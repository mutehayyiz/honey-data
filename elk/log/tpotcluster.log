[2022-04-22T17:20:12,753][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-22T17:20:12,765][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-22T17:20:12,766][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-22T17:20:20,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-22T17:20:20,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-22T17:20:20,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-22T17:20:20,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-22T17:20:20,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-22T17:20:20,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-22T17:20:20,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-22T17:20:20,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-22T17:20:20,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-22T17:20:20,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-22T17:20:20,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-22T17:20:20,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-22T17:20:20,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-22T17:20:20,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-22T17:20:20,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-22T17:20:20,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-22T17:20:20,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-22T17:20:20,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-22T17:20:20,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-22T17:20:20,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-22T17:20:20,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-22T17:20:20,892][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-22T17:20:20,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-22T17:20:20,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-22T17:20:20,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-22T17:20:20,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-22T17:20:20,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-22T17:20:20,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-22T17:20:20,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-22T17:20:20,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-22T17:20:20,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-22T17:20:20,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-22T17:20:20,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-22T17:20:20,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-22T17:20:20,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-22T17:20:20,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-22T17:20:20,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-22T17:20:20,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-22T17:20:20,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-22T17:20:20,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-22T17:20:20,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-22T17:20:20,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-22T17:20:20,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-22T17:20:20,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-22T17:20:20,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-22T17:20:20,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-22T17:20:20,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-22T17:20:20,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-22T17:20:20,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-22T17:20:20,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-22T17:20:20,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-22T17:20:20,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-22T17:20:20,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-22T17:20:20,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-22T17:20:20,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-22T17:20:20,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-22T17:20:20,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-22T17:20:20,908][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-22T17:20:20,909][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-22T17:20:21,008][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105gb], net total_space [125.8gb], types [ext4]
[2022-04-22T17:20:21,010][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-22T17:20:21,409][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-22T17:20:33,939][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-22T17:20:33,944][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-22T17:20:34,973][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-22T17:20:35,109][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-22T17:20:35,800][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-22T17:20:36,588][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-22T17:20:36,589][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-22T17:20:36,625][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-22T17:20:36,627][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-22T17:20:36,850][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-22T17:20:39,662][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-22T17:20:39,801][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{RK_BBTyBS5qShaXBunQesA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 294, version: 13240, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{RK_BBTyBS5qShaXBunQesA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-22T17:20:39,994][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{RK_BBTyBS5qShaXBunQesA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 294, version: 13240, reason: Publication{term=294, version=13240}
[2022-04-22T17:20:40,112][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-22T17:20:40,113][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-22T17:20:41,106][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-22T17:20:41,113][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [60] indices into cluster_state
[2022-04-22T17:20:41,952][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-22T17:20:41,953][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-22T17:20:42,721][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-22T17:20:42,907][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-22T17:20:43,181][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-22T17:20:43,184][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-22T17:20:43,185][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-22T17:20:43,364][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-22T17:20:43,676][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-22T17:20:43,867][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-22T17:20:46,775][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-22T17:20:57,504][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-22T17:20:57,771][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-22T17:21:08,052][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.22][0]]]).
[2022-04-22T17:21:43,313][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 761 finished with response BulkByScrollResponse[took=305.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-22T17:21:45,569][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 786 finished with response BulkByScrollResponse[took=2.2s,timed_out=false,sliceId=null,updated=922,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-22T17:21:54,435][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-22T17:22:47,983][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:22:48,181][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:22:48,922][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:28:41,702][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:45:52,199][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:50:17,801][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:51:23,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5e345158, interval=1s}] took [8486ms] which is above the warn threshold of [5000ms]
[2022-04-22T17:51:58,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5e345158, interval=1s}] took [15817ms] which is above the warn threshold of [5000ms]
[2022-04-22T17:57:17,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:02:00,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5584213184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:04:51,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/667406ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:10:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/667294017600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:14:35,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8m/589932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:17:34,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8m/590142526535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:23:10,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:26:46,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483870065147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:29:48,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:03,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433995127203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:30,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.8s/48871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:46,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48912996841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:50,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@69f89df, interval=5s}] took [48912ms] which is above the warn threshold of [5000ms]
[2022-04-22T18:31:03,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:31:28,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32785878887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:34:19,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:37:08,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/183631877970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:41:03,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353406ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:46:29,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353504062800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:53:45,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3m/742984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:53:45,709][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.7s/17757ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e]], which exceeds the warn threshold of [10s]
[2022-04-22T19:09:21,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3m/743353946350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:11:59,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2m/1157210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:57:22,350][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [743354ms] which is above the warn threshold of [5s]
[2022-04-22T19:11:15,537][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11046ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T19:15:48,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2m/1157069209024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:16:56,653][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.8s/32867ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T19:20:30,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/516663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T19:23:46,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/516248381896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:27:05,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396684ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T19:32:48,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396538674597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:37:03,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/549532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T19:52:45,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/549996974495ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:54:47,557][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [27.8m/1673317ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [31.1m/1869648ms]
[2022-04-22T19:56:59,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6m/1240648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:01:32,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6m/1240317792031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:02:13,752][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3860171ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [1.1h/4180327ms]
[2022-04-22T20:04:51,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/468194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:06:25,819][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.2h/4328766ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [1.2h/4353584ms]
[2022-04-22T20:08:12,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/468595552768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:13:02,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/498741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:12:48,754][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.5s/12550ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:16:25,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/498908334750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:19:29,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:17:36,855][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [38.3s/38353ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:23:18,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387394604175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:23:39,815][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.4h/5215069ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [1h/3691350ms]
[2022-04-22T20:25:25,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:26:51,118][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.5h/5563235ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [1h/3695469ms]
[2022-04-22T20:29:59,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348166371666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:31:58,619][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.4s/24407ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:33:32,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/486983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:36:40,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/487286143110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:37:06,626][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.4s/35404ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:40:09,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:40:41,883][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.7h/6448107ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [21.8m/1312406ms]
[2022-04-22T20:43:11,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397585643791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:36:53,184][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:58370}] took [348166ms] which is above the warn threshold of [5000ms]
[2022-04-22T20:46:47,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:49:47,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389395437257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:54:05,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:04:05,807][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-22T21:04:05,865][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-22T21:04:05,870][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-22T21:04:15,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-22T21:04:15,028][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-22T21:04:15,029][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-22T21:04:15,030][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-22T21:04:15,030][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-22T21:04:15,031][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-22T21:04:15,031][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-22T21:04:15,032][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-22T21:04:15,033][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-22T21:04:15,034][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-22T21:04:15,034][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-22T21:04:15,040][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-22T21:04:15,042][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-22T21:04:15,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-22T21:04:15,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-22T21:04:15,044][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-22T21:04:15,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-22T21:04:15,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-22T21:04:15,050][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-22T21:04:15,051][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-22T21:04:15,051][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-22T21:04:15,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-22T21:04:15,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-22T21:04:15,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-22T21:04:15,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-22T21:04:15,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-22T21:04:15,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-22T21:04:15,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-22T21:04:15,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-22T21:04:15,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-22T21:04:15,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-22T21:04:15,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-22T21:04:15,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-22T21:04:15,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-22T21:04:15,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-22T21:04:15,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-22T21:04:15,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-22T21:04:15,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-22T21:04:15,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-22T21:04:15,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-22T21:04:15,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-22T21:04:15,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-22T21:04:15,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-22T21:04:15,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-22T21:04:15,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-22T21:04:15,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-22T21:04:15,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-22T21:04:15,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-22T21:04:15,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-22T21:04:15,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-22T21:04:15,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-22T21:04:15,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-22T21:04:15,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-22T21:04:15,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-22T21:04:15,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-22T21:04:15,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-22T21:04:15,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-22T21:04:15,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-22T21:04:15,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-22T21:04:15,379][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.9gb], net total_space [125.8gb], types [ext4]
[2022-04-22T21:04:15,387][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-22T21:04:16,433][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-22T21:04:31,657][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-22T21:04:31,665][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-22T21:04:31,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-22T21:04:31,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-22T21:04:31,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-22T21:04:31,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-22T21:04:31,673][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-22T21:04:31,674][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-22T21:04:31,675][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-22T21:04:31,676][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-22T21:04:31,677][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-22T21:04:31,678][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-22T21:04:31,680][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-22T21:04:31,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-22T21:04:31,682][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-22T21:04:33,099][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-22T21:04:33,295][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-22T21:04:34,276][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-22T21:04:35,435][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-22T21:04:35,436][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-22T21:04:35,554][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-22T21:04:35,556][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-22T21:04:35,871][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-22T21:04:39,644][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-22T21:04:39,813][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 295, version: 13327, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-22T21:04:39,985][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 295, version: 13327, reason: Publication{term=295, version=13327}
[2022-04-22T21:04:40,147][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-22T21:04:40,150][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-22T21:04:41,614][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-22T21:04:41,636][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [60] indices into cluster_state
[2022-04-22T21:04:43,032][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-22T21:04:43,036][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-22T21:04:44,104][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-22T21:04:44,328][WARN ][o.e.i.t.Translog         ] [tpotcluster-node-01] [logstash-2022.04.22][0] deleted previously created, but not yet committed, next generation [translog-4.tlog]. This can happen due to a tragic exception when creating a new generation
[2022-04-22T21:04:44,522][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-22T21:04:44,527][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-22T21:04:44,529][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-22T21:04:45,096][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-22T21:04:45,350][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-22T21:04:45,531][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-22T21:04:46,404][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-22T21:04:48,248][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-22T21:05:06,568][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:05:08,346][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:05:58,130][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][61][17] duration [1.2s], collections [1]/[3.6s], total [1.2s]/[2.5s], memory [1.2gb]->[227.3mb]/[2gb], all_pools {[young] [1gb]->[4mb]/[0b]}{[old] [195.3mb]->[195.3mb]/[2gb]}{[survivor] [12mb]->[32mb]/[0b]}
[2022-04-22T21:05:58,452][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][61] overhead, spent [1.2s] collecting in the last [3.6s]
[2022-04-22T21:06:04,996][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][65] overhead, spent [484ms] collecting in the last [1.8s]
[2022-04-22T21:06:18,073][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][67][21] duration [6.8s], collections [1]/[9.1s], total [6.8s]/[10.8s], memory [301.4mb]->[236.1mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [227.3mb]->[227.3mb]/[2gb]}{[survivor] [6.1mb]->[8.8mb]/[0b]}
[2022-04-22T21:06:16,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:19,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][67] overhead, spent [6.8s] collecting in the last [9.1s]
[2022-04-22T21:06:20,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8553337014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:22,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7546ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:23,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7546098261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:23,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [7546ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:06:28,691][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][68][23] duration [2.3s], collections [2]/[13.1s], total [2.3s]/[13.1s], memory [236.1mb]->[246mb]/[2gb], all_pools {[young] [16mb]->[8mb]/[0b]}{[old] [227.3mb]->[234mb]/[2gb]}{[survivor] [8.8mb]->[11.9mb]/[0b]}
[2022-04-22T21:06:45,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:46,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7254099165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:46,947][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][72][24] duration [5.3s], collections [1]/[9s], total [5.3s]/[18.4s], memory [290mb]->[252.4mb]/[2gb], all_pools {[young] [48mb]->[20mb]/[0b]}{[old] [234mb]->[242.1mb]/[2gb]}{[survivor] [11.9mb]->[10.3mb]/[0b]}
[2022-04-22T21:06:46,948][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][72] overhead, spent [5.3s] collecting in the last [9s]
[2022-04-22T21:06:46,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [9293ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:07:01,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:07:01,763][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][78][25] duration [4.1s], collections [1]/[7.1s], total [4.1s]/[22.6s], memory [300.4mb]->[254.4mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [242.1mb]->[245mb]/[2gb]}{[survivor] [10.3mb]->[9.4mb]/[0b]}
[2022-04-22T21:07:01,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5908658322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:07:01,818][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][78] overhead, spent [4.1s] collecting in the last [7.1s]
[2022-04-22T21:07:01,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5908ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:07:07,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][79][26] duration [3s], collections [1]/[5.2s], total [3s]/[25.6s], memory [254.4mb]->[254.3mb]/[2gb], all_pools {[young] [8mb]->[4mb]/[0b]}{[old] [245mb]->[249.4mb]/[2gb]}{[survivor] [9.4mb]->[4.8mb]/[0b]}
[2022-04-22T21:07:07,882][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][79] overhead, spent [3s] collecting in the last [5.2s]
[2022-04-22T21:07:11,045][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][27] duration [923ms], collections [1]/[3.7s], total [923ms]/[26.5s], memory [254.3mb]->[254.9mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [249.4mb]->[249.4mb]/[2gb]}{[survivor] [4.8mb]->[5.5mb]/[0b]}
[2022-04-22T21:07:15,113][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][81][28] duration [1s], collections [1]/[3.7s], total [1s]/[27.6s], memory [254.9mb]->[259.1mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [249.4mb]->[249.4mb]/[2gb]}{[survivor] [5.5mb]->[9.6mb]/[0b]}
[2022-04-22T21:07:15,620][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][81] overhead, spent [1s] collecting in the last [3.7s]
[2022-04-22T21:07:20,337][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][82][29] duration [2.1s], collections [1]/[5.6s], total [2.1s]/[29.7s], memory [259.1mb]->[272.1mb]/[2gb], all_pools {[young] [4mb]->[16mb]/[0b]}{[old] [249.4mb]->[250.4mb]/[2gb]}{[survivor] [9.6mb]->[5.7mb]/[0b]}
[2022-04-22T21:07:21,394][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][82] overhead, spent [2.1s] collecting in the last [5.6s]
[2022-04-22T21:07:29,170][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][85][30] duration [1.7s], collections [1]/[4.5s], total [1.7s]/[31.5s], memory [328.1mb]->[257.7mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [250.4mb]->[250.4mb]/[2gb]}{[survivor] [5.7mb]->[7.3mb]/[0b]}
[2022-04-22T21:07:29,581][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][85] overhead, spent [1.7s] collecting in the last [4.5s]
[2022-04-22T21:07:34,953][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][86][31] duration [2.8s], collections [1]/[5.5s], total [2.8s]/[34.3s], memory [257.7mb]->[258.4mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [250.4mb]->[250.4mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-04-22T21:07:35,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][86] overhead, spent [2.8s] collecting in the last [5.5s]
[2022-04-22T21:07:42,657][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][88][32] duration [2.5s], collections [1]/[1.8s], total [2.5s]/[36.8s], memory [290.4mb]->[306.4mb]/[2gb], all_pools {[young] [36mb]->[88mb]/[0b]}{[old] [250.4mb]->[250.4mb]/[2gb]}{[survivor] [8mb]->[8mb]/[0b]}
[2022-04-22T21:07:42,807][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][88] overhead, spent [2.5s] collecting in the last [1.8s]
[2022-04-22T21:07:46,335][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][90] overhead, spent [610ms] collecting in the last [1.5s]
[2022-04-22T21:08:12,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:12,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [19675ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16633023274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59446}] took [17234ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59448}] took [18107ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [18107ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:14,559][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][92][35] duration [12.7s], collections [1]/[21.3s], total [12.7s]/[50.5s], memory [335.1mb]->[286.6mb]/[2gb], all_pools {[young] [72mb]->[20mb]/[0b]}{[old] [254.3mb]->[254.6mb]/[2gb]}{[survivor] [8.8mb]->[16mb]/[0b]}
[2022-04-22T21:08:15,480][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][92] overhead, spent [12.7s] collecting in the last [21.3s]
[2022-04-22T21:08:21,542][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][94][36] duration [874ms], collections [1]/[1.9s], total [874ms]/[51.4s], memory [298.6mb]->[302.6mb]/[2gb], all_pools {[young] [28mb]->[48mb]/[0b]}{[old] [254.6mb]->[254.6mb]/[2gb]}{[survivor] [16mb]->[16mb]/[0b]}
[2022-04-22T21:08:21,754][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [874ms] collecting in the last [1.9s]
[2022-04-22T21:08:34,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:38,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10532878643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:41,179][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][95][37] duration [6.5s], collections [1]/[3.9s], total [6.5s]/[57.9s], memory [302.6mb]->[361.4mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [254.6mb]->[276.4mb]/[2gb]}{[survivor] [16mb]->[5.2mb]/[0b]}
[2022-04-22T21:08:41,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:39,159][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.1s/15101ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-22T21:08:42,021][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][95] overhead, spent [6.5s] collecting in the last [3.9s]
[2022-04-22T21:08:42,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7490317370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:44,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [18423ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:00,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@48d28149] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:09,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:10,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5927734267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:10,325][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][97][38] duration [4.6s], collections [1]/[7.6s], total [4.6s]/[1m], memory [361.6mb]->[283mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [276.4mb]->[276.4mb]/[2gb]}{[survivor] [5.2mb]->[6.5mb]/[0b]}
[2022-04-22T21:09:10,424][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [4.6s] collecting in the last [7.6s]
[2022-04-22T21:09:10,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5927ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:11,482][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [20134ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:09:15,616][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][98][39] duration [2.2s], collections [1]/[2s], total [2.2s]/[1m], memory [283mb]->[371mb]/[2gb], all_pools {[young] [8mb]->[4mb]/[0b]}{[old] [276.4mb]->[276.4mb]/[2gb]}{[survivor] [6.5mb]->[10mb]/[0b]}
[2022-04-22T21:09:15,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [2.2s] collecting in the last [2s]
[2022-04-22T21:09:15,801][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [29s] publication of cluster state version [13386] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:09:30,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e40ccf, interval=30s}] took [10344ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:30,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:32,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10344238024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:36,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][102][40] duration [7.4s], collections [1]/[12.6s], total [7.4s]/[1.2m], memory [330.5mb]->[292.6mb]/[2gb], all_pools {[young] [52mb]->[24mb]/[0b]}{[old] [276.4mb]->[279.7mb]/[2gb]}{[survivor] [10mb]->[8.8mb]/[0b]}
[2022-04-22T21:09:38,996][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][102] overhead, spent [7.4s] collecting in the last [12.6s]
[2022-04-22T21:09:39,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [9141ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:10:17,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:11:37,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5734783020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:12:01,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:12:12,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111881205229ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:12:42,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:13:05,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34507370978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:13:49,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:14:35,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67879039945ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:14:48,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:14:54,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63485625465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:00,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:03,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882396961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:09,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:14,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389966429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@7cb6889d] took [12290ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,497][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [349124ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:15:21,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12290635105ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,785][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6m] publication of cluster state version [13387] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:15:24,425][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][103][42] duration [4.8s], collections [2]/[5.8m], total [4.8s]/[1.2m], memory [292.6mb]->[297mb]/[2gb], all_pools {[young] [24mb]->[8mb]/[0b]}{[old] [279.7mb]->[282.3mb]/[2gb]}{[survivor] [8.8mb]->[6.6mb]/[0b]}
[2022-04-22T21:15:24,480][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7m/422380ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [7m/422366ms]
[2022-04-22T21:15:26,584][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][104] overhead, spent [579ms] collecting in the last [1.8s]
[2022-04-22T21:15:32,473][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][107][44] duration [938ms], collections [1]/[1.2s], total [938ms]/[1.3m], memory [320.9mb]->[372.9mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [282.3mb]->[282.3mb]/[2gb]}{[survivor] [6.5mb]->[5.7mb]/[0b]}
[2022-04-22T21:15:32,699][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [938ms] collecting in the last [1.2s]
[2022-04-22T21:15:36,899][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-22T21:15:49,258][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][116][46] duration [2.9s], collections [1]/[6.1s], total [2.9s]/[1.3m], memory [372.3mb]->[292.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [282.3mb]->[284.2mb]/[2gb]}{[survivor] [9.9mb]->[8.3mb]/[0b]}
[2022-04-22T21:15:49,424][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][116] overhead, spent [2.9s] collecting in the last [6.1s]
[2022-04-22T21:15:55,067][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][121] overhead, spent [281ms] collecting in the last [1.1s]
[2022-04-22T21:16:04,858][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][126][48] duration [1s], collections [1]/[1.5s], total [1s]/[1.3m], memory [368.9mb]->[372.9mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [284.2mb]->[284.2mb]/[2gb]}{[survivor] [8.7mb]->[11mb]/[0b]}
[2022-04-22T21:16:05,487][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][126] overhead, spent [1s] collecting in the last [1.5s]
[2022-04-22T21:16:10,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8826ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:16:12,415][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][49] duration [3.1s], collections [1]/[9.9s], total [3.1s]/[1.4m], memory [372.9mb]->[301.3mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [284.2mb]->[286.4mb]/[2gb]}{[survivor] [11mb]->[6.8mb]/[0b]}
[2022-04-22T21:16:13,424][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [3.1s] collecting in the last [9.9s]
[2022-04-22T21:16:28,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:16:29,253][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][130][50] duration [5.2s], collections [1]/[1.4s], total [5.2s]/[1.5m], memory [361.3mb]->[361.3mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [6.8mb]->[7.8mb]/[0b]}
[2022-04-22T21:16:29,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][130] overhead, spent [5.2s] collecting in the last [1.4s]
[2022-04-22T21:16:29,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [9791ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:16:29,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7494580785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:16:35,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [6377ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:16:39,187][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][131][51] duration [3.5s], collections [1]/[19.2s], total [3.5s]/[1.5m], memory [361.3mb]->[297.6mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [7.8mb]->[7.1mb]/[0b]}
[2022-04-22T21:17:11,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:13,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11889830158ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:16,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:16,748][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][139][52] duration [8.2s], collections [1]/[1.4s], total [8.2s]/[1.7m], memory [357.6mb]->[381.6mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [7.1mb]->[8mb]/[0b]}
[2022-04-22T21:17:16,996][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59528}] took [5348ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:17:17,104][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [5348ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:17:17,826][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][139] overhead, spent [8.2s] collecting in the last [1.4s]
[2022-04-22T21:17:17,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5348410293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:18,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [17438ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:17:47,174][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [5404ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:07,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:19,705][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [15561ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:22,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7904ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:26,373][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59528}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:43,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:18:44,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12793917341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:18:45,712][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][53] duration [7.6s], collections [1]/[29.4s], total [7.6s]/[1.8m], memory [374.4mb]->[299.5mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [8mb]->[9mb]/[0b]}
[2022-04-22T21:18:47,027][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [7.6s] collecting in the last [29.4s]
[2022-04-22T21:18:49,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5671ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:19:59,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:19:06,736][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59542}] took [5804ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:06,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46297254740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:14,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:19,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15025163576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:18,617][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59542}] took [15025ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:20,087][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][54] duration [36.8s], collections [1]/[6.2s], total [36.8s]/[2.4m], memory [375.5mb]->[375.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [286.4mb]->[288.5mb]/[2gb]}{[survivor] [9mb]->[4.7mb]/[0b]}
[2022-04-22T21:20:26,243][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [36.8s] collecting in the last [6.2s]
[2022-04-22T21:20:26,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:32,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12152534435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:31,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [75476ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:38,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:44,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11948831396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:53,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [15203ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:53,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:02,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15203747005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:13,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:23,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18441318304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:33,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21408ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:41,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21408427898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:50,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:54,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@7613f8af] took [17629ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:21:56,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17629920548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:04,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:09,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12937712056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [12937ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:22:16,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [12456ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:22:16,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:24,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12456840789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:28,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:30,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11710879929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:32,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:44,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5003150501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [12616ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:22:46,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12616509685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:57,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:58,341][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][55] duration [4.2s], collections [1]/[2.4s], total [4.2s]/[2.5m], memory [357.3mb]->[365.3mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [4.7mb]->[4.5mb]/[0b]}
[2022-04-22T21:22:58,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6911796964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:58,935][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [4.2s] collecting in the last [2.4s]
[2022-04-22T21:23:00,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [11108ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:23:10,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5930ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:23:42,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7804ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:00,796][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [5259ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:28,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:24:29,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7643360964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:24:29,410][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][170][56] duration [5s], collections [1]/[12.5s], total [5s]/[2.6m], memory [377mb]->[305.3mb]/[2gb], all_pools {[young] [84mb]->[12mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [4.5mb]->[4.8mb]/[0b]}
[2022-04-22T21:24:29,441][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][170] overhead, spent [5s] collecting in the last [12.5s]
[2022-04-22T21:24:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [6201ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:49,298][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59574}] took [14573ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:50,958][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [13830ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:51,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@bf4b888] took [5038ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:25:07,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:25:17,651][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][175][57] duration [1.1s], collections [1]/[2.1s], total [1.1s]/[2.6m], memory [353.3mb]->[293.5mb]/[2gb], all_pools {[young] [60mb]->[36mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [4.8mb]->[5mb]/[0b]}
[2022-04-22T21:25:18,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][175] overhead, spent [1.1s] collecting in the last [2.1s]
[2022-04-22T21:25:25,750][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][58] duration [1.5s], collections [1]/[1.5s], total [1.5s]/[2.6m], memory [377.5mb]->[377.5mb]/[2gb], all_pools {[young] [84mb]->[84mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [5mb]->[5mb]/[0b]}
[2022-04-22T21:25:25,996][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.5s] collecting in the last [1.5s]
[2022-04-22T21:25:29,528][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][59] duration [1.8s], collections [1]/[6.9s], total [1.8s]/[2.6m], memory [377.5mb]->[293.2mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [5mb]->[4.7mb]/[0b]}
[2022-04-22T21:25:29,809][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [1.8s] collecting in the last [6.9s]
[2022-04-22T21:25:40,332][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][185][61] duration [844ms], collections [1]/[1.8s], total [844ms]/[2.7m], memory [332.5mb]->[299.2mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [288.5mb]->[288.6mb]/[2gb]}{[survivor] [8mb]->[6.6mb]/[0b]}
[2022-04-22T21:25:40,486][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][185] overhead, spent [844ms] collecting in the last [1.8s]
[2022-04-22T21:25:42,867][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][186] overhead, spent [644ms] collecting in the last [2.3s]
[2022-04-22T21:26:09,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:26:10,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19169704773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:26:25,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][63] duration [17.6s], collections [1]/[19.7s], total [17.6s]/[3m], memory [355.1mb]->[295mb]/[2gb], all_pools {[young] [68mb]->[60mb]/[0b]}{[old] [288.6mb]->[288.8mb]/[2gb]}{[survivor] [6.4mb]->[6.2mb]/[0b]}
[2022-04-22T21:26:31,393][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [17.6s] collecting in the last [19.7s]
[2022-04-22T21:26:34,353][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [10207ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:26:35,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [25776ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:26:59,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [14095ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:27:33,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:27:34,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9382073244ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:27:34,692][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][64] duration [5s], collections [1]/[18.4s], total [5s]/[3.1m], memory [375mb]->[298.6mb]/[2gb], all_pools {[young] [80mb]->[32mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [6.2mb]->[5.8mb]/[0b]}
[2022-04-22T21:27:36,130][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [5s] collecting in the last [18.4s]
[2022-04-22T21:27:59,479][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][65] duration [3.4s], collections [1]/[1.3s], total [3.4s]/[3.1m], memory [338.6mb]->[382.6mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [5.8mb]->[4.5mb]/[0b]}
[2022-04-22T21:28:00,476][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [3.4s] collecting in the last [1.3s]
[2022-04-22T21:28:00,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7364ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:28:26,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5471ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:28:45,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:50,052][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][210][66] duration [10.5s], collections [1]/[9.9s], total [10.5s]/[3.3m], memory [345.3mb]->[381.3mb]/[2gb], all_pools {[young] [56mb]->[52mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [4.5mb]->[6.7mb]/[0b]}
[2022-04-22T21:28:49,543][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [14748ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:28:48,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13389367126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:50,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:50,910][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][210] overhead, spent [10.5s] collecting in the last [9.9s]
[2022-04-22T21:28:52,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5671071417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:52,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [19260ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:05,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5575ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:19,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5080ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:25,522][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [6004ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:20,095][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1m/64274ms] ago, timed out [22s/22034ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2180]
[2022-04-22T21:29:31,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:06,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:09,747][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59574}] took [23854ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:08,896][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][215][67] duration [10.9s], collections [1]/[17.2s], total [10.9s]/[3.5m], memory [371.6mb]->[379.6mb]/[2gb], all_pools {[young] [76mb]->[88mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [6.7mb]->[6.7mb]/[0b]}
[2022-04-22T21:30:09,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20251201835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:10,955][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][215] overhead, spent [10.9s] collecting in the last [17.2s]
[2022-04-22T21:30:10,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:13,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [28933ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:12,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:13,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5480669567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:38,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8606ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:39,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8606243055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:42,943][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2180] timed out after [42240ms]
[2022-04-22T21:31:24,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:31:25,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13604436676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:31:25,665][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18408ms] which is above the warn threshold of [5s]
[2022-04-22T21:31:25,849][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][222][68] duration [9s], collections [1]/[2.6s], total [9s]/[3.6m], memory [377.6mb]->[377.6mb]/[2gb], all_pools {[young] [80mb]->[84mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [8.8mb]->[8.8mb]/[0b]}
[2022-04-22T21:31:26,518][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][222] overhead, spent [9s] collecting in the last [2.6s]
[2022-04-22T21:31:28,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [20321ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:31:35,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:31:38,421][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [48.9s/48968ms] to compute cluster state update for [put-mapping [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-22T21:32:15,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:32:16,086][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [18958ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:32:16,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17156433578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:32:16,086][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59574}] took [19158ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:32:18,990][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][228][69] duration [13.3s], collections [1]/[3s], total [13.3s]/[3.8m], memory [380.5mb]->[384.5mb]/[2gb], all_pools {[young] [84mb]->[48mb]/[0b]}{[old] [289.6mb]->[289.6mb]/[2gb]}{[survivor] [6.9mb]->[7.8mb]/[0b]}
[2022-04-22T21:32:20,674][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][228] overhead, spent [13.3s] collecting in the last [3s]
[2022-04-22T21:32:23,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [25982ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:32:42,716][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][229][70] duration [2.4s], collections [1]/[37.1s], total [2.4s]/[3.9m], memory [384.5mb]->[313mb]/[2gb], all_pools {[young] [48mb]->[16mb]/[0b]}{[old] [289.6mb]->[289.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-04-22T21:32:48,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [13366ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:33:13,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8405ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:33:57,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:33:20,045][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [90139ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:33:59,997][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [33380ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:33:59,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32779800689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:01,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:04,330][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.3m] publication of cluster state version [13391] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:34:06,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5643979620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:09,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:14,176][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [7490ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:34:13,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7489359548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:18,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9703ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:22,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9703708358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:27,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8058ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:29,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8057337672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:36,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:54,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9661752618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:56,807][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][71] duration [24.5s], collections [1]/[1.4m], total [24.5s]/[4.3m], memory [349mb]->[335.5mb]/[2gb], all_pools {[young] [56mb]->[40mb]/[0b]}{[old] [289.6mb]->[289.6mb]/[2gb]}{[survivor] [7.4mb]->[9.8mb]/[0b]}
[2022-04-22T21:34:57,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:00,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20846658493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:00,240][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][233] overhead, spent [24.5s] collecting in the last [1.4m]
[2022-04-22T21:35:03,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:03,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [30508ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:05,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5893377434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:09,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:09,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@26881dac] took [6315ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:12,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6315214266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:20,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:21,923][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [9247ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:23,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9247081750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:12,710][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6316ms] which is above the warn threshold of [5s]
[2022-04-22T21:35:25,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:28,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6780363576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:32,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:33,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [6380ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:33,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6380305384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:57,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:58,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11380053405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:36:00,052][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][236][72] duration [8.7s], collections [1]/[1.7s], total [8.7s]/[4.4m], memory [363.5mb]->[383.5mb]/[2gb], all_pools {[young] [64mb]->[36mb]/[0b]}{[old] [289.6mb]->[291.2mb]/[2gb]}{[survivor] [9.8mb]->[7.9mb]/[0b]}
[2022-04-22T21:36:01,094][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][236] overhead, spent [8.7s] collecting in the last [1.7s]
[2022-04-22T21:36:01,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [17051ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:01,650][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [5271ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:27,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:36:28,316][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [10078ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:28,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9678288212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:36:27,855][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=295, version=13391}] took [1.7m] which is above the warn threshold of [30s]: [running task [Publication{term=295, version=13391}]] took [61ms], [connecting to new nodes] took [263ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2e2c2a19] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@458356df] took [39108ms], [org.elasticsearch.script.ScriptService@4c7fa9bf] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@47310c8c] took [0ms], [org.elasticsearch.snapshots.RestoreService@8029a91] took [0ms], [org.elasticsearch.ingest.IngestService@57e800e0] took [2313ms], [org.elasticsearch.action.ingest.IngestActionForwarder@66eb79f3] took [75ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce420@29715cce] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@35bb9d63] took [83ms], [org.elasticsearch.tasks.TaskManager@22c37434] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1f5aa57e] took [1ms], [org.elasticsearch.cluster.InternalClusterInfoService@b4f4839] took [72ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7c35fe48] took [0ms], [org.elasticsearch.indices.SystemIndexManager@37946f33] took [2801ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@18519c31] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@6089c80c] took [217ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@3605aed] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1af4dbb8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@ed59c61] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@254bf147] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@60f3312a] took [31812ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@221cc90b] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@2e4361af] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@6ad3a1fd] took [5431ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1c37bb0f] took [160ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4823e815] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@7d58b6a7] took [15585ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@47310c8c] took [150ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@af32cb8] took [2145ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2b3d012a] took [37ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5759296f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@76db8471] took [1900ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@22dcf891] took [545ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7165c736] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@32bab212] took [104ms], [org.elasticsearch.node.ResponseCollectorService@7767dca4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@f54394f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@505703e4] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@379a11de] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@f364cff] took [0ms], [org.elasticsearch.indices.store.IndicesStore@4efac294] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5ed4375d] took [0ms], [org.elasticsearch.license.LicenseService@5878e82a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7d62863a] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@239e70ef] took [0ms], [org.elasticsearch.gateway.GatewayService@4c01fbbb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@5f56632e] took [0ms]
[2022-04-22T21:36:29,005][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][242][73] duration [6s], collections [1]/[5.9s], total [6s]/[4.5m], memory [367.2mb]->[387.2mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [291.2mb]->[291.2mb]/[2gb]}{[survivor] [7.9mb]->[7.6mb]/[0b]}
[2022-04-22T21:36:29,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][242] overhead, spent [6s] collecting in the last [5.9s]
[2022-04-22T21:36:29,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [10078ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:43,204][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][244][74] duration [2.7s], collections [1]/[12.2s], total [2.7s]/[4.6m], memory [366.9mb]->[299.6mb]/[2gb], all_pools {[young] [68mb]->[20mb]/[0b]}{[old] [291.2mb]->[291.2mb]/[2gb]}{[survivor] [7.6mb]->[8.3mb]/[0b]}
[2022-04-22T21:37:07,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:08,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15213011219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:13,328][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][247][75] duration [10.8s], collections [1]/[18.6s], total [10.8s]/[4.8m], memory [347.6mb]->[305.9mb]/[2gb], all_pools {[young] [48mb]->[4mb]/[0b]}{[old] [291.2mb]->[291.2mb]/[2gb]}{[survivor] [8.3mb]->[14.6mb]/[0b]}
[2022-04-22T21:37:14,046][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:37:14,084][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [6883ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:37:15,737][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][247] overhead, spent [10.8s] collecting in the last [18.6s]
[2022-04-22T21:37:18,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [26367ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:37:21,345][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [33s/33050ms] to compute cluster state update for [put-mapping [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-22T21:37:21,766][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59666}] took [8099ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:37:50,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:53,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17929757908ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:56,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@2145458a] took [6769ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:07,297][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][76] duration [10.2s], collections [1]/[50.6s], total [10.2s]/[4.9m], memory [305.9mb]->[327.9mb]/[2gb], all_pools {[young] [4mb]->[28mb]/[0b]}{[old] [291.2mb]->[296.8mb]/[2gb]}{[survivor] [14.6mb]->[7.1mb]/[0b]}
[2022-04-22T21:38:14,266][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [9052ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:15,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [16074ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:38,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [6922ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:41,368][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [8060ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:41,063][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10547ms] which is above the warn threshold of [5s]
[2022-04-22T21:38:53,295][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [63283ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:38:59,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5614ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:01,081][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.6m] publication of cluster state version [13392] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:39:11,144][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][251][77] duration [2.6s], collections [1]/[1.9s], total [2.6s]/[5m], memory [343.9mb]->[391.9mb]/[2gb], all_pools {[young] [40mb]->[64mb]/[0b]}{[old] [296.8mb]->[296.8mb]/[2gb]}{[survivor] [7.1mb]->[8.9mb]/[0b]}
[2022-04-22T21:39:12,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][251] overhead, spent [2.6s] collecting in the last [1.9s]
[2022-04-22T21:39:12,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [6555ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:26,011][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][257][80] duration [2.5s], collections [2]/[1.2s], total [2.5s]/[5m], memory [388.3mb]->[392.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [296.8mb]->[296.8mb]/[2gb]}{[survivor] [7.5mb]->[10.2mb]/[0b]}
[2022-04-22T21:39:26,268][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][257] overhead, spent [2.5s] collecting in the last [1.2s]
[2022-04-22T21:39:26,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5212ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:34,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][261][81] duration [1.7s], collections [1]/[4.2s], total [1.7s]/[5.1m], memory [370.7mb]->[306.9mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [297.9mb]->[297.9mb]/[2gb]}{[survivor] [8.8mb]->[9mb]/[0b]}
[2022-04-22T21:39:34,860][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][261] overhead, spent [1.7s] collecting in the last [4.2s]
[2022-04-22T21:39:39,762][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][263][83] duration [1.3s], collections [1]/[2.9s], total [1.3s]/[5.1m], memory [315.1mb]->[305.9mb]/[2gb], all_pools {[young] [12mb]->[8mb]/[0b]}{[old] [298.1mb]->[299.4mb]/[2gb]}{[survivor] [9mb]->[6.4mb]/[0b]}
[2022-04-22T21:39:39,912][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][263] overhead, spent [1.3s] collecting in the last [2.9s]
[2022-04-22T21:39:52,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8244ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:54,363][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [5217ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:40:00,515][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][266][84] duration [2.9s], collections [1]/[4.7s], total [2.9s]/[5.1m], memory [369.9mb]->[307mb]/[2gb], all_pools {[young] [88mb]->[36mb]/[0b]}{[old] [299.4mb]->[299.4mb]/[2gb]}{[survivor] [6.4mb]->[7.6mb]/[0b]}
[2022-04-22T21:40:01,024][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][266] overhead, spent [2.9s] collecting in the last [4.7s]
[2022-04-22T21:40:01,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5877ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:40:15,854][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][275][87] duration [1.2s], collections [1]/[2.3s], total [1.2s]/[5.2m], memory [342.3mb]->[312mb]/[2gb], all_pools {[young] [32mb]->[20mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [7.9mb]->[5.5mb]/[0b]}
[2022-04-22T21:40:16,130][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][275] overhead, spent [1.2s] collecting in the last [2.3s]
[2022-04-22T21:40:42,032][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][279][88] duration [4.4s], collections [1]/[5.2s], total [4.4s]/[5.2m], memory [340mb]->[396mb]/[2gb], all_pools {[young] [32mb]->[24mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [5.5mb]->[7.5mb]/[0b]}
[2022-04-22T21:40:41,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7837ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:40:42,234][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][279] overhead, spent [4.4s] collecting in the last [5.2s]
[2022-04-22T21:40:42,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7836881631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:40:42,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8893ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:40:49,881][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][283][89] duration [867ms], collections [1]/[2.6s], total [867ms]/[5.3m], memory [394mb]->[309.5mb]/[2gb], all_pools {[young] [84mb]->[28mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [7.5mb]->[7.1mb]/[0b]}
[2022-04-22T21:40:50,331][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][283] overhead, spent [867ms] collecting in the last [2.6s]
[2022-04-22T21:41:01,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][288][91] duration [1.7s], collections [1]/[1.4s], total [1.7s]/[5.3m], memory [345.4mb]->[381.4mb]/[2gb], all_pools {[young] [40mb]->[88mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [6.9mb]->[6.9mb]/[0b]}
[2022-04-22T21:41:01,461][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][288] overhead, spent [1.7s] collecting in the last [1.4s]
[2022-04-22T21:41:23,636][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [8672ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:31,669][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [9337ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:31,669][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59672}] took [9337ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:35,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@3c748761] took [7809ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:42,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5548ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:06,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [17817ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:06,984][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59666}] took [15390ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:21,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:28,615][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59672}] took [21698ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:27,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:30,344][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [16311ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:32,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14828232802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:26,049][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3191] timed out after [37476ms]
[2022-04-22T21:42:34,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:40,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7117887047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:12,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:12,632][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][298][94] duration [18.3s], collections [1]/[50.2s], total [18.3s]/[5.6m], memory [388.8mb]->[392.8mb]/[2gb], all_pools {[young] [80mb]->[88mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-04-22T21:43:17,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38418908084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:19,998][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][298] overhead, spent [18.3s] collecting in the last [50.2s]
[2022-04-22T21:43:34,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:36,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [59107ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:43:38,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20688355335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:38,060][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [20688ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:43:42,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:48,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8894780750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:52,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:57,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10125428867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:09,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:16,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14341965519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:18,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@535dd0cb] took [14341ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:44:23,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:33,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16273662083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:42,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:51,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18481455412ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:51,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [18481ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:44:59,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:59,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4d672f52, interval=1m}] took [17439ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:45:10,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17439804926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:19,384][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [17440ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:45:20,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20936ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:29,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20936018560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:35,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:41,297][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [14942ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:45:40,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14941627291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:45,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:50,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10041268916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:57,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:06,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215155681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:08,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [12215ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:46:18,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:03,825][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3320] timed out after [98114ms]
[2022-04-22T21:46:28,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19815891520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:39,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:48,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21671868025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:06,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:18,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25550795039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:35,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30774ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:56,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30773638884ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:10,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:22,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33655278840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:37,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:24,114][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [33656ms] which is above the warn threshold of [5s]
[2022-04-22T21:48:50,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27692998356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:53,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [27692ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:48:55,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:07,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19438786452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:14,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:58,310][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7m/421381ms] ago, timed out [6.3m/383905ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3191]
[2022-04-22T21:49:23,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18611035433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:40,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:55,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25166552192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:07,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:08,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [26933ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:50:16,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26933159738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:23,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:34,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16881224253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:47,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23976ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:58,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23976454840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:09,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:17,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21778073173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:26,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:38,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17159655487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:47,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:56,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [37412ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:52:04,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20252483940ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:52:18,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:02,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31464053535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:09,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:14,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111104268496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:21,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:27,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12259959893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:34,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:36,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13171340049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:35,114][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3420] timed out after [237994ms]
[2022-04-22T21:54:41,809][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18973ms] which is above the warn threshold of [5s]
[2022-04-22T21:55:00,725][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][303][95] duration [1.3m], collections [1]/[3.2m], total [1.3m]/[6.9m], memory [372.5mb]->[327.5mb]/[2gb], all_pools {[young] [64mb]->[20mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [6mb]->[5.1mb]/[0b]}
[2022-04-22T21:55:08,214][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][303] overhead, spent [1.3m] collecting in the last [3.2m]
[2022-04-22T21:54:51,361][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [10.6m/639722ms] ago, timed out [9m/541608ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3320]
[2022-04-22T21:55:14,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [35217ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:57:28,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [37998ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:58:28,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@44ec6075] took [45707ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:58:57,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [8867ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:59:32,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [15343ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:02:58,633][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [22555ms] which is above the warn threshold of [5s]
[2022-04-22T22:04:14,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [59445ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:04:53,359][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.3m/859786ms] ago, timed out [10.3m/621792ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3420]
[2022-04-22T22:08:11,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [12727ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:08:44,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [16088ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:11:20,498][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3564] timed out after [549897ms]
[2022-04-22T22:16:00,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:17:52,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6121166748ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:21:10,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/297573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:25:54,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/297105491761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:29:56,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:30:16,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@112ee234, interval=1m}] took [537380ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:33:34,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537380519471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:36:58,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/423005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:41:12,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422978887084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:44:43,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:49:07,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456880117742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:46:48,988][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10977ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250]], which exceeds the warn threshold of [10s]
[2022-04-22T22:53:15,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/520368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:45:36,800][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [879859ms] which is above the warn threshold of [5s]
[2022-04-22T22:57:05,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/520199151694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:57:02,547][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.2s/17295ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:01:10,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/462101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:02:00,677][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7.7m/462108ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [7.4m/447750ms]
[2022-04-22T23:05:04,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/462108375094ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:07:09,302][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.2s/10266ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d]], which exceeds the warn threshold of [10s]
[2022-04-22T23:09:22,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/472609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:13:21,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/472579938281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:13:11,868][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [40.1s/40123ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d]], which exceeds the warn threshold of [10s]
[2022-04-22T23:17:35,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/511031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:17:46,397][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [24.1m/1446101ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [16.6m/997107ms]
[2022-04-22T23:21:12,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/511412591251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:22:50,384][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.5s/30523ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:24:19,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:26:12,549][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [31m/1863716ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [12.5m/755373ms]
[2022-04-22T23:27:26,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417615353585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:29:24,910][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4h/5149285ms] ago, timed out [1.2h/4599388ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3564]
[2022-04-22T23:30:26,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:30:41,363][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17s/17027ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:33:28,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367603599968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:34:39,559][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.9s/30972ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:37:29,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:39:55,974][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [44.1m/2651212ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [13.8m/832255ms]
[2022-04-22T23:42:23,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419892485132ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:46:20,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8m/533975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:46:47,174][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.1s/12181ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:50:14,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8m/533965428048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:51:17,367][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32s/32097ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:54:59,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/509211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:56:18,647][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3694148ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [14.7m/887702ms]
[2022-04-22T23:59:01,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/508970136969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:18:14,589][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-23T00:18:14,627][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-23T00:18:14,628][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-23T00:18:19,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-23T00:18:19,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-23T00:18:19,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-23T00:18:19,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-23T00:18:19,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-23T00:18:19,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-23T00:18:19,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-23T00:18:19,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-23T00:18:19,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-23T00:18:19,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-23T00:18:19,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-23T00:18:19,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-23T00:18:19,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-23T00:18:19,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-23T00:18:19,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-23T00:18:19,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-23T00:18:19,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-23T00:18:19,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-23T00:18:19,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-23T00:18:19,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-23T00:18:19,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-23T00:18:19,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-23T00:18:19,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-23T00:18:19,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-23T00:18:19,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-23T00:18:19,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-23T00:18:19,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-23T00:18:19,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-23T00:18:19,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-23T00:18:19,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-23T00:18:19,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-23T00:18:19,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-23T00:18:19,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-23T00:18:19,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-23T00:18:19,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-23T00:18:19,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-23T00:18:19,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-23T00:18:19,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-23T00:18:19,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-23T00:18:19,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-23T00:18:19,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-23T00:18:19,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-23T00:18:19,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-23T00:18:19,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-23T00:18:19,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-23T00:18:19,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-23T00:18:19,610][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-23T00:18:19,610][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-23T00:18:19,610][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-23T00:18:19,611][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-23T00:18:19,611][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-23T00:18:19,611][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-23T00:18:19,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-23T00:18:19,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-23T00:18:19,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-23T00:18:19,613][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-23T00:18:19,613][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-23T00:18:19,614][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-23T00:18:19,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-23T00:18:19,703][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.9gb], net total_space [125.8gb], types [ext4]
[2022-04-23T00:18:19,705][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-23T00:18:20,258][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-23T00:18:30,141][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-23T00:18:30,165][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-23T00:18:30,170][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T00:18:30,177][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-23T00:18:30,178][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T00:18:30,179][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-23T00:18:30,179][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-23T00:18:30,180][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T00:18:30,181][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-23T00:18:30,182][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-23T00:18:30,182][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-23T00:18:30,186][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T00:18:30,195][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T00:18:30,196][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T00:18:30,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-23T00:18:32,008][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-23T00:18:32,216][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-23T00:18:33,231][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-23T00:18:34,323][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-23T00:18:34,330][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-23T00:18:34,481][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-23T00:18:34,486][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-23T00:18:34,797][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-23T00:18:38,171][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-23T00:18:38,396][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 296, version: 13393, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-23T00:18:38,669][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 296, version: 13393, reason: Publication{term=296, version=13393}
[2022-04-23T00:18:38,877][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-23T00:18:38,883][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-23T00:18:40,741][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-23T00:18:40,773][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [60] indices into cluster_state
[2022-04-23T00:18:42,245][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-23T00:18:42,260][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-23T00:20:12,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:20:15,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [19524ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:20:19,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12189858217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:20:23,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79473ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:20:23,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79629626880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:21:16,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:21:16,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [5709ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:21:23,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5509147182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:21:24,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:21:25,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8245ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:21:25,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8245749599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:21:28,420][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [23099ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-23T00:21:28,702][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [23.4s] publication of cluster state version [13401] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:21:31,455][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-23T00:21:31,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-23T00:21:31,637][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-23T00:21:32,690][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.encodeChangeCipherSpec(SSLSocketOutputRecord.java:233) ~[?:?]
	at sun.security.ssl.OutputRecord.changeWriteCiphers(OutputRecord.java:182) ~[?:?]
	at sun.security.ssl.ChangeCipherSpec$T10ChangeCipherSpecProducer.produce(ChangeCipherSpec.java:118) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.onProduceFinished(Finished.java:395) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.produce(Finished.java:379) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.ServerHelloDone$ServerHelloDoneConsumer.consume(ServerHelloDone.java:182) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-23T00:21:35,856][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][44] overhead, spent [475ms] collecting in the last [1.2s]
[2022-04-23T00:21:37,586][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T00:21:37,586][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T00:21:43,885][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T00:22:43,313][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.9s] publication of cluster state version [13429] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:22:54,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][94][14] duration [2.2s], collections [1]/[3.4s], total [2.2s]/[3.7s], memory [737.1mb]->[193.4mb]/[2gb], all_pools {[young] [592mb]->[0b]/[0b]}{[old] [133.8mb]->[133.8mb]/[2gb]}{[survivor] [11.2mb]->[59.6mb]/[0b]}
[2022-04-23T00:22:55,456][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [2.2s] collecting in the last [3.4s]
[2022-04-23T00:23:09,833][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][101][15] duration [1.8s], collections [1]/[3.4s], total [1.8s]/[5.6s], memory [213.4mb]->[199.2mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [133.8mb]->[191.2mb]/[2gb]}{[survivor] [59.6mb]->[8mb]/[0b]}
[2022-04-23T00:23:10,854][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][101] overhead, spent [1.8s] collecting in the last [3.4s]
[2022-04-23T00:23:19,874][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][105][16] duration [1.3s], collections [1]/[2.9s], total [1.3s]/[7s], memory [279.2mb]->[204.6mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [191.2mb]->[191.2mb]/[2gb]}{[survivor] [8mb]->[13.3mb]/[0b]}
[2022-04-23T00:23:19,968][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][105] overhead, spent [1.3s] collecting in the last [2.9s]
[2022-04-23T00:23:33,957][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][115][18] duration [1.1s], collections [1]/[2.6s], total [1.1s]/[8.3s], memory [295.3mb]->[210.8mb]/[2gb], all_pools {[young] [88mb]->[8mb]/[0b]}{[old] [203.3mb]->[203.3mb]/[2gb]}{[survivor] [4mb]->[7.4mb]/[0b]}
[2022-04-23T00:23:34,077][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][115] overhead, spent [1.1s] collecting in the last [2.6s]
[2022-04-23T00:23:36,992][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [636ms] collecting in the last [1.5s]
[2022-04-23T00:23:44,224][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][122] overhead, spent [330ms] collecting in the last [1s]
[2022-04-23T00:23:46,659][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][123][23] duration [960ms], collections [1]/[2.1s], total [960ms]/[10.8s], memory [240.5mb]->[222.7mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [207.5mb]->[212.3mb]/[2gb]}{[survivor] [13mb]->[10.3mb]/[0b]}
[2022-04-23T00:23:46,919][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][123] overhead, spent [960ms] collecting in the last [2.1s]
[2022-04-23T00:23:49,517][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.23] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-23T00:23:57,739][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [18.8s/18846ms] to compute cluster state update for [auto create [logstash-2022.04.23]], which exceeds the warn threshold of [10s]
[2022-04-23T00:25:34,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@4dc6d37] took [33767ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:25:37,178][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58944}] took [34969ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:25:55,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8024ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:26:12,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [10507ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:26:37,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:26:47,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19563177009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:26:52,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19623ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:26:59,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19623294234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:27:01,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [19623ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:27:22,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:27:48,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [28446ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:27:49,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28446424040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:27:59,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:07,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38568171515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:12,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:15,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10247419913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:01,595][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60106}] took [86638ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:28:20,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:26,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10696415741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:30,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:37,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9364914308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:41,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:45,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11464959189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:47,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:50,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5967437883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:28:55,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:07,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7938140354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:10,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:48,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6f06ec2b] took [60446ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:29:48,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15014993905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:51,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:52,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41190214565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:57,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:29:58,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5348138250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:30:00,056][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][161][24] duration [21.7s], collections [1]/[3.3m], total [21.7s]/[32.5s], memory [290.7mb]->[247mb]/[2gb], all_pools {[young] [68mb]->[16mb]/[0b]}{[old] [212.3mb]->[219.2mb]/[2gb]}{[survivor] [10.3mb]->[11.7mb]/[0b]}
[2022-04-23T00:30:02,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6228ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:30:19,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8856ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:30:37,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7605ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:31:20,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:31:22,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35853253441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:31:28,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7824ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:31:29,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7823471115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:34:31,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/177777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:34:44,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/177776659026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:34:57,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:10,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26579873808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:18,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20653ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:25,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20653753576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:29,631][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13437}] took [7m] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13437}]] took [0ms], [connecting to new nodes] took [18ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [265779ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [1448ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [709ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [75ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [1348ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [171ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [245ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [0ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [2146ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [525ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [277ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [90ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [85240ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [8192ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [342ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [62ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [8280ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [348ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [7099ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [180ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [40315ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [1049ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [284ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [77ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [76ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [515ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [0ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [125ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [0ms]
[2022-04-23T00:35:33,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:39,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@c990b6e] took [244738ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:35:42,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15620893138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:52,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:35:54,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [19293ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:35:32,000][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60134}] took [47233ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:35:59,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19293867347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:36:08,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:36:23,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16079414609ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:36:56,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:37:02,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48542436303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:37:08,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [64621ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:37:10,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:37:12,645][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1m/63101ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [122], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [122], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [126], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [126], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-23T00:37:19,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12787320056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:37:28,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:37:34,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17814912543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:38:12,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45123ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:38:26,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45122359163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:38:43,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:38:54,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29792109254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:39:06,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:39:19,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24217017135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:40:40,139][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][166][27] duration [3.8m], collections [3]/[8.4m], total [3.8m]/[4.3m], memory [291mb]->[314.8mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [219.2mb]->[235.4mb]/[2gb]}{[survivor] [11.7mb]->[9.1mb]/[0b]}
[2022-04-23T00:40:39,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:40:41,264][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][166] overhead, spent [3.8m] collecting in the last [8.4m]
[2022-04-23T00:40:41,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92862251984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:40:44,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [117079ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:40:44,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:40:45,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5054624892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:40:58,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:40:59,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8408380667ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:18,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:21,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13267221646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:24,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [15345ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:41:26,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8069ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:29,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8069565848ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:33,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:34,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6429920256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:41:28,555][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14068ms] which is above the warn threshold of [5s]
[2022-04-23T00:42:09,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:42:10,282][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][168][28] duration [19.8s], collections [1]/[29.4s], total [19.8s]/[4.6m], memory [312.6mb]->[324.6mb]/[2gb], all_pools {[young] [72mb]->[80mb]/[0b]}{[old] [235.4mb]->[235.4mb]/[2gb]}{[survivor] [9.1mb]->[9.1mb]/[0b]}
[2022-04-23T00:42:10,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36759163263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:42:11,474][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][168] overhead, spent [19.8s] collecting in the last [29.4s]
[2022-04-23T00:42:12,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [45643ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:42:13,400][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.6m/399371ms] ago, timed out [1.1m/67180ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [797]
[2022-04-23T00:42:21,991][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [102421ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [59] unchanged indices
[2022-04-23T00:42:29,159][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.1m] publication of cluster state version [13438] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:42:33,409][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [797] timed out after [332191ms]
[2022-04-23T00:42:47,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7082ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:42:58,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:03,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9423672298ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:03,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:04,518][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][30] duration [7.7s], collections [2]/[23.4s], total [7.7s]/[4.8m], memory [303mb]->[253.5mb]/[2gb], all_pools {[young] [56mb]->[4mb]/[0b]}{[old] [242.5mb]->[245.1mb]/[2gb]}{[survivor] [4.4mb]->[8.4mb]/[0b]}
[2022-04-23T00:43:04,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5936692087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:05,339][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [7.7s] collecting in the last [23.4s]
[2022-04-23T00:43:05,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7591ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:43:11,529][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13438}] took [39s] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13438}]] took [49ms], [connecting to new nodes] took [91ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [6302ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [0ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [360ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [0ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [114ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [0ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [373ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [164ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [58ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [136ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [23779ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [935ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [77ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [1082ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [61ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [3406ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [1270ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [155ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [142ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [93ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [0ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [0ms]
[2022-04-23T00:43:16,136][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7.3m/443555ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [8m/484350ms]
[2022-04-23T00:43:21,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@2d765f2] took [11044ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:43:29,585][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [936] timed out after [16048ms]
[2022-04-23T00:43:37,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:37,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6425687433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:41,185][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][184][32] duration [5.2s], collections [2]/[1.7s], total [5.2s]/[4.8m], memory [273.5mb]->[285.5mb]/[2gb], all_pools {[young] [20mb]->[84mb]/[0b]}{[old] [245.1mb]->[245.1mb]/[2gb]}{[survivor] [8.4mb]->[11.7mb]/[0b]}
[2022-04-23T00:43:42,247][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][184] overhead, spent [5.2s] collecting in the last [1.7s]
[2022-04-23T00:43:42,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [12426ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:43:45,911][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [937] timed out after [22540ms]
[2022-04-23T00:43:46,486][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [17242ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [59] unchanged indices
[2022-04-23T00:43:47,183][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [19.7s] publication of cluster state version [13439] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:43:58,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:58,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5046312995ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:43:58,724][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][191][33] duration [2.9s], collections [1]/[6.2s], total [2.9s]/[4.9m], memory [312.9mb]->[259.2mb]/[2gb], all_pools {[young] [56mb]->[72mb]/[0b]}{[old] [250.8mb]->[250.8mb]/[2gb]}{[survivor] [6mb]->[8.3mb]/[0b]}
[2022-04-23T00:43:59,995][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][191] overhead, spent [2.9s] collecting in the last [6.2s]
[2022-04-23T00:44:00,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5046ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:44:07,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][193][35] duration [2.1s], collections [1]/[1.3s], total [2.1s]/[4.9m], memory [318.8mb]->[338.8mb]/[2gb], all_pools {[young] [64mb]->[88mb]/[0b]}{[old] [250.8mb]->[250.8mb]/[2gb]}{[survivor] [8mb]->[8mb]/[0b]}
[2022-04-23T00:44:07,988][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][193] overhead, spent [2.1s] collecting in the last [1.3s]
[2022-04-23T00:44:11,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [9002ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:44:23,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@2892aea1] took [5871ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:44:24,014][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][36] duration [1.5s], collections [1]/[7.9s], total [1.5s]/[5m], memory [307.6mb]->[344.2mb]/[2gb], all_pools {[young] [44mb]->[80mb]/[0b]}{[old] [250.8mb]->[255.7mb]/[2gb]}{[survivor] [12.7mb]->[8.5mb]/[0b]}
[2022-04-23T00:44:32,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [7669ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:44:34,176][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][197][37] duration [2.9s], collections [1]/[9.9s], total [2.9s]/[5m], memory [344.2mb]->[271.1mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [255.7mb]->[255.7mb]/[2gb]}{[survivor] [8.5mb]->[7.4mb]/[0b]}
[2022-04-23T00:44:36,683][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][197] overhead, spent [2.9s] collecting in the last [9.9s]
[2022-04-23T00:44:42,783][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [957] timed out after [19588ms]
[2022-04-23T00:44:43,004][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][38] duration [1.6s], collections [1]/[6.1s], total [1.6s]/[5m], memory [271.1mb]->[351.1mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [255.7mb]->[255.7mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-04-23T00:44:43,255][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [1.6s] collecting in the last [6.1s]
[2022-04-23T00:44:37,848][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [956] timed out after [18369ms]
[2022-04-23T00:45:18,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [14579ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:46:33,100][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58942}] took [88268ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:46:27,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.3s/57323ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:46:34,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@bbcf06f] took [65212ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:46:34,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.3s/57322866039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:46:36,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:46:37,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9481388479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:46:38,846][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][39] duration [43.5s], collections [1]/[1.5m], total [43.5s]/[5.8m], memory [291.1mb]->[263.5mb]/[2gb], all_pools {[young] [28mb]->[4mb]/[0b]}{[old] [255.7mb]->[255.7mb]/[2gb]}{[survivor] [7.4mb]->[7.8mb]/[0b]}
[2022-04-23T00:46:39,982][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [43.5s] collecting in the last [1.5m]
[2022-04-23T00:46:44,127][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [962] timed out after [80260ms]
[2022-04-23T00:46:50,301][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [964] timed out after [22557ms]
[2022-04-23T00:46:55,648][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.7m/224552ms] ago, timed out [3.4m/208504ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [936]
[2022-04-23T00:47:02,264][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.5m/153237ms] ago, timed out [2.2m/133649ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [957]
[2022-04-23T00:47:02,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:47:02,227][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.6m/156946ms] ago, timed out [2.3m/138577ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [956]
[2022-04-23T00:47:02,264][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.5m/214507ms] ago, timed out [3.1m/191967ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [937]
[2022-04-23T00:47:02,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5186819007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:47:02,759][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][210][40] duration [2.2s], collections [1]/[6.8s], total [2.2s]/[5.8m], memory [335.5mb]->[264.7mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [255.7mb]->[255.7mb]/[2gb]}{[survivor] [7.8mb]->[9mb]/[0b]}
[2022-04-23T00:47:02,759][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][210] overhead, spent [2.2s] collecting in the last [6.8s]
[2022-04-23T00:47:02,903][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.6m/98835ms] ago, timed out [18.5s/18575ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [962]
[2022-04-23T00:47:02,903][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [35s/35024ms] ago, timed out [12.4s/12467ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [964]
[2022-04-23T00:47:09,035][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13440}] took [3m] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13440}]] took [34ms], [connecting to new nodes] took [35ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [175597ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [161ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [199ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [28ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [30ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [0ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [106ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [23ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [32ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [1594ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [2456ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [29ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [278ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [25ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [25ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [75ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [0ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [51ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [0ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [26ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [1ms]
[2022-04-23T00:47:17,376][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][215][41] duration [3.5s], collections [1]/[1.2s], total [3.5s]/[5.9m], memory [296.7mb]->[348.7mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [255.7mb]->[258.8mb]/[2gb]}{[survivor] [9mb]->[12.4mb]/[0b]}
[2022-04-23T00:47:19,106][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][215] overhead, spent [3.5s] collecting in the last [1.2s]
[2022-04-23T00:47:20,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [9406ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:47:23,556][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13s/13053ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3403ba25], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@849e635d], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@19371002], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2cce16f0]], which exceeds the warn threshold of [10s]
[2022-04-23T00:47:36,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:47:37,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8716338355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:47:37,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@460f8837] took [10717ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:47:38,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][217][42] duration [5.4s], collections [1]/[14.4s], total [5.4s]/[6m], memory [327.2mb]->[291.4mb]/[2gb], all_pools {[young] [60mb]->[24mb]/[0b]}{[old] [258.8mb]->[263.7mb]/[2gb]}{[survivor] [12.4mb]->[15.7mb]/[0b]}
[2022-04-23T00:47:39,475][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][217] overhead, spent [5.4s] collecting in the last [14.4s]
[2022-04-23T00:47:39,319][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A23042022%3Acount%3Adelay_threshold_exceeded?refresh=wait_for&require_alias=true&_source=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58938}] took [175936ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:47:54,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:47:56,587][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60204}] took [9836ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:48:25,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@4cd5583b] took [12408ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:48:34,135][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60204}] took [5204ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:48:33,974][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [54044ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [60] unchanged indices
[2022-04-23T00:48:36,306][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [57.6s] publication of cluster state version [13441] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:48:42,782][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][229][43] duration [1.7s], collections [1]/[3.5s], total [1.7s]/[6m], memory [331.4mb]->[287.5mb]/[2gb], all_pools {[young] [52mb]->[16mb]/[0b]}{[old] [263.7mb]->[271.5mb]/[2gb]}{[survivor] [15.7mb]->[16mb]/[0b]}
[2022-04-23T00:48:42,950][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][229] overhead, spent [1.7s] collecting in the last [3.5s]
[2022-04-23T00:48:47,435][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][230][44] duration [2.5s], collections [1]/[4.3s], total [2.5s]/[6m], memory [287.5mb]->[290.8mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [271.5mb]->[279.8mb]/[2gb]}{[survivor] [16mb]->[10.9mb]/[0b]}
[2022-04-23T00:48:47,690][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][230] overhead, spent [2.5s] collecting in the last [4.3s]
[2022-04-23T00:49:04,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7947ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:49:14,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:49:15,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8210673744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:49:22,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5171ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:49:23,224][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [35094ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [58] unchanged indices
[2022-04-23T00:49:31,196][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [43.3s] publication of cluster state version [13442] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:49:39,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7602ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:49:49,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:49:49,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [6462ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:49:52,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5261662544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:49:49,812][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1210] timed out after [24850ms]
[2022-04-23T00:49:56,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7354ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:49:57,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7353709154ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:49:57,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7353ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:50:15,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [9710ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:50:15,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:50:19,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8910100468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:50:22,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:50:22,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7623ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:50:23,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7623969593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:50:29,430][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/69468ms] ago, timed out [44.6s/44618ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1210]
[2022-04-23T00:50:45,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6ce376c9] took [14551ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:50:59,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8023ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:51:05,474][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60228}] took [5931ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:51:16,021][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13442}] took [1.5m] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13442}]] took [60ms], [connecting to new nodes] took [265ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [7415ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [0ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [365ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [0ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [140ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [64ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [142ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [1073ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [361ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [67ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [70ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [39815ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [7659ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [733ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [541ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [6074ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [1187ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [9889ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [108ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [9971ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [3880ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [1705ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [30ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [85ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [82ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [280ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [0ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [148ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [0ms]
[2022-04-23T00:51:20,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [7564ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:51:16,678][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7404ms] which is above the warn threshold of [5s]
[2022-04-23T00:51:34,024][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60228}] took [17603ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:52:00,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:04,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24493259371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:51:34,549][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1328] timed out after [42397ms]
[2022-04-23T00:52:08,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:12,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7953764175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:14,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:21,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5784662389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:22,131][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][243][45] duration [17.8s], collections [1]/[54.6s], total [17.8s]/[6.3m], memory [350.8mb]->[296mb]/[2gb], all_pools {[young] [68mb]->[12mb]/[0b]}{[old] [279.8mb]->[286.4mb]/[2gb]}{[survivor] [10.9mb]->[5.5mb]/[0b]}
[2022-04-23T00:52:28,432][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][243] overhead, spent [17.8s] collecting in the last [54.6s]
[2022-04-23T00:52:28,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:30,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [27881ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:52:30,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14142786032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:52:31,998][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [56.4s/56477ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-23T00:52:40,936][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/115195ms] ago, timed out [1.2m/72798ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1328]
[2022-04-23T00:52:50,297][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][245][46] duration [3.6s], collections [1]/[2.5s], total [3.6s]/[6.4m], memory [332mb]->[352mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [5.5mb]->[5.4mb]/[0b]}
[2022-04-23T00:52:50,539][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11319ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [57] unchanged indices
[2022-04-23T00:52:50,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][245] overhead, spent [3.6s] collecting in the last [2.5s]
[2022-04-23T00:52:50,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8317ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:52:56,762][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][246][47] duration [2.9s], collections [1]/[9.3s], total [2.9s]/[6.4m], memory [352mb]->[379.9mb]/[2gb], all_pools {[young] [0b]->[24mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [5.4mb]->[7.6mb]/[0b]}
[2022-04-23T00:52:57,173][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][246] overhead, spent [2.9s] collecting in the last [9.3s]
[2022-04-23T00:52:57,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5457ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:53:19,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:53:19,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [10524ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:53:20,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10324792618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:53:23,606][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][252][48] duration [7.3s], collections [1]/[14.7s], total [7.3s]/[6.6m], memory [362.1mb]->[324.5mb]/[2gb], all_pools {[young] [68mb]->[28mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [7.6mb]->[10mb]/[0b]}
[2022-04-23T00:53:24,906][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][252] overhead, spent [7.3s] collecting in the last [14.7s]
[2022-04-23T00:53:25,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6235ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:53:34,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@3d3e77d7] took [6538ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:53:35,914][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][253][49] duration [3.3s], collections [1]/[14.3s], total [3.3s]/[6.6m], memory [324.5mb]->[303.7mb]/[2gb], all_pools {[young] [28mb]->[12mb]/[0b]}{[old] [286.4mb]->[289.1mb]/[2gb]}{[survivor] [10mb]->[6.6mb]/[0b]}
[2022-04-23T00:53:38,755][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13443}] took [47.9s] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13443}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [46037ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [0ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [358ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [0ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [33ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [27ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [128ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [1ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [30ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [34ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [445ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [2ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [25ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [28ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [278ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [26ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [23ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [0ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [0ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@2d358347]]]] took [274ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4b9fbb56]]]] took [1ms]
[2022-04-23T00:53:55,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7237ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:53:55,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7236812029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:53:55,639][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][261][50] duration [5.5s], collections [1]/[8.8s], total [5.5s]/[6.7m], memory [379.7mb]->[294.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [289.1mb]->[289.1mb]/[2gb]}{[survivor] [6.6mb]->[5.2mb]/[0b]}
[2022-04-23T00:53:55,640][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][261] overhead, spent [5.5s] collecting in the last [8.8s]
[2022-04-23T00:53:55,636][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12986ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [60] unchanged indices
[2022-04-23T00:53:55,966][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.9s] publication of cluster state version [13444] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:54:21,375][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0]]]).
[2022-04-23T00:55:00,912][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13446}] took [30.3s] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13446}]] took [0ms], [connecting to new nodes] took [36ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [2721ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [0ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [399ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [30ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [30ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [29ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [396ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [61ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [39ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [12314ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [1ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [2107ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [153ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [3903ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [210ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [3087ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [82ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [2806ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [1178ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [326ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [40ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [28ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [0ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [35ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [0ms]
[2022-04-23T00:55:02,500][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.23][0]]]).
[2022-04-23T00:55:10,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][297][51] duration [3.1s], collections [1]/[1.6s], total [3.1s]/[6.8m], memory [314.3mb]->[382.3mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [289.1mb]->[289.1mb]/[2gb]}{[survivor] [5.2mb]->[10.2mb]/[0b]}
[2022-04-23T00:55:11,319][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][297] overhead, spent [3.1s] collecting in the last [1.6s]
[2022-04-23T00:55:11,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6892ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:55:34,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8216ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:55:35,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8216550815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:55:37,344][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][302][52] duration [6.7s], collections [1]/[12.9s], total [6.7s]/[6.9m], memory [307.4mb]->[300.7mb]/[2gb], all_pools {[young] [12mb]->[64mb]/[0b]}{[old] [289.1mb]->[291mb]/[2gb]}{[survivor] [10.2mb]->[9.7mb]/[0b]}
[2022-04-23T00:55:38,260][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][302] overhead, spent [6.7s] collecting in the last [12.9s]
[2022-04-23T00:55:38,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [12550ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:55:46,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5687ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:55:47,099][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][304][53] duration [4s], collections [1]/[6.7s], total [4s]/[6.9m], memory [380.7mb]->[299.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [291mb]->[293.4mb]/[2gb]}{[survivor] [9.7mb]->[6.2mb]/[0b]}
[2022-04-23T00:55:47,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5687063957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:55:47,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][304] overhead, spent [4s] collecting in the last [6.7s]
[2022-04-23T00:56:02,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6881ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:56:10,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6b04e264] took [6423ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:56:20,851][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [42505ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [60] unchanged indices
[2022-04-23T00:56:39,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [20648ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:56:28,962][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1m] publication of cluster state version [13447] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T00:57:23,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:57:30,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5737432020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:57:43,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:57:43,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [19834ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:57:58,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19834871812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:58:06,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:58:12,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [22980ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:58:11,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22980815049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:58:16,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:58:25,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10320738384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:58:13,522][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1782] timed out after [92225ms]
[2022-04-23T00:58:27,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11361ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:58:31,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11361016560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:11,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:12,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43543066207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:10,978][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11361ms] which is above the warn threshold of [5s]
[2022-04-23T00:59:20,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:22,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5469114127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:28,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:33,993][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][308][54] duration [24.9s], collections [1]/[1.4m], total [24.9s]/[7.4m], memory [355.6mb]->[333.9mb]/[2gb], all_pools {[young] [60mb]->[40mb]/[0b]}{[old] [293.4mb]->[293.4mb]/[2gb]}{[survivor] [6.2mb]->[8.5mb]/[0b]}
[2022-04-23T00:59:33,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11001790828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:37,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:37,651][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][308] overhead, spent [24.9s] collecting in the last [1.4m]
[2022-04-23T00:59:41,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10256249145ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:41,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [21258ms] which is above the warn threshold of [5000ms]
[2022-04-23T00:59:47,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T00:59:54,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9107062864ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:01,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:04,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12904773257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:07,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:13,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6adc349d] took [7688ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:00:12,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7688110196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:15,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:20,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8378718792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:23,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8378ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:00:24,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:26,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8420651906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:32,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:32,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [7789ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:00:32,158][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.2m/253657ms] ago, timed out [2.6m/161432ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1782]
[2022-04-23T01:00:36,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7789945119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:42,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:42,321][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=296, version=13447}] took [3.5m] which is above the warn threshold of [30s]: [running task [Publication{term=296, version=13447}]] took [345ms], [connecting to new nodes] took [808ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cc10f22] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@79724ef2] took [60169ms], [org.elasticsearch.script.ScriptService@6ae13522] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [0ms], [org.elasticsearch.snapshots.RestoreService@5680c335] took [0ms], [org.elasticsearch.ingest.IngestService@3d6e02c9] took [2855ms], [org.elasticsearch.action.ingest.IngestActionForwarder@5e456863] took [49ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cb300@4f9a8fc2] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1fd3746f] took [0ms], [org.elasticsearch.tasks.TaskManager@52e65516] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27953959] took [155ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ec08e8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3b6e8dd4] took [0ms], [org.elasticsearch.indices.SystemIndexManager@f0d0e87] took [924ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@7c2d8b4] took [226ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@4bb8bc72] took [376ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@568b4eab] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5cafba02] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@75d0c782] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@34b0e1bd] took [155ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1b931d36] took [82802ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3e8922a6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7c48618d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@ba3e0b0] took [19282ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3f8ee48b] took [447ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4d8234b3] took [247ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@72f97087] took [12463ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@721c7a4b] took [167ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@664bb808] took [10757ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2e29a2f6] took [156ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@70266d2b] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@cdca94a] took [9903ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7bc58f5c] took [6667ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@18b5df3a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@63a10d0a] took [1027ms], [org.elasticsearch.node.ResponseCollectorService@34c9d14d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@714a4604] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@47b01b18] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2c1287b9] took [214ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@62ecc12d] took [210ms], [org.elasticsearch.indices.store.IndicesStore@335e8767] took [1422ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3d2d729f] took [115ms], [org.elasticsearch.license.LicenseService@22449774] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@618701b4] took [65ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3fd6d221] took [0ms], [org.elasticsearch.gateway.GatewayService@22936c60] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@30fdca68] took [0ms]
[2022-04-23T01:00:47,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9748731984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:50,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [9748ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:00:52,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:55,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9961277582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:00:55,923][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.8m/412549ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [6.9m/416370ms]
[2022-04-23T01:00:58,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:01:00,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6136143091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:01:01,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6136ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:01:04,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:01:09,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6051381000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:02:21,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:02:29,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77263872241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:02:32,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@51862079] took [77263ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:02:36,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:02:44,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14827809601ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:02:53,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16844ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:00,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16844537034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:05,815][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][312][55] duration [52.5s], collections [1]/[1.6m], total [52.5s]/[8.2m], memory [381.9mb]->[300.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [293.4mb]->[293.4mb]/[2gb]}{[survivor] [8.5mb]->[7.3mb]/[0b]}
[2022-04-23T01:03:07,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13973ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:10,590][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][312] overhead, spent [52.5s] collecting in the last [1.6m]
[2022-04-23T01:03:15,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [45645ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:03:15,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13972922000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:23,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15884ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:27,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [15884ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:03:29,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15884269827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:36,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:42,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [13005ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:03:43,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13005158423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:48,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:03:55,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12484553344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:00,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:07,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12293094425ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:12,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:16,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10938341178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:21,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:26,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9831993526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:18,124][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10938ms] which is above the warn threshold of [5s]
[2022-04-23T01:04:36,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:40,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14628334433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:48,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:52,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10757972869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:55,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:04:54,753][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2030] timed out after [145469ms]
[2022-04-23T01:05:01,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8994578889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:02,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8994ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:05:05,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:06,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [9568ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:05:12,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9568707381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:17,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:22,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11421071619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:30,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:38,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13872108926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:42,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [13872ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:05:45,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:51,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14791961008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:05:56,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:00,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10988006491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:06,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:10,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9987628320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:09,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6ca13aa] took [9987ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:06:12,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:14,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5843573710ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:16,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5843ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:06:18,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:21,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5812196457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:23,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5367ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:27,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5367612592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:29,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:51,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6527241674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:54,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:57,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24422273547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:06:57,611][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][316][56] duration [11.9s], collections [1]/[38.4s], total [11.9s]/[8.4m], memory [324.7mb]->[300.6mb]/[2gb], all_pools {[young] [24mb]->[8mb]/[0b]}{[old] [293.4mb]->[293.4mb]/[2gb]}{[survivor] [7.3mb]->[7.2mb]/[0b]}
[2022-04-23T01:07:00,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:07:00,578][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][316] overhead, spent [11.9s] collecting in the last [38.4s]
[2022-04-23T01:07:02,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6372514103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:07:02,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [30794ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:07:09,651][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60312}] took [7561ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:07:11,036][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2156] timed out after [63307ms]
[2022-04-23T01:07:23,604][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6112ms] which is above the warn threshold of [5s]
[2022-04-23T01:07:28,442][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/80681ms] ago, timed out [17.3s/17374ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2156]
[2022-04-23T01:07:28,442][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5m/305774ms] ago, timed out [2.6m/160305ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2030]
[2022-04-23T01:07:33,161][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60312}] took [10663ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:07:36,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [9037ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:07:50,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6429ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:08:13,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [12962ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:08:17,907][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60312}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:08:32,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [6906ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:08:51,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:08:55,451][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60312}] took [8806ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:08:56,897][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2282] timed out after [30610ms]
[2022-04-23T01:09:13,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:09:26,805][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1m/60553ms] ago, timed out [29.9s/29943ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2282]
[2022-04-23T01:09:54,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:10:21,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:10:22,502][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][334][57] duration [4.9s], collections [1]/[3.7s], total [4.9s]/[8.5m], memory [376.6mb]->[384.6mb]/[2gb], all_pools {[young] [80mb]->[88mb]/[0b]}{[old] [293.4mb]->[293.4mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-04-23T01:10:23,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8760940841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:10:23,216][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][334] overhead, spent [4.9s] collecting in the last [3.7s]
[2022-04-23T01:10:24,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [11162ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:10:55,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:10:58,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16256598962ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:01,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6850ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:05,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6850017534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:08,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:09,389][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][336][58] duration [12.4s], collections [1]/[32.3s], total [12.4s]/[8.7m], memory [370.8mb]->[305.6mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [293.4mb]->[294.8mb]/[2gb]}{[survivor] [9.4mb]->[6.7mb]/[0b]}
[2022-04-23T01:11:12,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6318648468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:12,826][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][336] overhead, spent [12.4s] collecting in the last [32.3s]
[2022-04-23T01:11:15,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [13168ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:11:16,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:15,521][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60324}] took [13696ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:11:22,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7377072982ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:26,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:32,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10914186445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:38,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:26,669][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2486] timed out after [39604ms]
[2022-04-23T01:11:43,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12132284353ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:49,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:51,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [23035ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:11:53,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10903689102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:11:56,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [7117ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:11:56,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:02,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7117547116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:06,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:09,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9673989525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:15,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:20,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8841072234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:29,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:31,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [22856ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:12:38,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14015142180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:12:24,468][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8841ms] which is above the warn threshold of [5s]
[2022-04-23T01:12:48,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:13:02,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15997010011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:13:16,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30836ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:13:27,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6ca6c6e0] took [30835ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:13:28,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30835799816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:13:38,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:13:56,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21503942105ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:14:20,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:14:35,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37655857909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:14:53,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36415ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:15:05,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36415109191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:15:37,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42976ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:15:53,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42975964216ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:16:01,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:16:05,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [69124ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:16:12,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26148265162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:16:24,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:16:36,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23111485654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:16:55,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:17:05,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23552635370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:19:19,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:19:24,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150622050982ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:19:33,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:19:40,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15384413262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:19:52,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:20:03,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17630804610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:20:09,292][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.2m/555035ms] ago, timed out [8.5m/515431ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2486]
[2022-04-23T01:20:14,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:20:27,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22182231017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:20:07,398][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2617] timed out after [361985ms]
[2022-04-23T01:20:42,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:20:57,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27635485166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:21:12,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:20:48,561][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27635ms] which is above the warn threshold of [5s]
[2022-04-23T01:22:03,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31239658854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:22:09,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][340][59] duration [1.7m], collections [1]/[6.1m], total [1.7m]/[10.5m], memory [357.6mb]->[310.9mb]/[2gb], all_pools {[young] [60mb]->[8mb]/[0b]}{[old] [294.8mb]->[294.8mb]/[2gb]}{[survivor] [6.7mb]->[8.1mb]/[0b]}
[2022-04-23T01:22:13,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.7s/59711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:22:19,157][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][340] overhead, spent [1.7m] collecting in the last [6.1m]
[2022-04-23T01:22:26,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.7s/59710908878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:22:31,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [59710ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:22:38,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:22:48,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25735916996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:04,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:14,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26036076163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:23,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:31,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18558280021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:39,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:46,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16572681106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:23:54,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:24:20,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16009725606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:24:35,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:24:38,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33489036483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:24:52,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23304ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:24:59,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [23303ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:24:58,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23303539095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:05,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:09,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [13564ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:25:15,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13564246325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:23,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:30,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17898981888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:38,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:45,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15020137481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:50,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:25:56,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12658891704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:01,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:09,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9925629311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:05,708][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9926ms] which is above the warn threshold of [5s]
[2022-04-23T01:26:17,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:26,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16599562109ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:36,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18681ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:41,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [35280ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:26:50,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18680999929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:26:30,084][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2729] timed out after [125861ms]
[2022-04-23T01:27:00,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:14,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23815350586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:24,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:31,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24396622609ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:38,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:47,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14666986430ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:52,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:55,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [13612ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:27:56,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13612341724ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:27:58,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:28:02,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6223386259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:28:06,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:28:12,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [8087ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:28:12,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8087204434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:28:18,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:28:35,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [10344ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:28:40,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10344491309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:28:56,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:29:10,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37825759578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:29:15,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:29:25,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20143392317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:29:36,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:29:43,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20428191061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:29:50,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [20428ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:29:53,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:42:54,317][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.9m/358640ms] ago, timed out [3.8m/232779ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2729]
[2022-04-23T01:42:54,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17954506842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:43:11,655][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [16.6m/997321ms] ago, timed out [10.5m/635336ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2617]
[2022-04-23T01:43:15,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3m/799757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:43:26,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3m/799757768561ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:43:41,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:43:53,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26649405787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:44:12,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:44:28,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28030539093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:44:43,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:44:58,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34517479902ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:45:56,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:45:53,652][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2849] timed out after [114784ms]
[2022-04-23T01:46:00,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73572501290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:46:06,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:45:56,988][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [73572ms] which is above the warn threshold of [5s]
[2022-04-23T01:46:15,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10444573259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:46:27,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:46:37,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20766962801ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:46:47,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:47:52,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19301904869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:48:02,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:48:17,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76247344339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:48:32,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:48:51,226][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][346][60] duration [11.3m], collections [1]/[17.2m], total [11.3m]/[21.9m], memory [362.9mb]->[340.5mb]/[2gb], all_pools {[young] [76mb]->[44mb]/[0b]}{[old] [294.8mb]->[294.8mb]/[2gb]}{[survivor] [8.1mb]->[9.7mb]/[0b]}
[2022-04-23T01:48:53,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29102052242ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:49:08,601][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][346] overhead, spent [11.3m] collecting in the last [17.2m]
[2022-04-23T01:49:13,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:49:21,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [163560ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:49:23,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38909634599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:50:04,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:50:14,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51721514826ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:50:23,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19772ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:50:36,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19771261804ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:50:45,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:50:57,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22977223735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:51:10,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:51:24,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23121967119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:51:36,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27536ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:51:45,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27535865788ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:00,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:15,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22988442944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:24,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:31,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607369609ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:32,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@641194d9] took [25607ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:52:38,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:47,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12581730158ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:56,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:52:58,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [19300ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:53:02,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19300715411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:08,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:12,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [11740ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:52:59,563][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19301ms] which is above the warn threshold of [5s]
[2022-04-23T01:53:17,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11740287691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:27,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:34,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18116032622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:40,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:42,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [13763ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:53:46,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13763587892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:37,694][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2971] timed out after [61739ms]
[2022-04-23T01:53:53,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:53:57,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [13228ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:54:03,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13228880482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:09,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:18,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [16264ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:54:15,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16264256246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:25,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [14803ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:54:24,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:32,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14803743487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:38,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:45,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14932996341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:54:35,098][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] starting SLM retention snapshot cleanup task
[2022-04-23T01:54:57,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:55:08,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17297513823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:55:10,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [17297ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:55:19,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21939ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:55:29,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21939215424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:55:42,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:55:57,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23623079287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:56:13,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@6c3e21f0] took [30346ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:56:14,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30346ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:56:29,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30346865849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:56:42,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:56:58,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29193973852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:57:10,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:57:21,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27806467217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:57:32,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:57:40,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22983478847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:57:52,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:58:01,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17939533860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:58:13,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:58:26,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [22544ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:58:25,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22544777005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:58:35,045][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.8m/348408ms] ago, timed out [4.7m/286669ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2971]
[2022-04-23T01:58:34,415][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] there are no repositories to fetch, SLM retention snapshot cleanup task complete
[2022-04-23T01:58:35,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:58:37,563][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [30.2m/1814215ms] ago, timed out [28.3m/1699431ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2849]
[2022-04-23T01:58:47,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22572866025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:59:04,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:59:16,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28014500869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:59:26,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22991ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:59:28,972][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk?refresh=false&_source_includes=originId&require_alias=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58894}] took [137046ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:59:10,556][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28015ms] which is above the warn threshold of [5s]
[2022-04-23T01:59:34,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22990860870ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T01:59:41,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@592333a5, interval=5s}] took [14307ms] which is above the warn threshold of [5000ms]
[2022-04-23T01:59:42,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T01:59:33,948][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3078] timed out after [201403ms]
[2022-04-23T01:59:51,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14307007756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:00:03,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:00:23,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22507536594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:00:38,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34014ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:05:17,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34013462999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:05:34,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:05:42,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296800480342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:05:48,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:05:55,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14402401812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:06:05,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:06:13,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15883343729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:06:24,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:06:30,390][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][352][61] duration [4.1m], collections [1]/[7.8m], total [4.1m]/[26m], memory [380.5mb]->[303mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [294.8mb]->[295.4mb]/[2gb]}{[survivor] [9.7mb]->[7.5mb]/[0b]}
[2022-04-23T02:06:33,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19462385597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:06:41,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:06:42,588][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][352] overhead, spent [4.1m] collecting in the last [7.8m]
[2022-04-23T02:06:51,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [67107ms] which is above the warn threshold of [5000ms]
[2022-04-23T02:06:51,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17359169715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:00,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:11,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18198591159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:20,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:28,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21121859376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:36,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:45,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15897037918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:07:57,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:08:06,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19749405751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:08:17,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:08:24,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20261781854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:08:34,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17684ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:08:45,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17683648405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:08:59,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:09:02,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@55aa228] took [42463ms] which is above the warn threshold of [5000ms]
[2022-04-23T02:09:09,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24779595449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:09:41,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:10:13,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30130876161ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:10:39,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:11:09,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66376502310ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:11:37,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59123ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:12:06,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59123208119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:12:39,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.3s/59351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:13:05,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.3s/59351287667ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:13:37,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:14:18,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61766243383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:15:07,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88775ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:16:04,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88775533508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:17:02,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:17:38,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113370297205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:18:22,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:18:42,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82557259089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:19:03,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:19:28,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40654503582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:19:55,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:20:18,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52183326372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:22:33,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:20:24,573][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [52184ms] which is above the warn threshold of [5s]
[2022-04-23T02:22:43,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159582499164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:22:58,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:23:10,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24633255001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:23:45,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:24:45,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47552111540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:24:56,089][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][353][62] duration [1.2m], collections [1]/[16.9m], total [1.2m]/[27.3m], memory [303mb]->[302.6mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [295.4mb]->[295.4mb]/[2gb]}{[survivor] [7.5mb]->[7.1mb]/[0b]}
[2022-04-23T02:24:57,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:25:07,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [143613ms] which is above the warn threshold of [5000ms]
[2022-04-23T02:25:10,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71427925621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:25:25,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:25:39,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27927551416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:26:03,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:26:17,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [37683ms] which is above the warn threshold of [5000ms]
[2022-04-23T02:26:28,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37683374795ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:26:53,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46772ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:26:58,625][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [5ccc8905-dc69-431a-b6e6-f8f023a5423f][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=1000][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58948}] took [112383ms] which is above the warn threshold of [5000ms]
[2022-04-23T02:27:17,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46771860615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:27:30,302][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [31.1m/1866689ms] ago, timed out [27.7m/1665286ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{0R8VG7caQ_aGfkaiYVqMWA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3078]
[2022-04-23T02:27:46,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.1s/56124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:28:13,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56s/56009917276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:29:54,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:30:39,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127249044773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:31:03,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:31:19,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70113188261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:31:33,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:31:45,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30723576101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:32:07,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:32:24,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31046977267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:32:45,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:33:06,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40714870462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:33:28,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:33:50,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43160420772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:34:15,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:34:41,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46825656201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:34:59,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:35:28,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44574947511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:35:47,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:36:11,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47244947176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:36:35,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:37:03,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46924417769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:37:26,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:37:58,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51065727578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:38:33,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:38:51,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [66907ms] which is above the warn threshold of [5000ms]
[2022-04-23T02:39:17,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66907572483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:40:34,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:38:50,850][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [66907ms] which is above the warn threshold of [5s]
[2022-04-23T02:41:09,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121480773606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:41:52,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:42:41,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75721533361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:44:04,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/132038ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:45:46,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/132038658714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:47:16,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189845ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:48:42,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189844369863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:50:29,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/196341ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:52:30,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/196216490726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T02:54:59,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T02:57:25,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269276846288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:00:17,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/316640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:02:51,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/316625325291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:05:51,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:08:46,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333295733991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:10:58,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306416ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:13:04,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5119/0x00000008017dfdd0@33ea0132] took [639650ms] which is above the warn threshold of [5000ms]
[2022-04-23T03:13:27,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306354435347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:16:15,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:18:50,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318768936207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:21:44,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315038ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:25:00,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314619332496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:28:11,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:31:40,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362632114540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:36:40,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/501260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:39:10,919][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T03:41:37,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/500953879912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:44:49,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/522265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:47:57,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/522699023834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:45:57,354][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58944}] took [1700905ms] which is above the warn threshold of [5000ms]
[2022-04-23T03:50:55,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T03:55:30,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378649256152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T03:55:21,666][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:58942}] took [1402302ms] which is above the warn threshold of [5000ms]
[2022-04-23T03:58:38,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/458242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T04:02:05,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/458251705912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T04:05:22,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408169ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T04:07:20,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@c2bab3d, interval=1s}] took [407762ms] which is above the warn threshold of [5000ms]
[2022-04-23T04:08:26,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407762556725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T04:11:25,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363842ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T04:12:29,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77c36b67, interval=5s}] took [364371ms] which is above the warn threshold of [5000ms]
[2022-04-23T04:14:05,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364371953354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T04:17:01,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T04:59:17,911][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-23T04:59:17,927][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-23T04:59:17,929][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-23T04:59:25,720][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-23T04:59:25,728][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-23T04:59:25,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-23T04:59:25,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-23T04:59:25,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-23T04:59:25,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-23T04:59:25,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-23T04:59:25,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-23T04:59:25,738][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-23T04:59:25,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-23T04:59:25,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-23T04:59:25,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-23T04:59:25,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-23T04:59:25,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-23T04:59:25,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-23T04:59:25,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-23T04:59:25,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-23T04:59:25,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-23T04:59:25,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-23T04:59:25,748][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-23T04:59:25,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-23T04:59:25,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-23T04:59:25,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-23T04:59:25,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-23T04:59:25,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-23T04:59:25,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-23T04:59:25,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-23T04:59:25,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-23T04:59:25,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-23T04:59:25,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-23T04:59:25,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-23T04:59:25,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-23T04:59:25,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-23T04:59:25,773][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-23T04:59:25,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-23T04:59:25,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-23T04:59:25,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-23T04:59:25,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-23T04:59:25,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-23T04:59:25,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-23T04:59:25,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-23T04:59:25,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-23T04:59:25,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-23T04:59:25,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-23T04:59:25,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-23T04:59:25,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-23T04:59:25,788][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-23T04:59:25,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-23T04:59:25,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-23T04:59:25,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-23T04:59:25,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-23T04:59:25,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-23T04:59:25,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-23T04:59:25,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-23T04:59:25,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-23T04:59:25,800][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-23T04:59:25,800][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-23T04:59:25,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-23T04:59:25,804][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-23T04:59:25,938][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.9gb], net total_space [125.8gb], types [ext4]
[2022-04-23T04:59:25,939][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-23T04:59:26,747][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-23T04:59:43,367][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-23T04:59:43,385][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-23T04:59:43,388][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T04:59:43,403][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-23T04:59:43,403][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T04:59:43,404][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-23T04:59:43,405][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-23T04:59:43,405][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T04:59:43,406][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-23T04:59:43,407][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-23T04:59:43,412][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-23T04:59:43,413][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T04:59:43,414][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T04:59:43,415][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T04:59:43,415][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-23T04:59:45,704][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-23T04:59:45,926][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-23T04:59:47,378][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-23T04:59:48,775][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-23T04:59:48,777][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-23T04:59:48,869][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-23T04:59:48,871][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-23T04:59:49,258][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-23T04:59:54,712][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-23T04:59:54,932][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{CD5PEA66RTKMSHOUZzOSQQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 297, version: 13448, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{CD5PEA66RTKMSHOUZzOSQQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-23T04:59:55,321][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{CD5PEA66RTKMSHOUZzOSQQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 297, version: 13448, reason: Publication{term=297, version=13448}
[2022-04-23T04:59:55,486][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-23T04:59:55,487][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-23T04:59:57,547][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-23T04:59:57,561][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [61] indices into cluster_state
[2022-04-23T04:59:59,458][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-23T04:59:59,459][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-23T05:00:00,816][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-23T05:00:02,211][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-23T05:00:02,220][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-23T05:00:02,222][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-23T05:00:03,438][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T05:00:03,726][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T05:00:08,399][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-23T05:00:08,505][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-23T05:00:08,542][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-23T05:00:08,923][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T05:00:08,934][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T05:00:09,571][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T05:00:22,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-23T05:00:22,325][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-23T05:00:22,334][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-23T05:00:22,916][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-04-23T05:00:23,502][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-23T05:00:23,540][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-23T05:00:23,742][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T05:00:23,746][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T05:00:24,803][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T05:00:24,804][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T05:00:52,345][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:52,492][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:52,510][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:52,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:52,754][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:52,896][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,005][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,011][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,102][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,192][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,379][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,473][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,482][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,486][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,689][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:53,927][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:55,269][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:55,515][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:55,967][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:56,258][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:56,435][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:56,755][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:57,323][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:57,507][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:57,522][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:57,753][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,030][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,235][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,503][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,659][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,677][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,876][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,970][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,976][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:58,983][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:59,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:59,403][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:59,592][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:59,622][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:00:59,868][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:00,063][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:00,084][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:00,329][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:00,527][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:00,710][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:00,784][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:01,067][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:01,178][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:01,188][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:01,203][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:01,570][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:01,842][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:02,090][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:02,263][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:02,442][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:02,453][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:03,411][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:05,250][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:07,623][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:01:09,204][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:06:09,987][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:06:10,114][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:06:10,123][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:11:18,254][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:14:46,765][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:15:05,827][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:15:05,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:15:20,529][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:15:20,947][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T05:19:19,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3971caef, interval=1s}] took [7700ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:19:25,265][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk?refresh=false&_source_includes=originId&require_alias=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59704}] took [7871ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:20:09,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52b50f31, interval=5s}] took [5382ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:20:42,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3971caef, interval=1s}] took [13514ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:20:40,610][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:60838}] took [8723ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:27:16,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5838ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:31:50,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5473107367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T05:33:23,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1m/730160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:34:27,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1m/730525290783ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T05:35:17,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:36:58,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111746597250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T05:37:28,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/130629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:38:36,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/130629224217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T05:43:28,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/202858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:45:48,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/202858298266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T05:43:33,126][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59698}] took [130629ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:48:40,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:49:38,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@67bdb14e, interval=5s}] took [416612ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:50:54,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416612973112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T05:48:25,962][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59710}] took [416613ms] which is above the warn threshold of [5000ms]
[2022-04-23T05:56:14,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/504727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T05:59:11,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/504416731417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:02:17,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:04:39,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361756758808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:08:07,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:07:25,278][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [866174ms] which is above the warn threshold of [5s]
[2022-04-23T06:12:04,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334233498239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:14:10,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379851ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:16:17,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379423612475ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:18:50,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:21:46,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275758549845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:24:58,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/365422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:27:30,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/365593222479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:29:43,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/271247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:32:44,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/271118245700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:34:40,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:37:37,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320066851223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:40:31,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:43:11,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339459939234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:46:02,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:48:54,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327010817784ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:51:16,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T06:53:56,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320883537445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T06:57:11,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329976ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:00:18,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330108509893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:03:20,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:06:09,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387644143414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:08:54,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:11:10,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3971caef, interval=1s}] took [333921ms] which is above the warn threshold of [5000ms]
[2022-04-23T07:11:14,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333921821616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:14:01,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/316788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:12:55,044][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [23.4s/23474ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27107a3]], which exceeds the warn threshold of [10s]
[2022-04-23T07:16:16,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/316425289503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:19:20,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308239ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:21:46,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308020209424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:20:10,249][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.3s/10389ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a6aad8], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5a415410], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@eeda00b5]], which exceeds the warn threshold of [10s]
[2022-04-23T07:24:39,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:24:15,041][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59674}] took [308020ms] which is above the warn threshold of [5000ms]
[2022-04-23T07:27:05,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319140309012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:31:40,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:40:52,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431713241414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:43:36,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7m/705256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:46:27,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7m/705255637567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:47:05,762][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10.4m/627160ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [14.8m/890450ms]
[2022-04-23T07:53:48,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1m/611954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T07:57:11,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1m/611397694303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T07:57:07,962][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [39.5m/2375527ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [39.5m/2374182ms]
[2022-04-23T08:09:22,522][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-23T08:09:22,566][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-23T08:09:22,567][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-23T08:09:27,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-23T08:09:27,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-23T08:09:27,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-23T08:09:27,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-23T08:09:27,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-23T08:09:27,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-23T08:09:27,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-23T08:09:27,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-23T08:09:27,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-23T08:09:27,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-23T08:09:27,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-23T08:09:27,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-23T08:09:27,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-23T08:09:27,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-23T08:09:27,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-23T08:09:27,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-23T08:09:27,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-23T08:09:27,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-23T08:09:27,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-23T08:09:27,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-23T08:09:27,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-23T08:09:27,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-23T08:09:27,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-23T08:09:27,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-23T08:09:27,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-23T08:09:27,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-23T08:09:27,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-23T08:09:27,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-23T08:09:27,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-23T08:09:27,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-23T08:09:27,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-23T08:09:27,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-23T08:09:27,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-23T08:09:27,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-23T08:09:27,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-23T08:09:27,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-23T08:09:27,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-23T08:09:27,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-23T08:09:27,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-23T08:09:27,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-23T08:09:27,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-23T08:09:27,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-23T08:09:27,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-23T08:09:27,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-23T08:09:27,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-23T08:09:27,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-23T08:09:27,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-23T08:09:27,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-23T08:09:27,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-23T08:09:27,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-23T08:09:27,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-23T08:09:27,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-23T08:09:27,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-23T08:09:27,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-23T08:09:27,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-23T08:09:27,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-23T08:09:27,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-23T08:09:27,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-23T08:09:27,979][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-23T08:09:28,077][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.8gb], net total_space [125.8gb], types [ext4]
[2022-04-23T08:09:28,078][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-23T08:09:28,652][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-23T08:09:38,922][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-23T08:09:38,931][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-23T08:09:38,933][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T08:09:38,934][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-23T08:09:38,934][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T08:09:38,935][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-23T08:09:38,936][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-23T08:09:38,936][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T08:09:38,937][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-23T08:09:38,937][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-23T08:09:38,938][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-23T08:09:38,938][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T08:09:38,939][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T08:09:38,940][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T08:09:38,941][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-23T08:09:40,151][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-23T08:09:40,321][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-23T08:09:41,328][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-23T08:09:42,334][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-23T08:09:42,336][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-23T08:09:42,477][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-23T08:09:42,479][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-23T08:09:42,789][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-23T08:09:47,141][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-23T08:09:47,434][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 298, version: 13571, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-23T08:09:48,428][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 298, version: 13571, reason: Publication{term=298, version=13571}
[2022-04-23T08:09:48,850][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-23T08:09:48,853][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-23T08:09:50,013][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-23T08:09:50,075][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [61] indices into cluster_state
[2022-04-23T08:10:12,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [5243ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:10:33,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@35ee925f] took [8963ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:10:32,603][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-23T08:10:34,072][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=298, version=13574}] took [34.8s] which is above the warn threshold of [30s]: [running task [Publication{term=298, version=13574}]] took [0ms], [connecting to new nodes] took [98ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@19587bee] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@65c51b7a] took [32620ms], [org.elasticsearch.script.ScriptService@160c7fc7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [0ms], [org.elasticsearch.snapshots.RestoreService@27e53106] took [0ms], [org.elasticsearch.ingest.IngestService@6a36e04b] took [50ms], [org.elasticsearch.action.ingest.IngestActionForwarder@1fb1488d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016ccee0@2723f32d] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@29408bce] took [1ms], [org.elasticsearch.tasks.TaskManager@74291215] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@61c9d93d] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@37d9ef9e] took [134ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e80fb67] took [19ms], [org.elasticsearch.indices.SystemIndexManager@5ec23e41] took [55ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2b4694ff] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@6604a4fa] took [18ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@58a75f22] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@72256b7f] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013be000@5182f1d5] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2a9354b8] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3bdb555] took [690ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3ea44717] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5320530c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4fa02a80] took [183ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3d663aa1] took [62ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4a3f35d4] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1b56a7ac] took [26ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [40ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@556c7a6c] took [26ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@5953e152] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@723df2fc] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@421444e2] took [2ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3642b950] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@639076c4] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@432100f1] took [46ms], [org.elasticsearch.node.ResponseCollectorService@608aa9fc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@38824d6b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@58c42ae9] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@13299f88] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@106ba9b2] took [0ms], [org.elasticsearch.indices.store.IndicesStore@6c86e02f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@392ba327] took [602ms], [org.elasticsearch.license.LicenseService@49003cc2] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5f373553] took [23ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4fd9d010] took [0ms], [org.elasticsearch.gateway.GatewayService@507c41a9] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7396af42] took [0ms]
[2022-04-23T08:10:34,219][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-23T08:10:52,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [7602ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:10:52,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:10:53,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7602165683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:11:00,764][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33404}] took [29881ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:11:00,764][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33410}] took [29881ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:11:00,764][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33408}] took [29881ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:11:22,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [7123ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:11:22,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:11:23,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5121819890ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:11:24,434][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.1s/16114ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [222], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [222], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-23T08:11:40,869][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [13.2s] publication of cluster state version [13575] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T08:12:29,479][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=298, version=13575}] took [47.5s] which is above the warn threshold of [30s]: [running task [Publication{term=298, version=13575}]] took [0ms], [connecting to new nodes] took [63ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@19587bee] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@65c51b7a] took [5554ms], [org.elasticsearch.script.ScriptService@160c7fc7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [0ms], [org.elasticsearch.snapshots.RestoreService@27e53106] took [0ms], [org.elasticsearch.ingest.IngestService@6a36e04b] took [299ms], [org.elasticsearch.action.ingest.IngestActionForwarder@1fb1488d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016ccee0@2723f32d] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@29408bce] took [0ms], [org.elasticsearch.tasks.TaskManager@74291215] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@61c9d93d] took [94ms], [org.elasticsearch.cluster.InternalClusterInfoService@37d9ef9e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e80fb67] took [30ms], [org.elasticsearch.indices.SystemIndexManager@5ec23e41] took [2463ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2b4694ff] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@6604a4fa] took [152ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@58a75f22] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@72256b7f] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013be000@5182f1d5] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2a9354b8] took [30ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3bdb555] took [6059ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3ea44717] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5320530c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4fa02a80] took [4445ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3d663aa1] took [102ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4a3f35d4] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1b56a7ac] took [5635ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [289ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@556c7a6c] took [5656ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@5953e152] took [46ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@723df2fc] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@421444e2] took [5417ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3642b950] took [5932ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@639076c4] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@432100f1] took [1706ms], [org.elasticsearch.node.ResponseCollectorService@608aa9fc] took [61ms], [org.elasticsearch.snapshots.SnapshotShardsService@38824d6b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@58c42ae9] took [59ms], [org.elasticsearch.shutdown.PluginShutdownService@13299f88] took [123ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@106ba9b2] took [410ms], [org.elasticsearch.indices.store.IndicesStore@6c86e02f] took [350ms], [org.elasticsearch.persistent.PersistentTasksNodeService@392ba327] took [0ms], [org.elasticsearch.license.LicenseService@49003cc2] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5f373553] took [160ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4fd9d010] took [0ms], [org.elasticsearch.gateway.GatewayService@507c41a9] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7396af42] took [0ms]
[2022-04-23T08:12:40,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@2ea17a8] took [26751ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:14:05,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:16:08,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8890163951ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:16:33,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [9290ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:16:42,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/202194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:17:03,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/202194544922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:17:22,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.8s/42828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:17:35,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.8s/42827659391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:17:40,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:17:45,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18456739297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:17:54,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:05,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12667831384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:12,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1faac1e4, interval=5s}] took [92782ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:18:12,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:19,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18830631880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:23,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:30,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10293701832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:37,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:46,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13736763765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:51,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:52,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14757995065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:18:52,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@4ee60210] took [38788ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:18:54,356][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [356235ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [58] unchanged indices
[2022-04-23T08:18:54,474][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [5.9m] publication of cluster state version [13577] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T08:19:42,421][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][81] overhead, spent [621ms] collecting in the last [1.5s]
[2022-04-23T08:19:49,936][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=298, version=13577}] took [54.9s] which is above the warn threshold of [30s]: [running task [Publication{term=298, version=13577}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@19587bee] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@65c51b7a] took [34438ms], [org.elasticsearch.script.ScriptService@160c7fc7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [0ms], [org.elasticsearch.snapshots.RestoreService@27e53106] took [0ms], [org.elasticsearch.ingest.IngestService@6a36e04b] took [0ms], [org.elasticsearch.action.ingest.IngestActionForwarder@1fb1488d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016ccee0@2723f32d] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@29408bce] took [0ms], [org.elasticsearch.tasks.TaskManager@74291215] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@61c9d93d] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@37d9ef9e] took [1ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e80fb67] took [0ms], [org.elasticsearch.indices.SystemIndexManager@5ec23e41] took [89ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2b4694ff] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@6604a4fa] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@58a75f22] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@72256b7f] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013be000@5182f1d5] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2a9354b8] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3bdb555] took [208ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3ea44717] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5320530c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4fa02a80] took [4ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3d663aa1] took [23ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4a3f35d4] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1b56a7ac] took [38ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [27ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@556c7a6c] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@5953e152] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@723df2fc] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@421444e2] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3642b950] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@639076c4] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@432100f1] took [0ms], [org.elasticsearch.node.ResponseCollectorService@608aa9fc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@38824d6b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@58c42ae9] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@13299f88] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@106ba9b2] took [0ms], [org.elasticsearch.indices.store.IndicesStore@6c86e02f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@392ba327] took [0ms], [org.elasticsearch.license.LicenseService@49003cc2] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5f373553] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4fd9d010] took [20347ms], [org.elasticsearch.gateway.GatewayService@507c41a9] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7396af42] took [0ms]
[2022-04-23T08:19:40,417][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
	at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1715) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
	Suppressed: java.net.SocketException: Broken pipe
		at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
		at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
		at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
		at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
		at sun.security.ssl.SSLSocketOutputRecord.encodeAlert(SSLSocketOutputRecord.java:82) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:400) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:312) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:463) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
		at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
		at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
		at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
		at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
		at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: java.io.EOFException: SSL peer shut down incorrectly
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:483) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:472) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	... 25 more
[2022-04-23T08:20:24,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1faac1e4, interval=5s}] took [6287ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:20:24,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:20:27,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5087110228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:20:29,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5006ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:20:29,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [5005ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:20:34,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5005813078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:20:36,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:20:36,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6860976967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:20:38,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@6471db7] took [8940ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:20:37,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33446}] took [6861ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:20:45,622][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=298, version=13579}] took [45.3s] which is above the warn threshold of [30s]: [running task [Publication{term=298, version=13579}]] took [40ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@19587bee] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@65c51b7a] took [38818ms], [org.elasticsearch.script.ScriptService@160c7fc7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [0ms], [org.elasticsearch.snapshots.RestoreService@27e53106] took [0ms], [org.elasticsearch.ingest.IngestService@6a36e04b] took [47ms], [org.elasticsearch.action.ingest.IngestActionForwarder@1fb1488d] took [43ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016ccee0@2723f32d] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@29408bce] took [40ms], [org.elasticsearch.tasks.TaskManager@74291215] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@61c9d93d] took [82ms], [org.elasticsearch.cluster.InternalClusterInfoService@37d9ef9e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e80fb67] took [0ms], [org.elasticsearch.indices.SystemIndexManager@5ec23e41] took [66ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2b4694ff] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@6604a4fa] took [91ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@58a75f22] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@72256b7f] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013be000@5182f1d5] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2a9354b8] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3bdb555] took [467ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3ea44717] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5320530c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4fa02a80] took [845ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3d663aa1] took [44ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4a3f35d4] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1b56a7ac] took [1925ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [83ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@556c7a6c] took [2339ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@5953e152] took [26ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@723df2fc] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@421444e2] took [337ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3642b950] took [51ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@639076c4] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@432100f1] took [0ms], [org.elasticsearch.node.ResponseCollectorService@608aa9fc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@38824d6b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@58c42ae9] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@13299f88] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@106ba9b2] took [0ms], [org.elasticsearch.indices.store.IndicesStore@6c86e02f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@392ba327] took [0ms], [org.elasticsearch.license.LicenseService@49003cc2] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5f373553] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4fd9d010] took [0ms], [org.elasticsearch.gateway.GatewayService@507c41a9] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7396af42] took [0ms]
[2022-04-23T08:20:46,941][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:49682}] took [32333ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:20:47,988][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-23T08:20:50,777][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][108] overhead, spent [527ms] collecting in the last [1.6s]
[2022-04-23T08:20:51,057][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-23T08:20:51,079][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-23T08:20:57,179][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][11] duration [989ms], collections [1]/[1s], total [989ms]/[2.4s], memory [174.7mb]->[97.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [86.7mb]->[86.7mb]/[2gb]}{[survivor] [7.9mb]->[10.9mb]/[0b]}
[2022-04-23T08:20:57,471][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [989ms] collecting in the last [1s]
[2022-04-23T08:21:02,260][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][114][12] duration [1.6s], collections [1]/[3.2s], total [1.6s]/[4s], memory [137.7mb]->[101.1mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [86.7mb]->[90.5mb]/[2gb]}{[survivor] [10.9mb]->[10.5mb]/[0b]}
[2022-04-23T08:21:03,098][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][114] overhead, spent [1.6s] collecting in the last [3.2s]
[2022-04-23T08:21:13,278][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][119] overhead, spent [586ms] collecting in the last [1.5s]
[2022-04-23T08:21:13,608][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T08:21:13,946][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T08:21:17,333][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][121][14] duration [1.1s], collections [1]/[2.3s], total [1.1s]/[5.7s], memory [157.2mb]->[106mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [94.9mb]->[99.9mb]/[2gb]}{[survivor] [10.2mb]->[6mb]/[0b]}
[2022-04-23T08:21:17,553][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][121] overhead, spent [1.1s] collecting in the last [2.3s]
[2022-04-23T08:21:30,011][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][128][16] duration [2.7s], collections [1]/[4s], total [2.7s]/[8.6s], memory [187.9mb]->[108.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [99.9mb]->[99.9mb]/[2gb]}{[survivor] [8mb]->[8.5mb]/[0b]}
[2022-04-23T08:21:31,142][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][128] overhead, spent [2.7s] collecting in the last [4s]
[2022-04-23T08:21:38,102][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][132][17] duration [921ms], collections [1]/[2.5s], total [921ms]/[9.6s], memory [168.5mb]->[113.1mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [99.9mb]->[102.8mb]/[2gb]}{[survivor] [8.5mb]->[10.3mb]/[0b]}
[2022-04-23T08:21:38,502][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][132] overhead, spent [921ms] collecting in the last [2.5s]
[2022-04-23T08:21:43,804][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][135][18] duration [868ms], collections [1]/[2.3s], total [868ms]/[10.4s], memory [185.1mb]->[115.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [102.8mb]->[104.9mb]/[2gb]}{[survivor] [10.3mb]->[10.9mb]/[0b]}
[2022-04-23T08:21:44,018][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][135] overhead, spent [868ms] collecting in the last [2.3s]
[2022-04-23T08:21:45,319][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T08:21:59,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:22:00,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5403497766ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:22:03,403][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][142][19] duration [3.5s], collections [1]/[6.4s], total [3.5s]/[14s], memory [191.9mb]->[117.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [104.9mb]->[107.1mb]/[2gb]}{[survivor] [10.9mb]->[10.3mb]/[0b]}
[2022-04-23T08:22:04,535][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][142] overhead, spent [3.5s] collecting in the last [6.4s]
[2022-04-23T08:22:05,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [6704ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:22:18,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [7657ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:22:33,972][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [42214ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [60] unchanged indices
[2022-04-23T08:22:34,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@21e0baaa] took [11408ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:22:35,408][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [44.9s] publication of cluster state version [13587] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T08:23:06,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [19136ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:23:31,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f74b184, interval=5s}] took [11589ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:23:40,830][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:49718}] took [56379ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:24:07,066][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10746ms] which is above the warn threshold of [5s]
[2022-04-23T08:24:54,203][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=298, version=13587}] took [2.2m] which is above the warn threshold of [30s]: [running task [Publication{term=298, version=13587}]] took [0ms], [connecting to new nodes] took [65ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@19587bee] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@65c51b7a] took [712ms], [org.elasticsearch.script.ScriptService@160c7fc7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [0ms], [org.elasticsearch.snapshots.RestoreService@27e53106] took [0ms], [org.elasticsearch.ingest.IngestService@6a36e04b] took [294ms], [org.elasticsearch.action.ingest.IngestActionForwarder@1fb1488d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016ccee0@2723f32d] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@29408bce] took [0ms], [org.elasticsearch.tasks.TaskManager@74291215] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@61c9d93d] took [80ms], [org.elasticsearch.cluster.InternalClusterInfoService@37d9ef9e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e80fb67] took [0ms], [org.elasticsearch.indices.SystemIndexManager@5ec23e41] took [346ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2b4694ff] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@6604a4fa] took [34ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@58a75f22] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@72256b7f] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013be000@5182f1d5] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2a9354b8] took [40ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3bdb555] took [79808ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3ea44717] took [167ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5320530c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4fa02a80] took [12993ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3d663aa1] took [591ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4a3f35d4] took [371ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1b56a7ac] took [7803ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7db8a254] took [1733ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@556c7a6c] took [12329ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@5953e152] took [50ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@723df2fc] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@421444e2] took [7395ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3642b950] took [7266ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@639076c4] took [493ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@432100f1] took [2238ms], [org.elasticsearch.node.ResponseCollectorService@608aa9fc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@38824d6b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@58c42ae9] took [250ms], [org.elasticsearch.shutdown.PluginShutdownService@13299f88] took [117ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@106ba9b2] took [60ms], [org.elasticsearch.indices.store.IndicesStore@6c86e02f] took [435ms], [org.elasticsearch.persistent.PersistentTasksNodeService@392ba327] took [63ms], [org.elasticsearch.license.LicenseService@49003cc2] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5f373553] took [193ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4fd9d010] took [0ms], [org.elasticsearch.gateway.GatewayService@507c41a9] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7396af42] took [0ms]
[2022-04-23T08:26:06,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:26:09,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29271685844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:26:10,338][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [53.4s/53454ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [hyU9a5upSKy1sLhrI58WrQ], primary term [13], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [hyU9a5upSKy1sLhrI58WrQ], primary term [13], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [hyU9a5upSKy1sLhrI58WrQ], primary term [13], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [hyU9a5upSKy1sLhrI58WrQ], primary term [13], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-23T08:26:12,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6287ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:26:13,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6286726410ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:26:26,645][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33510}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:26:32,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@1b40aa61] took [165023ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:26:50,134][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33510}] took [13608ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:26:52,987][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][149][20] duration [19.6s], collections [1]/[3.8m], total [19.6s]/[33.6s], memory [173.4mb]->[124mb]/[2gb], all_pools {[young] [56mb]->[8mb]/[0b]}{[old] [107.1mb]->[111.5mb]/[2gb]}{[survivor] [10.3mb]->[8.4mb]/[0b]}
[2022-04-23T08:26:57,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [16610ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:27:25,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:27:40,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:27:49,140][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33510}] took [7005ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:28:44,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@79cbd0ac] took [50596ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:28:42,935][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [119641ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [60] unchanged indices
[2022-04-23T08:28:45,445][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.2m] publication of cluster state version [13588] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ppTPVqZLSemDG3RGmHAmaA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-23T08:28:47,981][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.9m/419735ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [6.9m/419793ms]
[2022-04-23T08:29:00,693][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][21] duration [1.1s], collections [1]/[2.3s], total [1.1s]/[34.8s], memory [188mb]->[123mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [111.5mb]->[111.5mb]/[2gb]}{[survivor] [8.4mb]->[11.4mb]/[0b]}
[2022-04-23T08:29:01,025][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [1.1s] collecting in the last [2.3s]
[2022-04-23T08:29:05,882][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][22] duration [1.3s], collections [1]/[5.1s], total [1.3s]/[36.1s], memory [123mb]->[123.3mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [111.5mb]->[115.3mb]/[2gb]}{[survivor] [11.4mb]->[7.9mb]/[0b]}
[2022-04-23T08:29:06,120][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [1.3s] collecting in the last [5.1s]
[2022-04-23T08:29:08,701][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60600}] took [11032ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:29:08,701][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60598}] took [11032ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:29:09,026][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60602}] took [11233ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:29:19,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [7936ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:29:24,592][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][173] overhead, spent [269ms] collecting in the last [1s]
[2022-04-23T08:29:33,384][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][180] overhead, spent [394ms] collecting in the last [1s]
[2022-04-23T08:29:37,343][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][183] overhead, spent [316ms] collecting in the last [1s]
[2022-04-23T08:29:39,564][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][184][32] duration [704ms], collections [1]/[2.4s], total [704ms]/[38.8s], memory [248.2mb]->[182.8mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [161.2mb]->[167.7mb]/[2gb]}{[survivor] [13.3mb]->[15mb]/[0b]}
[2022-04-23T08:29:39,769][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][184] overhead, spent [704ms] collecting in the last [2.4s]
[2022-04-23T08:29:42,203][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][186] overhead, spent [351ms] collecting in the last [1.3s]
[2022-04-23T08:29:54,591][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][190][34] duration [1.8s], collections [1]/[2.6s], total [1.8s]/[40.9s], memory [227.1mb]->[195.7mb]/[2gb], all_pools {[young] [36mb]->[16mb]/[0b]}{[old] [175.1mb]->[189.9mb]/[2gb]}{[survivor] [16mb]->[5.7mb]/[0b]}
[2022-04-23T08:29:54,830][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][190] overhead, spent [1.8s] collecting in the last [2.6s]
[2022-04-23T08:29:58,041][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][193] overhead, spent [283ms] collecting in the last [1s]
[2022-04-23T08:29:58,470][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T08:29:58,516][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T08:29:58,539][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.23/3_9gRcMrTjGfinZT_Xu0Fw] update_mapping [_doc]
[2022-04-23T08:29:59,709][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][194] overhead, spent [546ms] collecting in the last [1.4s]
[2022-04-23T08:30:03,207][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][38] duration [946ms], collections [1]/[3.4s], total [946ms]/[42.9s], memory [212.2mb]->[216.5mb]/[2gb], all_pools {[young] [4mb]->[4mb]/[0b]}{[old] [193.7mb]->[201.9mb]/[2gb]}{[survivor] [14.4mb]->[14.5mb]/[0b]}
[2022-04-23T08:30:03,421][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [946ms] collecting in the last [3.4s]
[2022-04-23T08:30:10,296][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][201] overhead, spent [301ms] collecting in the last [1.1s]
[2022-04-23T08:30:26,897][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][214] overhead, spent [643ms] collecting in the last [2s]
[2022-04-23T08:30:38,453][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-23T08:35:28,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [8105ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:35:53,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d2498@2e27558d] took [9403ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:36:10,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [5947ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:37:45,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51c08c67, interval=1s}] took [40827ms] which is above the warn threshold of [5000ms]
[2022-04-23T08:42:34,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:43:43,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6956556358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:45:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/480302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:47:39,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/480336178342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T08:48:48,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/227526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T08:50:43,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/227645024086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:02:18,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1faac1e4, interval=5s}] took [227645ms] which is above the warn threshold of [5000ms]
[2022-04-23T09:03:08,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3m/858742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:06:18,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3m/858249975746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:08:58,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:11:36,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351296023211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:14:22,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:16:07,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324260473674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:19:16,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/277308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:31:32,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/277476405359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:34:12,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2m/912805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:37:05,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2m/913126673765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:40:37,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:46:41,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384062641573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:49:34,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:52:22,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/538018534996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T09:56:04,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T09:59:52,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389252289925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T10:09:48,543][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-23T10:09:48,559][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-23T10:09:48,561][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-23T10:09:53,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-23T10:09:53,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-23T10:09:53,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-23T10:09:53,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-23T10:09:53,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-23T10:09:53,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-23T10:09:53,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-23T10:09:53,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-23T10:09:53,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-23T10:09:53,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-23T10:09:53,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-23T10:09:53,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-23T10:09:53,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-23T10:09:53,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-23T10:09:53,892][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-23T10:09:53,892][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-23T10:09:53,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-23T10:09:53,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-23T10:09:53,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-23T10:09:53,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-23T10:09:53,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-23T10:09:53,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-23T10:09:53,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-23T10:09:53,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-23T10:09:53,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-23T10:09:53,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-23T10:09:53,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-23T10:09:53,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-23T10:09:53,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-23T10:09:53,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-23T10:09:53,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-23T10:09:53,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-23T10:09:53,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-23T10:09:53,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-23T10:09:53,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-23T10:09:53,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-23T10:09:53,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-23T10:09:53,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-23T10:09:53,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-23T10:09:53,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-23T10:09:53,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-23T10:09:53,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-23T10:09:53,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-23T10:09:53,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-23T10:09:53,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-23T10:09:53,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-23T10:09:53,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-23T10:09:53,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-23T10:09:53,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-23T10:09:53,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-23T10:09:53,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-23T10:09:53,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-23T10:09:53,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-23T10:09:53,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-23T10:09:53,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-23T10:09:53,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-23T10:09:53,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-23T10:09:53,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-23T10:09:53,908][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-23T10:09:54,004][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.8gb], net total_space [125.8gb], types [ext4]
[2022-04-23T10:09:54,005][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-23T10:09:54,711][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-23T10:10:42,958][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-23T10:10:48,487][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-23T10:10:50,556][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-23T10:10:50,714][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-23T10:10:50,715][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T10:10:50,716][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-23T10:10:50,716][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-23T10:10:50,740][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T10:10:50,741][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-23T10:10:50,741][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-23T10:10:50,742][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-23T10:10:50,742][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-23T10:10:50,743][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-23T10:10:50,743][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-23T10:10:50,788][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-23T10:11:49,341][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-23T10:11:49,543][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-23T10:11:50,622][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-23T10:11:51,766][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-23T10:11:51,777][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-23T10:11:51,985][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-23T10:11:51,994][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-23T10:11:56,821][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-23T11:16:06,943][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-23T11:16:07,002][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-23T11:16:07,003][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-23T11:16:12,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-23T11:16:12,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-23T11:16:12,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-23T11:16:12,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-23T11:16:12,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-23T11:16:12,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-23T11:16:12,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-23T11:16:12,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-23T11:16:12,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-23T11:16:12,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-23T11:16:12,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-23T11:16:12,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-23T11:16:12,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-23T11:16:12,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-23T11:16:12,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-23T11:16:12,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-23T11:16:12,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-23T11:16:12,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-23T11:16:12,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-23T11:16:12,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-23T11:16:12,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-23T11:16:12,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-23T11:16:12,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-23T11:16:12,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-23T11:16:12,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-23T11:16:12,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-23T11:16:12,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-23T11:16:12,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-23T11:16:12,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-23T11:16:12,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-23T11:16:12,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-23T11:16:12,479][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-23T11:16:12,479][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-23T11:16:12,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-23T11:16:12,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-23T11:16:12,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-23T11:16:12,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-23T11:16:12,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-23T11:16:12,482][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-23T11:16:12,482][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-23T11:16:12,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-23T11:16:12,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-23T11:16:12,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-23T11:16:12,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-23T11:16:12,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-23T11:16:12,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-23T11:16:12,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-23T11:16:12,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-23T11:16:12,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-23T11:16:12,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-23T11:16:12,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-23T11:16:12,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-23T11:16:12,488][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-23T11:16:12,488][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-23T11:16:12,488][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-23T11:16:12,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-23T11:16:12,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-23T11:16:12,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-23T11:16:12,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-23T11:16:12,608][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.8gb], net total_space [125.8gb], types [ext4]
[2022-04-23T11:16:12,609][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-23T11:16:13,373][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-23T11:16:28,836][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-23T11:16:28,850][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-23T11:16:47,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4c1efa83, interval=5s}] took [16970ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:16:50,440][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-23T11:16:50,651][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-23T11:16:51,721][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-23T11:16:52,967][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-23T11:16:52,979][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-23T11:16:53,154][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-23T11:16:53,170][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-23T11:16:53,722][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-23T11:17:13,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [5298ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:17:29,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [8886ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:17:57,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [8900ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:19:20,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@59c0c1f2] took [52256ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:19:30,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [6048ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:19:40,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [5309ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:19:42,261][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5766ms] which is above the warn threshold of [5s]
[2022-04-23T11:20:02,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [5447ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:20:15,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [7296ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:20:36,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [6305ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:20:51,067][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [215240ms] which is above the warn threshold of [10s]; wrote full state with [61] indices
[2022-04-23T11:20:51,971][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-23T11:20:52,780][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Mp4TeqajQgW8zeYn2J7Tfw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 299, version: 13633, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Mp4TeqajQgW8zeYn2J7Tfw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-23T11:20:53,023][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Mp4TeqajQgW8zeYn2J7Tfw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 299, version: 13633, reason: Publication{term=299, version=13633}
[2022-04-23T11:20:53,197][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-23T11:20:53,198][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-23T11:21:10,721][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-23T11:21:10,036][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-23T11:21:11,106][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [61] indices into cluster_state
[2022-04-23T11:24:30,066][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33360}] took [115217ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:31:56,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5101/0x00000008017e8fb8@76b34e19] took [631892ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:32:57,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [53246ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:33:20,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:33:44,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14908521814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:33:49,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35824ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:02,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35824533772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:04,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:11,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16206247361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:06,258][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34476}] took [52031ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:34:16,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:20,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11602137504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:25,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:29,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8249026589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:34,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:38,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9340114658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:44,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:48,391][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34476}] took [19170ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:34:50,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9829598344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:34:56,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:35:06,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12557300709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:35:16,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19748ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:35:22,654][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34476}] took [19748ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:34:15,946][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A23042022%3Acount%3Adelay_threshold_exceeded, params: {require_alias=true, refresh=wait_for, index=.kibana_7.17.0, _source=true, id=usage-counters:eventLoop:23042022:count:delay_threshold_exceeded}
org.elasticsearch.action.UnavailableShardsException: [.kibana_7.17.0_001][0] [1] shardIt, [0] active : Timeout waiting for [1m], request: indices:data/write/update
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.retry(TransportInstanceSingleOperationAction.java:231) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.doStart(TransportInstanceSingleOperationAction.java:181) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$2.onTimeout(TransportInstanceSingleOperationAction.java:254) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:345) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:263) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:660) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-23T11:35:27,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19748006248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:35:39,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:36:38,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22913884494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:36:55,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:37:04,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76131113343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:37:16,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:37:33,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20614429020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:37:42,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:37:53,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26072792461ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:38:09,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:38:19,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27026833837ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:38:32,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:38:37,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22614645728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:38:45,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:38:54,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13924645191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:02,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:12,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16738579695ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:20,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:27,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16961204437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:34,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:42,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14578381587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:39:53,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:40:00,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19047059111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:40:08,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:40:44,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14959351611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:40:53,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:04,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45205267322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:13,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:21,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19686338338ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:24,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:29,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11593441185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:32,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7836ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:35,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7836141271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:39,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:43,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7297179941ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:44,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5101/0x00000008017e8fb8@6efd90e7] took [506558ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:41:48,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:49,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8526978702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:41:49,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [8526ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:42:09,978][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34466}] took [1241246ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:43:06,213][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34478}] took [6542ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:43:08,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5101/0x00000008017e8fb8@a1341dd] took [16763ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:43:16,866][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34480}] took [7405ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:43:36,549][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34484}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:44:39,241][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34492}] took [5503ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:45:19,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:45:28,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9844ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:45:33,196][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34492}] took [33594ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:45:34,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18143209835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:45:41,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:45:50,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12805504683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:45:58,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:02,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34492}] took [17294ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:46:03,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17294318100ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:10,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11303ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:18,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11303378290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:27,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:36,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17091475724ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:42,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:49,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15235951339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:46:57,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:07,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14750960316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:14,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17855ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:23,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17854402218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:31,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:38,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14763489907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:46,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:52,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15084476571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:47:52,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5101/0x00000008017e8fb8@7aca7574] took [243036ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:47:57,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:04,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13064007962ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:11,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13394ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:18,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13394667874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:19,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [13394ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:48:25,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:25,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16e53fb2, interval=30s}] took [14335ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:48:33,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14335581203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:39,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:44,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14257896919ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:43,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [14257ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:48:48,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:48:42,164][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14258ms] which is above the warn threshold of [5s]
[2022-04-23T11:48:56,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8563062610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:04,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:10,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15886610083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:18,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:23,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13726148354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:32,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:38,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14714833827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:43,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:49:53,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11111104073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:50:00,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:50:09,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16375951938ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:50:19,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:50:31,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18523713452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:50:47,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28354ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:50:59,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28353940399ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:06,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:13,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18605039789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:22,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:29,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15856440374ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:36,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14842ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:42,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14841956540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:49,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12701ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:51:57,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12701535553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:05,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:12,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16716683395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:22,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:30,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15931454096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:39,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:47,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17620043596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:52:54,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:02,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15448281915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:09,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:19,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891433489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:28,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:30,568][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34494}] took [33378ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:53:35,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19486905155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:40,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5101/0x00000008017e8fb8@141dff21] took [288355ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:53:41,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13318ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:45,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13318110334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:49,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:51,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6716317462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:51,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [6716ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:53:53,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:53,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16e53fb2, interval=30s}] took [6076ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:53:54,239][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34494}] took [12792ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:53:56,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6076058610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:59,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T11:53:59,794][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5289ms] which is above the warn threshold of [5s]
[2022-04-23T11:53:59,661][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34494}] took [5289ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:54:01,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5288957642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T11:54:02,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [5288ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:54:22,416][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:34502}] took [10221ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:54:28,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [10055ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:55:13,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [28198ms] which is above the warn threshold of [5000ms]
[2022-04-23T11:59:48,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:00:08,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6375714795ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:00:33,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44556ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:01:07,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44556643518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:01:28,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.7s/59768ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:01:48,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.7s/59767366617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:02:01,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32497ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:02:12,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32497825459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:02:28,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:02:46,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28269157628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:02:44,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5101/0x00000008017e8fb8@41049d6] took [426055ms] which is above the warn threshold of [5000ms]
[2022-04-23T12:03:03,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:11,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32399674864ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:19,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:26,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18181223334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:35,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@288df430, interval=1s}] took [34698ms] which is above the warn threshold of [5000ms]
[2022-04-23T12:03:35,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:41,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16516894466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:47,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11950ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:57,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11949735400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:03:59,716][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11950ms] which is above the warn threshold of [5s]
[2022-04-23T12:04:03,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16408ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:04:14,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16407916482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:04:34,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28338ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:04:47,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28337682482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:05:08,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:05:28,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35213523677ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:05:47,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:06:08,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39403125809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:06:28,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41038ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:06:56,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41037563650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:07:34,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:08:24,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65413280179ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:08:51,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76168ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:09:41,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76167615530ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:10:51,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120154ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:12:04,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120072572581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:13:37,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:15:05,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165352535207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:20:31,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/412130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:22:57,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/411755091660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:25:32,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/302371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:30:06,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/302616730937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:33:25,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/444880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:37:22,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445009636036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:42:49,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:46:44,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571060519761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:50:28,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/480624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:53:27,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/480770179413ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T12:56:11,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T12:59:03,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340664174238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T13:01:44,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T13:04:09,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322087025158ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T13:12:02,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/601232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-23T13:14:32,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/601724725733ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-23T13:17:49,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374976ms] on absolute clock which is above the warn threshold of [5000ms]
