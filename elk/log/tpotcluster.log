[2022-04-11T15:52:44,135][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T15:52:44,172][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T15:52:44,173][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T15:52:50,304][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T15:52:50,305][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T15:52:50,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T15:52:50,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T15:52:50,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T15:52:50,307][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T15:52:50,307][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T15:52:50,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T15:52:50,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T15:52:50,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T15:52:50,309][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T15:52:50,309][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T15:52:50,310][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T15:52:50,310][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T15:52:50,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T15:52:50,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T15:52:50,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T15:52:50,312][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T15:52:50,312][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T15:52:50,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T15:52:50,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T15:52:50,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T15:52:50,314][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T15:52:50,314][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T15:52:50,315][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T15:52:50,315][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T15:52:50,316][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T15:52:50,316][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T15:52:50,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T15:52:50,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T15:52:50,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T15:52:50,318][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T15:52:50,318][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T15:52:50,319][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T15:52:50,319][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T15:52:50,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T15:52:50,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T15:52:50,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T15:52:50,321][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T15:52:50,321][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T15:52:50,322][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T15:52:50,322][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T15:52:50,323][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T15:52:50,323][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T15:52:50,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T15:52:50,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T15:52:50,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T15:52:50,325][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T15:52:50,325][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T15:52:50,326][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T15:52:50,326][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T15:52:50,327][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T15:52:50,327][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T15:52:50,327][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T15:52:50,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T15:52:50,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T15:52:50,329][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T15:52:50,329][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T15:52:50,330][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T15:52:50,397][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.7gb], net total_space [125.8gb], types [ext4]
[2022-04-11T15:52:50,398][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T15:52:50,648][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T15:53:02,147][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-11T15:53:02,151][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-11T15:53:03,318][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-11T15:53:03,448][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-11T15:53:04,204][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-11T15:53:04,920][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-11T15:53:04,921][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-11T15:53:04,954][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-11T15:53:04,956][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-11T15:53:05,181][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-11T15:53:07,668][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-11T15:53:07,776][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{kZcfg0TbRnKG2C5trZnzkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 229, version: 8817, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{kZcfg0TbRnKG2C5trZnzkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-11T15:53:07,962][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{kZcfg0TbRnKG2C5trZnzkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 229, version: 8817, reason: Publication{term=229, version=8817}
[2022-04-11T15:53:08,091][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-11T15:53:08,092][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-11T15:53:08,886][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-11T15:53:08,892][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [45] indices into cluster_state
[2022-04-11T15:53:09,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-11T15:53:09,594][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-11T15:53:10,238][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-11T15:53:10,389][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-11T15:53:10,762][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-11T15:53:10,829][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-11T15:53:10,835][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-11T15:53:10,839][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T15:53:11,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T15:53:11,427][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T15:53:14,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T15:53:23,440][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.10][0]]]).
[2022-04-11T15:53:29,522][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-11T15:53:29,738][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-11T15:54:13,342][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 640 finished with response BulkByScrollResponse[took=357.7ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-11T15:54:15,013][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 666 finished with response BulkByScrollResponse[took=1.6s,timed_out=false,sliceId=null,updated=1016,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-11T15:54:24,222][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-11T15:55:05,723][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.11] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-11T15:55:05,863][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.11][0]]]).
[2022-04-11T15:55:06,032][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,148][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,180][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,199][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,346][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,353][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,442][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,614][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,633][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,793][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,890][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:07,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,355][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,423][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,432][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,440][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,567][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:10,361][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:14,384][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:14,474][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:50,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:07,250][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:09,477][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:09,611][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:11,483][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:14,464][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:20,507][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:20,579][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:34,486][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:22,573][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:31,658][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:32,371][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:46,685][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:59:08,694][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:01:54,862][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:03:08,750][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] to [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:08,805][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] to [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:08,863][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [.kibana-event-log-7.16.2-000002] creating index, cause [rollover_index], templates [.kibana-event-log-7.16.2-template], shards [1]/[1]
[2022-04-11T16:03:08,867][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] updating number_of_replicas to [0] for indices [.kibana-event-log-7.16.2-000002]
[2022-04-11T16:03:08,974][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [.ds-ilm-history-5-2022.04.11-000002] creating index, cause [rollover_data_stream], templates [ilm-history], shards [1]/[0]
[2022-04-11T16:03:09,099][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.04.11-000002][0]]]).
[2022-04-11T16:03:09,134][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000002] from [null] to [{"phase":"new","action":"complete","name":"complete"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,136][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] to [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,137][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.04.11-000002] from [null] to [{"phase":"new","action":"complete","name":"complete"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,138][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] to [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,189][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.04.11-000002] from [{"phase":"new","action":"complete","name":"complete"}] to [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,191][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000002] from [{"phase":"new","action":"complete","name":"complete"}] to [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,195][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] to [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,197][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] to [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,198][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] to [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,200][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] to [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,386][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.04.11-000002] from [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] to [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,387][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000002] from [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] to [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,388][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] to [{"phase":"hot","action":"complete","name":"complete"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,389][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] to [{"phase":"hot","action":"complete","name":"complete"}] in policy [kibana-event-log-policy]
[2022-04-11T16:04:29,673][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:04:29,999][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:04:30,085][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:08:34,780][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:08:35,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:10:07,043][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:10:07,115][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:10:08,045][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:12:23,402][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:18:35,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:18:35,815][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:21:19,330][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:31:50,554][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:36:57,756][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:36:58,560][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:36:58,626][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:37:02,405][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/0jugN3uXT1aOat1QmEPt3w] deleting index
[2022-04-11T16:39:49,877][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:12,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:12,846][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:14,891][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:37,755][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:52:37,693][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:53:09,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [25202ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:53:50,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [13068ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:54:23,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50261f32, interval=5s}] took [8366ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:55:08,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [6214ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:56:08,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [27524ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:58:51,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:02:47,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:04:07,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295520430430ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:04:17,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:04:50,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215722607389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:06:13,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:08:04,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90505192583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:08:20,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/151780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:08:43,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/152353407516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:10:06,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:11:08,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103468523952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:13:01,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:15:10,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173572821534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:16:43,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [173572ms] which is above the warn threshold of [5000ms]
[2022-04-11T17:16:44,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:14:43,696][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:57516}] took [152354ms] which is above the warn threshold of [5000ms]
[2022-04-11T17:18:55,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225665885443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:21:08,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/261349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:21:56,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260978937718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:23:43,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/155818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:25:45,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/156188086658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:26:16,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d4f44c5, interval=5s}] took [417167ms] which is above the warn threshold of [5000ms]
[2022-04-11T17:27:16,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:29:18,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212151066267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:31:20,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/242693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:33:16,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/242857030006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:36:08,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287585ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:38:22,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287728436838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:36:16,844][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [242857ms] which is above the warn threshold of [5s]
[2022-04-11T17:40:35,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:39:46,247][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14s/14058ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@4f5159d4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e3ea0679]], which exceeds the warn threshold of [10s]
[2022-04-11T17:42:54,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268587441654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:45:48,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:45:19,897][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.4s/13423ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@feb6b09c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9d860bdd]], which exceeds the warn threshold of [10s]
[2022-04-11T17:47:56,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313137089808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:49:35,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/226870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:52:22,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/226909676362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:55:07,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:57:29,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314399869232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:59:53,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:02:25,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303137068077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:04:44,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:05:20,007][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [9.6m/581724ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [12.1m/728430ms]
[2022-04-11T18:06:36,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290760863056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:08:01,517][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [28.6m/1716932ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [13.9m/834543ms]
[2022-04-11T18:09:17,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:11:27,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273197806781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:11:20,264][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15s/15085ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@4f5159d4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e3ea0679]], which exceeds the warn threshold of [10s]
[2022-04-11T18:13:14,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:13:34,484][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [37.1m/2226907ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [10.7m/643608ms]
[2022-04-11T18:15:51,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236777489874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:18:23,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:21:08,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308549713001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:23:47,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:26:23,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325095965728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:28:11,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/264073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:30:31,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263752169920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:33:16,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:35:49,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304232246380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:09:14,679][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T19:09:14,699][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T19:09:14,702][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T19:09:21,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T19:09:21,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T19:09:21,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T19:09:21,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T19:09:21,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T19:09:21,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T19:09:21,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T19:09:21,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T19:09:21,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T19:09:21,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T19:09:21,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T19:09:21,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T19:09:21,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T19:09:21,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T19:09:21,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T19:09:21,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T19:09:21,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T19:09:21,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T19:09:21,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T19:09:21,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T19:09:21,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T19:09:21,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T19:09:21,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T19:09:21,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T19:09:21,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T19:09:21,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T19:09:21,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T19:09:21,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T19:09:21,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T19:09:21,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T19:09:21,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T19:09:21,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T19:09:21,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T19:09:21,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T19:09:21,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T19:09:21,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T19:09:21,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T19:09:21,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T19:09:21,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T19:09:21,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T19:09:21,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T19:09:21,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T19:09:21,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T19:09:21,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T19:09:21,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T19:09:21,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T19:09:21,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T19:09:21,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T19:09:21,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T19:09:21,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T19:09:21,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T19:09:21,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T19:09:21,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T19:09:21,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T19:09:21,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T19:09:21,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T19:09:21,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T19:09:21,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T19:09:21,093][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T19:09:21,194][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.5gb], net total_space [125.8gb], types [ext4]
[2022-04-11T19:09:21,196][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T19:09:21,739][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T19:09:31,999][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-11T19:09:32,010][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-11T19:09:32,012][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T19:09:32,015][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-11T19:09:32,016][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T19:09:32,017][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-11T19:09:32,020][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-11T19:09:32,021][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T19:09:32,023][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-11T19:09:32,024][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-11T19:09:32,025][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-11T19:09:32,026][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T19:09:32,027][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T19:09:32,028][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T19:09:32,029][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-11T19:09:33,111][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-11T19:09:33,285][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-11T19:09:34,056][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-11T19:09:34,858][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-11T19:09:34,859][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-11T19:09:34,958][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-11T19:09:34,963][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-11T19:09:35,232][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-11T19:09:37,673][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-11T19:09:37,846][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 230, version: 8948, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-11T19:09:38,028][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 230, version: 8948, reason: Publication{term=230, version=8948}
[2022-04-11T19:09:38,143][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-11T19:09:38,146][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-11T19:09:38,895][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-11T19:09:38,907][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [47] indices into cluster_state
[2022-04-11T19:09:41,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-11T19:09:41,622][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-11T19:13:39,019][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.9m/179178ms] to notify listeners on successful publication of cluster state (version: 8952, uuid: OgWZStazRUq684aukeT3lQ) for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-11T19:12:03,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:15:24,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9273075080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:15:46,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:14:52,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@466b153f, interval=5s}] took [13961ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:16:42,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324591934119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:16:51,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:17:01,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66120545706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:17:53,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:02,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63157895588ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:20,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18697633436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:24,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [81855ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:18:34,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:45,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20705724732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:54,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:05,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21594229627ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:10,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:07,819][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [52.3s/52300ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-11T19:19:58,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14754602111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:58,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:59,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49982002165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:59,710][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59676}] took [65646ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:20:00,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@acb6090] took [88040ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:20:01,131][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [9.6m/581760ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [10.3m/619186ms]
[2022-04-11T19:20:07,996][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
	at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1715) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
	Suppressed: java.net.SocketException: Broken pipe
		at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
		at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
		at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
		at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
		at sun.security.ssl.SSLSocketOutputRecord.encodeAlert(SSLSocketOutputRecord.java:82) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:400) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:312) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:463) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
		at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
		at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
		at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
		at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
		at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: java.io.EOFException: SSL peer shut down incorrectly
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:483) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:472) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	... 25 more
[2022-04-11T19:20:14,704][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-11T19:20:24,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:20:25,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7347603309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:20:26,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-11T19:20:28,390][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T19:21:06,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [5655ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:21:25,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:29,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8409194725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:33,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:44,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7810226333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:51,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:52,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13517025970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:52,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@2221fab3] took [30336ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:21:55,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:05,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101138162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:06,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:07,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10459700521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:06,839][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59708}] took [10460ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:22:11,328][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [77162ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [44] unchanged indices
[2022-04-11T19:22:11,447][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.3m] publication of cluster state version [8961] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-11T19:22:35,030][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][46] overhead, spent [485ms] collecting in the last [1.4s]
[2022-04-11T19:22:36,795][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59708}] took [5248ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:22:36,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [21302ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:06,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [11813ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:22,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [5209ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:52,210][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T19:23:52,210][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T19:23:51,313][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [66584ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [46] unchanged indices
[2022-04-11T19:23:53,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@34c70944] took [14784ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:55,512][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.2m] publication of cluster state version [8962] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-11T19:24:17,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15701ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:22,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15701009904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:25,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:28,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7641428985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:29,675][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59726}] took [7641ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:24:32,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:36,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7420418102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:42,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:44,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9504380025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:43,118][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59726}] took [9504ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:24:45,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][56][12] duration [11.6s], collections [1]/[18.5s], total [11.6s]/[13.5s], memory [187.9mb]->[105.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [95.9mb]->[95.9mb]/[2gb]}{[survivor] [8mb]->[9.9mb]/[0b]}
[2022-04-11T19:24:48,841][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][56] overhead, spent [11.6s] collecting in the last [18.5s]
[2022-04-11T19:24:53,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [35871ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:25:01,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@490ed723] took [5277ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:25:29,897][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=230, version=8962}] took [1.3m] which is above the warn threshold of [30s]: [running task [Publication{term=230, version=8962}]] took [76ms], [connecting to new nodes] took [46ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@da71c6e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1e79b753] took [29934ms], [org.elasticsearch.script.ScriptService@1b3834c1] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [1ms], [org.elasticsearch.snapshots.RestoreService@61e8eb39] took [0ms], [org.elasticsearch.ingest.IngestService@32780b78] took [10457ms], [org.elasticsearch.action.ingest.IngestActionForwarder@683cafca] took [176ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d2640@7272efec] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@54e573b8] took [6ms], [org.elasticsearch.tasks.TaskManager@26349197] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@2b7b3864] took [117ms], [org.elasticsearch.cluster.InternalClusterInfoService@4f422da9] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@11f2c0c1] took [0ms], [org.elasticsearch.indices.SystemIndexManager@3fe52349] took [959ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@3bda7ebe] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@74c62010] took [405ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7731947] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@448dbc41] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013ba000@5189d28] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@4a4e7366] took [60ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1a79674e] took [30846ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@a100ad6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@36a69f18] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3c6a65a0] took [6673ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@207c6ae6] took [99ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@61349821] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@63688540] took [1741ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [36ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@cc0bbe8] took [370ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f28cbcc] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@246b80b2] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@21a0698] took [2ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@41ebba80] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@370ef08a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@753c2c19] took [38ms], [org.elasticsearch.node.ResponseCollectorService@691fa784] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@40f6a806] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3c9a32] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@14f432c8] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@454bf96f] took [0ms], [org.elasticsearch.indices.store.IndicesStore@610d8b7c] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6350bd9] took [0ms], [org.elasticsearch.license.LicenseService@7e5054da] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@69116b70] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3d6dc9c5] took [0ms], [org.elasticsearch.gateway.GatewayService@14843365] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@33f528e4] took [0ms]
[2022-04-11T19:25:42,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [8111ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:26:48,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:26:51,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16332570136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:01,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:10,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13658727913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:20,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19346ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:32,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19346266631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:40,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19424ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:50,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19423875845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:01,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:12,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21544947806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:21,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:27,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19493835335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:28,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@1b9eb550] took [148387ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:28:34,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:40,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12756532327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:46,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@28976841, interval=5s}] took [12243ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:28:46,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:51,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12243515448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:55,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:59,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9011875458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:03,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:08,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8494735711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:13,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:17,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9506856892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:21,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:25,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8954281751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:25,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [8954ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:29:30,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:29,642][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8954ms] which is above the warn threshold of [5s]
[2022-04-11T19:29:32,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8559279738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:35,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:39,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [5352ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:29:39,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5352579788ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:42,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:45,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6499859307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:45,174][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [212357ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [45] unchanged indices
[2022-04-11T19:29:47,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:50,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5480915140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:53,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:54,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [10948ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:29:50,699][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.8m] publication of cluster state version [8963] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-11T19:29:55,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5467593308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:30:01,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@28976841, interval=5s}] took [7962ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:31:16,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24619ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:19,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24618382148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:25,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:29,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9576439562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:34,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:37,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9395965854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:38,360][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59750}] took [9396ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:31:44,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:52,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8592619796ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:00,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@35cd7aa3] took [112244ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:31:59,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:06,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16127582896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:13,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:19,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13956378652ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:24,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:24,484][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59750}] took [13956ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:32:31,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10580728981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:31,240][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][69][13] duration [16.5s], collections [1]/[2.3m], total [16.5s]/[30.1s], memory [177.9mb]->[110.9mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [95.9mb]->[100.2mb]/[2gb]}{[survivor] [9.9mb]->[6.7mb]/[0b]}
[2022-04-11T19:32:40,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [24537ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:32:40,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:46,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15328778045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:53,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:58,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13672486579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:03,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:08,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10472388688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:13,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9318ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:12,127][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=230, version=8963}] took [3.1m] which is above the warn threshold of [30s]: [running task [Publication{term=230, version=8963}]] took [65ms], [connecting to new nodes] took [406ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@da71c6e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1e79b753] took [13164ms], [org.elasticsearch.script.ScriptService@1b3834c1] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [0ms], [org.elasticsearch.snapshots.RestoreService@61e8eb39] took [0ms], [org.elasticsearch.ingest.IngestService@32780b78] took [6041ms], [org.elasticsearch.action.ingest.IngestActionForwarder@683cafca] took [69ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d2640@7272efec] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@54e573b8] took [67ms], [org.elasticsearch.tasks.TaskManager@26349197] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@2b7b3864] took [111ms], [org.elasticsearch.cluster.InternalClusterInfoService@4f422da9] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@11f2c0c1] took [71ms], [org.elasticsearch.indices.SystemIndexManager@3fe52349] took [3635ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@3bda7ebe] took [72ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@74c62010] took [443ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7731947] took [156ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@448dbc41] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013ba000@5189d28] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@4a4e7366] took [73ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1a79674e] took [82691ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@a100ad6] took [1ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@36a69f18] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3c6a65a0] took [22672ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@207c6ae6] took [1330ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@61349821] took [527ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@63688540] took [20443ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [930ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@cc0bbe8] took [17431ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f28cbcc] took [177ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@246b80b2] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@21a0698] took [11814ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@41ebba80] took [6478ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@370ef08a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@753c2c19] took [1566ms], [org.elasticsearch.node.ResponseCollectorService@691fa784] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@40f6a806] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3c9a32] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@14f432c8] took [161ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@454bf96f] took [149ms], [org.elasticsearch.indices.store.IndicesStore@610d8b7c] took [294ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6350bd9] took [0ms], [org.elasticsearch.license.LicenseService@7e5054da] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@69116b70] took [80ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3d6dc9c5] took [0ms], [org.elasticsearch.gateway.GatewayService@14843365] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@33f528e4] took [0ms]
[2022-04-11T19:33:18,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9318075504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:27,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:33,453][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7.8m/471501ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [10.7m/645342ms]
[2022-04-11T19:33:34,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14693026452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:40,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:31,182][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14693ms] which is above the warn threshold of [5s]
[2022-04-11T19:33:45,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [12885ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:33:46,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12885162150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:51,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@466b153f, interval=5s}] took [10411ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:33:51,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:57,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10411997646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:02,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:09,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11313843586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:15,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:19,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [12596ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:34:19,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12596497893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:23,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:28,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8693472973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:46,617][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [59.1s/59185ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-11T19:36:00,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:37:44,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92218373512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:39:52,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/229649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:41:35,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/229649346783ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:42:50,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/172490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:44:24,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/172240658825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:46:03,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/195551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:48:12,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/195800793034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:50:31,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:54:00,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/267993854474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:57:10,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:00:38,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396678744625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:03:25,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:06:36,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367954352579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:10:17,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:13:04,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415659430698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:16:15,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:19:44,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340176881629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:23:11,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/423672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:26:24,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424185702636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:29:22,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/383559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:32:50,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/383104077230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:36:28,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/421078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:39:57,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/421355020849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:43:10,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:46:29,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404867254215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:50:08,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/402656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:57:58,288][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T20:57:58,344][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T20:57:58,345][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T20:58:05,155][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T20:58:05,156][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T20:58:05,157][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T20:58:05,157][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T20:58:05,158][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T20:58:05,159][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T20:58:05,160][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T20:58:05,161][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T20:58:05,163][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T20:58:05,164][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T20:58:05,165][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T20:58:05,166][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T20:58:05,167][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T20:58:05,168][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T20:58:05,169][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T20:58:05,170][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T20:58:05,172][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T20:58:05,173][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T20:58:05,173][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T20:58:05,175][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T20:58:05,175][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T20:58:05,177][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T20:58:05,178][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T20:58:05,179][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T20:58:05,180][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T20:58:05,181][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T20:58:05,182][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T20:58:05,182][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T20:58:05,183][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T20:58:05,184][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T20:58:05,185][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T20:58:05,186][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T20:58:05,187][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T20:58:05,188][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T20:58:05,189][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T20:58:05,190][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T20:58:05,191][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T20:58:05,192][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T20:58:05,193][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T20:58:05,194][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T20:58:05,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T20:58:05,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T20:58:05,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T20:58:05,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T20:58:05,203][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T20:58:05,203][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T20:58:05,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T20:58:05,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T20:58:05,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T20:58:05,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T20:58:05,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T20:58:05,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T20:58:05,206][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T20:58:05,206][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T20:58:05,207][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T20:58:05,207][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T20:58:05,207][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T20:58:05,208][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T20:58:05,209][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T20:58:05,310][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.5gb], net total_space [125.8gb], types [ext4]
[2022-04-11T20:58:05,312][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T20:58:06,085][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T20:58:23,872][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-11T20:58:23,876][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-11T20:58:23,877][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T20:58:23,878][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-11T20:58:23,879][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T20:58:23,879][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp]
[2022-04-11T20:58:23,880][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-11T20:58:23,881][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-11T20:58:23,881][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T20:58:23,882][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T20:58:23,883][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T20:58:23,884][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-11T20:58:26,538][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-11T20:58:26,738][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-11T20:58:28,753][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-11T20:58:30,260][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-11T20:58:30,261][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-11T20:58:30,321][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-11T20:58:30,323][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-11T20:58:30,637][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-11T20:58:34,621][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-11T20:58:34,855][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NQHbqHqgR-a7MMmSAMORoA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 231, version: 8964, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NQHbqHqgR-a7MMmSAMORoA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-11T20:58:35,115][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NQHbqHqgR-a7MMmSAMORoA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 231, version: 8964, reason: Publication{term=231, version=8964}
[2022-04-11T20:58:35,376][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-11T20:58:35,378][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-11T20:58:36,883][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-11T20:58:36,898][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [47] indices into cluster_state
[2022-04-11T20:58:38,617][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-11T20:58:38,623][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-11T20:58:39,874][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-11T20:58:40,465][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-11T20:58:41,321][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-11T20:58:41,334][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-11T20:58:41,339][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T20:58:41,754][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-11T20:58:42,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T20:58:42,454][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T20:58:46,871][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T20:58:58,662][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.11][0]]]).
[2022-04-11T21:00:01,271][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T21:00:01,289][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T21:00:01,291][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T21:00:08,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T21:00:08,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T21:00:08,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T21:00:08,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T21:00:08,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T21:00:08,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T21:00:08,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T21:00:08,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T21:00:08,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T21:00:08,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T21:00:08,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T21:00:08,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T21:00:08,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T21:00:08,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T21:00:08,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T21:00:08,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T21:00:08,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T21:00:08,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T21:00:08,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T21:00:08,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T21:00:08,479][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T21:00:08,479][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T21:00:08,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T21:00:08,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T21:00:08,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T21:00:08,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T21:00:08,482][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T21:00:08,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T21:00:08,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T21:00:08,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T21:00:08,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T21:00:08,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T21:00:08,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T21:00:08,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T21:00:08,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T21:00:08,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T21:00:08,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T21:00:08,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T21:00:08,488][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T21:00:08,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T21:00:08,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T21:00:08,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T21:00:08,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T21:00:08,492][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T21:00:08,492][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T21:00:08,492][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T21:00:08,493][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T21:00:08,493][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T21:00:08,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T21:00:08,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T21:00:08,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T21:00:08,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T21:00:08,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T21:00:08,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T21:00:08,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T21:00:08,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T21:00:08,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T21:00:08,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T21:00:08,499][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T21:00:08,587][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.5gb], net total_space [125.8gb], types [ext4]
[2022-04-11T21:00:08,588][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T21:00:09,112][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T21:00:25,808][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-11T21:00:25,813][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-11T21:00:25,815][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T21:00:25,817][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-11T21:00:25,819][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T21:00:25,820][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-11T21:00:25,821][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-11T21:00:25,822][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T21:00:25,822][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-11T21:00:25,823][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-11T21:00:25,824][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-11T21:00:25,825][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T21:00:25,826][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T21:00:25,826][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T21:00:25,827][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-11T21:00:27,454][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-11T21:00:27,687][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-11T21:00:29,350][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-11T21:00:30,847][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-11T21:00:30,849][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-11T21:00:30,937][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-11T21:00:30,940][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-11T21:00:31,255][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-11T21:00:35,692][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-11T21:00:35,838][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 232, version: 9025, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-11T21:00:36,152][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 232, version: 9025, reason: Publication{term=232, version=9025}
[2022-04-11T21:00:36,403][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-11T21:00:36,404][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-11T21:00:37,961][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-11T21:00:37,984][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [47] indices into cluster_state
[2022-04-11T21:00:39,873][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-11T21:00:39,877][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-11T21:00:41,151][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-11T21:00:41,995][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-11T21:00:42,809][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-11T21:00:42,876][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-11T21:00:42,885][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-11T21:00:42,891][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T21:00:43,709][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T21:00:43,891][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T21:00:49,770][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T21:00:51,124][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][20] overhead, spent [382ms] collecting in the last [1s]
[2022-04-11T21:00:56,216][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-11T21:01:28,480][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-1970.01.01] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-11T21:01:28,699][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:28,811][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-1970.01.01][0]]]).
[2022-04-11T21:01:29,075][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-1970.01.01/9Y6aaqQuTfir-K-bt-BQiw] update_mapping [_doc]
[2022-04-11T21:01:29,178][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:29,464][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:29,652][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:30,163][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:30,370][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:31,099][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:31,687][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:32,130][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:33,267][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:33,685][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:34,380][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:36,187][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:37,764][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:37,995][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:38,251][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:39,095][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:39,527][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:01:40,839][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:19:23,619][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T21:22:48,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [32776ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:23:53,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [6033ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:24:08,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [8000ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:24:53,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [8702ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:27:53,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:30:41,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5248600084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:31:50,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/390762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:32:16,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391257461143ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:32:53,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64069ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:33:08,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64069603143ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:33:21,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:33:38,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28799896041ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:33:54,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:34:17,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31889629513ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:34:43,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:35:11,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45731915162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:36:05,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:36:24,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85846877006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:36:39,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:36:41,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2f524db2, interval=5s}] took [35329ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:37:01,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35329042634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:37:25,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:37:38,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44309560962ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:39:28,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:39:52,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122394741253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:40:11,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:40:34,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [42550ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:41:00,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42550227482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:41:09,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:41:19,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61417015763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:41:33,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:41:49,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22699960278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:42:04,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:42:29,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29089230012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:42:45,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:43:08,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43102648140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:43:57,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:44:23,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67049958817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:44:30,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:43:58,548][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [17.3m/1040142ms] ago, timed out [16.7m/1005385ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10741]
[2022-04-11T21:44:47,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38694122267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:44:56,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:45:10,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26633515514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:45:15,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [18278ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:45:14,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:45:30,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18278461642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:45:35,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [19824ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:45:34,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19824ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:45:58,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19824018510ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:45:33,276][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18278ms] which is above the warn threshold of [5s]
[2022-04-11T21:46:27,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:46:27,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [45812ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:46:35,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45812674788ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:11,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:19,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51279450343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:18,821][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10741] timed out after [34757ms]
[2022-04-11T21:47:22,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10538ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:26,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10537382667ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:31,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:31,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@5d70c66e, interval=1m}] took [8831ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:47:39,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8831167895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:44,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:52,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12842835098ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:47:57,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:04,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12966152192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:11,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:18,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14527801790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:30,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13902ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:35,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13901757593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:39,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:49,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14756808941ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:48:55,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:01,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14610352471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:11,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:16,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13071247422ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:22,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:28,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14060512144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:31,003][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1337][36] duration [17.9s], collections [1]/[1.6m], total [17.9s]/[19.9s], memory [1.3gb]->[250.3mb]/[2gb], all_pools {[young] [1.1gb]->[20mb]/[0b]}{[old] [223.4mb]->[223.4mb]/[2gb]}{[survivor] [6.3mb]->[6.8mb]/[0b]}
[2022-04-11T21:49:33,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:39,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10984562834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:40,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [121722ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:49:45,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:49:54,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388153237ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:50:03,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:50:22,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18678628624ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:50:27,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T21:50:30,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24015310780ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T21:50:31,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [24015ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:50:32,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [5171ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:50:51,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [6204ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:51:46,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [19813ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:52:44,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [5866ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:52:59,676][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6004ms] which is above the warn threshold of [5s]
[2022-04-11T21:53:49,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [34467ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:54:17,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [13145ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:54:58,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5103/0x00000008017e2020@3271a754] took [6971ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:56:10,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [36426ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:57:06,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [5907ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:57:05,022][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [11084] timed out after [113584ms]
[2022-04-11T21:56:59,802][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12435ms] which is above the warn threshold of [5s]
[2022-04-11T21:57:10,432][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.2m/132049ms] ago, timed out [18.8s/18872ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [11085]
[2022-04-11T21:57:14,065][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.3m/141133ms] ago, timed out [27.5s/27549ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [11084]
[2022-04-11T21:57:24,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [6611ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:57:16,890][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [11085] timed out after [113177ms]
[2022-04-11T21:58:39,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [8805ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:58:29,614][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [11134] timed out after [17983ms]
[2022-04-11T21:58:30,187][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [11135] timed out after [17383ms]
[2022-04-11T21:58:40,561][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [32.7s/32793ms] ago, timed out [15.4s/15410ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [11135]
[2022-04-11T21:58:40,561][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [33.3s/33393ms] ago, timed out [15.4s/15410ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [11134]
[2022-04-11T21:58:52,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [6404ms] which is above the warn threshold of [5000ms]
[2022-04-11T21:59:17,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [7604ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:00:23,803][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1396][37] duration [3.1s], collections [1]/[4.9s], total [3.1s]/[23.1s], memory [310.3mb]->[234mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [223.4mb]->[223.5mb]/[2gb]}{[survivor] [6.8mb]->[10.5mb]/[0b]}
[2022-04-11T22:00:28,514][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1396] overhead, spent [3.1s] collecting in the last [4.9s]
[2022-04-11T22:00:29,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [5590ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:00:31,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1397][38] duration [2.6s], collections [1]/[6.8s], total [2.6s]/[25.7s], memory [234mb]->[264.6mb]/[2gb], all_pools {[young] [0b]->[28mb]/[0b]}{[old] [223.5mb]->[225.4mb]/[2gb]}{[survivor] [10.5mb]->[11.1mb]/[0b]}
[2022-04-11T22:00:32,543][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1397] overhead, spent [2.6s] collecting in the last [6.8s]
[2022-04-11T22:00:50,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:00:52,294][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1401][39] duration [4.5s], collections [1]/[8.1s], total [4.5s]/[30.3s], memory [304.6mb]->[236.3mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [225.4mb]->[227.6mb]/[2gb]}{[survivor] [11.1mb]->[8.7mb]/[0b]}
[2022-04-11T22:00:56,709][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1401] overhead, spent [4.5s] collecting in the last [8.1s]
[2022-04-11T22:00:52,294][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:53468}] took [6966ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:00:56,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6365551429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:00:56,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [6365ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:00:56,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:00:56,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6861391882ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:00:56,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [6861ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:01:02,006][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1402][41] duration [4.9s], collections [2]/[11s], total [4.9s]/[35.2s], memory [236.3mb]->[236.2mb]/[2gb], all_pools {[young] [12mb]->[24mb]/[0b]}{[old] [227.6mb]->[230.9mb]/[2gb]}{[survivor] [8.7mb]->[5.3mb]/[0b]}
[2022-04-11T22:01:02,405][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1402] overhead, spent [4.9s] collecting in the last [11s]
[2022-04-11T22:01:24,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:01:34,481][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1407][43] duration [9.3s], collections [2]/[1.8s], total [9.3s]/[44.6s], memory [276.2mb]->[324.2mb]/[2gb], all_pools {[young] [48mb]->[28mb]/[0b]}{[old] [230.9mb]->[230.9mb]/[2gb]}{[survivor] [5.3mb]->[8.8mb]/[0b]}
[2022-04-11T22:01:34,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6945244923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:01:38,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:01:37,971][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1407] overhead, spent [9.3s] collecting in the last [1.8s]
[2022-04-11T22:01:38,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13997114279ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:01:38,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [21542ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:01:40,787][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1408][44] duration [2.1s], collections [1]/[23.6s], total [2.1s]/[46.7s], memory [324.2mb]->[264.8mb]/[2gb], all_pools {[young] [28mb]->[28mb]/[0b]}{[old] [230.9mb]->[232.6mb]/[2gb]}{[survivor] [8.8mb]->[4.2mb]/[0b]}
[2022-04-11T22:02:02,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:24,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14466680507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:24,505][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1412][46] duration [24.9s], collections [2]/[16.1s], total [24.9s]/[1.1m], memory [288.8mb]->[240mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [232.6mb]->[232.6mb]/[2gb]}{[survivor] [4.2mb]->[7.4mb]/[0b]}
[2022-04-11T22:02:27,479][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1412] overhead, spent [24.9s] collecting in the last [16.1s]
[2022-04-11T22:02:26,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:31,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [39375ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:02:32,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24909207751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:35,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [8210ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:02:35,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:37,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8210095448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:41,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6601ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:44,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6600605957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:48,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [6516ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:02:48,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:53,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6516408599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:02:49,069][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33012}] took [6517ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:02:58,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:02,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10715628896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:02,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [10715ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:03:08,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:12,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9216729782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:15,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:16,646][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:53452}] took [8100ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:03:19,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8100412485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:19,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [8100ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:03:22,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6868ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:31,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6868169174ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:40,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:46,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16344574983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:52,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:03:42,900][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33018}] took [16344ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:03:55,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5103/0x00000008017e2020@b317c8f] took [13020ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:03:57,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13020892693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:04:03,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:04:10,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [11250ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:04:12,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11250403689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:04:11,635][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33018}] took [11250ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:04:20,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:04:28,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16607521115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:04:37,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16968876686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:29,774][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1416][47] duration [30.2s], collections [1]/[1m], total [30.2s]/[1.6m], memory [316.6mb]->[324.6mb]/[2gb], all_pools {[young] [76mb]->[84mb]/[0b]}{[old] [232.6mb]->[232.6mb]/[2gb]}{[survivor] [8mb]->[6.4mb]/[0b]}
[2022-04-11T22:05:32,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.4s/55467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:32,287][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1416] overhead, spent [30.2s] collecting in the last [1m]
[2022-04-11T22:05:34,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [89043ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:05:35,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.4s/55467186955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:39,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:43,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7044780100ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:46,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:53,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227744368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:05:45,836][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7045ms] which is above the warn threshold of [5s]
[2022-04-11T22:06:01,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:06:09,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14271012523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:06:25,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:06:33,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23667320386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:08:00,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:08:12,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95176258344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:08:22,618][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1417][48] duration [1m], collections [1]/[1.9m], total [1m]/[2.7m], memory [324.6mb]->[319mb]/[2gb], all_pools {[young] [84mb]->[88mb]/[0b]}{[old] [232.6mb]->[232.6mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-04-11T22:08:30,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:08:33,995][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1417] overhead, spent [1m] collecting in the last [1.9m]
[2022-04-11T22:08:41,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30733493136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:08:38,935][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33018}] took [30734ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:08:44,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [149577ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:08:49,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:09:03,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18391161031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:09:09,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:09:24,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20555788966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:09:39,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:09:53,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29352121073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:10:06,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:10:20,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27132610677ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:10:34,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:10:46,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28888748550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:11:03,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27838ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:11:19,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [56726ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:10:56,745][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28889ms] which is above the warn threshold of [5s]
[2022-04-11T22:11:21,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27837986906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:11:45,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41142ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:11:48,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@60ec8889, interval=5s}] took [41141ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:12:03,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41141815228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:11:51,124][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [11648] timed out after [217876ms]
[2022-04-11T22:12:21,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:12:24,175][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/378718ms] ago, timed out [2.6m/160842ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I-JEfLsbRoeRxhc3lMJqYA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [11648]
[2022-04-11T22:12:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35840987221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:12:39,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:12:49,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19063664584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:13:01,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:13:14,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22957521661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:13:27,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:13:37,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24876554622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:13:49,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:14:03,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22900352792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T22:14:09,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@70507abe, interval=1s}] took [22900ms] which is above the warn threshold of [5000ms]
[2022-04-11T22:14:17,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T22:14:30,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27916228893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:22:07,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1h/3865991ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:26:12,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1h/3865990576951ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:30:52,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8m/651631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:35:15,114][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T23:35:15,163][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T23:35:15,164][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T23:48:31,266][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T23:48:31,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T23:48:31,272][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T23:48:31,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T23:48:31,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T23:48:31,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T23:48:31,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T23:48:31,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T23:48:31,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T23:48:31,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T23:48:31,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T23:48:31,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T23:48:31,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T23:48:31,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T23:48:31,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T23:48:31,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T23:48:31,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T23:48:31,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T23:48:31,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T23:48:31,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T23:48:31,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T23:48:31,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T23:48:31,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T23:48:31,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T23:48:31,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T23:48:31,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T23:48:31,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T23:48:31,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T23:48:31,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T23:48:31,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T23:48:31,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T23:48:31,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T23:48:31,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T23:48:31,284][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T23:48:31,284][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T23:48:31,284][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T23:48:31,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T23:48:31,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T23:48:31,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T23:48:31,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T23:48:31,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T23:48:31,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T23:48:31,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T23:48:31,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T23:48:31,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T23:48:31,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T23:48:31,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T23:48:31,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T23:48:31,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T23:48:31,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T23:48:31,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T23:48:31,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T23:48:31,290][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T23:48:31,290][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T23:48:31,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T23:48:31,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T23:48:31,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T23:48:31,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T23:48:31,292][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T23:48:35,462][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.5gb], net total_space [125.8gb], types [ext4]
[2022-04-11T23:48:35,846][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T23:48:37,039][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T23:50:06,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:25,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7073821718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:29,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:32,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112756061888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:36,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:39,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6246744200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:42,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:51:50,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6934177080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:52:00,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16216ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:52:12,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16216120572ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:52:22,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23497ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:52:27,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23496260291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:52:38,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:53:01,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14931017778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:52:38,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@42e8dbc6, interval=1m}] took [23496ms] which is above the warn threshold of [5000ms]
[2022-04-11T23:53:15,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36359ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:53:31,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36359638498ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:53:56,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:53:58,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c63529e, interval=5s}] took [34855ms] which is above the warn threshold of [5000ms]
[2022-04-11T23:54:08,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34855641990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:54:19,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:54:20,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c63529e, interval=5s}] took [30421ms] which is above the warn threshold of [5000ms]
[2022-04-11T23:54:30,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30421856201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:54:37,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:54:41,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17373641886ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:54:48,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:54:50,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c63529e, interval=5s}] took [11734ms] which is above the warn threshold of [5000ms]
[2022-04-11T23:54:54,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11734924478ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:01,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:10,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12376852664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:19,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:21,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7d28a36d, interval=30s}] took [17943ms] which is above the warn threshold of [5000ms]
[2022-04-11T23:55:28,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17943092074ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:33,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:38,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14567268324ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:43,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:55:45,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9947366789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T23:56:43,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7d28a36d, interval=30s}] took [10693ms] which is above the warn threshold of [5000ms]
[2022-04-11T23:59:09,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T23:58:57,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c63529e, interval=5s}] took [17666ms] which is above the warn threshold of [5000ms]
