[2022-03-29T00:00:04,448][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.29] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-29T00:00:05,289][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.29][0]]]).
[2022-03-29T00:00:06,951][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:13,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:21,225][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:31,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:35,801][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:39,859][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:41,252][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:42,864][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:43,895][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:44,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:45,414][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:46,334][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:46,922][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:49,079][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:53,957][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:58,787][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:03,534][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:10,514][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:15,193][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:23,095][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:34,583][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:50,986][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11.4s] publication of cluster state version [4215] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:02:35,529][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:02:42,148][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.8s/16817ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:02:59,005][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12002ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:03:03,982][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.6s] publication of cluster state version [4216] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:03:19,281][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:31,797][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:45,375][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:46,554][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:07,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:09,457][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:09,458][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][820][93] duration [1.1s], collections [1]/[2.7s], total [1.1s]/[3.1m], memory [1.3gb]->[240.6mb]/[2gb], all_pools {[young] [1.1gb]->[24mb]/[0b]}{[old] [203.6mb]->[204.6mb]/[2gb]}{[survivor] [10.9mb]->[12mb]/[0b]}
[2022-03-29T00:04:09,649][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][820] overhead, spent [1.1s] collecting in the last [2.7s]
[2022-03-29T00:04:18,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:22,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][828][94] duration [1.4s], collections [1]/[1.1s], total [1.4s]/[3.1m], memory [276.6mb]->[280.6mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [204.6mb]->[204.6mb]/[2gb]}{[survivor] [12mb]->[12mb]/[0b]}
[2022-03-29T00:04:22,985][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][828] overhead, spent [1.4s] collecting in the last [1.1s]
[2022-03-29T00:04:25,718][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][829][95] duration [1.1s], collections [1]/[5.2s], total [1.1s]/[3.2m], memory [280.6mb]->[216.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [204.6mb]->[211.9mb]/[2gb]}{[survivor] [12mb]->[4.9mb]/[0b]}
[2022-03-29T00:04:30,878][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:40,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][836][96] duration [5s], collections [1]/[6.1s], total [5s]/[3.2m], memory [280.9mb]->[217.5mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [4.9mb]->[5.5mb]/[0b]}
[2022-03-29T00:04:40,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:40,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5825300232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:40,434][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][836] overhead, spent [5s] collecting in the last [6.1s]
[2022-03-29T00:04:52,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,399][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][837][97] duration [7s], collections [1]/[1.9s], total [7s]/[3.4m], memory [217.5mb]->[297.5mb]/[2gb], all_pools {[young] [0b]->[20mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [5.5mb]->[6.5mb]/[0b]}
[2022-03-29T00:04:53,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][837] overhead, spent [7s] collecting in the last [1.9s]
[2022-03-29T00:04:53,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [10206ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9542982608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,933][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [21199ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:04:54,131][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [22.4s] publication of cluster state version [4222] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:04:59,748][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:02,960][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][842][98] duration [1.6s], collections [1]/[3.8s], total [1.6s]/[3.4m], memory [274.5mb]->[222.9mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.5mb]->[6.9mb]/[0b]}
[2022-03-29T00:05:03,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][842] overhead, spent [1.6s] collecting in the last [3.8s]
[2022-03-29T00:05:10,710][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][848][99] duration [738ms], collections [1]/[1.7s], total [738ms]/[3.4m], memory [278.9mb]->[276.5mb]/[2gb], all_pools {[young] [64mb]->[56mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.9mb]->[8.5mb]/[0b]}
[2022-03-29T00:05:10,869][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][848] overhead, spent [738ms] collecting in the last [1.7s]
[2022-03-29T00:05:15,066][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][849][100] duration [2.1s], collections [1]/[4.3s], total [2.1s]/[3.4m], memory [276.5mb]->[286.1mb]/[2gb], all_pools {[young] [56mb]->[72mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [8.5mb]->[6.1mb]/[0b]}
[2022-03-29T00:05:15,560][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][849] overhead, spent [2.1s] collecting in the last [4.3s]
[2022-03-29T00:05:15,406][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:23,360][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][853][101] duration [1.5s], collections [1]/[1.3s], total [1.5s]/[3.5m], memory [302.1mb]->[306.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.1mb]->[5.7mb]/[0b]}
[2022-03-29T00:05:23,553][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][853] overhead, spent [1.5s] collecting in the last [1.3s]
[2022-03-29T00:05:24,498][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:38,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][862][102] duration [1.9s], collections [1]/[3.6s], total [1.9s]/[3.5m], memory [297.7mb]->[226.6mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [5.7mb]->[6.6mb]/[0b]}
[2022-03-29T00:05:39,351][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][862] overhead, spent [1.9s] collecting in the last [3.6s]
[2022-03-29T00:05:49,638][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:50,083][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][867][103] duration [3.2s], collections [1]/[4.9s], total [3.2s]/[3.5m], memory [274.6mb]->[219.9mb]/[2gb], all_pools {[young] [60mb]->[48mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.6mb]->[7.9mb]/[0b]}
[2022-03-29T00:05:50,196][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][867] overhead, spent [3.2s] collecting in the last [4.9s]
[2022-03-29T00:05:50,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5207ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:05:50,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:01,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][873][104] duration [1.9s], collections [1]/[3.1s], total [1.9s]/[3.6m], memory [303.9mb]->[218mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [7.9mb]->[6mb]/[0b]}
[2022-03-29T00:06:01,826][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][873] overhead, spent [1.9s] collecting in the last [3.1s]
[2022-03-29T00:06:03,738][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:07,177][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][875][105] duration [2s], collections [1]/[1.2s], total [2s]/[3.6m], memory [270mb]->[306mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6mb]->[9.8mb]/[0b]}
[2022-03-29T00:06:07,378][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][875] overhead, spent [2s] collecting in the last [1.2s]
[2022-03-29T00:06:09,191][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][876][106] duration [861ms], collections [1]/[4.9s], total [861ms]/[3.6m], memory [306mb]->[222mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [211.9mb]->[213.2mb]/[2gb]}{[survivor] [9.8mb]->[8.8mb]/[0b]}
[2022-03-29T00:06:16,352][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][882] overhead, spent [263ms] collecting in the last [1s]
[2022-03-29T00:06:24,023][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:39,759][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][901] overhead, spent [370ms] collecting in the last [1.2s]
[2022-03-29T00:06:56,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][910][112] duration [3.6s], collections [1]/[5.6s], total [3.6s]/[3.7m], memory [295.6mb]->[221.1mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [213.2mb]->[213.2mb]/[2gb]}{[survivor] [6.3mb]->[7.8mb]/[0b]}
[2022-03-29T00:07:00,366][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][910] overhead, spent [3.6s] collecting in the last [5.6s]
[2022-03-29T00:07:01,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5455ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:07:09,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:09,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][911][113] duration [2.2s], collections [1]/[7.8s], total [2.2s]/[3.7m], memory [221.1mb]->[293.2mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [213.2mb]->[213.2mb]/[2gb]}{[survivor] [7.8mb]->[7.9mb]/[0b]}
[2022-03-29T00:07:10,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5274535049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:10,293][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][911] overhead, spent [2.2s] collecting in the last [7.8s]
[2022-03-29T00:07:10,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5674ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:07:12,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][912][114] duration [3.8s], collections [1]/[7.6s], total [3.8s]/[3.8m], memory [293.2mb]->[256.9mb]/[2gb], all_pools {[young] [80mb]->[36mb]/[0b]}{[old] [213.2mb]->[214mb]/[2gb]}{[survivor] [7.9mb]->[6.9mb]/[0b]}
[2022-03-29T00:07:12,844][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][912] overhead, spent [3.8s] collecting in the last [7.6s]
[2022-03-29T00:07:45,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:45,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11568672739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:46,839][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][920][115] duration [8.1s], collections [1]/[2.9s], total [8.1s]/[3.9m], memory [304.9mb]->[304.9mb]/[2gb], all_pools {[young] [84mb]->[84mb]/[0b]}{[old] [214mb]->[214mb]/[2gb]}{[survivor] [6.9mb]->[6.9mb]/[0b]}
[2022-03-29T00:07:51,154][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][920] overhead, spent [8.1s] collecting in the last [2.9s]
[2022-03-29T00:07:53,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [24156ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:25,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:26,172][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][922][116] duration [10.7s], collections [1]/[17.3s], total [10.7s]/[4.1m], memory [263.4mb]->[219.2mb]/[2gb], all_pools {[young] [68mb]->[28mb]/[0b]}{[old] [214mb]->[214.2mb]/[2gb]}{[survivor] [5.3mb]->[4.9mb]/[0b]}
[2022-03-29T00:08:26,563][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][922] overhead, spent [10.7s] collecting in the last [17.3s]
[2022-03-29T00:08:26,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [14086ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:26,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14086186061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:27,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:08:31,917][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24s/24014ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:08:51,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,543][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][930][117] duration [6.4s], collections [1]/[1.6s], total [6.4s]/[4.2m], memory [287.2mb]->[287.2mb]/[2gb], all_pools {[young] [68mb]->[72mb]/[0b]}{[old] [214.2mb]->[214.2mb]/[2gb]}{[survivor] [4.9mb]->[4.9mb]/[0b]}
[2022-03-29T00:08:51,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8110480180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][930] overhead, spent [6.4s] collecting in the last [1.6s]
[2022-03-29T00:08:51,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [10915ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,585][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38574}] took [8511ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:59,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][932][118] duration [3.1s], collections [1]/[1.4s], total [3.1s]/[4.3m], memory [248.1mb]->[252.1mb]/[2gb], all_pools {[young] [28mb]->[88mb]/[0b]}{[old] [214.2mb]->[214.2mb]/[2gb]}{[survivor] [5.8mb]->[5.8mb]/[0b]}
[2022-03-29T00:08:59,515][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][932] overhead, spent [3.1s] collecting in the last [1.4s]
[2022-03-29T00:08:59,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5359ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:58,854][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [15160ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:09:07,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:09:07,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][933][119] duration [5.2s], collections [1]/[6.3s], total [5.2s]/[4.4m], memory [252.1mb]->[219.6mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [214.2mb]->[214.4mb]/[2gb]}{[survivor] [5.8mb]->[5.1mb]/[0b]}
[2022-03-29T00:09:08,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6468673732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:09:08,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][933] overhead, spent [5.2s] collecting in the last [6.3s]
[2022-03-29T00:09:08,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6468ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:09:00,546][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.1s] publication of cluster state version [4229] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:09:17,459][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][939][120] duration [1s], collections [1]/[1.8s], total [1s]/[4.4m], memory [299.6mb]->[241mb]/[2gb], all_pools {[young] [80mb]->[44mb]/[0b]}{[old] [214.4mb]->[214.4mb]/[2gb]}{[survivor] [5.1mb]->[6.6mb]/[0b]}
[2022-03-29T00:09:18,067][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][939] overhead, spent [1s] collecting in the last [1.8s]
[2022-03-29T00:09:25,905][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][944] overhead, spent [550ms] collecting in the last [1.4s]
[2022-03-29T00:09:33,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][949][122] duration [1s], collections [1]/[2s], total [1s]/[4.4m], memory [297.3mb]->[220.8mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [214.4mb]->[214.4mb]/[2gb]}{[survivor] [6.9mb]->[6.3mb]/[0b]}
[2022-03-29T00:09:33,523][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][949] overhead, spent [1s] collecting in the last [2s]
[2022-03-29T00:09:41,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][955] overhead, spent [421ms] collecting in the last [1.1s]
[2022-03-29T00:09:50,430][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][962][124] duration [727ms], collections [1]/[1.7s], total [727ms]/[4.4m], memory [280.7mb]->[221.8mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [214.5mb]->[214.5mb]/[2gb]}{[survivor] [6.1mb]->[7.2mb]/[0b]}
[2022-03-29T00:09:50,735][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][962] overhead, spent [727ms] collecting in the last [1.7s]
[2022-03-29T00:09:58,941][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][968] overhead, spent [394ms] collecting in the last [1.2s]
[2022-03-29T00:10:06,183][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][974] overhead, spent [314ms] collecting in the last [1.1s]
[2022-03-29T00:10:34,686][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][998] overhead, spent [301ms] collecting in the last [1s]
[2022-03-29T00:19:22,359][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [7559] timed out after [15963ms]
[2022-03-29T00:19:29,227][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [25.7s/25720ms] ago, timed out [9.7s/9757ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [7559]
[2022-03-29T00:20:43,320][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1532][144] duration [3s], collections [1]/[5.4s], total [3s]/[4.5m], memory [1.3gb]->[224.8mb]/[2gb], all_pools {[young] [1gb]->[0b]/[0b]}{[old] [216.7mb]->[216.8mb]/[2gb]}{[survivor] [6.4mb]->[8mb]/[0b]}
[2022-03-29T00:20:46,638][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1532] overhead, spent [3s] collecting in the last [5.4s]
[2022-03-29T00:20:48,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1533][145] duration [1.8s], collections [1]/[5.6s], total [1.8s]/[4.6m], memory [224.8mb]->[260.3mb]/[2gb], all_pools {[young] [0b]->[36mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [8mb]->[7.4mb]/[0b]}
[2022-03-29T00:20:49,249][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1533] overhead, spent [1.8s] collecting in the last [5.6s]
[2022-03-29T00:20:55,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1536][146] duration [1.4s], collections [1]/[2.6s], total [1.4s]/[4.6m], memory [264.3mb]->[232.2mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [7.4mb]->[7.3mb]/[0b]}
[2022-03-29T00:20:55,988][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1536] overhead, spent [1.4s] collecting in the last [2.6s]
[2022-03-29T00:21:07,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1540][147] duration [2.6s], collections [1]/[5.4s], total [2.6s]/[4.6m], memory [248.2mb]->[226.1mb]/[2gb], all_pools {[young] [24mb]->[8mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [7.3mb]->[5.2mb]/[0b]}
[2022-03-29T00:21:08,578][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1540] overhead, spent [2.6s] collecting in the last [5.4s]
[2022-03-29T00:21:26,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:21:27,423][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1547][148] duration [3.9s], collections [1]/[6.3s], total [3.9s]/[4.7m], memory [290.1mb]->[223.5mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [5.2mb]->[6.7mb]/[0b]}
[2022-03-29T00:21:27,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1547] overhead, spent [3.9s] collecting in the last [6.3s]
[2022-03-29T00:21:27,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5198938076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:21:27,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5198ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:22:10,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:22:11,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740151619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:22:12,039][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1559][149] duration [9.9s], collections [1]/[14.7s], total [9.9s]/[4.8m], memory [279.5mb]->[224.3mb]/[2gb], all_pools {[young] [56mb]->[36mb]/[0b]}{[old] [216.8mb]->[216.9mb]/[2gb]}{[survivor] [6.7mb]->[7.4mb]/[0b]}
[2022-03-29T00:22:13,598][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1559] overhead, spent [9.9s] collecting in the last [14.7s]
[2022-03-29T00:22:19,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [21678ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:22:32,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [7550ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:22,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:22,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@b7390be] took [22529ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:23,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20728762092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:24,139][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1561][150] duration [15.1s], collections [1]/[31.2s], total [15.1s]/[5.1m], memory [268.3mb]->[233.5mb]/[2gb], all_pools {[young] [44mb]->[52mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[8.6mb]/[0b]}
[2022-03-29T00:23:24,901][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1561] overhead, spent [15.1s] collecting in the last [31.2s]
[2022-03-29T00:23:37,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,393][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38574}] took [10536ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735577427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1562][151] duration [6.3s], collections [1]/[15.2s], total [6.3s]/[5.2m], memory [233.5mb]->[268.6mb]/[2gb], all_pools {[young] [52mb]->[48mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [8.6mb]->[7.6mb]/[0b]}
[2022-03-29T00:23:38,801][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1562] overhead, spent [6.3s] collecting in the last [15.2s]
[2022-03-29T00:23:56,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77bf7db2, interval=5s}] took [7379ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:56,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:57,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379302816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:57,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1568][152] duration [5.9s], collections [1]/[8.6s], total [5.9s]/[5.3m], memory [284.6mb]->[244.3mb]/[2gb], all_pools {[young] [88mb]->[28mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.6mb]->[7.4mb]/[0b]}
[2022-03-29T00:23:57,275][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1568] overhead, spent [5.9s] collecting in the last [8.6s]
[2022-03-29T00:23:57,544][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:23:57,721][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.7s/13783ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:24:05,783][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1573][153] duration [1.2s], collections [1]/[2.7s], total [1.2s]/[5.3m], memory [272.3mb]->[232.4mb]/[2gb], all_pools {[young] [48mb]->[32mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-29T00:24:06,128][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1573] overhead, spent [1.2s] collecting in the last [2.7s]
[2022-03-29T00:24:34,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7917ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:24:34,574][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1585][154] duration [5.7s], collections [1]/[2.4s], total [5.7s]/[5.4m], memory [308.4mb]->[312.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[6.4mb]/[0b]}
[2022-03-29T00:24:34,793][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1585] overhead, spent [5.7s] collecting in the last [2.4s]
[2022-03-29T00:24:34,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7916834588ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:24:34,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [9009ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:24:41,062][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=146, version=4230}] took [33.6s] which is above the warn threshold of [30s]: [running task [Publication{term=146, version=4230}]] took [103ms], [connecting to new nodes] took [26ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [146ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [1409ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [137ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [24ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016a7960@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [29ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [144ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801316e58@7c05388] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013da000@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [34ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [12953ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [13048ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [83ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [1166ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [855ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [2138ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [44ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [492ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [42ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [232ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [24ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [127ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-03-29T00:24:52,201][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1592][155] duration [2.8s], collections [1]/[1.6s], total [2.8s]/[5.5m], memory [283.4mb]->[311.4mb]/[2gb], all_pools {[young] [60mb]->[20mb]/[0b]}{[old] [216.9mb]->[217.2mb]/[2gb]}{[survivor] [6.4mb]->[6.3mb]/[0b]}
[2022-03-29T00:24:54,240][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1592] overhead, spent [2.8s] collecting in the last [1.6s]
[2022-03-29T00:24:54,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7540ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:43,612][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1598][156] duration [21.2s], collections [1]/[2.9s], total [21.2s]/[5.8m], memory [279.5mb]->[287.5mb]/[2gb], all_pools {[young] [60mb]->[12mb]/[0b]}{[old] [217.2mb]->[217.3mb]/[2gb]}{[survivor] [6.3mb]->[6.9mb]/[0b]}
[2022-03-29T00:25:42,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:43,298][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38900}] took [26419ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:44,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25522368770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:44,144][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1598] overhead, spent [21.2s] collecting in the last [2.9s]
[2022-03-29T00:25:47,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [25722ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:47,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:54,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5748842916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:00,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:06,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13093898888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:10,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [9183ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:26:09,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:04,483][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13094ms] which is above the warn threshold of [5s]
[2022-03-29T00:26:13,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9183731479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:15,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:17,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738345569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:18,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5738ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:26:38,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:39,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18586912199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:40,206][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1601][157] duration [14.6s], collections [1]/[24.5s], total [14.6s]/[6.1m], memory [248.3mb]->[230.6mb]/[2gb], all_pools {[young] [32mb]->[8mb]/[0b]}{[old] [217.3mb]->[217.4mb]/[2gb]}{[survivor] [6.9mb]->[9.2mb]/[0b]}
[2022-03-29T00:26:43,668][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1601] overhead, spent [14.6s] collecting in the last [24.5s]
[2022-03-29T00:26:45,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7764ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:00,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:27:01,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5198711914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:27:01,190][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1605][158] duration [3.6s], collections [1]/[1.4s], total [3.6s]/[6.1m], memory [302.6mb]->[310.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [217.4mb]->[218.5mb]/[2gb]}{[survivor] [9.2mb]->[8.8mb]/[0b]}
[2022-03-29T00:27:01,507][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1605] overhead, spent [3.6s] collecting in the last [1.4s]
[2022-03-29T00:27:01,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:03,156][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:27:08,612][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1610][160] duration [722ms], collections [1]/[2s], total [722ms]/[6.2m], memory [300.1mb]->[229.8mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [218.5mb]->[219.3mb]/[2gb]}{[survivor] [9.5mb]->[6.4mb]/[0b]}
[2022-03-29T00:27:08,995][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1610] overhead, spent [722ms] collecting in the last [2s]
[2022-03-29T00:27:09,602][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:27:17,569][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1615][161] duration [1.8s], collections [1]/[3.5s], total [1.8s]/[6.2m], memory [281.8mb]->[224.6mb]/[2gb], all_pools {[young] [56mb]->[60mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [6.4mb]->[5.2mb]/[0b]}
[2022-03-29T00:27:17,782][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1615] overhead, spent [1.8s] collecting in the last [3.5s]
[2022-03-29T00:27:28,364][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1620][162] duration [2.1s], collections [1]/[3.9s], total [2.1s]/[6.2m], memory [304.6mb]->[225.8mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [5.2mb]->[6.4mb]/[0b]}
[2022-03-29T00:27:30,705][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1620] overhead, spent [2.1s] collecting in the last [3.9s]
[2022-03-29T00:27:32,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7268ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:34,658][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1621][163] duration [1.1s], collections [1]/[6.4s], total [1.1s]/[6.2m], memory [225.8mb]->[242.1mb]/[2gb], all_pools {[young] [28mb]->[16mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [6.4mb]->[6.7mb]/[0b]}
[2022-03-29T00:27:43,997][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1624][164] duration [2.5s], collections [1]/[4.9s], total [2.5s]/[6.3m], memory [246.1mb]->[268.8mb]/[2gb], all_pools {[young] [24mb]->[44mb]/[0b]}{[old] [219.3mb]->[219.5mb]/[2gb]}{[survivor] [6.7mb]->[5.3mb]/[0b]}
[2022-03-29T00:27:44,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1624] overhead, spent [2.5s] collecting in the last [4.9s]
[2022-03-29T00:27:46,126][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:28:25,542][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=146, version=4233}] took [33.9s] which is above the warn threshold of [30s]: [running task [Publication{term=146, version=4233}]] took [19ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [4283ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [154ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016a7960@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [104ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801316e58@7c05388] took [1ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013da000@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [22ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [12294ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [2013ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [70ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [2565ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [128ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [4484ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [59ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [4367ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [2060ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [635ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [64ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [22ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [55ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [0ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.bulk.TransportShardBulkAction$1@5b3a0f6a]]]] took [209ms]
[2022-03-29T00:28:36,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:28:36,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@48bc072c] took [8851ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:28:37,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8451787139ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:28:38,232][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1645][165] duration [5.2s], collections [1]/[10.6s], total [5.2s]/[6.4m], memory [304.8mb]->[249mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.3mb]->[5.5mb]/[0b]}
[2022-03-29T00:28:42,212][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1645] overhead, spent [5.2s] collecting in the last [10.6s]
[2022-03-29T00:28:44,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7916ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:13,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:15,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17099018287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:16,693][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1649][166] duration [12.3s], collections [1]/[2.7s], total [12.3s]/[6.6m], memory [265mb]->[313mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.5mb]->[8.4mb]/[0b]}
[2022-03-29T00:29:18,361][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1649] overhead, spent [12.3s] collecting in the last [2.7s]
[2022-03-29T00:29:26,870][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38922}] took [9030ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:26,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [31292ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:26,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:27,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7628433739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:28,582][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1650][167] duration [4.9s], collections [1]/[32.6s], total [4.9s]/[6.7m], memory [313mb]->[283.5mb]/[2gb], all_pools {[young] [12mb]->[56mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [8.4mb]->[8mb]/[0b]}
[2022-03-29T00:29:36,134][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1653][168] duration [2s], collections [1]/[4s], total [2s]/[6.7m], memory [311.5mb]->[227.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [8mb]->[7.5mb]/[0b]}
[2022-03-29T00:29:36,607][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1653] overhead, spent [2s] collecting in the last [4s]
[2022-03-29T00:29:58,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:59,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388181396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:59,987][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1659][169] duration [9.4s], collections [1]/[1.5s], total [9.4s]/[6.8m], memory [291.1mb]->[315.1mb]/[2gb], all_pools {[young] [64mb]->[36mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [7.5mb]->[5.5mb]/[0b]}
[2022-03-29T00:30:00,470][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1659] overhead, spent [9.4s] collecting in the last [1.5s]
[2022-03-29T00:30:00,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [12825ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:30:02,668][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:04,681][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.5s/22508ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:30:12,225][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1664][170] duration [2.5s], collections [1]/[1.3s], total [2.5s]/[6.9m], memory [281mb]->[313mb]/[2gb], all_pools {[young] [56mb]->[32mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.5mb]->[6.3mb]/[0b]}
[2022-03-29T00:30:13,033][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1664] overhead, spent [2.5s] collecting in the last [1.3s]
[2022-03-29T00:30:13,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5250ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:30:22,342][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1669] overhead, spent [1.5s] collecting in the last [1.3s]
[2022-03-29T00:30:19,539][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:34,416][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10007ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:30:35,603][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.3s] publication of cluster state version [4235] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:30:54,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:58,155][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.9s/12912ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:31:05,528][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1694][172] duration [2.2s], collections [1]/[4.3s], total [2.2s]/[7m], memory [313.8mb]->[224.6mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [6.2mb]->[5.1mb]/[0b]}
[2022-03-29T00:31:05,999][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1694] overhead, spent [2.2s] collecting in the last [4.3s]
[2022-03-29T00:31:11,293][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:31:32,738][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1710][173] duration [2.7s], collections [1]/[5.2s], total [2.7s]/[7m], memory [304.6mb]->[225.7mb]/[2gb], all_pools {[young] [80mb]->[68mb]/[0b]}{[old] [219.5mb]->[219.7mb]/[2gb]}{[survivor] [5.1mb]->[5.9mb]/[0b]}
[2022-03-29T00:31:34,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1710] overhead, spent [2.7s] collecting in the last [5.2s]
[2022-03-29T00:31:49,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:31:49,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1714][174] duration [4.5s], collections [1]/[7.5s], total [4.5s]/[7.1m], memory [305.7mb]->[230.6mb]/[2gb], all_pools {[young] [80mb]->[56mb]/[0b]}{[old] [219.7mb]->[219.8mb]/[2gb]}{[survivor] [5.9mb]->[6.7mb]/[0b]}
[2022-03-29T00:31:49,947][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1714] overhead, spent [4.5s] collecting in the last [7.5s]
[2022-03-29T00:31:49,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6081ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:31:49,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6081482670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:31:58,829][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1717][175] duration [2.7s], collections [1]/[5.2s], total [2.7s]/[7.1m], memory [294.6mb]->[229.1mb]/[2gb], all_pools {[young] [68mb]->[20mb]/[0b]}{[old] [219.8mb]->[219.8mb]/[2gb]}{[survivor] [6.7mb]->[9.2mb]/[0b]}
[2022-03-29T00:31:59,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1717] overhead, spent [2.7s] collecting in the last [5.2s]
[2022-03-29T00:32:09,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:10,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7104933778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:09,944][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1718][176] duration [5.5s], collections [1]/[4.3s], total [5.5s]/[7.2m], memory [229.1mb]->[313.1mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [219.8mb]->[220.5mb]/[2gb]}{[survivor] [9.2mb]->[7.8mb]/[0b]}
[2022-03-29T00:32:10,292][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1718] overhead, spent [5.5s] collecting in the last [4.3s]
[2022-03-29T00:32:10,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7305ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:19,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:20,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8130486128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:24,608][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1719][177] duration [6.2s], collections [1]/[19.7s], total [6.2s]/[7.3m], memory [313.1mb]->[235.5mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [220.5mb]->[220.6mb]/[2gb]}{[survivor] [7.8mb]->[6.9mb]/[0b]}
[2022-03-29T00:32:25,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1719] overhead, spent [6.2s] collecting in the last [19.7s]
[2022-03-29T00:32:28,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:51,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@2a5a3cfa] took [13915ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:51,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:52,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13715138373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:53,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1722][178] duration [10.5s], collections [1]/[18.9s], total [10.5s]/[7.5m], memory [239.5mb]->[260.5mb]/[2gb], all_pools {[young] [16mb]->[32mb]/[0b]}{[old] [220.6mb]->[220.6mb]/[2gb]}{[survivor] [6.9mb]->[7.8mb]/[0b]}
[2022-03-29T00:32:54,063][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1722] overhead, spent [10.5s] collecting in the last [18.9s]
[2022-03-29T00:33:05,135][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1726][179] duration [2.5s], collections [1]/[1.4s], total [2.5s]/[7.5m], memory [280.5mb]->[284.5mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [220.6mb]->[220.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-03-29T00:33:05,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1726] overhead, spent [2.5s] collecting in the last [1.4s]
[2022-03-29T00:33:05,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5580ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:33:15,928][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1732][180] duration [2.2s], collections [1]/[3.5s], total [2.2s]/[7.6m], memory [280.1mb]->[230.6mb]/[2gb], all_pools {[young] [52mb]->[28mb]/[0b]}{[old] [220.6mb]->[220.7mb]/[2gb]}{[survivor] [7.4mb]->[5.9mb]/[0b]}
[2022-03-29T00:33:16,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1732] overhead, spent [2.2s] collecting in the last [3.5s]
[2022-03-29T00:33:25,614][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1737][181] duration [1.5s], collections [1]/[2.5s], total [1.5s]/[7.6m], memory [262.6mb]->[225.9mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [5.9mb]->[5.2mb]/[0b]}
[2022-03-29T00:33:26,286][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1737] overhead, spent [1.5s] collecting in the last [2.5s]
[2022-03-29T00:33:32,869][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1740][182] duration [1.4s], collections [1]/[1.1s], total [1.4s]/[7.6m], memory [249.9mb]->[313.9mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [5.2mb]->[7.7mb]/[0b]}
[2022-03-29T00:33:33,862][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1740] overhead, spent [1.4s] collecting in the last [1.1s]
[2022-03-29T00:33:39,606][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1741][183] duration [3.3s], collections [1]/[8.8s], total [3.3s]/[7.7m], memory [313.9mb]->[244mb]/[2gb], all_pools {[young] [0b]->[36mb]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [7.7mb]->[7.2mb]/[0b]}
[2022-03-29T00:33:40,175][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1741] overhead, spent [3.3s] collecting in the last [8.8s]
[2022-03-29T00:33:40,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5913ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:33:54,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:33:55,548][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1745][184] duration [5.4s], collections [1]/[1.9s], total [5.4s]/[7.8m], memory [300mb]->[316mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [220.7mb]->[221.4mb]/[2gb]}{[survivor] [7.2mb]->[6.5mb]/[0b]}
[2022-03-29T00:33:55,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251125898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:33:57,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1745] overhead, spent [5.4s] collecting in the last [1.9s]
[2022-03-29T00:33:58,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [12188ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:36:54,576][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38940}] took [157718ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:36:58,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:36:58,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1751] overhead, spent [2.4m] collecting in the last [4.2s]
[2022-03-29T00:37:13,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157518145053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:16,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [158118ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:37:23,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:30,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32403ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:37:32,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32403076861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:41,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:53,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719061400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:03,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:11,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21871101760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:28,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:42,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22754831399ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:52,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:59,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@59dfe26f] took [25444ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:39:07,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25444858617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:29,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:47,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38253402042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:05,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:19,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [72483ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:40:30,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34229894778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:48,169][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38254ms] which is above the warn threshold of [5s]
[2022-03-29T00:40:55,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50282ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:59,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [50282ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:41:10,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50282026189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:24,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:37,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29904802093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:54,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:10,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30366249200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:38,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44361ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:58,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44360774509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:13,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:25,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35052698535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:42,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28188267875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:29,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:48,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45659958352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:09,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:22,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41202298523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:36,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:45,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222358581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:56,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20596ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:54,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [27222ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:45:31,110][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8761] timed out after [262450ms]
[2022-03-29T00:46:04,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20596147625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:19,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:28,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22772399224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:39,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:48,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18872320898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:57,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:10,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19129165867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:36,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:50,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37193522889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:48:10,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:48:18,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [73354ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:48:26,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36160644844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:58,594][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [37194ms] which is above the warn threshold of [5s]
[2022-03-29T00:48:50,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32263ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:48:53,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:08,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32263282354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:23,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:42,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40920058378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:58,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:14,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34033118200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:34,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:59,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35330950039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:19,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:34,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [40554ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:51:33,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40554729451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:53,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32970ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:51:50,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:09,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32970699836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:24,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:47,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37233563416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:07,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:46,571][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [13.5m/812754ms] ago, timed out [9.1m/550304ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8761]
[2022-03-29T00:53:12,636][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [43430ms] which is above the warn threshold of [5s]
[2022-03-29T00:53:17,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43430766005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:30,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:40,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21443317887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:53,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [21443ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:53:53,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:31,131][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8831] timed out after [183810ms]
[2022-03-29T00:54:04,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24962833494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:16,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:24,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21769557309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:34,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:34,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6229f0b7, interval=1m}] took [19207ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:54:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19207660074ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:51,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:05,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17114582095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:15,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:23,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22786695025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:34,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19992ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:44,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19991202445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:47,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [19991ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:55:54,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:03,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18543956361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:12,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:20,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [18888ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:56:22,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18888706251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:34,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:43,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21960425086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:53,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:04,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19812415737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:15,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:31,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21524731309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:45,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:55,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508088434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:10,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:22,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25139815057ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:31,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20703ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:37,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20702881384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [9734ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.7m/587831ms] ago, timed out [6.7m/404021ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8831]
[2022-03-29T00:58:38,416][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20703ms] which is above the warn threshold of [5s]
[2022-03-29T00:58:42,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9734592903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:42,965][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8893] timed out after [106610ms]
[2022-03-29T00:59:11,399][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:53020}] took [9368ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:59:11,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6766ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:59:29,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [8541ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:00:59,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [50720ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:02:54,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77bf7db2, interval=5s}] took [6563ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:02:59,669][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15514ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@53fb3fe1]], which exceeds the warn threshold of [10s]
[2022-03-29T01:06:06,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [109173ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:06:19,123][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [19.7s/19726ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf767083], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8cb6533a]], which exceeds the warn threshold of [10s]
[2022-03-29T01:12:21,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:14:53,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/147912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:17:02,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/148304069485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:19:42,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:22:24,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292470109777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:25:31,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:28:32,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343476050295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:31:44,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:33:17,109][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [34.8m/2089711ms] ago, timed out [33m/1983101ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8893]
[2022-03-29T01:34:55,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379004857617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:38:21,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:38:57,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@40d0cf29] took [396462ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:41:47,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396462211638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:45:41,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:48:34,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398362757695ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:04:55,636][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T03:04:55,649][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T03:04:55,650][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T03:05:01,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T03:05:01,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T03:05:01,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T03:05:01,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T03:05:01,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T03:05:01,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T03:05:01,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T03:05:01,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T03:05:01,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T03:05:01,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T03:05:01,945][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T03:05:01,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T03:05:01,947][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T03:05:01,949][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T03:05:01,950][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T03:05:01,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T03:05:01,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T03:05:01,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T03:05:01,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T03:05:01,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T03:05:01,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T03:05:01,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T03:05:01,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T03:05:01,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T03:05:01,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T03:05:01,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T03:05:01,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T03:05:01,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T03:05:01,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T03:05:01,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T03:05:01,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T03:05:01,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T03:05:01,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T03:05:01,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T03:05:01,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T03:05:01,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T03:05:01,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T03:05:01,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T03:05:01,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T03:05:01,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T03:05:01,988][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T03:05:01,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T03:05:01,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T03:05:01,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T03:05:01,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T03:05:01,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T03:05:01,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T03:05:01,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T03:05:01,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T03:05:01,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T03:05:01,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T03:05:01,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T03:05:01,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T03:05:01,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T03:05:02,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T03:05:02,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T03:05:02,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T03:05:02,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T03:05:02,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T03:05:02,098][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T03:05:02,099][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T03:05:02,589][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T03:05:16,597][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T03:05:16,604][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,606][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T03:05:16,608][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,609][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,610][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,611][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,613][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T03:05:16,614][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,615][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,616][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T03:05:16,617][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T03:05:16,619][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,620][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T03:05:17,853][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T03:05:18,122][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T03:05:19,094][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T03:05:20,124][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T03:05:20,132][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T03:05:20,273][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T03:05:20,276][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T03:05:20,608][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T03:05:23,623][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T03:05:23,826][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 147, version: 4238, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T03:05:24,110][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 147, version: 4238, reason: Publication{term=147, version=4238}
[2022-03-29T03:05:24,350][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T03:05:24,354][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T03:07:09,953][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T03:07:10,008][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T03:07:10,009][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T03:07:16,336][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T03:07:16,339][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T03:07:16,340][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T03:07:16,341][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T03:07:16,343][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T03:07:16,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T03:07:16,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T03:07:16,346][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T03:07:16,347][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T03:07:16,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T03:07:16,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T03:07:16,349][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T03:07:16,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T03:07:16,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T03:07:16,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T03:07:16,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T03:07:16,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T03:07:16,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T03:07:16,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T03:07:16,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T03:07:16,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T03:07:16,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T03:07:16,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T03:07:16,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T03:07:16,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T03:07:16,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T03:07:16,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T03:07:16,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T03:07:16,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T03:07:16,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T03:07:16,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T03:07:16,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T03:07:16,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T03:07:16,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T03:07:16,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T03:07:16,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T03:07:16,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T03:07:16,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T03:07:16,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T03:07:16,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T03:07:16,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T03:07:16,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T03:07:16,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T03:07:16,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T03:07:16,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T03:07:16,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T03:07:16,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T03:07:16,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T03:07:16,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T03:07:16,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T03:07:16,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T03:07:16,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T03:07:16,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T03:07:16,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T03:07:16,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T03:07:16,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T03:07:16,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T03:07:16,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T03:07:16,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T03:07:16,489][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T03:07:16,490][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T03:07:16,875][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T03:07:31,606][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T03:07:31,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T03:07:34,015][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T03:07:36,314][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T03:07:38,729][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T03:07:43,741][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T03:07:43,748][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T03:07:43,885][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T03:07:43,887][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T03:07:44,537][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T03:08:02,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5859ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:08:57,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@398115be] took [11029ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:09:21,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [7242ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:09:42,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [10806ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:05,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:28,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [6149ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:25,638][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9597ms] which is above the warn threshold of [5s]
[2022-03-29T03:10:45,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [9606ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:01,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5484ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:12,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:26,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5808ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:13:01,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [38032ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:13:38,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [5961ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:16:14,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [40546ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:19:59,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [83485ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:19:50,145][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [94481ms] which is above the warn threshold of [5s]
[2022-03-29T03:20:37,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [6396ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:21:37,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [42602ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:22:18,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [12325ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:23:06,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [30931ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:24:32,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [23687ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:24:09,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:24:05,332][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26489ms] which is above the warn threshold of [5s]
[2022-03-29T03:25:03,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11161483891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:17,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79343124036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:42,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:38,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [79343ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:25:50,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24763936888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:02,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:02,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [19946ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:26:12,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19946505707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:25,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:35,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22695058496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:44,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:50,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [43491ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:26:53,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20796434931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:01,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [16083ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:00,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:10,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16083290578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:20,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:23,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [19569ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:31,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19569527119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:47,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18675149083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:57,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:59,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [36440ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:07,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17765484221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:18,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:18,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [20201ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:27,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20201923492ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:31,665][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20202ms] which is above the warn threshold of [5s]
[2022-03-29T03:28:37,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:48,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20314416752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:50,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [20314ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:56,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:04,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19405727056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:13,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:20,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [16199ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:29:18,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199006411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:27,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:34,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14607565815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:40,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:44,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [13883ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:29:49,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13883279594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:56,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:02,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713177217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:07,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:07,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [11672ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:13,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11672257278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:22,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [13445ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:21,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:25,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13445878624ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:31,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:35,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [10105ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:35,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105106284ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:50,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:58,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031601477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:10,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:20,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20637983234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:22,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [20637ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:31:38,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:00,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26049946844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:21,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:39,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472240626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:54,990][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [41472ms] which is above the warn threshold of [5s]
[2022-03-29T03:33:01,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:16,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [82028ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:33:27,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40556092909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:38,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:52,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39247869018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:05,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:16,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25870121889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:29,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:39,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [49280ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:34:39,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23409927622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:53,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:55,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [24766ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:35:04,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24766154007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:18,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:29,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25961384899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:38,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:47,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [45986ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:35:46,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20024870484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:01,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:03,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@398115be] took [22584ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:36:16,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22584537964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:33,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:46,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30082484704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:01,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:19,364][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28500ms] which is above the warn threshold of [5s]
[2022-03-29T03:37:19,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28500744792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:37,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:47,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [65236ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:37:59,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36735996836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:17,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:19,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [38450ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:38:39,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38450590497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:53,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:10,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39023656366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:19,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [39023ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:39:33,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:51,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37382014529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:16,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:30,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43710863222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:47,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:59,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [73192ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:41:04,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29482028140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:28,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:40,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40986691765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:53,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:53,294][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40987ms] which is above the warn threshold of [5s]
[2022-03-29T03:42:07,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27148630597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:23,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:42,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [56030ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:42:37,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28882264745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:57,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:43:12,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35067403308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:43:29,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:00,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30558250915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:14,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [30558ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:44:22,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:40,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53907127544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:00,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:16,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38762859207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:35,151][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27834ms] which is above the warn threshold of [5s]
[2022-03-29T03:45:41,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [66597ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:45:42,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27834696628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:58,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [29621ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:45:57,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:11,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29621062502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:25,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:27,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [27311ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:46:38,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27311254743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:51,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:05,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26629393927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:18,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [26629ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:47:21,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:37,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29213070582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [31681ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:47:53,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:09,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31681459836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:27,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:40,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32890472264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:54,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:33,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [61472ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:50:38,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28582525458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:52,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:57,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [119075ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:51:04,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119075138476ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:51:16,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:51:16,565][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23023ms] which is above the warn threshold of [5s]
[2022-03-29T03:51:50,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23022818511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:54:30,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:56:03,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [182378ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:57:21,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182378665365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:00:40,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [352800ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:00:48,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:05:07,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352800395322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:08:50,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/484518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:11:04,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [484849ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:12:54,422][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [484849ms] which is above the warn threshold of [5s]
[2022-03-29T04:13:03,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/484849088500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:17:13,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:21:08,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525459272172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:25:37,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:25:39,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [503200ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:21,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T04:28:43,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503200862111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:32:14,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:35:19,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399200492532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:38:29,619][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [399200ms] which is above the warn threshold of [5s]
[2022-03-29T04:38:36,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:40:41,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [382448ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:42:09,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382448855253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:45:20,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:45:53,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [404738ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:48:05,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404738509552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:50:47,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:53:34,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326522723966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:56:50,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:57:45,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [361595ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:59:53,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361595940180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:02:55,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367708ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:03:00,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@731a4124, interval=1m}] took [367838ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:05:58,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367838217160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:09:01,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:10:42,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [357487ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:12:11,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357487506339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:04:45,476][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [367839ms] which is above the warn threshold of [5s]
[2022-03-29T05:16:57,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:20:08,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481305887369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:23:11,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:24:54,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [372270ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:26:18,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372270076985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:29:40,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/385176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:30:10,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [384905ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:59:28,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T05:32:36,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384905563778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:35:27,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:35:36,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [354537ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:38:14,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354537806701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:41:14,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:44:38,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342536657470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:44:54,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [342536ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:46:07,973][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [342537ms] which is above the warn threshold of [5s]
[2022-03-29T05:48:18,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:51:23,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426527140739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:55:22,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:58:11,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [415449ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:59:17,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415449313320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:02:01,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:05:41,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407743001912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:10:41,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:16:37,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [481674ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:19:00,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481674227636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:19:24,092][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [481674ms] which is above the warn threshold of [5s]
[2022-03-29T06:23:08,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7m/764779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:23:52,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [764703ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:26:48,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7m/764703805274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:32:18,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/551987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:35:40,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/551848255984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:39:10,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:05:56,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T06:43:06,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416961644651ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:47:47,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:47:51,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [514544ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:49:52,733][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [514545ms] which is above the warn threshold of [5s]
[2022-03-29T06:52:49,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514544217224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:05:32,659][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T07:05:32,709][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T07:05:32,710][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T07:05:39,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T07:05:39,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T07:05:39,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T07:05:39,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T07:05:39,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T07:05:39,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T07:05:39,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T07:05:39,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T07:05:39,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T07:05:39,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T07:05:39,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T07:05:39,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T07:05:39,748][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T07:05:39,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T07:05:39,751][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T07:05:39,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T07:05:39,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T07:05:39,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T07:05:39,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T07:05:39,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T07:05:39,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T07:05:39,759][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T07:05:39,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T07:05:39,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T07:05:39,762][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T07:05:39,763][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T07:05:39,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T07:05:39,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T07:05:39,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T07:05:39,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T07:05:39,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T07:05:39,770][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T07:05:39,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T07:05:39,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T07:05:39,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T07:05:39,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T07:05:39,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T07:05:39,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T07:05:39,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T07:05:39,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T07:05:39,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T07:05:39,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T07:05:39,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T07:05:39,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T07:05:39,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T07:05:39,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T07:05:39,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T07:05:39,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T07:05:39,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T07:05:39,788][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T07:05:39,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T07:05:39,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T07:05:39,790][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T07:05:39,791][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T07:05:39,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T07:05:39,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T07:05:39,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T07:05:39,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T07:05:39,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T07:05:39,888][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T07:05:39,889][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T07:05:40,424][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T07:05:53,019][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T07:05:53,023][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T07:05:54,795][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T07:05:55,031][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T07:05:56,442][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T07:05:57,819][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T07:05:57,820][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T07:05:57,947][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T07:05:57,949][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T07:05:58,262][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T07:06:00,569][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T07:06:00,732][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 148, version: 4240, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T07:06:00,943][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 148, version: 4240, reason: Publication{term=148, version=4240}
[2022-03-29T07:06:01,069][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T07:06:01,069][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T07:06:02,599][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T07:06:02,618][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T07:06:04,154][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T07:06:04,162][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T07:07:15,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [47233ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:07:34,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:07:44,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7505454611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:07:48,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:07:51,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17331856262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:00,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:02,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12032651436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:05,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:08,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5187547000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:12,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:18,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7335549680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:22,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:25,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9262079576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:28,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:30,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6140802228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:43,009][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=148, version=4246}] took [2.6m] which is above the warn threshold of [30s]: [running task [Publication{term=148, version=4246}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [2663ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [12ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [29ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1980@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [24ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@7c05388] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bec08@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [82ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [78ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [20ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [107ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [23ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [23ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [1ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [9ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [2ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [1ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [154752ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-03-29T07:08:43,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@4c8ef8e9] took [72894ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:08:50,300][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.encodeChangeCipherSpec(SSLSocketOutputRecord.java:233) ~[?:?]
	at sun.security.ssl.OutputRecord.changeWriteCiphers(OutputRecord.java:182) ~[?:?]
	at sun.security.ssl.ChangeCipherSpec$T10ChangeCipherSpecProducer.produce(ChangeCipherSpec.java:118) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.onProduceFinished(Finished.java:395) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.produce(Finished.java:379) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.ServerHelloDone$ServerHelloDoneConsumer.consume(ServerHelloDone.java:182) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T07:09:02,279][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][30] overhead, spent [693ms] collecting in the last [2.3s]
[2022-03-29T07:09:13,864][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T07:09:16,214][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T07:09:16,496][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T07:09:24,842][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T07:09:24,842][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T07:09:31,233][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T07:09:45,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5377ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:09:46,254][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38468}] took [11184ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:09:46,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][55][15] duration [3.1s], collections [1]/[6.4s], total [3.1s]/[5.1s], memory [191.7mb]->[132.1mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [101.9mb]->[106.4mb]/[2gb]}{[survivor] [9.7mb]->[9.6mb]/[0b]}
[2022-03-29T07:09:46,750][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][55] overhead, spent [3.1s] collecting in the last [6.4s]
[2022-03-29T07:10:02,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:10:02,497][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][61][16] duration [4.1s], collections [1]/[1.3s], total [4.1s]/[9.2s], memory [160.1mb]->[184.1mb]/[2gb], all_pools {[young] [52mb]->[80mb]/[0b]}{[old] [106.4mb]->[106.4mb]/[2gb]}{[survivor] [9.6mb]->[9.6mb]/[0b]}
[2022-03-29T07:10:03,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5751711551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:10:03,630][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][61] overhead, spent [4.1s] collecting in the last [1.3s]
[2022-03-29T07:10:04,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6151ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:10:20,723][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38464}] took [32821ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:10:20,560][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38466}] took [45292ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:10:54,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5627ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:11:11,831][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [43112ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [30] unchanged indices
[2022-03-29T07:11:16,711][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [50.3s] publication of cluster state version [4257] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T07:11:19,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@73030288] took [18185ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:11:43,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7511735217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,508][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][87][17] duration [4.5s], collections [1]/[9.1s], total [4.5s]/[13.7s], memory [202.2mb]->[128.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [111.6mb]->[115.1mb]/[2gb]}{[survivor] [10.5mb]->[13.7mb]/[0b]}
[2022-03-29T07:11:44,642][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][87] overhead, spent [4.5s] collecting in the last [9.1s]
[2022-03-29T07:11:44,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7261323415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,935][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39942}] took [7261ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:11:46,759][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16464ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T07:11:46,961][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.6s] publication of cluster state version [4258] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T07:11:52,251][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [540ms] collecting in the last [1.5s]
[2022-03-29T07:11:57,352][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [996ms] collecting in the last [1s]
[2022-03-29T07:11:59,776][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [285ms] collecting in the last [1s]
[2022-03-29T07:12:01,447][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [491ms] collecting in the last [1.4s]
[2022-03-29T07:12:16,082][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][110][25] duration [747ms], collections [1]/[1.8s], total [747ms]/[17.2s], memory [233mb]->[170.3mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [151mb]->[158.6mb]/[2gb]}{[survivor] [14mb]->[11.7mb]/[0b]}
[2022-03-29T07:12:16,526][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][110] overhead, spent [747ms] collecting in the last [1.8s]
[2022-03-29T07:12:20,480][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][26] duration [899ms], collections [1]/[2.2s], total [899ms]/[18.1s], memory [222.3mb]->[173.2mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [158.6mb]->[165.2mb]/[2gb]}{[survivor] [11.7mb]->[8mb]/[0b]}
[2022-03-29T07:12:21,925][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [899ms] collecting in the last [2.2s]
[2022-03-29T07:12:25,384][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][114][27] duration [802ms], collections [1]/[1.7s], total [802ms]/[18.9s], memory [233.2mb]->[174.7mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [165.2mb]->[165.2mb]/[2gb]}{[survivor] [8mb]->[9.5mb]/[0b]}
[2022-03-29T07:12:25,567][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][114] overhead, spent [802ms] collecting in the last [1.7s]
[2022-03-29T07:12:25,752][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [.kibana-event-log-7.16.2-000001][0], [logstash-2022.03.13][0]]]).
[2022-03-29T07:12:37,833][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:12:39,689][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:12:42,302][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [481ms] collecting in the last [1.4s]
[2022-03-29T07:12:43,814][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:12:45,484][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][129] overhead, spent [374ms] collecting in the last [1s]
[2022-03-29T07:12:52,334][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][133] overhead, spent [671ms] collecting in the last [1.1s]
[2022-03-29T07:12:56,227][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][134][34] duration [1.6s], collections [1]/[6s], total [1.6s]/[22.6s], memory [266mb]->[231.9mb]/[2gb], all_pools {[young] [0b]->[52mb]/[0b]}{[old] [172.1mb]->[172.1mb]/[2gb]}{[survivor] [6.7mb]->[7.7mb]/[0b]}
[2022-03-29T07:12:56,431][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][134] overhead, spent [1.6s] collecting in the last [6s]
[2022-03-29T07:13:12,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][36] duration [1.4s], collections [1]/[2.7s], total [1.4s]/[24.2s], memory [255.6mb]->[192.7mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [172.1mb]->[172.1mb]/[2gb]}{[survivor] [7.4mb]->[8.6mb]/[0b]}
[2022-03-29T07:13:12,722][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [1.4s] collecting in the last [2.7s]
[2022-03-29T07:13:36,310][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][156][38] duration [1.1s], collections [1]/[2.5s], total [1.1s]/[25.9s], memory [247.8mb]->[182.4mb]/[2gb], all_pools {[young] [68mb]->[16mb]/[0b]}{[old] [172.1mb]->[172.1mb]/[2gb]}{[survivor] [7.6mb]->[10.3mb]/[0b]}
[2022-03-29T07:13:36,580][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][156] overhead, spent [1.1s] collecting in the last [2.5s]
[2022-03-29T07:13:38,111][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A29032022%3Acount%3Adelay_threshold_exceeded?refresh=wait_for&require_alias=true&_source=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38520}] took [7056ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:13:44,389][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][159][39] duration [2.9s], collections [1]/[5s], total [2.9s]/[28.8s], memory [250.4mb]->[183.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [172.1mb]->[173.2mb]/[2gb]}{[survivor] [10.3mb]->[10.4mb]/[0b]}
[2022-03-29T07:13:44,732][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][159] overhead, spent [2.9s] collecting in the last [5s]
[2022-03-29T07:13:44,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5230ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:13:54,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [5120ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:13:55,773][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:13:55,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][40] duration [2.4s], collections [1]/[6.5s], total [2.4s]/[31.3s], memory [235.6mb]->[203.4mb]/[2gb], all_pools {[young] [56mb]->[24mb]/[0b]}{[old] [173.2mb]->[175.4mb]/[2gb]}{[survivor] [10.4mb]->[8mb]/[0b]}
[2022-03-29T07:13:55,923][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [2.4s] collecting in the last [6.5s]
[2022-03-29T07:14:01,683][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:14:01,935][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][41] duration [3.7s], collections [1]/[5.8s], total [3.7s]/[35s], memory [203.4mb]->[183mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [175.4mb]->[175.4mb]/[2gb]}{[survivor] [8mb]->[7.5mb]/[0b]}
[2022-03-29T07:14:02,318][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [3.7s] collecting in the last [5.8s]
[2022-03-29T07:14:03,186][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.6s/14683ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T07:14:14,886][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.4s] publication of cluster state version [4272] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T07:14:18,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][173][42] duration [1.5s], collections [1]/[2.8s], total [1.5s]/[36.6s], memory [235mb]->[182.2mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [175.4mb]->[175.4mb]/[2gb]}{[survivor] [7.5mb]->[6.8mb]/[0b]}
[2022-03-29T07:14:18,522][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][173] overhead, spent [1.5s] collecting in the last [2.8s]
[2022-03-29T07:14:21,189][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][174][43] duration [889ms], collections [1]/[3.1s], total [889ms]/[37.5s], memory [182.2mb]->[182.6mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [175.4mb]->[175.4mb]/[2gb]}{[survivor] [6.8mb]->[7.1mb]/[0b]}
[2022-03-29T07:14:21,391][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][174] overhead, spent [889ms] collecting in the last [3.1s]
[2022-03-29T07:14:30,639][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:14:45,349][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][191] overhead, spent [659ms] collecting in the last [1s]
[2022-03-29T07:14:55,625][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][48] duration [1.1s], collections [1]/[3.2s], total [1.1s]/[39.8s], memory [234.4mb]->[185.6mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [5.3mb]->[8.5mb]/[0b]}
[2022-03-29T07:14:55,866][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [1.1s] collecting in the last [3.2s]
[2022-03-29T07:15:06,400][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][203][49] duration [1.8s], collections [1]/[3.1s], total [1.8s]/[41.7s], memory [265.6mb]->[185.1mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [8.5mb]->[8mb]/[0b]}
[2022-03-29T07:15:06,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][203] overhead, spent [1.8s] collecting in the last [3.1s]
[2022-03-29T07:15:14,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:15,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5827491514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:15,104][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][50] duration [4.6s], collections [1]/[2s], total [4.6s]/[46.3s], memory [185.1mb]->[273.1mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [8mb]->[9.5mb]/[0b]}
[2022-03-29T07:15:15,801][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [4.6s] collecting in the last [2s]
[2022-03-29T07:15:18,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [10351ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:15:32,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:32,803][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][207][51] duration [3.7s], collections [1]/[1.6s], total [3.7s]/[50s], memory [190.6mb]->[206.6mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [9.5mb]->[9.5mb]/[0b]}
[2022-03-29T07:15:32,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5628212958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:32,827][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][207] overhead, spent [3.7s] collecting in the last [1.6s]
[2022-03-29T07:15:32,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [8036ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:15:47,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][214][52] duration [2.8s], collections [1]/[4.7s], total [2.8s]/[52.9s], memory [249.9mb]->[204.9mb]/[2gb], all_pools {[young] [88mb]->[20mb]/[0b]}{[old] [177.9mb]->[177.9mb]/[2gb]}{[survivor] [8mb]->[10.9mb]/[0b]}
[2022-03-29T07:15:48,101][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][214] overhead, spent [2.8s] collecting in the last [4.7s]
[2022-03-29T07:15:48,635][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:15:48,820][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.1s/13197ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T07:15:59,278][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][217][53] duration [2.8s], collections [1]/[4.1s], total [2.8s]/[55.7s], memory [268.9mb]->[268.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [177.9mb]->[180.8mb]/[2gb]}{[survivor] [10.9mb]->[4.9mb]/[0b]}
[2022-03-29T07:15:59,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][217] overhead, spent [2.8s] collecting in the last [4.1s]
[2022-03-29T07:15:59,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5137ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:16:18,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6101ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:16:28,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:16:28,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [6960ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:16:29,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5759648029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:16:29,609][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][224][54] duration [3.4s], collections [1]/[17.5s], total [3.4s]/[59.2s], memory [253.8mb]->[226.4mb]/[2gb], all_pools {[young] [84mb]->[40mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [4.9mb]->[5.6mb]/[0b]}
[2022-03-29T07:16:45,613][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][230][55] duration [3.3s], collections [1]/[6.3s], total [3.3s]/[1m], memory [246.4mb]->[186.9mb]/[2gb], all_pools {[young] [60mb]->[28mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [5.6mb]->[6.1mb]/[0b]}
[2022-03-29T07:16:46,319][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][230] overhead, spent [3.3s] collecting in the last [6.3s]
[2022-03-29T07:16:49,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9672ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:17:09,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:17:10,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][236][56] duration [3.3s], collections [1]/[6.3s], total [3.3s]/[1m], memory [246.9mb]->[188.1mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [6.1mb]->[7.2mb]/[0b]}
[2022-03-29T07:17:10,260][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][236] overhead, spent [3.3s] collecting in the last [6.3s]
[2022-03-29T07:17:10,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5515632988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:17:10,218][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39956}] took [5916ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:17:12,851][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][237][57] duration [1.2s], collections [1]/[2.7s], total [1.2s]/[1.1m], memory [188.1mb]->[189.4mb]/[2gb], all_pools {[young] [4mb]->[4mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [7.2mb]->[8.6mb]/[0b]}
[2022-03-29T07:17:12,998][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][237] overhead, spent [1.2s] collecting in the last [2.7s]
[2022-03-29T07:17:17,750][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][238][58] duration [1.9s], collections [1]/[1.4s], total [1.9s]/[1.1m], memory [189.4mb]->[265.4mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [8.6mb]->[8.6mb]/[0b]}
[2022-03-29T07:17:18,333][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][238] overhead, spent [1.9s] collecting in the last [1.4s]
[2022-03-29T07:17:22,651][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][240][60] duration [1s], collections [1]/[1.2s], total [1s]/[1.1m], memory [190.9mb]->[189.1mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [8.8mb]->[7mb]/[0b]}
[2022-03-29T07:17:22,993][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][240] overhead, spent [1s] collecting in the last [1.2s]
[2022-03-29T07:17:30,588][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][245][61] duration [1.2s], collections [1]/[2.5s], total [1.2s]/[1.1m], memory [273.1mb]->[191.3mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7mb]->[5.2mb]/[0b]}
[2022-03-29T07:17:31,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][245] overhead, spent [1.2s] collecting in the last [2.5s]
[2022-03-29T07:17:34,090][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:17:41,181][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][252][62] duration [889ms], collections [1]/[2s], total [889ms]/[1.2m], memory [247.3mb]->[189mb]/[2gb], all_pools {[young] [60mb]->[20mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [5.2mb]->[6.8mb]/[0b]}
[2022-03-29T07:17:41,427][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][252] overhead, spent [889ms] collecting in the last [2s]
[2022-03-29T07:17:45,824][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][253][63] duration [1.6s], collections [1]/[4.2s], total [1.6s]/[1.2m], memory [189mb]->[210.1mb]/[2gb], all_pools {[young] [20mb]->[20mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [6.8mb]->[8mb]/[0b]}
[2022-03-29T07:17:46,473][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][253] overhead, spent [1.6s] collecting in the last [4.2s]
[2022-03-29T07:17:53,565][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][256][64] duration [1.7s], collections [1]/[3.3s], total [1.7s]/[1.2m], memory [214.1mb]->[188.1mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [8mb]->[6mb]/[0b]}
[2022-03-29T07:17:53,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][256] overhead, spent [1.7s] collecting in the last [3.3s]
[2022-03-29T07:18:01,130][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][260][66] duration [797ms], collections [1]/[2.5s], total [797ms]/[1.2m], memory [264.1mb]->[190.1mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7.9mb]->[8mb]/[0b]}
[2022-03-29T07:18:01,304][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][260] overhead, spent [797ms] collecting in the last [2.5s]
[2022-03-29T07:18:09,312][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][264][67] duration [1.4s], collections [1]/[3.2s], total [1.4s]/[1.3m], memory [226.1mb]->[189.9mb]/[2gb], all_pools {[young] [40mb]->[8mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [8mb]->[7.8mb]/[0b]}
[2022-03-29T07:18:09,504][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][264] overhead, spent [1.4s] collecting in the last [3.2s]
[2022-03-29T07:18:36,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6198ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:19:02,600][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39956}] took [32318ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:19:15,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [14263ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:19:39,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [11044ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:20:24,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@639ce543] took [6470ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:20:39,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [11019ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:21:21,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:21,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [30915ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:21:23,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29915261054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:25,885][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][277][70] duration [23s], collections [1]/[57.4s], total [23s]/[1.7m], memory [205.2mb]->[209.9mb]/[2gb], all_pools {[young] [20mb]->[24mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7.1mb]->[7.8mb]/[0b]}
[2022-03-29T07:21:27,127][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][277] overhead, spent [23s] collecting in the last [57.4s]
[2022-03-29T07:21:37,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][282][71] duration [2s], collections [1]/[4s], total [2s]/[1.7m], memory [233.9mb]->[191.5mb]/[2gb], all_pools {[young] [44mb]->[24mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7.8mb]->[9.3mb]/[0b]}
[2022-03-29T07:21:38,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][282] overhead, spent [2s] collecting in the last [4s]
[2022-03-29T07:21:47,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:48,149][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][284][72] duration [4.2s], collections [1]/[7.6s], total [4.2s]/[1.8m], memory [231.5mb]->[190mb]/[2gb], all_pools {[young] [40mb]->[20mb]/[0b]}{[old] [182.1mb]->[183.5mb]/[2gb]}{[survivor] [9.3mb]->[6.5mb]/[0b]}
[2022-03-29T07:21:48,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6679219441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:49,123][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][284] overhead, spent [4.2s] collecting in the last [7.6s]
[2022-03-29T07:21:56,819][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][287][73] duration [2.9s], collections [1]/[4.4s], total [2.9s]/[1.8m], memory [270mb]->[189.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.5mb]->[5.8mb]/[0b]}
[2022-03-29T07:21:57,191][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][287] overhead, spent [2.9s] collecting in the last [4.4s]
[2022-03-29T07:22:07,526][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][291][74] duration [2.8s], collections [1]/[1.4s], total [2.8s]/[1.9m], memory [265.4mb]->[277.4mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [5.8mb]->[5mb]/[0b]}
[2022-03-29T07:22:08,892][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][291] overhead, spent [2.8s] collecting in the last [1.4s]
[2022-03-29T07:22:09,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7315ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:22:18,018][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][297] overhead, spent [336ms] collecting in the last [1.1s]
[2022-03-29T07:22:22,401][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][298][76] duration [1.6s], collections [1]/[3.1s], total [1.6s]/[1.9m], memory [266.4mb]->[190.1mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.8mb]->[6.5mb]/[0b]}
[2022-03-29T07:22:22,589][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][298] overhead, spent [1.6s] collecting in the last [3.1s]
[2022-03-29T07:22:36,285][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][305][77] duration [2.1s], collections [1]/[1.5s], total [2.1s]/[1.9m], memory [246.1mb]->[188.7mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.5mb]->[5.1mb]/[0b]}
[2022-03-29T07:22:36,562][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][305] overhead, spent [2.1s] collecting in the last [1.5s]
[2022-03-29T07:22:47,255][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][310][78] duration [759ms], collections [1]/[2.6s], total [759ms]/[1.9m], memory [244.7mb]->[206mb]/[2gb], all_pools {[young] [56mb]->[24mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [5.1mb]->[6.4mb]/[0b]}
[2022-03-29T07:22:47,444][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][310] overhead, spent [759ms] collecting in the last [2.6s]
[2022-03-29T07:23:02,749][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][316][79] duration [2.9s], collections [1]/[1.5s], total [2.9s]/[2m], memory [222mb]->[226mb]/[2gb], all_pools {[young] [32mb]->[40mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-03-29T07:23:03,177][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][316] overhead, spent [2.9s] collecting in the last [1.5s]
[2022-03-29T07:23:03,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7216ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:23:25,992][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][322][80] duration [1.6s], collections [1]/[1.5s], total [1.6s]/[2m], memory [271mb]->[279mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [183.5mb]->[183.6mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-29T07:23:26,079][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][322] overhead, spent [1.6s] collecting in the last [1.5s]
[2022-03-29T07:23:31,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][325][82] duration [1.8s], collections [1]/[3.1s], total [1.8s]/[2.1m], memory [263.4mb]->[191.1mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [183.6mb]->[183.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-03-29T07:23:32,105][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][325] overhead, spent [1.8s] collecting in the last [3.1s]
[2022-03-29T07:24:21,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][359][86] duration [1.9s], collections [1]/[3.3s], total [1.9s]/[2.1m], memory [253.9mb]->[194.4mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [183.7mb]->[183.8mb]/[2gb]}{[survivor] [6.1mb]->[6.6mb]/[0b]}
[2022-03-29T07:24:21,186][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][359] overhead, spent [1.9s] collecting in the last [3.3s]
[2022-03-29T07:24:32,214][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][364][87] duration [2s], collections [1]/[3.8s], total [2s]/[2.1m], memory [258.4mb]->[191.8mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [183.8mb]->[183.8mb]/[2gb]}{[survivor] [6.6mb]->[8mb]/[0b]}
[2022-03-29T07:24:32,698][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][364] overhead, spent [2s] collecting in the last [3.8s]
[2022-03-29T07:24:41,475][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][367][88] duration [2.9s], collections [1]/[4.7s], total [2.9s]/[2.2m], memory [271.8mb]->[190.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [183.8mb]->[183.8mb]/[2gb]}{[survivor] [8mb]->[6.7mb]/[0b]}
[2022-03-29T07:24:41,908][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][367] overhead, spent [2.9s] collecting in the last [4.7s]
[2022-03-29T07:25:07,108][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][381][89] duration [1.8s], collections [1]/[1.9s], total [1.8s]/[2.2m], memory [274.6mb]->[274.6mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [183.8mb]->[183.9mb]/[2gb]}{[survivor] [6.7mb]->[4.6mb]/[0b]}
[2022-03-29T07:25:07,239][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][381] overhead, spent [1.8s] collecting in the last [1.9s]
[2022-03-29T07:25:07,539][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_transform/endpoint.metadata_*/_stats][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38466}] took [63568ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:25:09,430][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:25:13,206][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][386] overhead, spent [264ms] collecting in the last [1s]
[2022-03-29T07:25:29,546][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][395][91] duration [1.3s], collections [1]/[2.6s], total [1.3s]/[2.2m], memory [269.8mb]->[229.7mb]/[2gb], all_pools {[young] [80mb]->[40mb]/[0b]}{[old] [183.9mb]->[184mb]/[2gb]}{[survivor] [5.9mb]->[5.7mb]/[0b]}
[2022-03-29T07:25:29,747][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][395] overhead, spent [1.3s] collecting in the last [2.6s]
[2022-03-29T07:25:35,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][398][92] duration [1.5s], collections [1]/[2.7s], total [1.5s]/[2.3m], memory [241.7mb]->[192.6mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [184mb]->[184mb]/[2gb]}{[survivor] [5.7mb]->[8.5mb]/[0b]}
[2022-03-29T07:25:36,212][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][398] overhead, spent [1.5s] collecting in the last [2.7s]
[2022-03-29T07:25:41,870][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][399][93] duration [2.3s], collections [1]/[6.7s], total [2.3s]/[2.3m], memory [192.6mb]->[191.5mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [184mb]->[184mb]/[2gb]}{[survivor] [8.5mb]->[7.4mb]/[0b]}
[2022-03-29T07:25:42,135][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][399] overhead, spent [2.3s] collecting in the last [6.7s]
[2022-03-29T07:25:50,954][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][405][94] duration [1s], collections [1]/[2.6s], total [1s]/[2.3m], memory [259.5mb]->[193.2mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [184mb]->[184mb]/[2gb]}{[survivor] [7.4mb]->[9.1mb]/[0b]}
[2022-03-29T07:25:51,233][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][405] overhead, spent [1s] collecting in the last [2.6s]
[2022-03-29T07:26:38,325][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][440][100] duration [1.6s], collections [1]/[3.2s], total [1.6s]/[2.4m], memory [275.2mb]->[192mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [185.2mb]->[185.5mb]/[2gb]}{[survivor] [5.9mb]->[6.4mb]/[0b]}
[2022-03-29T07:26:38,823][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][440] overhead, spent [1.6s] collecting in the last [3.2s]
[2022-03-29T07:26:56,201][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][451][101] duration [1.6s], collections [1]/[2.8s], total [1.6s]/[2.4m], memory [276mb]->[193.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [185.5mb]->[185.5mb]/[2gb]}{[survivor] [6.4mb]->[7.8mb]/[0b]}
[2022-03-29T07:26:56,351][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][451] overhead, spent [1.6s] collecting in the last [2.8s]
[2022-03-29T07:27:00,101][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][453][102] duration [982ms], collections [1]/[2.3s], total [982ms]/[2.4m], memory [201.4mb]->[195.8mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [185.5mb]->[185.6mb]/[2gb]}{[survivor] [7.8mb]->[10.2mb]/[0b]}
[2022-03-29T07:27:00,367][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][453] overhead, spent [982ms] collecting in the last [2.3s]
[2022-03-29T07:27:02,180][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][454] overhead, spent [698ms] collecting in the last [2.2s]
[2022-03-29T07:27:10,679][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][460][104] duration [780ms], collections [1]/[1.5s], total [780ms]/[2.4m], memory [278mb]->[194.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [186.4mb]->[186.4mb]/[2gb]}{[survivor] [7.5mb]->[7.9mb]/[0b]}
[2022-03-29T07:27:10,798][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][460] overhead, spent [780ms] collecting in the last [1.5s]
[2022-03-29T07:27:48,710][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:54228}] took [5852ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:27:53,526][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][484][107] duration [2.5s], collections [1]/[5.5s], total [2.5s]/[2.5m], memory [278.4mb]->[192.8mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [186.9mb]->[187.9mb]/[2gb]}{[survivor] [7.4mb]->[4.8mb]/[0b]}
[2022-03-29T07:27:53,663][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][484] overhead, spent [2.5s] collecting in the last [5.5s]
[2022-03-29T07:28:11,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][492][108] duration [3.8s], collections [1]/[6.3s], total [3.8s]/[2.6m], memory [236.8mb]->[194.6mb]/[2gb], all_pools {[young] [44mb]->[4mb]/[0b]}{[old] [187.9mb]->[187.9mb]/[2gb]}{[survivor] [4.8mb]->[6.6mb]/[0b]}
[2022-03-29T07:28:12,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][492] overhead, spent [3.8s] collecting in the last [6.3s]
[2022-03-29T07:28:14,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7712ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:28:31,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:28:31,900][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40062}] took [7938ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:28:32,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7137852621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:28:32,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][498][109] duration [5.7s], collections [1]/[7.7s], total [5.7s]/[2.7m], memory [266.6mb]->[197.6mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [187.9mb]->[188mb]/[2gb]}{[survivor] [6.6mb]->[5.6mb]/[0b]}
[2022-03-29T07:28:32,380][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][498] overhead, spent [5.7s] collecting in the last [7.7s]
[2022-03-29T07:28:32,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7137ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:28:49,670][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][510] overhead, spent [625ms] collecting in the last [1.6s]
[2022-03-29T07:29:29,563][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][541][113] duration [1.3s], collections [1]/[2.6s], total [1.3s]/[2.7m], memory [274.2mb]->[194.8mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [188mb]->[188mb]/[2gb]}{[survivor] [6.1mb]->[6.7mb]/[0b]}
[2022-03-29T07:29:30,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][541] overhead, spent [1.3s] collecting in the last [2.6s]
[2022-03-29T07:29:36,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:29:37,272][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40062}] took [6069ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:29:37,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5269062366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:29:39,012][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][542][114] duration [3.9s], collections [1]/[2.8s], total [3.9s]/[2.8m], memory [194.8mb]->[282.8mb]/[2gb], all_pools {[young] [16mb]->[12mb]/[0b]}{[old] [188mb]->[188mb]/[2gb]}{[survivor] [6.7mb]->[6.4mb]/[0b]}
[2022-03-29T07:29:39,746][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][542] overhead, spent [3.9s] collecting in the last [2.8s]
[2022-03-29T07:29:39,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [8705ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:29:38,976][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6270ms] which is above the warn threshold of [5s]
[2022-03-29T07:30:06,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@59aaf69e] took [11406ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:30:18,265][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][553][115] duration [2.8s], collections [1]/[5.1s], total [2.8s]/[2.8m], memory [262.5mb]->[210.7mb]/[2gb], all_pools {[young] [84mb]->[36mb]/[0b]}{[old] [188mb]->[188.3mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-03-29T07:30:18,296][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:54250}] took [6118ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:30:18,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][553] overhead, spent [2.8s] collecting in the last [5.1s]
[2022-03-29T07:30:28,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][558][116] duration [2.2s], collections [1]/[3.6s], total [2.2s]/[2.8m], memory [238.7mb]->[198.6mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [6.4mb]->[6.2mb]/[0b]}
[2022-03-29T07:30:29,229][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][558] overhead, spent [2.2s] collecting in the last [3.6s]
[2022-03-29T07:30:51,447][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40110}] took [5577ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:31:09,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [5816ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:31:19,019][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40110}] took [7875ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:31:31,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [12813ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:09,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:25,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9521ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:41,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9169ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:51,793][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40062}] took [5477ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:33:35,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:33:35,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [39876ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:33:37,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39026060035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:33:47,976][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][573][117] duration [30s], collections [1]/[1m], total [30s]/[3.3m], memory [274.6mb]->[222.6mb]/[2gb], all_pools {[young] [80mb]->[56mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [6.2mb]->[6.2mb]/[0b]}
[2022-03-29T07:33:55,024][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][573] overhead, spent [30s] collecting in the last [1m]
[2022-03-29T07:34:02,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [27460ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:34:31,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:34:38,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7704770514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:34:43,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:34:51,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [11942ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:34:52,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11942608871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:01,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:11,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18325469405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:14,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [18325ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:35:37,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:42,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35742185185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:47,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9875ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:56,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9875306828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:02,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:04,161][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38628}] took [25042ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:36:17,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15166584671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:22,799][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=100][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38700}] took [25042ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:36:31,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [28310ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:36:33,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:44,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28310510904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:56,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:07,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25840587903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:31,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:43,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34728158666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:57,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:38:08,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25714824231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:38:37,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [25714ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:38:47,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:38:59,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48398127368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:39:26,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:39:38,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40898842282ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:39:56,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29169ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:07,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29169325018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:24,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:36,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28261342791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:47,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23441ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:01,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23440873809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:37,992][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28261ms] which is above the warn threshold of [5s]
[2022-03-29T07:41:17,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:27,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [52738ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:41:27,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29297210155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:40,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23218ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:43,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [23217ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:41:51,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23217537548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:42:02,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:42:21,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21340543286ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:19,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:24,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138418817371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:28,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:22,130][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2156] timed out after [99941ms]
[2022-03-29T07:44:34,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8602048987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:36,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:36,680][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][578][118] duration [1.3m], collections [1]/[3.8m], total [1.3m]/[4.7m], memory [262.6mb]->[212.3mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [6.2mb]->[8mb]/[0b]}
[2022-03-29T07:44:41,409][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][578] overhead, spent [1.3m] collecting in the last [3.8m]
[2022-03-29T07:44:40,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8979344890ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:44,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:44,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [8979ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:44:35,931][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11.6m/696632ms] ago, timed out [9.9m/596691ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2156]
[2022-03-29T07:44:48,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7792705073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:53,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:56,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7757070612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:00,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [7731ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:00,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:05,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7731644947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:09,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:16,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9271146047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:08,057][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7732ms] which is above the warn threshold of [5s]
[2022-03-29T07:45:21,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:21,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [11364ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:25,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11364707484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:30,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:35,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8936818156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:36,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@7297343e] took [8936ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:39,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:43,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421263256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:44,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@69cd45e3, interval=1m}] took [9421ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:47,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:52,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8243183524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:59,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:01,392][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_monitoring/bulk?system_id=kibana&system_api_version=7&interval=10000ms][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38694}] took [11293ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:03,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11293535437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:05,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6810ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:05,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:08,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6810526296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:13,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:16,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7270372278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:08,046][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2285] timed out after [35769ms]
[2022-03-29T07:46:20,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [14149ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:19,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:24,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6879048073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:30,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:33,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9671282857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:35,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9671ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:36,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:38,169][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/67131ms] ago, timed out [31.3s/31362ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2285]
[2022-03-29T07:46:39,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7541861527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:44,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:49,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7507477226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:55,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:01,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10533671631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:07,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [10533ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:47:07,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:15,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12240162256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:20,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:26,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@61d90192] took [13336ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:47:25,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13336896820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:30,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:36,204][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][586][119] duration [53.3s], collections [1]/[43.9s], total [53.3s]/[5.6m], memory [256.3mb]->[280.3mb]/[2gb], all_pools {[young] [60mb]->[8mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [8mb]->[6.8mb]/[0b]}
[2022-03-29T07:48:35,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9670705647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:37,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:38,332][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][586] overhead, spent [53.3s] collecting in the last [43.9s]
[2022-03-29T07:48:43,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67205262355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:44,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [76875ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:48:50,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:57,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572251730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:02,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:07,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11995468373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:16,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:18,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [13909ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:49:24,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13909033218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:32,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:36,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [14686ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:49:43,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14686141112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:51,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:57,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@692a4f58] took [20886ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:49:59,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20886574803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:04,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:11,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12735738521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:18,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:25,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891789933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:30,576][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38696}] took [26628ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:50:32,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:38,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14033471915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:48,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:56,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12725474984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:00,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [26758ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:51:05,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19974ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:05,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [19974ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:51:16,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19974013283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:23,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:31,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17898220188ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:41,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:48,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18037105202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:33,690][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2385] timed out after [91259ms]
[2022-03-29T07:51:55,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:06,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14178356488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:16,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:21,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [33415ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:52:23,141][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38708}] took [33415ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:52:22,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19236858736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:32,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:39,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17631929253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:50,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:57,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18027436508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:04,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:12,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14655507473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:21,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:27,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15968847691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:33,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:35,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [28885ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:53:40,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12916261873ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:48,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:50,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@44375fb2] took [14477ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:53:53,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14477496695ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:59,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:02,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [11320ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:54:07,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320224802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:11,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:14,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11890051372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:17,693][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38684}] took [23210ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:54:20,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:25,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9350ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:54:29,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9350397501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:15,457][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.3m/259599ms] ago, timed out [2.8m/168340ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2385]
[2022-03-29T07:54:33,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:41,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13059267599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:48,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:54,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15063832894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:05,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [30753ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:55:05,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:11,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15689946309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:18,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:25,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@7ad0ca0a] took [13604ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:55:24,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13604585799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:30,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:34,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12208729163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:39,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:43,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7746847389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:40,393][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7746ms] which is above the warn threshold of [5s]
[2022-03-29T07:55:50,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:54,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12325257436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:57,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [12325ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:55:58,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:05,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8159140907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:13,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:20,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14371681794ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:32,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18447191323ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:48,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [18447ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:56:51,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:00,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20121145144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:07,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:10,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [16278ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:57:14,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16278975176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:22,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:02,710][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2493] timed out after [40440ms]
[2022-03-29T07:57:29,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13745966369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:37,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:46,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15787310759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:55,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:58:06,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17855445456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:58:10,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [33642ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:58:18,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:59:27,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23568470925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:01:10,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/170420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:03:46,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/170076598377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:06:32,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:09:12,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321529498715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:12:59,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:16:28,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386036560177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:20:06,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:23:06,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422426985185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:26:53,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/412337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:30:33,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/411911925578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:30:10,458][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.5s/21570ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2bd125f7]], which exceeds the warn threshold of [10s]
[2022-03-29T08:33:41,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:37:31,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/395053021612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:37:03,735][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.5s/10530ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2bd125f7]], which exceeds the warn threshold of [10s]
[2022-03-29T08:32:22,854][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [411912ms] which is above the warn threshold of [5s]
[2022-03-29T08:40:58,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/450979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:42:46,017][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.1m/66576ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@43cd1a8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@648c3950]], which exceeds the warn threshold of [10s]
[2022-03-29T08:44:28,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/450974927741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:47:53,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/414313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:46:09,397][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.3s/17332ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@43cd1a8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@648c3950]], which exceeds the warn threshold of [10s]
[2022-03-29T08:52:20,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/414329269587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:53:26,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [414329ms] which is above the warn threshold of [5000ms]
[2022-03-29T08:53:12,056][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38530}] took [865304ms] which is above the warn threshold of [5000ms]
[2022-03-29T08:55:22,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:55:06,056][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [52.5m/3152955ms] ago, timed out [51.8m/3112515ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2493]
[2022-03-29T08:58:27,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449701722167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:01:36,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372684ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:02:35,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@436b2eef] took [372685ms] which is above the warn threshold of [5000ms]
[2022-03-29T09:04:55,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372685985099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:16:59,872][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T09:16:59,889][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T09:16:59,891][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T09:17:05,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T09:17:05,792][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T09:17:05,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T09:17:05,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T09:17:05,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T09:17:05,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T09:17:05,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T09:17:05,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T09:17:05,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T09:17:05,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T09:17:05,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T09:17:05,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T09:17:05,800][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T09:17:05,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T09:17:05,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T09:17:05,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T09:17:05,803][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T09:17:05,804][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T09:17:05,804][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T09:17:05,805][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T09:17:05,806][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T09:17:05,806][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T09:17:05,807][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T09:17:05,808][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T09:17:05,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T09:17:05,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T09:17:05,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T09:17:05,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T09:17:05,812][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T09:17:05,813][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T09:17:05,813][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T09:17:05,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T09:17:05,815][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T09:17:05,815][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T09:17:05,816][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T09:17:05,817][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T09:17:05,817][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T09:17:05,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T09:17:05,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T09:17:05,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T09:17:05,820][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T09:17:05,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T09:17:05,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T09:17:05,822][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T09:17:05,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T09:17:05,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T09:17:05,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T09:17:05,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T09:17:05,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T09:17:05,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T09:17:05,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T09:17:05,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T09:17:05,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T09:17:05,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T09:17:05,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T09:17:05,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T09:17:05,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T09:17:05,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T09:17:05,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T09:17:05,921][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.5gb], net total_space [125.8gb], types [ext4]
[2022-03-29T09:17:05,922][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T09:17:06,355][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T09:17:19,590][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T09:17:19,597][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T09:17:19,599][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T09:17:19,600][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T09:17:19,601][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T09:17:19,602][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T09:17:19,603][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T09:17:19,604][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T09:17:19,605][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T09:17:19,606][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T09:17:19,607][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T09:17:19,608][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T09:17:19,610][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T09:17:19,611][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T09:17:19,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T09:17:21,507][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T09:17:21,772][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T09:17:22,987][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T09:17:24,022][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T09:17:24,023][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T09:17:24,116][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T09:17:24,118][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T09:17:24,393][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T09:17:27,139][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T09:17:27,266][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_1oazSPuS7avvc7WYchbVA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 149, version: 4277, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_1oazSPuS7avvc7WYchbVA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T09:17:27,440][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_1oazSPuS7avvc7WYchbVA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 149, version: 4277, reason: Publication{term=149, version=4277}
[2022-03-29T09:17:27,599][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T09:17:27,601][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T09:17:29,163][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T09:17:29,177][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T09:17:31,165][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T09:17:31,168][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T09:17:44,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1c26d19d, interval=1s}] took [5233ms] which is above the warn threshold of [5000ms]
[2022-03-29T09:21:49,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:23:52,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6693405550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:24:13,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/238754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:24:29,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/238754402561ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:24:52,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:25:13,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38517949531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:25:31,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:25:51,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39460151470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:26:11,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:26:34,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38506373616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:26:52,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:27:17,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41248972038ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:27:42,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:28:07,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50226842748ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:28:31,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.2s/49205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:28:52,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.2s/49204647382ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:29:13,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:29:37,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40794706147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:29:59,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:30:27,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47136877006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:30:50,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:31:13,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50988531730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:31:37,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:31:53,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46562620871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:32:19,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:32:30,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41450963315ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:32:45,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:33:00,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27389290216ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:33:12,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:33:31,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27370817433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:34:07,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.4s/55427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:34:18,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.4s/55427016834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:34:29,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:34:37,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23036710153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:34:48,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:35:12,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17317404640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:35:22,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:35:31,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34726731389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:35:40,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18600ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:35:47,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18600303058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:35:57,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:36:08,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16817935108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:36:18,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:36:27,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20052064661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:36:35,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:36:46,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17994315729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:36:55,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20585ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:37:04,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20584121017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:37:14,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:37:27,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18692162167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:37:34,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:37:41,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20253917227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:37:54,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:38:08,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17800203007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:38:24,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:38:40,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29801791821ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:38:53,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:39:05,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29664770853ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:39:20,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:39:35,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27140184483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:39:54,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:40:08,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32250898715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:40:25,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31691ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:40:37,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31690423215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:40:53,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:41:07,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27816035449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:41:30,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:42:03,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37209522248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:42:25,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.6s/49675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:42:48,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.6s/49674675455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:43:11,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:43:28,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47518666674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:43:47,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:44:31,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39048090772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:44:50,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64414ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:45:10,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64414183300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:45:27,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37038ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:45:48,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37038166201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:46:12,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:46:40,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39156553167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:46:58,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:47:18,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51762940989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:47:49,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:48:16,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49174324705ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:48:43,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.1s/54120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:49:07,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.1s/54119714587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:49:36,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:49:58,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52814458261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:50:12,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:50:23,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38764797725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:50:38,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:50:50,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25576637828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:51:02,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:51:12,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24260414880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:51:23,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:51:33,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22389734371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:51:42,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:51:55,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18085229967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:52:01,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:52:08,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19789469857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:52:16,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14394ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:52:24,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14394642839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:52:34,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:52:44,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17242929191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:53:30,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:53:39,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53290835861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:53:53,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:54:06,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24644135483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:54:19,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:54:31,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27655831059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:54:39,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:54:47,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20122099103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:54:55,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:55:02,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15693416817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:55:14,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:55:29,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17636746747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:55:47,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:55:57,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35092991164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:56:11,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:56:29,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22391794754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:56:45,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:59:14,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33377869913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:02:29,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328850ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:05:23,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328849722995ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:09:06,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/393611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:12:29,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/393198815842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:16:09,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:19:10,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397722678498ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:22:19,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:25:30,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403493922083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:28:40,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370619ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:32:00,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370619121049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:35:58,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:40:05,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431532681379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:49:02,968][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T10:49:03,043][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T10:49:03,045][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T10:49:09,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T10:49:09,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T10:49:09,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T10:49:09,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T10:49:09,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T10:49:09,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T10:49:09,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T10:49:09,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T10:49:09,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T10:49:09,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T10:49:09,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T10:49:09,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T10:49:09,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T10:49:09,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T10:49:09,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T10:49:09,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T10:49:09,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T10:49:09,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T10:49:09,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T10:49:09,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T10:49:09,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T10:49:09,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T10:49:09,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T10:49:09,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T10:49:09,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T10:49:09,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T10:49:09,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T10:49:09,479][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T10:49:09,479][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T10:49:09,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T10:49:09,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T10:49:09,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T10:49:09,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T10:49:09,484][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T10:49:09,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T10:49:09,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T10:49:09,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T10:49:09,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T10:49:09,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T10:49:09,492][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T10:49:09,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T10:49:09,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T10:49:09,499][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T10:49:09,499][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T10:49:09,500][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T10:49:09,500][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T10:49:09,501][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T10:49:09,502][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T10:49:09,503][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T10:49:09,503][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T10:49:09,504][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T10:49:09,504][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T10:49:09,505][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T10:49:09,506][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T10:49:09,509][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T10:49:09,509][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T10:49:09,510][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T10:49:09,511][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T10:49:09,512][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T10:49:09,606][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T10:49:09,608][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T10:49:10,087][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T10:49:25,714][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T10:49:25,728][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T10:49:28,107][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T10:49:28,809][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T10:49:30,840][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T10:49:33,464][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T10:49:33,858][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T10:49:34,524][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T10:49:34,530][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T10:49:35,101][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T10:50:30,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:00,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6224711373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:50:52,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [34343ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:51:05,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:11,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47138071636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:18,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:18,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@527e8959, interval=5s}] took [12721ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:51:23,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12721181012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:31,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13058ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:38,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13058294489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:48,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:51:58,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16369334361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:03,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:09,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16003757930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:14,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [32373ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:52:21,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:30,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17583132650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:35,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:39,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14258304199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:47,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:52:53,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12049055896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:00,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:07,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12498908925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:18,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16884ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:28,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16884545366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:37,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:42,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19647448078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:43,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@55ac543b] took [75338ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:53:50,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13687ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:53:57,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13687632711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:06,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:15,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15387273372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:22,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:30,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16059643942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:38,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:42,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [32525ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:54:45,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16466213312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:53,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:54:53,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@55ee596d, interval=1m}] took [14331ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:54:58,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14331386712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:07,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:13,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14123950173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:19,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:22,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@527e8959, interval=5s}] took [12985ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:55:13,919][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44922ms] which is above the warn threshold of [5s]
[2022-03-29T10:55:26,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12985917420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:33,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:37,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13525430923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:42,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:46,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9002545240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:46,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [22527ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:55:49,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T10:55:50,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8233614103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T10:56:18,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [8234ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:57:37,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [6208ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:58:25,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:58:36,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [5464ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:59:06,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-03-29T10:59:39,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [5225ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:00:29,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [6111ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:00:54,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [5613ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:03:50,327][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T11:03:50,507][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{emehft6PQT6FgUT6kpVEmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 150, version: 4284, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{emehft6PQT6FgUT6kpVEmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T11:03:50,676][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{emehft6PQT6FgUT6kpVEmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 150, version: 4284, reason: Publication{term=150, version=4284}
[2022-03-29T11:03:50,870][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T11:03:50,872][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T11:04:34,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:41766}] took [8666ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:04:36,012][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:55810}] took [10667ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:04:39,958][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T11:04:58,075][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:40320}] took [19405ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:04:58,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e14e0@6f92aa6] took [5010ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:05:51,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [32438ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:05:52,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:06:38,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14388652366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:06:50,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:07:01,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72518157380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:07:19,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:07:33,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28142378873ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:07:47,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:08:02,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29159944065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:08:15,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:08:27,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27516177701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:08:40,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:08:53,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25503328892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:09:03,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:09:13,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22666788200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:09:30,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:09:39,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26711504823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:09:48,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:09:58,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17775528262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:10:12,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:10:21,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24654282611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:10:31,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:10:42,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18999459601ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:11:06,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:11:17,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33987517723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:11:35,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:11:46,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29090595734ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:12:01,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:12:20,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26141477151ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:12:35,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:12:52,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33335916145ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:13:06,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:13:29,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31766294904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:14:00,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53s/53081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:14:20,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53s/53081256519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:14:42,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:15:02,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40582883292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:15:23,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42701ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:15:39,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e14e0@31ac597d] took [584335ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:15:41,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42701788489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:15:58,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:16:18,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35483504700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:16:40,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:17:04,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [77045ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:17:00,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41562340865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:17:36,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.6s/55638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:17:37,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34a2a628, interval=5s}] took [55637ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:18:06,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.6s/55637906791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:18:44,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:19:32,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58398481674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:19:59,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:18:57,969][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [58398ms] which is above the warn threshold of [5s]
[2022-03-29T11:20:24,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83125487258ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:21:03,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:21:23,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61596654730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:21:40,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.2s/42260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:22:01,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.2s/42260077014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:22:15,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:22:32,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35258199423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:22:44,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:22:56,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27610895977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:23:06,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:23:16,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23278326992ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:23:27,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:23:43,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20941945756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:23:54,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:24:07,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26575310245ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:24:20,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:24:33,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25545888066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:24:50,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:25:08,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30128117633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:25:24,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:25:38,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32635444367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:25:51,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29147ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:26:05,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29146897210ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:26:31,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39239ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:26:43,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39238625242ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:27:00,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:27:15,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29149376919ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:27:30,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:27:45,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29987113356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:28:03,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:28:06,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e14e0@7bc10600] took [627666ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:28:23,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32789930941ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:28:38,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:28:53,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34745901917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:29:16,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [34745ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:29:22,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:29:40,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43357758830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:29:55,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:30:07,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34569355954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:30:21,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:30:38,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24610707131ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:30:56,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:30:28,282][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24610ms] which is above the warn threshold of [5s]
[2022-03-29T11:31:20,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34802600115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:31:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.5s/55545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:32:26,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.5s/55544639758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:33:51,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:34:22,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118730006787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:35:01,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:35:36,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71331316987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:36:01,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.4s/58426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:36:31,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.4s/58425227060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:36:49,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:37:11,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46592722833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:37:29,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43s/43090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:38:43,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43s/43090168881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:39:10,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:40:03,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99416256365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:41:02,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/110997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:41:59,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/110996978072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:42:34,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:42:57,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94512766510ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:43:37,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62367ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:43:59,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62366489129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:44:37,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:45:10,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58273818273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:45:43,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:46:02,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68410055797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:46:22,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:46:41,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40206876708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:47:04,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:47:39,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37267399572ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:48:04,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:48:32,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63203304910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:48:59,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e14e0@3a63ac60] took [1122350ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:49:34,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:50:23,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88527724116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:50:50,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73acf3a5, interval=1s}] took [70983ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:50:46,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:51:19,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70983450797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:51:40,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:51:41,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@55ee596d, interval=1m}] took [56664ms] which is above the warn threshold of [5000ms]
[2022-03-29T11:52:00,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56664774306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:52:33,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51154ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:53:06,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50994350000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:54:10,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:55:01,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96022381818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:56:00,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:54:38,806][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [147017ms] which is above the warn threshold of [5s]
[2022-03-29T11:56:45,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105687845326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:57:48,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T11:58:28,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109123908325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T11:59:07,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:00:07,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82377778488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:01:28,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:02:51,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138474112328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:05:16,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:07:34,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225794647003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:09:43,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/267426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:12:37,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/267063970400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:15:40,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:20:21,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338383363712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:24:16,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/517109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:27:59,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/517476067167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:31:32,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/447827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:34:37,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/447585746971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:37:21,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/346729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:42:34,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/346970433419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:45:14,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:47:46,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/482753255042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:50:20,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303825ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:53:07,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303600482509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T12:55:32,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/310173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T12:58:21,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309899312028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T13:01:22,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T13:04:26,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331020833723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T13:08:00,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T13:16:25,258][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T13:16:25,289][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T13:16:25,292][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T13:16:31,620][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T13:16:31,622][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T13:16:31,622][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T13:16:31,623][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T13:16:31,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T13:16:31,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T13:16:31,625][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T13:16:31,626][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T13:16:31,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T13:16:31,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T13:16:31,628][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T13:16:31,629][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T13:16:31,629][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T13:16:31,630][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T13:16:31,631][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T13:16:31,631][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T13:16:31,632][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T13:16:31,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T13:16:31,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T13:16:31,634][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T13:16:31,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T13:16:31,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T13:16:31,636][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T13:16:31,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T13:16:31,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T13:16:31,638][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T13:16:31,638][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T13:16:31,639][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T13:16:31,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T13:16:31,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T13:16:31,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T13:16:31,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T13:16:31,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T13:16:31,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T13:16:31,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T13:16:31,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T13:16:31,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T13:16:31,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T13:16:31,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T13:16:31,647][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T13:16:31,647][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T13:16:31,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T13:16:31,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T13:16:31,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T13:16:31,650][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T13:16:31,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T13:16:31,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T13:16:31,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T13:16:31,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T13:16:31,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T13:16:31,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T13:16:31,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T13:16:31,655][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T13:16:31,656][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T13:16:31,656][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T13:16:31,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T13:16:31,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T13:16:31,658][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T13:16:31,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T13:16:31,745][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T13:16:31,747][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T13:16:32,090][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T13:16:49,576][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T13:16:49,584][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T13:16:52,710][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T13:16:53,165][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T13:16:55,430][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T13:16:57,275][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T13:16:57,283][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T13:16:57,346][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T13:16:57,353][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T13:16:57,837][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T13:17:04,405][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T13:17:04,693][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{05yw0Po6TeuSYCjYmd4o2A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 151, version: 4286, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{05yw0Po6TeuSYCjYmd4o2A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T13:17:05,070][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{05yw0Po6TeuSYCjYmd4o2A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 151, version: 4286, reason: Publication{term=151, version=4286}
[2022-03-29T13:17:05,366][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T13:17:05,370][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T13:17:10,561][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T13:17:10,602][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T13:17:13,928][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T13:17:13,938][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T13:17:16,824][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-03-29T13:17:20,909][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T13:17:20,934][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T13:17:20,950][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T13:17:22,705][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][25] overhead, spent [265ms] collecting in the last [1s]
[2022-03-29T13:17:27,385][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T13:17:27,611][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T13:17:35,129][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T13:17:35,338][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-03-29T13:17:35,472][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-03-29T13:17:37,016][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T13:17:37,034][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T13:17:41,404][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-29T13:17:43,222][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T13:17:51,308][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T13:17:51,346][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-03-29T13:17:51,352][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-03-29T13:17:53,624][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T13:17:53,772][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-03-29T13:17:54,378][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T13:17:54,386][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T13:17:57,398][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T13:17:57,401][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T13:19:00,300][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:03,292][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:11,548][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:15,184][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:17,255][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:24,576][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:26,333][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:32,727][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:33,157][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:19:34,229][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:23:00,018][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:29:50,215][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:29:50,429][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:29:50,714][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T13:38:45,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@731a4124, interval=1s}] took [12701ms] which is above the warn threshold of [5000ms]
[2022-03-29T13:43:56,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T13:54:05,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15776165551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:04:44,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24m/1440724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:05:49,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@731a4124, interval=1s}] took [1440155ms] which is above the warn threshold of [5000ms]
[2022-03-29T14:06:51,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24m/1440155787417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:11:11,761][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [18s/18048ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c703f0fa]], which exceeds the warn threshold of [10s]
[2022-03-29T14:15:58,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6m/760595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:26:03,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6m/760980099278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:18:48,922][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:40800}] took [1440155ms] which is above the warn threshold of [5000ms]
[2022-03-29T14:28:10,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2m/733829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:26:58,070][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.1m/68371ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c703f0fa], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@4b88c058], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@23f46c09], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8443d3b1]], which exceeds the warn threshold of [10s]
[2022-03-29T14:31:47,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2m/733782146554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:33:59,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:36:27,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349334960776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:38:49,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/289702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:39:36,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5098/0x00000008017e5730@e25d072] took [289702ms] which is above the warn threshold of [5000ms]
[2022-03-29T14:40:46,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/289702680067ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:43:11,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:55:05,561][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T14:55:05,616][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T14:55:05,618][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T14:55:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T14:55:14,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T14:55:14,846][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T14:55:14,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T14:55:14,849][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T14:55:14,850][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T14:55:14,852][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T14:55:14,853][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T14:55:14,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T14:55:14,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T14:55:14,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T14:55:14,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T14:55:14,858][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T14:55:14,859][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T14:55:14,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T14:55:14,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T14:55:14,861][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T14:55:14,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T14:55:14,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T14:55:14,864][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T14:55:14,865][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T14:55:14,866][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T14:55:14,867][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T14:55:14,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T14:55:14,869][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T14:55:14,870][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T14:55:14,871][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T14:55:14,871][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T14:55:14,872][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T14:55:14,872][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T14:55:14,873][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T14:55:14,874][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T14:55:14,875][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T14:55:14,875][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T14:55:14,876][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T14:55:14,877][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T14:55:14,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T14:55:14,879][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T14:55:14,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T14:55:14,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T14:55:14,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T14:55:14,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T14:55:14,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T14:55:14,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T14:55:14,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T14:55:14,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T14:55:14,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T14:55:14,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T14:55:14,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T14:55:14,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T14:55:14,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T14:55:14,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T14:55:14,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T14:55:14,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T14:55:14,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T14:55:14,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T14:55:14,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T14:55:14,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T14:55:14,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T14:55:15,104][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.5gb], net total_space [125.8gb], types [ext4]
[2022-03-29T14:55:15,109][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T14:55:16,525][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T14:55:30,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1983a6dc, interval=5s}] took [6350ms] which is above the warn threshold of [5000ms]
[2022-03-29T14:56:23,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1983a6dc, interval=5s}] took [9514ms] which is above the warn threshold of [5000ms]
[2022-03-29T14:56:40,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:56:56,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9514202248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:56:56,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:56:58,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35650557836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:57:26,406][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T14:57:26,413][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T14:57:26,414][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T14:57:26,416][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T14:57:26,417][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T14:57:26,418][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T14:57:26,419][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T14:57:26,420][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T14:57:26,421][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T14:57:26,421][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T14:57:26,422][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T14:57:26,423][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T14:57:26,425][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T14:57:26,426][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T14:57:26,426][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T14:57:27,906][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T14:57:28,105][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T14:57:29,179][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T14:57:30,051][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T14:57:30,052][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T14:57:30,138][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T14:57:30,139][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T14:57:30,383][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T14:57:32,873][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T14:57:33,015][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 152, version: 4338, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T14:57:33,192][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 152, version: 4338, reason: Publication{term=152, version=4338}
[2022-03-29T14:57:33,316][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T14:57:33,317][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T14:57:34,815][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T14:57:34,844][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T14:57:37,346][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T14:57:37,355][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T14:57:45,265][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][11][12] duration [1.1s], collections [1]/[2.9s], total [1.1s]/[1.6s], memory [143.5mb]->[84.9mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [54.4mb]->[77.3mb]/[2gb]}{[survivor] [25mb]->[7.5mb]/[0b]}
[2022-03-29T14:57:45,738][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][11] overhead, spent [1.1s] collecting in the last [2.9s]
[2022-03-29T14:58:27,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:58:28,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6899763634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:59:17,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:59:19,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7450215994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T14:59:22,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T14:59:24,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5213514249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:00:08,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@193b9018] took [124390ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:00:38,480][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5203ms] which is above the warn threshold of [5s]
[2022-03-29T15:01:07,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [7248ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:01:07,224][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [64] timed out after [30749ms]
[2022-03-29T15:01:28,784][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/80559ms] ago, timed out [49.8s/49810ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [64]
[2022-03-29T15:01:37,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [7431ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:01:54,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@428cf098] took [5429ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:02:10,723][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [71] timed out after [15814ms]
[2022-03-29T15:02:13,979][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [72] timed out after [18216ms]
[2022-03-29T15:02:35,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [6642ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:02:44,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@61ff4c0f, interval=5s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:03:05,532][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43370}] took [249613ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:03:11,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@20e36ae3] took [21649ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:03:18,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:03:20,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5984373037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:03:22,576][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [79] timed out after [31604ms]
[2022-03-29T15:03:25,495][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.5m/92898ms] ago, timed out [1.2m/77084ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [71]
[2022-03-29T15:03:25,495][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.5m/91097ms] ago, timed out [1.2m/72881ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [72]
[2022-03-29T15:03:26,395][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [80] timed out after [15176ms]
[2022-03-29T15:03:27,693][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [16.5s/16580ms] ago, timed out [1.4s/1404ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [80]
[2022-03-29T15:03:27,770][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [37s/37029ms] ago, timed out [5.4s/5425ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [79]
[2022-03-29T15:03:44,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5006ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:04:04,926][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43412}] took [6505ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:04:06,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@779f4615] took [7905ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:04:08,898][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=152, version=4344}] took [6.5m] which is above the warn threshold of [30s]: [running task [Publication{term=152, version=4344}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2cebe063] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@4b8fb07d] took [1531ms], [org.elasticsearch.script.ScriptService@2b94fbe3] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5cf4959] took [0ms], [org.elasticsearch.snapshots.RestoreService@7654597b] took [0ms], [org.elasticsearch.ingest.IngestService@406e0394] took [0ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4ca9a399] took [15ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d0000@3353bed4] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@6718b644] took [0ms], [org.elasticsearch.tasks.TaskManager@575a6b90] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@566ebf89] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@6527cc74] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7e7a688d] took [0ms], [org.elasticsearch.indices.SystemIndexManager@6d3a0c20] took [3ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@19c70829] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@52216045] took [2ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4c72b24e] took [2ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@67d7825c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@34f76d9c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@311aad45] took [1ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@7e1bf833] took [73ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@1efcde33] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@57f3d4e0] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@2f86856b] took [31ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@59d33cf5] took [40ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@6c6ddcdd] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@25704164] took [153ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5cf4959] took [12ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3e46257c] took [932ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@7e3282d5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@540584ad] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@50714b4f] took [235ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@1b5c2631] took [27ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@6040c70f] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@625327d0] took [6ms], [org.elasticsearch.node.ResponseCollectorService@18c4f805] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@3028b236] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@56366383] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2d20b332] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1e79d2af] took [15ms], [org.elasticsearch.indices.store.IndicesStore@324c4276] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@78018046] took [0ms], [org.elasticsearch.license.LicenseService@8dff27c] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@86c03fa] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1db70b5e] took [387025ms], [org.elasticsearch.gateway.GatewayService@6b8ce10d] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@140bff1d] took [0ms]
[2022-03-29T15:04:10,836][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.encodeChangeCipherSpec(SSLSocketOutputRecord.java:233) ~[?:?]
	at sun.security.ssl.OutputRecord.changeWriteCiphers(OutputRecord.java:182) ~[?:?]
	at sun.security.ssl.ChangeCipherSpec$T10ChangeCipherSpecProducer.produce(ChangeCipherSpec.java:118) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.onProduceFinished(Finished.java:395) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.produce(Finished.java:379) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.ServerHelloDone$ServerHelloDoneConsumer.consume(ServerHelloDone.java:182) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T15:04:20,561][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][67] overhead, spent [326ms] collecting in the last [1.1s]
[2022-03-29T15:04:21,408][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T15:04:21,791][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T15:04:21,856][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T15:04:26,311][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][71][14] duration [706ms], collections [1]/[2.1s], total [706ms]/[2.7s], memory [130.2mb]->[94.3mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [77.3mb]->[82.3mb]/[2gb]}{[survivor] [8.8mb]->[12mb]/[0b]}
[2022-03-29T15:04:26,414][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][71] overhead, spent [706ms] collecting in the last [2.1s]
[2022-03-29T15:04:33,430][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][73][15] duration [2.3s], collections [1]/[4.7s], total [2.3s]/[5s], memory [146.3mb]->[105.8mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [82.3mb]->[92.8mb]/[2gb]}{[survivor] [12mb]->[8.9mb]/[0b]}
[2022-03-29T15:04:33,974][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][73] overhead, spent [2.3s] collecting in the last [4.7s]
[2022-03-29T15:04:42,156][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T15:04:42,157][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T15:04:49,005][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][79][16] duration [2.2s], collections [1]/[6.9s], total [2.2s]/[7.3s], memory [181.8mb]->[104.2mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [92.8mb]->[92.8mb]/[2gb]}{[survivor] [8.9mb]->[11.3mb]/[0b]}
[2022-03-29T15:04:49,262][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][79] overhead, spent [2.2s] collecting in the last [6.9s]
[2022-03-29T15:04:49,915][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10s] publication of cluster state version [4352] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_APPLY_COMMIT]
[2022-03-29T15:04:59,001][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][84][17] duration [2s], collections [1]/[4.2s], total [2s]/[9.3s], memory [156.2mb]->[108.5mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [92.8mb]->[98.5mb]/[2gb]}{[survivor] [11.3mb]->[10mb]/[0b]}
[2022-03-29T15:04:59,503][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][84] overhead, spent [2s] collecting in the last [4.2s]
[2022-03-29T15:05:05,138][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][88] overhead, spent [673ms] collecting in the last [1.7s]
[2022-03-29T15:05:07,446][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:57476}] took [18271ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:05:10,647][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][92] overhead, spent [375ms] collecting in the last [1.4s]
[2022-03-29T15:05:13,072][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T15:05:17,178][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][96][20] duration [727ms], collections [1]/[2.1s], total [727ms]/[11.1s], memory [185.3mb]->[118.6mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [103.9mb]->[106.6mb]/[2gb]}{[survivor] [9.3mb]->[12mb]/[0b]}
[2022-03-29T15:05:17,354][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [727ms] collecting in the last [2.1s]
[2022-03-29T15:05:19,964][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [649ms] collecting in the last [1.3s]
[2022-03-29T15:05:22,951][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][100] overhead, spent [475ms] collecting in the last [1.8s]
[2022-03-29T15:05:25,487][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][102] overhead, spent [390ms] collecting in the last [1.1s]
[2022-03-29T15:05:31,735][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [460ms] collecting in the last [1.2s]
[2022-03-29T15:05:37,171][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][109][27] duration [2.4s], collections [1]/[3.6s], total [2.4s]/[16s], memory [218.8mb]->[147.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [134.8mb]->[134.8mb]/[2gb]}{[survivor] [8mb]->[12.7mb]/[0b]}
[2022-03-29T15:05:37,445][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][109] overhead, spent [2.4s] collecting in the last [3.6s]
[2022-03-29T15:05:40,389][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][111] overhead, spent [549ms] collecting in the last [1.4s]
[2022-03-29T15:05:43,629][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][113] overhead, spent [481ms] collecting in the last [1.6s]
[2022-03-29T15:05:57,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:05:57,601][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][117][30] duration [7.4s], collections [1]/[9.4s], total [7.4s]/[24.5s], memory [222.1mb]->[160.6mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [146.5mb]->[153.4mb]/[2gb]}{[survivor] [11.5mb]->[7.2mb]/[0b]}
[2022-03-29T15:05:58,404][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [7.4s] collecting in the last [9.4s]
[2022-03-29T15:05:58,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8507996056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:06:04,074][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][120] overhead, spent [462ms] collecting in the last [1.3s]
[2022-03-29T15:06:10,553][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][123][32] duration [1.4s], collections [1]/[2.9s], total [1.4s]/[26.5s], memory [219.8mb]->[173.2mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [153.4mb]->[159.9mb]/[2gb]}{[survivor] [14.4mb]->[13.2mb]/[0b]}
[2022-03-29T15:06:10,765][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][123] overhead, spent [1.4s] collecting in the last [2.9s]
[2022-03-29T15:06:15,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][124][33] duration [1.5s], collections [1]/[1.8s], total [1.5s]/[28s], memory [173.2mb]->[245.2mb]/[2gb], all_pools {[young] [0b]->[80mb]/[0b]}{[old] [159.9mb]->[159.9mb]/[2gb]}{[survivor] [13.2mb]->[13.2mb]/[0b]}
[2022-03-29T15:06:15,845][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][124] overhead, spent [1.5s] collecting in the last [1.8s]
[2022-03-29T15:06:31,211][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][133][34] duration [1.8s], collections [1]/[3.1s], total [1.8s]/[29.9s], memory [254.8mb]->[175.5mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [168.4mb]->[168.4mb]/[2gb]}{[survivor] [6.3mb]->[7.1mb]/[0b]}
[2022-03-29T15:06:31,601][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][133] overhead, spent [1.8s] collecting in the last [3.1s]
[2022-03-29T15:06:52,182][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][35] duration [1.6s], collections [1]/[2.8s], total [1.6s]/[31.5s], memory [259.5mb]->[175.5mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [168.4mb]->[168.4mb]/[2gb]}{[survivor] [7.1mb]->[7.1mb]/[0b]}
[2022-03-29T15:06:52,402][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [1.6s] collecting in the last [2.8s]
[2022-03-29T15:07:01,552][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][149][36] duration [1.7s], collections [1]/[1.2s], total [1.7s]/[33.2s], memory [235.5mb]->[267.5mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [168.4mb]->[168.4mb]/[2gb]}{[survivor] [7.1mb]->[15mb]/[0b]}
[2022-03-29T15:07:02,881][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][149] overhead, spent [1.7s] collecting in the last [1.2s]
[2022-03-29T15:07:03,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5028ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:07:07,154][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][150][37] duration [1.9s], collections [1]/[8.5s], total [1.9s]/[35.2s], memory [267.5mb]->[189mb]/[2gb], all_pools {[young] [4mb]->[40mb]/[0b]}{[old] [168.4mb]->[181.3mb]/[2gb]}{[survivor] [15mb]->[3.7mb]/[0b]}
[2022-03-29T15:07:11,828][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][38] duration [1.7s], collections [1]/[2.4s], total [1.7s]/[36.9s], memory [189mb]->[189.9mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [181.3mb]->[181.3mb]/[2gb]}{[survivor] [3.7mb]->[8.5mb]/[0b]}
[2022-03-29T15:07:11,993][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [1.7s] collecting in the last [2.4s]
[2022-03-29T15:07:20,233][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12956ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [30] unchanged indices
[2022-03-29T15:07:20,648][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.2s] publication of cluster state version [4371] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T15:07:25,971][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][160][39] duration [1s], collections [1]/[2.3s], total [1s]/[37.9s], memory [269.9mb]->[192.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [181.3mb]->[183.8mb]/[2gb]}{[survivor] [8.5mb]->[8.7mb]/[0b]}
[2022-03-29T15:07:26,111][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][160] overhead, spent [1s] collecting in the last [2.3s]
[2022-03-29T15:07:30,010][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][161][40] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[39.5s], memory [192.6mb]->[193.7mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [183.8mb]->[188.2mb]/[2gb]}{[survivor] [8.7mb]->[5.5mb]/[0b]}
[2022-03-29T15:07:30,099][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-03-29T15:07:35,102][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-29T15:07:38,961][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][166][41] duration [1.3s], collections [1]/[2.7s], total [1.3s]/[40.9s], memory [253.7mb]->[193.8mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [188.2mb]->[188.2mb]/[2gb]}{[survivor] [5.5mb]->[5.6mb]/[0b]}
[2022-03-29T15:07:39,238][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][166] overhead, spent [1.3s] collecting in the last [2.7s]
[2022-03-29T15:07:46,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:07:46,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3491a0fc, interval=1m}] took [5216ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:07:46,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5216325339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:07:46,880][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][168][42] duration [4.1s], collections [1]/[6s], total [4.1s]/[45s], memory [277.8mb]->[194.6mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [188.2mb]->[188.2mb]/[2gb]}{[survivor] [5.6mb]->[6.4mb]/[0b]}
[2022-03-29T15:07:46,927][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][168] overhead, spent [4.1s] collecting in the last [6s]
[2022-03-29T15:08:01,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:08:01,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6866194551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:08:01,785][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][171][43] duration [5.5s], collections [1]/[11.4s], total [5.5s]/[50.6s], memory [206.6mb]->[196.7mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [188.2mb]->[188.2mb]/[2gb]}{[survivor] [6.4mb]->[8.5mb]/[0b]}
[2022-03-29T15:08:01,786][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][171] overhead, spent [5.5s] collecting in the last [11.4s]
[2022-03-29T15:08:20,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:08:20,810][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][177][44] duration [5.3s], collections [1]/[1.4s], total [5.3s]/[55.9s], memory [276.7mb]->[280.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [188.2mb]->[188.8mb]/[2gb]}{[survivor] [8.5mb]->[6.9mb]/[0b]}
[2022-03-29T15:08:20,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9946006002ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:08:20,859][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][177] overhead, spent [5.3s] collecting in the last [1.4s]
[2022-03-29T15:08:20,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [9946ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:08:36,876][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43470}] took [5146ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:08:37,311][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][185][45] duration [3.4s], collections [1]/[5.5s], total [3.4s]/[59.3s], memory [255.8mb]->[219.4mb]/[2gb], all_pools {[young] [60mb]->[48mb]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [6.9mb]->[6.6mb]/[0b]}
[2022-03-29T15:08:37,312][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][185] overhead, spent [3.4s] collecting in the last [5.5s]
[2022-03-29T15:08:46,115][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][187][46] duration [2.5s], collections [1]/[4.8s], total [2.5s]/[1m], memory [263.4mb]->[196mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [6.6mb]->[7.1mb]/[0b]}
[2022-03-29T15:08:46,416][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [2.5s] collecting in the last [4.8s]
[2022-03-29T15:09:26,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:09:27,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@61ff4c0f, interval=5s}] took [12594ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:09:27,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12594612947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:09:28,675][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][47] duration [9.1s], collections [1]/[19.3s], total [9.1s]/[1.1m], memory [264mb]->[227.9mb]/[2gb], all_pools {[young] [72mb]->[64mb]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [7.1mb]->[7.1mb]/[0b]}
[2022-03-29T15:09:29,805][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [9.1s] collecting in the last [19.3s]
[2022-03-29T15:09:37,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5619ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:09:45,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5619240813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:09:45,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][199][48] duration [5.6s], collections [1]/[9.1s], total [5.6s]/[1.2m], memory [227.9mb]->[283.9mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [7.1mb]->[5.9mb]/[0b]}
[2022-03-29T15:09:45,738][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][199] overhead, spent [5.6s] collecting in the last [9.1s]
[2022-03-29T15:09:45,820][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_monitoring/bulk?system_id=kibana&system_api_version=7&interval=10000ms][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:42002}] took [76556ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:09:45,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9216ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:09:45,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [9216ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:09:45,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9216103390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:10:22,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:10:24,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7301049578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:10:25,829][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][214][49] duration [5.7s], collections [1]/[1.5s], total [5.7s]/[1.3m], memory [274.7mb]->[278.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [5.9mb]->[5.7mb]/[0b]}
[2022-03-29T15:10:27,416][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][214] overhead, spent [5.7s] collecting in the last [1.5s]
[2022-03-29T15:10:29,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [14561ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:10:51,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5071ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:11:36,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5092ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:11:38,324][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43438}] took [6764ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:12:13,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:12:13,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [19380ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:12:14,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16927725100ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:12:17,427][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][232][50] duration [13.7s], collections [1]/[22.7s], total [13.7s]/[1.6m], memory [278.5mb]->[215.9mb]/[2gb], all_pools {[young] [84mb]->[20mb]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [5.7mb]->[7.1mb]/[0b]}
[2022-03-29T15:12:18,289][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][232] overhead, spent [13.7s] collecting in the last [22.7s]
[2022-03-29T15:12:47,945][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43438}] took [7941ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:12:49,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5151ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:13:15,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5059ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:13:25,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5066ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:13:28,098][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43438}] took [20546ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:13:42,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:13:44,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10254177662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:13:49,316][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][51] duration [7.6s], collections [1]/[13.1s], total [7.6s]/[1.7m], memory [243.9mb]->[198.1mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [188.8mb]->[188.8mb]/[2gb]}{[survivor] [7.1mb]->[9.3mb]/[0b]}
[2022-03-29T15:13:50,318][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][248] overhead, spent [7.6s] collecting in the last [13.1s]
[2022-03-29T15:13:51,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [8727ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:14:11,797][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43438}] took [7098ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:14:33,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [12955ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:14:33,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11354ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:14:36,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11354673037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:14:39,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6281ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:14:41,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6280851026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:14:41,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@6c7c8b29] took [6280ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:14:44,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:14:46,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5108102136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:14:55,346][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][259][52] duration [8s], collections [1]/[28.1s], total [8s]/[1.8m], memory [278.1mb]->[206.5mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [188.8mb]->[189.7mb]/[2gb]}{[survivor] [9.3mb]->[8.7mb]/[0b]}
[2022-03-29T15:14:57,737][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][259] overhead, spent [8s] collecting in the last [28.1s]
[2022-03-29T15:14:59,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [15208ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:15:07,098][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [24.9s/24919ms] ago, timed out [2.2s/2201ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [861]
[2022-03-29T15:15:07,554][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [861] timed out after [22718ms]
[2022-03-29T15:15:25,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [8210ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:15:18,584][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5636ms] which is above the warn threshold of [5s]
[2022-03-29T15:15:40,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5701ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:15:56,070][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43438}] took [12380ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:15:57,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [10314ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:16:07,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@287b9b2f] took [6800ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:16:22,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [6641ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:16:44,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5314ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:16:40,920][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [927] timed out after [31190ms]
[2022-03-29T15:16:48,518][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43438}] took [14697ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:16:58,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5835ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:17:19,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [12132ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:17:27,159][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.2m/77287ms] ago, timed out [46s/46097ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [927]
[2022-03-29T15:17:38,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5306ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:17:58,792][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10091ms] which is above the warn threshold of [5s]
[2022-03-29T15:18:06,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [14175ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:18:25,867][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [973] timed out after [38564ms]
[2022-03-29T15:18:43,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [6272ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:19:06,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [9495ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:20:18,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:20,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62536867979ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:23,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:28,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6032018639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:35,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:41,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11108270060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:49,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14646ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:20:52,128][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][275][53] duration [54.8s], collections [1]/[1.4m], total [54.8s]/[2.7m], memory [270.5mb]->[200.8mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [189.7mb]->[190.3mb]/[2gb]}{[survivor] [8.7mb]->[10.4mb]/[0b]}
[2022-03-29T15:21:00,425][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][275] overhead, spent [54.8s] collecting in the last [1.4m]
[2022-03-29T15:20:56,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14645757015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:07,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [25754ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:21:10,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:17,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20640653133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:23,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@1a6e3d2c] took [11053ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:21:21,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11053ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:28,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11053100173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:36,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:43,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14467161293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:49,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:55,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12271836275ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:21:50,118][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12272ms] which is above the warn threshold of [5s]
[2022-03-29T15:22:00,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:07,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12503513961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:15,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [12503ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:22:20,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:32,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19242016841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:37,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:40,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17736151691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:34,676][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1008] timed out after [69537ms]
[2022-03-29T15:22:36,414][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1010] timed out after [58484ms]
[2022-03-29T15:22:43,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:50,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6443120245ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:22:59,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:04,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [15015ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:23:07,054][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.8m/108732ms] ago, timed out [39.1s/39195ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1008]
[2022-03-29T15:23:08,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015891737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:11,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:21,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11815321504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:26,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:31,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [15558ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:23:35,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15558117839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:39,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:42,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13019444390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:51,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:23:55,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@2c78a9d] took [11047ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:23:58,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11047777850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:05,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:13,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14783709725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:13,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [14783ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:24:17,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:17,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@26231ba9, interval=30s}] took [12571ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:24:19,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12571281335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:24,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:29,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5900414061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:35,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12175ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:38,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12175073108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:37,486][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12175ms] which is above the warn threshold of [5s]
[2022-03-29T15:24:38,313][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1020] timed out after [33255ms]
[2022-03-29T15:24:35,647][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1019] timed out after [44303ms]
[2022-03-29T15:24:42,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:45,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [6745ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:24:44,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6745812274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T15:24:45,870][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.9m/418169ms] ago, timed out [6.3m/379605ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [973]
[2022-03-29T15:24:47,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1983a6dc, interval=5s}] took [5146ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:24:57,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5849ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:25:03,921][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.6m/221607ms] ago, timed out [2.7m/163123ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1010]
[2022-03-29T15:25:10,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [7071ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:25:34,224][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.8m/113939ms] ago, timed out [1.1m/69636ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1019]
[2022-03-29T15:25:49,415][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1054] timed out after [18973ms]
[2022-03-29T15:25:52,215][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/119907ms] ago, timed out [1.4m/86652ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1020]
[2022-03-29T15:25:58,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [8440ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:26:24,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [14366ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:26:50,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [6840ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:27:05,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@6d86df9e] took [9691ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:27:22,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [9706ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:27:25,031][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8378ms] which is above the warn threshold of [5s]
[2022-03-29T15:27:38,614][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1076] timed out after [38398ms]
[2022-03-29T15:27:43,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [5887ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:27:38,740][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1079] timed out after [32860ms]
[2022-03-29T15:27:46,870][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [46s/46087ms] ago, timed out [7.6s/7689ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1076]
[2022-03-29T15:27:57,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [9033ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:28:18,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [12397ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:28:44,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [9363ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:29:04,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@22989ee8] took [13704ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:29:42,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [19213ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:30:30,019][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1093] timed out after [91079ms]
[2022-03-29T15:30:47,075][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.6m/217829ms] ago, timed out [3m/184969ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1079]
[2022-03-29T15:30:44,300][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1094] timed out after [95519ms]
[2022-03-29T15:31:00,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [15813ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:31:05,510][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.5m/333264ms] ago, timed out [5.2m/314291ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1054]
[2022-03-29T15:31:01,005][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12964ms] which is above the warn threshold of [5s]
[2022-03-29T15:31:40,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [11928ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:31:57,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [8893ms] which is above the warn threshold of [5000ms]
[2022-03-29T15:59:53,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4m/1648146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T15:59:58,059][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.2m/196778ms] ago, timed out [1.7m/105699ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tML8VwhRRTuK70x_-QopTQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1093]
[2022-03-29T16:02:44,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1983a6dc, interval=5s}] took [1648946ms] which is above the warn threshold of [5000ms]
[2022-03-29T16:03:22,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4m/1648146101310ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T16:07:03,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/437786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T16:10:19,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/437467282746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T16:14:03,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/410379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T16:18:13,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/410221426146ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T16:22:25,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9m/479806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T16:28:26,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/480282394627ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T16:29:07,428][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][299][54] duration [24.3m], collections [1]/[45.1m], total [24.3m]/[27.1m], memory [280.8mb]->[201.5mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [190.3mb]->[192.6mb]/[2gb]}{[survivor] [10.4mb]->[4.8mb]/[0b]}
[2022-03-29T16:33:51,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9m/718181ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T16:36:56,035][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][299] overhead, spent [24.3m] collecting in the last [45.1m]
[2022-03-29T16:40:25,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9m/717858485126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T16:43:25,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23ddf060, interval=1s}] took [1198140ms] which is above the warn threshold of [5000ms]
[2022-03-29T16:44:57,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11m/664598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T16:49:04,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11m/664407444447ns] on relative clock which is above the warn threshold of [5000ms]
