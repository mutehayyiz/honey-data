[2022-04-04T16:08:07,309][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-04T16:08:07,331][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-04T16:08:07,331][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-04T16:08:14,761][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-04T16:08:14,762][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-04T16:08:14,763][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-04T16:08:14,763][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-04T16:08:14,764][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-04T16:08:14,764][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-04T16:08:14,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-04T16:08:14,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-04T16:08:14,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-04T16:08:14,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-04T16:08:14,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-04T16:08:14,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-04T16:08:14,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-04T16:08:14,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-04T16:08:14,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-04T16:08:14,770][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-04T16:08:14,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-04T16:08:14,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-04T16:08:14,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-04T16:08:14,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-04T16:08:14,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-04T16:08:14,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-04T16:08:14,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-04T16:08:14,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-04T16:08:14,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-04T16:08:14,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-04T16:08:14,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-04T16:08:14,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-04T16:08:14,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-04T16:08:14,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-04T16:08:14,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-04T16:08:14,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-04T16:08:14,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-04T16:08:14,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-04T16:08:14,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-04T16:08:14,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-04T16:08:14,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-04T16:08:14,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-04T16:08:14,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-04T16:08:14,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-04T16:08:14,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-04T16:08:14,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-04T16:08:14,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-04T16:08:14,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-04T16:08:14,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-04T16:08:14,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-04T16:08:14,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-04T16:08:14,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-04T16:08:14,792][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-04T16:08:14,792][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-04T16:08:14,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-04T16:08:14,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-04T16:08:14,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-04T16:08:14,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-04T16:08:14,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-04T16:08:14,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-04T16:08:14,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-04T16:08:14,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-04T16:08:14,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-04T16:08:14,888][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.6gb], net total_space [125.8gb], types [ext4]
[2022-04-04T16:08:14,889][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-04T16:08:15,192][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-04T16:08:26,100][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-04T16:08:26,104][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-04T16:08:27,051][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-04T16:08:27,162][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-04T16:08:27,862][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-04T16:08:28,632][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-04T16:08:28,633][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-04T16:08:28,670][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-04T16:08:28,672][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-04T16:08:28,884][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-04T16:08:30,971][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-04T16:08:31,103][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 201, version: 7170, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-04T16:08:31,269][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 201, version: 7170, reason: Publication{term=201, version=7170}
[2022-04-04T16:08:31,377][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-04T16:08:31,378][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-04T16:08:32,300][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-04T16:08:32,306][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [37] indices into cluster_state
[2022-04-04T16:08:33,014][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-04T16:08:33,015][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-04T16:08:33,629][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-04T16:08:34,081][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-04T16:08:34,084][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-04T16:08:34,090][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-04T16:08:34,830][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T16:08:34,904][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T16:08:36,548][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-04T16:08:36,577][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-04T16:08:36,589][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-04T16:08:36,965][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T16:08:36,989][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T16:08:37,890][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T16:08:39,592][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-04-04T16:08:41,313][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-04T16:08:41,325][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-04T16:08:41,327][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-04T16:08:41,976][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-04T16:08:41,999][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-04T16:08:42,167][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T16:08:42,169][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T16:08:42,805][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T16:08:42,806][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T16:08:52,684][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-04T16:08:52,870][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-04T16:09:36,267][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 871 finished with response BulkByScrollResponse[took=585.6ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-04T16:09:39,061][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 891 finished with response BulkByScrollResponse[took=3s,timed_out=false,sliceId=null,updated=1028,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-04T16:09:46,349][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-04T16:10:17,352][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.04] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-04T16:10:17,615][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.04][0]]]).
[2022-04-04T16:10:17,941][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,037][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,047][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,063][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,320][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:33,766][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:36,995][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:47,732][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:48,014][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:48,173][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:49,378][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:50,810][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:06,547][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:06,737][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:07,856][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:08,039][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:32,023][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:32,107][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:33,046][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:33,125][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,075][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,173][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,184][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,305][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,375][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:40,638][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:56,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:14:08,831][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:16:03,249][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:16:18,264][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,059][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,280][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,401][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,480][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,543][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:50,448][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:19:49,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:21:40,555][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:22:15,587][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:28:00,585][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:18,785][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:18,884][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:18,960][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:19,065][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:19,089][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:36:34,190][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:37:11,846][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_all/_settings?expand_wildcards=open%2Cclosed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:40528}] took [7401ms] which is above the warn threshold of [5000ms]
[2022-04-04T16:38:34,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [5931ms] which is above the warn threshold of [5000ms]
[2022-04-04T16:39:22,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:43:07,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9741431358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:43:35,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:44:41,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269455526717ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:45:12,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:46:43,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101901173230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:48:47,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:50:00,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216264378815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:50:20,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:50:37,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93096028806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:51:51,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:52:06,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90380217583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:52:17,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:53:05,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26716455966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:53:35,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:55:13,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76659168792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:53:44,171][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26717ms] which is above the warn threshold of [5s]
[2022-04-04T16:55:54,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:56:16,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139879703406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:56:35,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:56:57,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38828504887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:16,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:33,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42976796494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:46,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:57,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30737680945ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:58:17,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:58:32,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30514317768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:58:47,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30356ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:59:19,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30355938894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:00:53,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:17,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126193552490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:16,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [126193ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:01:20,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [27982ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:01:20,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:26,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27982990803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:30,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:44,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9475555569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:09,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:14,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [38493ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:17,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38493638085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:39,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:12,912][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38493ms] which is above the warn threshold of [5s]
[2022-04-04T17:02:43,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30554748514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:44,651][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cluster/state/metadata/.apm-agent-configuration,.apm-custom-link,.async-search,.kibana_7.16.2_001,.kibana_7.17.0_001,.kibana_task_manager_7.16.2_001,.kibana_task_manager_7.17.0_001,.tasks,logstash-1970.01.01,logstash-2022.03.13,logstash-2022.03.14,logstash-2022.03.15,logstash-2022.03.16,logstash-2022.03.17,logstash-2022.03.18,logstash-2022.03.19,logstash-2022.03.20,logstash-2022.03.21,logstash-2022.03.22,logstash-2022.03.23,logstash-2022.03.24,logstash-2022.03.25,logstash-2022.03.26,logstash-2022.03.27,logstash-2022.03.28,logstash-2022.03.29,logstash-2022.03.30,logstash-2022.03.31,logstash-2022.04.01,logstash-2022.04.02,logstash-2022.04.03,logstash-2022.04.04][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:40528}] took [1522956ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:45,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:46,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6118231182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:56,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:56,845][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:44774}] took [11125ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:57,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [6229ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:58,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6229365615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:49,782][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [23.7m/1426584ms] ago, timed out [1.4m/84642ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13137]
[2022-04-04T17:03:07,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:22,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [11108ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:03:22,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11108278030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:26,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [17876ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:03:25,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:29,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17876592731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:33,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:37,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8023543515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:52,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:56,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19229398331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:56,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [19229ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:01,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@451d2502, interval=1m}] took [8668ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:01,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:06,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8668875824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:10,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:10,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [9264ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:12,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9264159509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:03,289][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.apm-agent-configuration,.apm-custom-link,.async-search,.kibana_7.16.2_001,.kibana_7.17.0_001,.kibana_task_manager_7.16.2_001,.kibana_task_manager_7.17.0_001,.tasks,logstash-1970.01.01,logstash-2022.03.13,logstash-2022.03.14,logstash-2022.03.15,logstash-2022.03.16,logstash-2022.03.17,logstash-2022.03.18,logstash-2022.03.19,logstash-2022.03.20,logstash-2022.03.21,logstash-2022.03.22,logstash-2022.03.23,logstash-2022.03.24,logstash-2022.03.25,logstash-2022.03.26,logstash-2022.03.27,logstash-2022.03.28,logstash-2022.03.29,logstash-2022.03.30,logstash-2022.03.31,logstash-2022.04.01,logstash-2022.04.02,logstash-2022.04.03,logstash-2022.04.04/_stats/store,docs][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:40540}] took [8669ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:12,711][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13137] timed out after [1341942ms]
[2022-04-04T17:04:34,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:38,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [11966ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:38,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9364396127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:57,359][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:57,822][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:58,000][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:58,009][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:58,482][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:59,176][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:59,305][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:05:05,312][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:05:05,895][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:05:25,076][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1768][36] duration [1.1s], collections [1]/[2.8s], total [1.1s]/[2.4s], memory [1.3gb]->[207.9mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [197.7mb]->[197.7mb]/[2gb]}{[survivor] [5mb]->[10.1mb]/[0b]}
[2022-04-04T17:05:26,084][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1768] overhead, spent [1.1s] collecting in the last [2.8s]
[2022-04-04T17:05:27,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [9578ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:05:29,868][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1770] overhead, spent [646ms] collecting in the last [1.1s]
[2022-04-04T17:05:32,316][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1771][38] duration [1s], collections [1]/[2.6s], total [1s]/[4.1s], memory [220.6mb]->[207.2mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [2.6mb]->[1.3mb]/[0b]}
[2022-04-04T17:05:32,612][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1771] overhead, spent [1s] collecting in the last [2.6s]
[2022-04-04T17:05:41,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [6030ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:06:13,893][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1778][39] duration [2.2s], collections [1]/[3.5s], total [2.2s]/[6.4s], memory [295.2mb]->[210.4mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [1.3mb]->[4.4mb]/[0b]}
[2022-04-04T17:06:15,061][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1778] overhead, spent [2.2s] collecting in the last [3.5s]
[2022-04-04T17:06:25,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [5423ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:07:12,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [28004ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:11:22,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@19ed9812] took [188780ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:12:25,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:12:24,787][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45584}] took [22415ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:11:43,633][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45580}] took [12526ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:12:53,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10746530555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:05,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.9s/47956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:18,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.9s/47956186367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:32,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:40,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26980603717ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:52,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:09,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19565791434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:10,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [19565ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:13:36,615][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45582}] took [26980ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:14:21,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:38,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29145893445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:15:31,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:23,070][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [29146ms] which is above the warn threshold of [5s]
[2022-04-04T17:15:51,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68637139438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:16:07,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:16:50,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37189761097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:17:48,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101536ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:18:02,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [138725ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:18:17,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101535969927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:19:08,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:20:22,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78630037614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:21:08,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:23:33,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119489165939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:23:51,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/164774ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:24:08,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/164774132379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:10,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:17,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43649999898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:27,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:35,972][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33118}] took [439911ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:26:36,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111998260661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:27:11,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.2s/44244ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:27:52,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.2s/44244655187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:01,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:06,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50195498363ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:08,560][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33110}] took [50195ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:28:11,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:04,047][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13628] timed out after [915415ms]
[2022-04-04T17:28:13,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984729677ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:29,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:36,640][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45586}] took [18634ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:28:42,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18633661913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:54,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:29:26,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25447638287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:29:46,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52s/52099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:29:52,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@451d2502, interval=1m}] took [52099ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:29:59,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52s/52099364439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:30:13,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:30:28,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26513162573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:30:56,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.6s/41680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:19,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.6s/41679339936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:36,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:09,897][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [41679ms] which is above the warn threshold of [5s]
[2022-04-04T17:31:40,287][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1783][40] duration [1m], collections [1]/[14.3m], total [1m]/[1.2m], memory [242.4mb]->[243.3mb]/[2gb], all_pools {[young] [36mb]->[32mb]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [4.4mb]->[5.3mb]/[0b]}
[2022-04-04T17:31:49,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40541648713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:58,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [82220ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:32:03,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:19,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28179574852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:34,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31142ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:44,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31141572596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:53,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:09,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19118258501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:21,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:29,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27054436662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:31,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@1e5c2a57] took [27054ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:33:34,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:43,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14356930488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:56,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:06,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20725969807ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:13,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:23,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17206508767ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:28,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [17206ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:34:31,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:36,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18678964660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:46,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:57,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14442756600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:06,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:09,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [19632ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:35:13,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19632716029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:25,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19434ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:38,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19434009833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:58,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:36:25,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33309814725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:36:36,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:36:40,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [71680ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:36:13,288][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [33310ms] which is above the warn threshold of [5s]
[2022-04-04T17:36:52,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38370449888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:18,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6m/941849ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:21,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6m/941848721409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:18,545][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13726] timed out after [85411ms]
[2022-04-04T17:52:24,290][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [41.2m/2472792ms] ago, timed out [25.9m/1557377ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13628]
[2022-04-04T17:52:23,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:24,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5536665331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:33,860][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1786][41] duration [14.9m], collections [1]/[16.5m], total [14.9m]/[16.1m], memory [259.3mb]->[258.8mb]/[2gb], all_pools {[young] [52mb]->[48mb]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [5.3mb]->[4.8mb]/[0b]}
[2022-04-04T17:52:36,760][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1786] overhead, spent [14.9m] collecting in the last [16.5m]
[2022-04-04T17:52:38,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [12807ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:52:39,326][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [19.3m/1158817ms] ago, timed out [17.8m/1073406ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13726]
[2022-04-04T17:53:28,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47287ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:38,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47287531328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:45,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:53,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17673451824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:40,367][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [47888ms] which is above the warn threshold of [5s]
[2022-04-04T17:54:01,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:04,675][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1787][42] duration [36.3s], collections [1]/[13.8s], total [36.3s]/[16.7m], memory [258.8mb]->[298.8mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [4.8mb]->[4.7mb]/[0b]}
[2022-04-04T17:54:08,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16034082805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:15,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1787] overhead, spent [36.3s] collecting in the last [13.8s]
[2022-04-04T17:54:18,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:23,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [98252ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:54:26,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17057006150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:36,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:38,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [17900ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:54:45,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17900061666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:54,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:02,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18352383537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:12,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:19,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17293979736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:25,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:34,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13308033099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:42,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:56,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [16667ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:55:56,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16667219280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:06,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:18,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23932195655ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:37,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:56,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31708916148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:57:14,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35782ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:57:26,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35782489547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:57:49,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:06,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35320749451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:14,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [35320ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:58:23,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34333ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:36,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34332731637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:47,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:01,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24063289769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:13,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26166903554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:31,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:09,156][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13842] timed out after [85616ms]
[2022-04-04T17:59:42,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18570769955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:58,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:00:10,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26403012443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:38,230][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44737ms] which is above the warn threshold of [5s]
[2022-04-04T18:00:25,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:00:42,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27322542441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:00:55,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:06,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [57214ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:01:10,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29891720142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:24,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:37,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28980055345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:54,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:06,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28866437565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:23,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:38,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29662917778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:54,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:56,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@4915e650] took [28813ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:03:06,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28813755871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:03:19,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:03:33,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27438881572ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:03:45,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25753ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:03,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25753250949ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:09,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [25753ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:04:16,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:28,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30833156211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:39,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:52,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23210481390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:06,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:20,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26711820219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:29,061][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.8m/593741ms] ago, timed out [8.4m/508125ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13842]
[2022-04-04T18:05:43,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:23,622][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26712ms] which is above the warn threshold of [5s]
[2022-04-04T18:06:11,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37035349720ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:19,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:27,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36881779119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:37,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:50,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17941337358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:57,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:08,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19997909080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@12b63aa8] took [23784ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:07:32,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23784077858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:40,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:49,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18515982589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:58,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:01,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [36188ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:08:05,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17672636680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:15,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:17,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [17579ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:08:26,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17579313899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:37,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21554ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:40,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [21553ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:08:48,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21553878090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:57,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:12,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21031219501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:23,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:35,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25310303968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:51,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:02,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758301906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:18,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:32,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27265617257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:53,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:11:15,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27121327091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:11:24,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [27121ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:11:33,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:11:48,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47787048396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:04,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30463ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:05,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [30463ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:12:21,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30463017791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:39,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:23,899][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [14007] timed out after [173205ms]
[2022-04-04T18:12:54,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35018464866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:25,385][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30463ms] which is above the warn threshold of [5s]
[2022-04-04T18:13:20,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:13:36,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41017708560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:13:57,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:14:13,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37997529966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:14:33,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:14:58,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34758291466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:15:16,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:15:34,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43511446866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:15:40,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [43511ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:16:01,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:16:28,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43155212334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:16:48,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:17:10,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48165740181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:17:28,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:17:43,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40553338740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:00,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:17,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32401898022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:42,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:19:01,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42459924965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:59,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@72dbb473] took [42459ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:19:23,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:19:40,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40373965500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:03,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:25,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39104617058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:48,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:21:14,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40761932024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:21:45,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:41,471][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [13m/784360ms] ago, timed out [10.1m/611155ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [14007]
[2022-04-04T18:22:05,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61643464930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:22:30,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:22:48,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45701637386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:23:17,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:23:36,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46352956835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:24:21,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:25:01,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63735067184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:25:47,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:26:04,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [79184ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:26:32,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79184662515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:27:47,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:28:45,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127215336118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:29:23,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:30:04,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96088589917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:31:04,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:32:14,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99821815689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:33:01,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:34:05,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118319182518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:36:23,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/201398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:37:51,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/201397994181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:39:24,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/180532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:40:12,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/180532665910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:40:58,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:42:53,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88134721466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:43:37,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166231ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:44:01,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166231049372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:44:34,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.2s/56225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:46:19,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.2s/56225232434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:46:49,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/130120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:47:14,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/130119701001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:47:34,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:48:35,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50427959823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:49:45,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:50:42,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126814198694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:50:39,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@4b82d07d] took [126814ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:51:37,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:52:07,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116046972337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:53:14,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:53:55,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96915002772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:54:33,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:55:20,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78325864501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:56:08,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:56:02,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [78325ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:57:06,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95624450037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:58:55,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:59:45,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165676870689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:01:51,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:03:57,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176378460972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:05:43,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232004ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:02:09,138][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [342055ms] which is above the warn threshold of [5s]
[2022-04-04T19:07:48,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231744533697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:10:05,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/261139ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:12:43,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260871492204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:15:19,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:18:31,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314523540219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:22:57,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:27:18,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456717467346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:32:52,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9m/595769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:37:19,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9m/595770995381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:41:21,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/509180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:45:01,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/509146794334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:47:55,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:49:50,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394408344863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:53:00,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:56:05,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304634990104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:58:45,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T20:01:31,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344855409527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T20:00:04,898][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [52s/52034ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@470bfe7f]], which exceeds the warn threshold of [10s]
[2022-04-04T20:04:58,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/356421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T20:07:57,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/356149644022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T20:07:59,303][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [27.6s/27666ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ea1b8370], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ea052ce], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a75b6627]], which exceeds the warn threshold of [10s]
[2022-04-04T20:10:57,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T20:11:34,298][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.8s/26895ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ea1b8370], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ea052ce], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a75b6627]], which exceeds the warn threshold of [10s]
[2022-04-04T20:20:52,409][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-04T20:20:52,434][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-04T20:20:52,435][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-04T21:21:33,762][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-04T21:21:33,790][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-04T21:21:33,793][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-04T21:21:39,534][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-04T21:21:39,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-04T21:21:39,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-04T21:21:39,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-04T21:21:39,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-04T21:21:39,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-04T21:21:39,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-04T21:21:39,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-04T21:21:39,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-04T21:21:39,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-04T21:21:39,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-04T21:21:39,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-04T21:21:39,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-04T21:21:39,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-04T21:21:39,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-04T21:21:39,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-04T21:21:39,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-04T21:21:39,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-04T21:21:39,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-04T21:21:39,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-04T21:21:39,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-04T21:21:39,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-04T21:21:39,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-04T21:21:39,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-04T21:21:39,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-04T21:21:39,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-04T21:21:39,545][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-04T21:21:39,545][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-04T21:21:39,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-04T21:21:39,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-04T21:21:39,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-04T21:21:39,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-04T21:21:39,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-04T21:21:39,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-04T21:21:39,549][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-04T21:21:39,549][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-04T21:21:39,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-04T21:21:39,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-04T21:21:39,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-04T21:21:39,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-04T21:21:39,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-04T21:21:39,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-04T21:21:39,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-04T21:21:39,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-04T21:21:39,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-04T21:21:39,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-04T21:21:39,556][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-04T21:21:39,630][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-04T21:21:39,632][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-04T21:21:40,086][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-04T21:21:51,800][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-04T21:21:51,809][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-04T21:21:51,810][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T21:21:51,812][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-04T21:21:51,813][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-04T21:21:51,814][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-04T21:21:51,815][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-04T21:21:51,816][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-04T21:21:51,817][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-04T21:21:51,818][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-04T21:21:51,819][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-04T21:21:51,820][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T21:21:51,822][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T21:21:51,823][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-04T21:21:51,826][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-04T21:21:53,780][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-04T21:21:53,966][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-04T21:21:55,194][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-04T21:21:56,075][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-04T21:21:56,076][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-04T21:21:56,155][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-04T21:21:56,157][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-04T21:21:56,405][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-04T21:21:59,066][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-04T21:21:59,201][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 202, version: 7270, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-04T21:21:59,365][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 202, version: 7270, reason: Publication{term=202, version=7270}
[2022-04-04T21:21:59,494][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-04T21:21:59,495][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-04T21:22:00,811][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-04T21:22:00,820][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [38] indices into cluster_state
[2022-04-04T21:22:02,243][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-04T21:22:02,259][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-04T21:22:03,929][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-04T21:22:03,937][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-04T21:22:03,938][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-04T21:22:04,858][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T21:22:04,878][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T21:22:06,197][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][10] overhead, spent [339ms] collecting in the last [1s]
[2022-04-04T21:22:07,441][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-04T21:22:08,113][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T21:22:11,445][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [.kibana-event-log-7.16.2-000001][0]]]).
[2022-04-04T21:22:26,723][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:27,570][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:29,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:31,799][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:54,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5975ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:22:57,701][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16432ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [37] unchanged indices
[2022-04-04T21:23:06,431][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [30s] publication of cluster state version [7310] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-04T21:23:16,300][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:23:32,164][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:23:35,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:23:36,806][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:24:08,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5703ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:16,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [5087ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:17,412][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46120}] took [9088ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:17,412][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [9088ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:30,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5031ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:37,197][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:24:42,433][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:24:58,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@28cee0ee] took [5892ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:25:28,860][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:28:27,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [22200ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:28:27,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21855ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:28:28,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21855099369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:29:13,544][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48840}] took [12270ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:30:43,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:45,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977872114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:49,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:52,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7226746666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:58,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:01,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9246177365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:07,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:11,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8872781864ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:13,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:15,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6224912526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:16,311][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][325][18] duration [5.5s], collections [1]/[9.1s], total [5.5s]/[6.9s], memory [1.3gb]->[200.5mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [191.4mb]->[191.4mb]/[2gb]}{[survivor] [8.6mb]->[9mb]/[0b]}
[2022-04-04T21:31:26,960][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][325] overhead, spent [5.5s] collecting in the last [9.1s]
[2022-04-04T21:31:31,881][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [6262ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:31:36,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [62817ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:31:51,819][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [5928ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:03,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@15c7a3ae] took [10065ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:10,858][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [6261ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:09,720][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46124}] took [5260ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:20,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [8004ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:57,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:32:58,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [30962ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:02,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29761402099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:04,152][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [31367ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:04,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:05,637][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48854}] took [42274ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:05,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8375843755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:07,624][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8376ms] which is above the warn threshold of [5s]
[2022-04-04T21:33:26,075][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][327][19] duration [19.6s], collections [1]/[1m], total [19.6s]/[26.6s], memory [232.5mb]->[242.8mb]/[2gb], all_pools {[young] [36mb]->[40mb]/[0b]}{[old] [191.4mb]->[191.4mb]/[2gb]}{[survivor] [9mb]->[11.3mb]/[0b]}
[2022-04-04T21:33:32,396][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][327] overhead, spent [19.6s] collecting in the last [1m]
[2022-04-04T21:33:41,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:46,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [23412ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:51,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8468554913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:45,831][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [15207ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:54,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:00,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13079030213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:03,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9328ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:04,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [9328ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:06,599][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:34486}] took [22407ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:07,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9328044536ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:10,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@699488cc, interval=1m}] took [5385ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:09,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5386ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:19,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5385981144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:25,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:25,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@a2855c7] took [16079ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:32,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16079675490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:48,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:56,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [23376ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:57,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23376511838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:59,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:01,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10487589592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:00,377][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [33864ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:35:17,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:17,919][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][329][20] duration [5.2s], collections [1]/[44.9s], total [5.2s]/[31.8s], memory [274.8mb]->[239mb]/[2gb], all_pools {[young] [72mb]->[72mb]/[0b]}{[old] [191.4mb]->[195.1mb]/[2gb]}{[survivor] [11.3mb]->[7.9mb]/[0b]}
[2022-04-04T21:35:17,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13487358761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:24,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:25,616][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][330][21] duration [4.3s], collections [1]/[1.1s], total [4.3s]/[36.2s], memory [239mb]->[287mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [7.9mb]->[7mb]/[0b]}
[2022-04-04T21:35:25,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5283815124ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:29,286][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][330] overhead, spent [4.3s] collecting in the last [1.1s]
[2022-04-04T21:35:39,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [13424ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:35:40,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:41,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7624588175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:45,231][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46204}] took [5150ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:35:57,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][338][22] duration [1s], collections [1]/[2.7s], total [1s]/[37.2s], memory [262.1mb]->[205.8mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [7mb]->[6.7mb]/[0b]}
[2022-04-04T21:35:58,098][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][338] overhead, spent [1s] collecting in the last [2.7s]
[2022-04-04T21:36:26,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@7c78f1e] took [5565ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:36:43,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:45,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16160869698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:46,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][350][23] duration [10.3s], collections [1]/[26.8s], total [10.3s]/[47.6s], memory [285.8mb]->[254.6mb]/[2gb], all_pools {[young] [84mb]->[52mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [6.7mb]->[7.5mb]/[0b]}
[2022-04-04T21:36:48,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:48,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5246687026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:48,954][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][350] overhead, spent [10.3s] collecting in the last [26.8s]
[2022-04-04T21:36:49,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5246ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:36:53,280][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][351][24] duration [897ms], collections [1]/[7s], total [897ms]/[48.5s], memory [254.6mb]->[271.1mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [7.5mb]->[8mb]/[0b]}
[2022-04-04T21:36:53,377][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [2036] timed out after [29279ms]
[2022-04-04T21:36:53,698][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2037] timed out after [26477ms]
[2022-04-04T21:36:58,189][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [31.9s/31998ms] ago, timed out [5.5s/5521ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2037]
[2022-04-04T21:36:58,101][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [34.8s/34800ms] ago, timed out [5.5s/5521ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2036]
[2022-04-04T21:37:40,142][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][382][27] duration [1.5s], collections [1]/[3.2s], total [1.5s]/[50.5s], memory [278.6mb]->[207.2mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [3.5mb]->[4.1mb]/[0b]}
[2022-04-04T21:37:41,511][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][382] overhead, spent [1.5s] collecting in the last [3.2s]
[2022-04-04T21:37:44,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7180ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:28,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@2c7d6a86] took [6651ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:40,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6617ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:40,502][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [7218ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:54,649][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [25.8s/25879ms] ago, timed out [2.5s/2547ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2206]
[2022-04-04T21:38:53,483][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2206] timed out after [23332ms]
[2022-04-04T21:39:18,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:39:32,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5309ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:39:43,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6533ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:07,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [18292ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:07,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:39:42,937][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6134ms] which is above the warn threshold of [5s]
[2022-04-04T21:40:07,788][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [18893ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:08,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16891727782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:40:09,110][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][399][28] duration [11.4s], collections [1]/[31.7s], total [11.4s]/[1m], memory [275.2mb]->[241.3mb]/[2gb], all_pools {[young] [76mb]->[64mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [4.1mb]->[6.2mb]/[0b]}
[2022-04-04T21:40:12,117][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][399] overhead, spent [11.4s] collecting in the last [31.7s]
[2022-04-04T21:40:14,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7403ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:25,368][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][401][29] duration [2s], collections [1]/[4.4s], total [2s]/[1m], memory [281.3mb]->[200.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [6.2mb]->[5.5mb]/[0b]}
[2022-04-04T21:40:25,706][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][401] overhead, spent [2s] collecting in the last [4.4s]
[2022-04-04T21:40:59,941][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][423][30] duration [1.4s], collections [1]/[3.1s], total [1.4s]/[1m], memory [280.6mb]->[203.8mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [5.5mb]->[4.7mb]/[0b]}
[2022-04-04T21:41:00,650][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][423] overhead, spent [1.4s] collecting in the last [3.1s]
[2022-04-04T21:42:17,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:20,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [14512ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:20,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11896490423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:24,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:24,619][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [12297ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:28,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8036769285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:34,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9619ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:37,747][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][447][31] duration [7.3s], collections [1]/[23.1s], total [7.3s]/[1.2m], memory [283.8mb]->[203.1mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [4.7mb]->[8mb]/[0b]}
[2022-04-04T21:42:41,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9619017568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:44,278][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][447] overhead, spent [7.3s] collecting in the last [23.1s]
[2022-04-04T21:42:47,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:50,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [31277ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:52,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13621711709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:55,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:59,421][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [7696ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:59,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7695668594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:43:02,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@710cc29a] took [7451ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:43:02,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:43:05,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7451315763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:43:03,045][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7451ms] which is above the warn threshold of [5s]
[2022-04-04T21:43:12,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [9315ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:43:59,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:04,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37504541565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:08,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:09,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [9227ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:44:15,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9227140480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:21,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:25,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][449][32] duration [25.5s], collections [1]/[1.1m], total [25.5s]/[1.6m], memory [255.1mb]->[242.5mb]/[2gb], all_pools {[young] [56mb]->[56mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [8mb]->[7.4mb]/[0b]}
[2022-04-04T21:44:27,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12392676343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:32,378][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][449] overhead, spent [25.5s] collecting in the last [1.1m]
[2022-04-04T21:44:35,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:39,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [26612ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:44:39,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14219764236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:44,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:45,828][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46204}] took [8662ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:44:49,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8661880585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:55,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:00,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11035757354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:14,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:14,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [18993ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:45:21,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18993924564ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:00,641][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2500] timed out after [100988ms]
[2022-04-04T21:45:31,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:37,497][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46204}] took [16224ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:45:40,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16224438112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:48,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [16224ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:45:50,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:46:53,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20305212381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:46:57,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:00,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67108307721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:46:58,398][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.7m/167548ms] ago, timed out [1.1m/66560ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2500]
[2022-04-04T21:47:00,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [67108ms] which is above the warn threshold of [5s]
[2022-04-04T21:47:02,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:04,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5116869710ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:07,041][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][451][33] duration [46.5s], collections [1]/[1.6m], total [46.5s]/[2.4m], memory [286.5mb]->[206.9mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.2mb]/[2gb]}{[survivor] [7.4mb]->[7.6mb]/[0b]}
[2022-04-04T21:47:09,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:14,072][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][451] overhead, spent [46.5s] collecting in the last [1.6m]
[2022-04-04T21:47:14,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6832855751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:21,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11949ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:47:21,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:29,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627329739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:33,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:33,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@4efe2eed] took [12182ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:47:34,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12182815116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:50,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6420ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:48:03,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5127ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:48:29,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:33,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22309355948ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:41,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:49,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12377372285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:55,525][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][455][34] duration [15.4s], collections [1]/[33.6s], total [15.4s]/[2.6m], memory [278.9mb]->[222.1mb]/[2gb], all_pools {[young] [76mb]->[20mb]/[0b]}{[old] [195.2mb]->[195.2mb]/[2gb]}{[survivor] [7.6mb]->[6.8mb]/[0b]}
[2022-04-04T21:49:01,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:04,683][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][455] overhead, spent [15.4s] collecting in the last [33.6s]
[2022-04-04T21:49:10,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19502549747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:14,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [31879ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:49:17,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:23,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17108805960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:33,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:40,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15032179324ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:48,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:59,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14359911396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:06,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:13,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19501640697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:23,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [14940ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:50:22,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:30,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14940888758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:41,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:24,127][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14941ms] which is above the warn threshold of [5s]
[2022-04-04T21:50:47,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20070632899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:58,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:04,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17371606502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:10,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [17371ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:50:58,984][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2661] timed out after [48802ms]
[2022-04-04T21:51:15,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:17,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15557537127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:19,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:21,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5599444142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:26,152][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:34588}] took [5959ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:51:30,633][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/117971ms] ago, timed out [1.1m/69169ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2661]
[2022-04-04T21:51:48,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:52:46,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7993ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:52:29,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7753898691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:38,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [107846ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:53:38,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:53,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107846637336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:55,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20181ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:57,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20181937776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:01,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:05,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5465054091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:10,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@75186ca9] took [9502ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:10,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:12,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9502803814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:21,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:21,274][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][475][35] duration [4.6s], collections [1]/[2.5m], total [4.6s]/[2.7m], memory [278.1mb]->[282.1mb]/[2gb], all_pools {[young] [76mb]->[84mb]/[0b]}{[old] [195.2mb]->[195.2mb]/[2gb]}{[survivor] [6.8mb]->[6.8mb]/[0b]}
[2022-04-04T21:54:21,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10947405366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:21,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [10947ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:30,633][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][482][36] duration [944ms], collections [1]/[2s], total [944ms]/[2.7m], memory [248.7mb]->[199.7mb]/[2gb], all_pools {[young] [51.9mb]->[0b]/[0b]}{[old] [195.4mb]->[195.4mb]/[2gb]}{[survivor] [5.2mb]->[4.2mb]/[0b]}
[2022-04-04T21:54:30,887][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][482] overhead, spent [944ms] collecting in the last [2s]
[2022-04-04T21:54:46,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6947ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:56,503][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46244}] took [14566ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:56,503][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46240}] took [8721ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:59,560][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46242}] took [10709ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:04,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [5192ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:46,180][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46244}] took [7067ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:46,910][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46240}] took [7067ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:46,180][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46242}] took [10675ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:53,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [27894ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:56:28,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:56:32,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@6694988f] took [18189ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:56:46,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5007815499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:57:57,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:57:59,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90055161178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:57:59,216][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46244}] took [90055ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:58:00,687][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][490][38] duration [52.7s], collections [1]/[2.5m], total [52.7s]/[3.6m], memory [265.9mb]->[244.1mb]/[2gb], all_pools {[young] [64mb]->[40mb]/[0b]}{[old] [195.5mb]->[195.5mb]/[2gb]}{[survivor] [6.4mb]->[8.5mb]/[0b]}
[2022-04-04T21:58:02,239][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][490] overhead, spent [52.7s] collecting in the last [2.5m]
[2022-04-04T21:58:04,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6314ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:58:15,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][496][39] duration [1.2s], collections [1]/[2.2s], total [1.2s]/[3.6m], memory [276.1mb]->[205.4mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [195.5mb]->[195.5mb]/[2gb]}{[survivor] [8.5mb]->[5.9mb]/[0b]}
[2022-04-04T21:58:15,831][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][496] overhead, spent [1.2s] collecting in the last [2.2s]
[2022-04-04T21:58:30,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7942ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:58:57,886][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46250}] took [9972ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:10,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5086ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:25,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@4fcd556d] took [8897ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:37,410][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46254}] took [8625ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:58,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [14164ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:06,432][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46256}] took [12141ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:14,331][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [12098ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:24,414][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46250}] took [15542ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:43,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [29884ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:08,026][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46256}] took [14795ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:12,712][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [10201ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:10,766][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [15154ms] which is above the warn threshold of [5s]
[2022-04-04T22:01:35,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [33313ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:39,305][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2974] timed out after [87008ms]
[2022-04-04T22:01:59,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [7770ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:02:54,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [9244ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:03:16,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@5c13cc89] took [8106ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:03:39,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11372ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:03:59,965][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [11044ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:04:42,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11959ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:04:51,128][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16706ms] which is above the warn threshold of [5s]
[2022-04-04T22:04:55,761][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [6523ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:04:38,060][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3067] timed out after [74392ms]
[2022-04-04T22:05:09,716][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46256}] took [14191ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:05:21,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [6113ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:05:40,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [9247ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:07:23,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:27,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [71954ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:07:29,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69764931177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:39,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:47,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16758865775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:58,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:04,866][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][508][40] duration [55.1s], collections [1]/[2.1m], total [55.1s]/[4.5m], memory [269.4mb]->[231.9mb]/[2gb], all_pools {[young] [72mb]->[28mb]/[0b]}{[old] [195.5mb]->[195.5mb]/[2gb]}{[survivor] [5.9mb]->[8.4mb]/[0b]}
[2022-04-04T22:08:07,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19099492570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:16,654][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][508] overhead, spent [55.1s] collecting in the last [2.1m]
[2022-04-04T22:08:19,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:24,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [56953ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:08:24,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21094733653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:31,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:40,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12148014411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:49,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@f49fd32] took [17846ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:08:49,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:51,537][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.1m/547127ms] ago, timed out [7.6m/460119ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2974]
[2022-04-04T22:08:54,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17846164305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:59,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:02,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9908595250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:07,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:09,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@22b0b54, interval=30s}] took [8402ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:09:13,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8402753944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:19,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:24,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11751ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:09:25,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11751141849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:33,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:28,269][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11751ms] which is above the warn threshold of [5s]
[2022-04-04T22:09:43,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14075381371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:44,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [14075ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:09:49,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:53,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15547642529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:00,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:04,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10870218465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:57,965][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3147] timed out after [30063ms]
[2022-04-04T22:10:17,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:26,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16239333629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:37,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:38,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [36777ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:10:47,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20537715519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:59,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:11,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21503090877ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:20,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:29,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21794797389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:35,128][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.5m/150631ms] ago, timed out [2m/120568ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3147]
[2022-04-04T22:11:39,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:40,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@6888a970] took [18712ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:11:50,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18712842696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:04,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:12,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24659884278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:18,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [24659ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:12:19,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15877ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:28,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15877300346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:37,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:41,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@23f18c76] took [16246ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:12:45,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246309898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:59,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:11,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23227842713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:23,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19828766215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:41,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [22179ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:13:41,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:23,138][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19829ms] which is above the warn threshold of [5s]
[2022-04-04T22:13:47,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22179064702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:55,407][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [3193] timed out after [65236ms]
[2022-04-04T22:13:56,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15261459858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:12,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:19,352][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.6m/96212ms] ago, timed out [30.9s/30976ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3193]
[2022-04-04T22:14:22,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15715071633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:34,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:15:41,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [37844ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:15:44,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22129186581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:15:48,109][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3194] timed out after [65236ms]
[2022-04-04T22:15:55,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:05,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81111481528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:08,504][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [12.6m/759349ms] ago, timed out [11.4m/684957ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3067]
[2022-04-04T22:16:13,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:24,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18381155958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:34,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:43,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20959715292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:54,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19356ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:03,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19355907736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:14,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:26,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19721859001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:35,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:44,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21530937150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:46,551][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][515][41] duration [52.6s], collections [1]/[2.8m], total [52.6s]/[5.4m], memory [283.9mb]->[211.3mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [195.5mb]->[195.6mb]/[2gb]}{[survivor] [8.4mb]->[7.6mb]/[0b]}
[2022-04-04T22:17:55,949][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][515] overhead, spent [52.6s] collecting in the last [2.8m]
[2022-04-04T22:17:55,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:05,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [60510ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:18:09,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19257728093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:22,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:22,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [27450ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:18:32,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27450776175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:41,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:56,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19736962297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:08,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:44,074][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19737ms] which is above the warn threshold of [5s]
[2022-04-04T22:19:22,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@78f6c272] took [26059ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:19:22,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26059075172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:33,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25854ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:43,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25853904079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:56,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:09,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21916724990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:25,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:35,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29642521505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:45,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:51,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [50028ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:20:54,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20385664294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:05,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:15,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20053894360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:26,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:32,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21010499360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:39,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:20,834][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [3233] timed out after [117853ms]
[2022-04-04T22:21:46,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13272462839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:56,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:06,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16075109078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:16,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [16075ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:22:19,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:22,769][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.3m/560118ms] ago, timed out [8.2m/494882ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3194]
[2022-04-04T22:22:28,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23597941295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:10,223][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3234] timed out after [117853ms]
[2022-04-04T22:22:36,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:41,510][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/208969ms] ago, timed out [1.5m/91116ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3233]
[2022-04-04T22:22:46,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17159778182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:46,247][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17160ms] which is above the warn threshold of [5s]
[2022-04-04T22:23:00,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:06,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20260062255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:12,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:19,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16011784996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:20,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [16011ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:23:55,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:05,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41445972636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:19,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:23,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [25178ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:24:37,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25178135099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:51,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:50,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [30936ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:25:03,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30936518134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:25:14,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:25:28,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24468637524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:25:42,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:01,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27165321559ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:10,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:20,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207655369701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:31,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:33,485][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][519][42] duration [2.8m], collections [1]/[2.4m], total [2.8m]/[8.3m], memory [279.3mb]->[287.3mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [195.6mb]->[195.8mb]/[2gb]}{[survivor] [7.6mb]->[7.8mb]/[0b]}
[2022-04-04T22:29:38,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21555543113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:42,281][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][519] overhead, spent [2.8m] collecting in the last [2.4m]
[2022-04-04T22:29:50,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:51,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [273461ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:29:58,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17085261205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:09,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:19,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20740837273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:35,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25860ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:41,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@699488cc, interval=1m}] took [25860ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:30:54,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25860315536ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:31:13,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:31:33,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37454910155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:31:53,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:32:14,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40368561618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:32:30,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:32:45,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37640683062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:03,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:20,460][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.7m/465796ms] ago, timed out [30.2s/30269ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3317]
[2022-04-04T22:33:17,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30269580044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:34,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:50,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33332286844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:27,411][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30269ms] which is above the warn threshold of [5s]
[2022-04-04T22:34:06,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:34:25,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31379423861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:34:42,458][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.9m/897778ms] ago, timed out [12.9m/779925ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3234]
[2022-04-04T22:34:42,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:34:59,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36411097570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:35:16,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:35:19,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [70644ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:35:33,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34233472134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:35:11,387][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3317] timed out after [435527ms]
[2022-04-04T22:35:46,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:03,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30420920688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:18,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31679717438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:49,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:10,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30071183404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:19,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:27,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30907423774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:39,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:44,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [18366ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:37:48,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18366219967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:01,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:07,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22829045427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:15,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:22,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14900631560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:25,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@1c1fa413] took [14900ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:38:12,893][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [22829ms] which is above the warn threshold of [5s]
[2022-04-04T22:38:34,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:44,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18921253362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:56,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22547ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:39:14,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22546509987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:39:29,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:39:45,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [53601ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:39:48,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31055287066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:40:05,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:40:19,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37647648786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:40:45,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:02,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39674179728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:25,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:39,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38879229091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:58,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:13,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33888983880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:28,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:30,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [64486ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:42:41,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30597016728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:53,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:43:04,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24779594615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:43:18,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:57,220][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3400] timed out after [149844ms]
[2022-04-04T22:43:41,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24886054820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:43:55,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:44:09,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36901028175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:44:28,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:44:44,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33319325103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:03,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [33319ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:45:04,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:13,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34462188023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:26,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:35,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23664432958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:45,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:55,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17458558709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:04,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:13,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20676231077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:23,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:36,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18135343578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:45,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:51,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@1126dfed] took [22084ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:46:52,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22084088038ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:00,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:10,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15840895290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:18,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:24,567][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9m/542620ms] ago, timed out [6.5m/392776ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3400]
[2022-04-04T22:47:25,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [33043ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:47:26,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17202498208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:44,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:50,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [24120ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:48:04,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24120359859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:48:38,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:49:04,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53423123623ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:49:27,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:49:42,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52409280078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:50:03,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:50:55,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35498709367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:51:21,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:51:36,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [75637ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:51:53,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75637033503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:52:12,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:52:30,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52634908852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:52:54,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:03,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [41321ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:53:15,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41321054048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:36,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.8s/42895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:56,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.8s/42895440169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:17,393][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3520] timed out after [198494ms]
[2022-04-04T22:54:17,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:54:47,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39315723304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:54:18,687][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15s/15093ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3db32d6]], which exceeds the warn threshold of [10s]
[2022-04-04T22:55:19,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:55:47,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60969356159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:56:32,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:04,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70164925930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:24,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.5s/56558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:34,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.5s/56557642657ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:45,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:26,702][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [56558ms] which is above the warn threshold of [5s]
[2022-04-04T22:57:57,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22534452175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:08,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [22534ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:58:08,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:19,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23318872903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:34,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:47,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25250698566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:57,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:06,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22956932481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:21,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:33,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24613723541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:35,470][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [12.6m/756665ms] ago, timed out [9.3m/558171ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3520]
[2022-04-04T22:59:43,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:55,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22135634804ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:55,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@446047b4] took [22135ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:00:04,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:16,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20948706466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:26,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:39,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21914790108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:58,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:01:12,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [29709ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:01:17,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29709342501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:01:37,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:01:39,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [40592ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:01:48,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40592948488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:00,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:17,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22187321731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:33,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:50,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33676567702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:03:13,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:03:37,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40660407227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:02,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:23,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48518255481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:41,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:56,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38897946006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:05:20,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:05:40,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [38215ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:05:46,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38215845809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:05:01,341][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38898ms] which is above the warn threshold of [5s]
[2022-04-04T23:06:23,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:06:24,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [59607ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:06:44,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59607993168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:07:09,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:06:51,121][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3640] timed out after [258208ms]
[2022-04-04T23:07:40,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48419132392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:08:28,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:09:00,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81223781150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:09:51,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:10:22,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79745094271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:10:51,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:11:13,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60189232509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:11:51,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:12:19,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62027755219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:12:41,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:12:56,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51262741400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:07,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:23,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26071253942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:39,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:52,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [31488ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:13:59,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31488761223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:23,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:39,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43501481654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:51,823][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.6m/878859ms] ago, timed out [10.3m/620651ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3640]
[2022-04-04T23:14:54,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:56,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31696952325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:15:00,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:15:01,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@8d28e73] took [5896ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:15:01,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5896063865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:15:00,439][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5896ms] which is above the warn threshold of [5s]
[2022-04-04T23:15:18,952][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3726] timed out after [17237ms]
[2022-04-04T23:16:26,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:31,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [61771ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:16:36,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59569760262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:46,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19367ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:51,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19366663933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:30,625][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [24.4s/24411ms] ago, timed out [7.1s/7174ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3726]
[2022-04-04T23:16:59,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:07,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13045409526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:19,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:32,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][536][43] duration [51.7s], collections [1]/[1.3m], total [51.7s]/[9.1m], memory [287.6mb]->[203.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [195.8mb]->[195.8mb]/[2gb]}{[survivor] [7.8mb]->[7.5mb]/[0b]}
[2022-04-04T23:17:43,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19934313273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:55,792][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][536] overhead, spent [51.7s] collecting in the last [1.3m]
[2022-04-04T23:17:58,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:05,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [91833ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:18:10,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39486621965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:19,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:30,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20679284443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:40,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:49,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20633272573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:02,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:14,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22651602337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:26,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:37,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@5cc22377] took [23685ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:19:39,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23685843549ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:48,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21685ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:00,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21685023969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:14,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:17,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [45882ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:19:54,583][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21685ms] which is above the warn threshold of [5s]
[2022-04-04T23:20:24,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24197756662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:32,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:40,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19115545240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:49,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:57,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18125060316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:05,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:04,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [15226ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:21:14,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15226825422ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:31,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:42,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26923812905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:53,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:00,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [21976ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:22:07,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21976737238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:15,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:23,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22012613530ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:34,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:45,554][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.9s/16997ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b6f8725]], which exceeds the warn threshold of [10s]
[2022-04-04T23:22:49,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300448360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:57,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:41,040][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3804] timed out after [98350ms]
[2022-04-04T23:23:11,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24059187678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:29,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:39,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30027684729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:54,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:56,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [56088ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:24:08,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26061026401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:22,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:43,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28488721009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:59,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:25:16,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36399531021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:45,949][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28488ms] which is above the warn threshold of [5s]
[2022-04-04T23:25:29,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:25:42,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30124124956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:25:58,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:11,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28860278981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:27,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:41,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29063809368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:39,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [57924ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:26:57,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:27:16,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27540803613ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:27:36,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39842ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:27:49,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39841339072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:11,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:36,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37017358838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:57,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:44,098][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.7m/525048ms] ago, timed out [7.1m/426698ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3804]
[2022-04-04T23:29:07,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46015049522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:29:27,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:29:38,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30104843637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:29:50,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:01,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22118185209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:10,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:22,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20632077155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:33,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [20632ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:30:35,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:45,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24820120030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:53,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:59,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18242684878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:05,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:09,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11995220568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:13,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:02,756][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3924] timed out after [220549ms]
[2022-04-04T23:31:02,766][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18243ms] which is above the warn threshold of [5s]
[2022-04-04T23:31:15,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [8644ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:17,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8644739260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:22,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:24,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [8706ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:27,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8706469542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:34,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:35,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [11833ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:39,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11833759835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:46,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:47,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11923ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:52,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11923666599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:57,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:01,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10998244224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:05,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:06,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7919ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:32:10,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7919322076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:12,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:12,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@774ccb83] took [7908ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:32:13,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7908052195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:18,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5069ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:32:35,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [10709ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:34:36,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [26054ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:35:26,729][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.5m/510410ms] ago, timed out [4.8m/289861ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3924]
[2022-04-04T23:35:28,806][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [22313ms] which is above the warn threshold of [5s]
[2022-04-04T23:35:11,736][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [4044] timed out after [69297ms]
[2022-04-04T23:36:57,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [35247ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:37:17,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [6093ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:39:09,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@6888a970] took [26279ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:39:24,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:39:10,090][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.7m/405187ms] ago, timed out [5.5m/335890ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [4044]
[2022-04-04T23:42:26,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5439868730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:44:16,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:45:36,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291876189532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:47:20,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:48:59,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184907767311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:49:48,668][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.4s/12412ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a6eab7c7]], which exceeds the warn threshold of [10s]
[2022-04-04T23:50:52,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:51:19,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211392600087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:51:42,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:51:57,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@43f594c2] took [260851ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:52:14,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49458794669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:52:58,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:53:32,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76251689235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:53:59,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:54:34,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [61740ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:54:53,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61740760667ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:55:37,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:56:08,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97277007972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:57:07,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:57:15,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [91020ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:58:12,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91020999537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:59:31,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139783ms] on absolute clock which is above the warn threshold of [5000ms]
