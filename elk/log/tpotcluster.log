[2022-04-27T16:30:53,249][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-27T16:30:53,290][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-27T16:30:53,293][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-27T16:30:59,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-27T16:30:59,012][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-27T16:30:59,013][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-27T16:30:59,013][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-27T16:30:59,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-27T16:30:59,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-27T16:30:59,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-27T16:30:59,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-27T16:30:59,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-27T16:30:59,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-27T16:30:59,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-27T16:30:59,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-27T16:30:59,018][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-27T16:30:59,018][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-27T16:30:59,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-27T16:30:59,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-27T16:30:59,021][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-27T16:30:59,021][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-27T16:30:59,022][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-27T16:30:59,022][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-27T16:30:59,023][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-27T16:30:59,023][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-27T16:30:59,024][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-27T16:30:59,024][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-27T16:30:59,025][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-27T16:30:59,026][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-27T16:30:59,026][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-27T16:30:59,027][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-27T16:30:59,027][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-27T16:30:59,027][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-27T16:30:59,028][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-27T16:30:59,028][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-27T16:30:59,029][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-27T16:30:59,029][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-27T16:30:59,029][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-27T16:30:59,030][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-27T16:30:59,030][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-27T16:30:59,032][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-27T16:30:59,032][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-27T16:30:59,033][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-27T16:30:59,035][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-27T16:30:59,036][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-27T16:30:59,036][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-27T16:30:59,037][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-27T16:30:59,037][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-27T16:30:59,037][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-27T16:30:59,038][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-27T16:30:59,038][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-27T16:30:59,038][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-27T16:30:59,039][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-27T16:30:59,039][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-27T16:30:59,039][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-27T16:30:59,040][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-27T16:30:59,040][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-27T16:30:59,041][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-27T16:30:59,041][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-27T16:30:59,041][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-27T16:30:59,041][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-27T16:30:59,042][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-27T16:30:59,100][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.8gb], net total_space [125.8gb], types [ext4]
[2022-04-27T16:30:59,101][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-27T16:30:59,343][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-27T16:31:10,174][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-27T16:31:10,184][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-27T16:31:11,377][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-27T16:31:11,550][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-27T16:31:12,511][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-27T16:31:13,527][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-27T16:31:13,528][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-27T16:31:13,558][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-27T16:31:13,559][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-27T16:31:13,762][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-27T16:31:17,679][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-27T16:31:17,883][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Rk_nYcIVQkql87tIMmQsyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 317, version: 14517, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Rk_nYcIVQkql87tIMmQsyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-27T16:31:18,223][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Rk_nYcIVQkql87tIMmQsyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 317, version: 14517, reason: Publication{term=317, version=14517}
[2022-04-27T16:31:18,318][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-27T16:31:18,319][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-27T16:31:19,433][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-27T16:31:19,444][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [66] indices into cluster_state
[2022-04-27T16:31:20,343][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-27T16:31:20,346][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-27T16:31:21,532][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-27T16:31:21,854][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-27T16:31:22,354][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-27T16:31:22,658][WARN ][o.e.i.t.Translog         ] [tpotcluster-node-01] [logstash-1970.01.01][0] deleted previously created, but not yet committed, next generation [translog-14.tlog]. This can happen due to a tragic exception when creating a new generation
[2022-04-27T16:31:23,221][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-27T16:31:23,224][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-27T16:31:23,224][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-27T16:31:24,091][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-27T16:31:24,575][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-27T16:31:24,963][WARN ][o.e.i.t.Translog         ] [tpotcluster-node-01] [.ds-.logs-deprecation.elasticsearch-default-2022.04.23-000004][0] deleted previously created, but not yet committed, next generation [translog-16.tlog]. This can happen due to a tragic exception when creating a new generation
[2022-04-27T16:31:28,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-27T16:31:36,197][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.26][0]]]).
[2022-04-27T16:31:42,141][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-27T16:31:42,329][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-27T16:32:25,296][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 857 finished with response BulkByScrollResponse[took=256.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-27T16:32:27,030][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 883 finished with response BulkByScrollResponse[took=1.7s,timed_out=false,sliceId=null,updated=921,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-27T16:32:34,805][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-27T16:32:56,682][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:32:56,692][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:32:56,835][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:32:56,840][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:32:56,983][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:33:02,345][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:33:03,156][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:33:03,400][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:33:05,430][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:33:34,914][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:33:50,128][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [18.8s/18898ms] to compute cluster state update for [put-mapping [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA][_doc]], which exceeds the warn threshold of [10s]
[2022-04-27T16:33:47,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6150cf74, interval=1s}] took [9827ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:33:55,630][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50402}] took [8755ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:34:28,295][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50406}] took [12385ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:36:39,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:36:39,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7317975164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:36:40,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:36:40,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79950508968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:36:41,927][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [133690ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [65] unchanged indices
[2022-04-27T16:36:43,077][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.7m] publication of cluster state version [14620] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Rk_nYcIVQkql87tIMmQsyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-27T16:36:51,698][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:36:52,408][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:36:53,541][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:36:55,304][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:36:57,568][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:36:59,257][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:37:17,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5323ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:38:51,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6150cf74, interval=1s}] took [8512ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:39:01,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:39:01,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111942464125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:39:01,607][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50402}] took [111943ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:39:02,869][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [123338ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [65] unchanged indices
[2022-04-27T16:39:03,366][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2m] publication of cluster state version [14626] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Rk_nYcIVQkql87tIMmQsyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-27T16:39:06,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:39:09,030][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:39:12,940][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:39:14,360][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:39:16,470][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:39:22,272][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:39:56,539][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50402}] took [6336ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:39:56,539][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50442}] took [6136ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:40:12,430][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][16] duration [3.9s], collections [1]/[6.5s], total [3.9s]/[4.9s], memory [1.4gb]->[292.9mb]/[2gb], all_pools {[young] [1.1gb]->[16mb]/[0b]}{[old] [271.9mb]->[271.9mb]/[2gb]}{[survivor] [10mb]->[12.9mb]/[0b]}
[2022-04-27T16:40:24,396][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [3.9s] collecting in the last [6.5s]
[2022-04-27T16:40:37,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6150cf74, interval=1s}] took [58512ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:40:55,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50444}] took [8205ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:41:12,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:41:30,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5079606698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:41:40,684][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50402}] took [5881ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:41:44,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:41:54,012][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50442}] took [31435ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:42:31,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31435137395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:42:46,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.1s/58196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:42:54,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.1s/58196221419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:43:09,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:43:19,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26758609365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:42:56,007][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50430}] took [58196ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:43:33,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:43:34,913][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][PUT][/.kibana_7.17.0/_doc/event_loop_delays_daily%3A9c25d902-b01c-4008-bc7b-9f03bccf35db%3A%3A1%3A%3A27042022?refresh=wait_for&require_alias=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:47250}] took [108786ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:43:40,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23831353661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:44:09,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35939ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:44:14,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35939085824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:44:16,355][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50430}] took [35939ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:44:34,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][17] duration [3.5s], collections [1]/[4.4m], total [3.5s]/[8.5s], memory [292.9mb]->[320.9mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [271.9mb]->[281.3mb]/[2gb]}{[survivor] [12.9mb]->[3.8mb]/[0b]}
[2022-04-27T16:44:34,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:44:34,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6150cf74, interval=1s}] took [61750ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:44:34,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25811287250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:44:35,501][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:47304}] took [26967ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:44:43,113][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][170] overhead, spent [550ms] collecting in the last [1.5s]
[2022-04-27T16:45:12,538][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][191][19] duration [910ms], collections [1]/[3.2s], total [910ms]/[10s], memory [371.5mb]->[299.8mb]/[2gb], all_pools {[young] [88mb]->[60mb]/[0b]}{[old] [281.3mb]->[281.3mb]/[2gb]}{[survivor] [2.1mb]->[6.4mb]/[0b]}
[2022-04-27T16:45:13,099][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][191] overhead, spent [910ms] collecting in the last [3.2s]
[2022-04-27T16:45:17,350][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:45:29,144][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50490}] took [5905ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:45:34,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][20] duration [3.4s], collections [1]/[2.8s], total [3.4s]/[13.4s], memory [363.8mb]->[375.8mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [281.3mb]->[281.3mb]/[2gb]}{[survivor] [6.4mb]->[8mb]/[0b]}
[2022-04-27T16:45:34,919][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [3.4s] collecting in the last [2.8s]
[2022-04-27T16:45:38,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6150cf74, interval=1s}] took [14731ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:45:45,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@39077c4d, interval=5s}] took [5388ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:45:45,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T16:45:45,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5188277155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T16:45:47,543][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [17759ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [65] unchanged indices
[2022-04-27T16:45:48,151][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.7s] publication of cluster state version [14633] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Rk_nYcIVQkql87tIMmQsyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-27T16:45:48,164][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][DELETE][/logstash-1970.01.01?master_timeout=30s][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:58772}] took [33507ms] which is above the warn threshold of [5000ms]
[2022-04-27T16:45:51,236][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][199][21] duration [1s], collections [1]/[1.4s], total [1s]/[14.5s], memory [361.3mb]->[291.3mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [281.3mb]->[281.3mb]/[2gb]}{[survivor] [8mb]->[9.9mb]/[0b]}
[2022-04-27T16:45:50,749][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/zfXz1uXsTXiGc4GmdIkUag] deleting index
[2022-04-27T16:45:51,417][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][199] overhead, spent [1s] collecting in the last [1.4s]
[2022-04-27T16:45:53,667][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][200] overhead, spent [524ms] collecting in the last [2s]
[2022-04-27T16:45:58,565][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:45:58,635][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:45:58,647][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:45:58,658][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:45:59,014][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:47:04,641][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:52:28,865][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:53:29,914][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:53:30,002][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:54:10,954][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:54:43,953][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:54:44,219][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:55:33,106][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:56:52,875][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-1970.01.01] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-27T16:56:52,975][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-1970.01.01][0]]]).
[2022-04-27T16:56:53,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-1970.01.01/dT_-bXESTqOmxna6SslMrQ] update_mapping [_doc]
[2022-04-27T16:57:29,135][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:58:54,122][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T16:58:54,306][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:01:43,545][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:01:44,237][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:02:00,340][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:34,332][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:35,018][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:35,318][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:50,038][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:50,116][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:50,330][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:50,402][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:50,540][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:03:50,546][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:05:08,385][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:12:25,213][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:12:38,236][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.27/b6_FBWm6Q2SD_jgnX2KpIA] update_mapping [_doc]
[2022-04-27T17:15:46,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6150cf74, interval=1s}] took [29563ms] which is above the warn threshold of [5000ms]
[2022-04-27T17:16:36,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@39077c4d, interval=5s}] took [10861ms] which is above the warn threshold of [5000ms]
[2022-04-27T17:18:15,899][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50490}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-04-27T17:22:56,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T17:26:01,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60505685903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T17:28:18,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/602830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T17:31:08,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/602652651546ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T17:34:57,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T17:38:44,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400676117813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T17:41:22,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T17:45:13,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384632011817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T17:48:07,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T17:45:58,767][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.9s/13951ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@354f13ca], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.23-000004], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@110e7722]], which exceeds the warn threshold of [10s]
[2022-04-27T17:50:52,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405075946193ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T17:53:38,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T17:56:14,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329718275740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T17:56:17,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5112/0x00000008017e1ba0@4d399bc3] took [329718ms] which is above the warn threshold of [5000ms]
[2022-04-27T17:59:04,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:02:30,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327166949678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:12:53,402][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-27T18:12:53,453][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-27T18:12:53,454][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-27T18:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-27T18:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-27T18:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-27T18:12:59,409][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-27T18:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-27T18:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-27T18:12:59,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-27T18:12:59,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-27T18:12:59,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-27T18:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-27T18:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-27T18:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-27T18:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-27T18:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-27T18:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-27T18:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-27T18:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-27T18:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-27T18:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-27T18:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-27T18:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-27T18:12:59,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-27T18:12:59,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-27T18:12:59,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-27T18:12:59,418][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-27T18:12:59,418][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-27T18:12:59,418][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-27T18:12:59,419][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-27T18:12:59,419][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-27T18:12:59,419][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-27T18:12:59,420][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-27T18:12:59,420][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-27T18:12:59,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-27T18:12:59,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-27T18:12:59,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-27T18:12:59,422][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-27T18:12:59,422][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-27T18:12:59,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-27T18:12:59,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-27T18:12:59,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-27T18:12:59,424][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-27T18:12:59,424][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-27T18:12:59,425][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-27T18:12:59,425][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-27T18:12:59,425][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-27T18:12:59,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-27T18:12:59,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-27T18:12:59,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-27T18:12:59,427][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-27T18:12:59,427][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-27T18:12:59,427][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-27T18:12:59,427][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-27T18:12:59,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-27T18:12:59,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-27T18:12:59,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-27T18:12:59,429][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-27T18:12:59,429][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-27T18:12:59,429][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-27T18:12:59,431][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-27T18:12:59,559][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.7gb], net total_space [125.8gb], types [ext4]
[2022-04-27T18:12:59,561][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-27T18:13:00,314][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-27T18:15:01,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:15:03,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91297319365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:15:03,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:15:03,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19110658563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:15:11,496][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-27T18:15:11,505][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-27T18:15:11,506][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-27T18:15:11,509][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-27T18:15:11,510][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-27T18:15:11,510][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-27T18:15:11,511][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-27T18:15:11,512][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-27T18:15:11,512][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-27T18:15:11,512][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-27T18:15:11,513][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-27T18:15:11,513][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-27T18:15:11,514][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-27T18:15:11,515][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-27T18:15:11,516][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-27T18:15:12,775][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-27T18:15:13,003][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-27T18:15:14,203][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-27T18:15:19,575][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-27T18:15:19,580][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-27T18:15:20,167][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-27T18:15:20,200][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-27T18:15:20,611][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-27T18:16:44,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:17:50,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35188582671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:18:54,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/136212ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:19:00,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/136211843956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:19:03,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:19:02,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [47691ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:19:04,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9156954762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:19:49,979][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10606ms] which is above the warn threshold of [10s]; wrote full state with [66] indices
[2022-04-27T18:19:53,068][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-27T18:19:54,197][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lgccun-eSrOjYywswM7i-A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 318, version: 14665, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lgccun-eSrOjYywswM7i-A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-27T18:19:54,444][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lgccun-eSrOjYywswM7i-A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 318, version: 14665, reason: Publication{term=318, version=14665}
[2022-04-27T18:19:54,612][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-27T18:19:54,613][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-27T18:20:23,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [22920ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:21:40,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [45880ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:22:10,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@3bd14dde] took [17708ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:23:15,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [46213ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:22:52,980][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16978ms] which is above the warn threshold of [5s]
[2022-04-27T18:24:25,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [42688ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:24:44,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@83098b3] took [11006ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:25:22,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [18523ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:26:03,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [17015ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:26:27,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:26:39,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [6866ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:26:44,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5866148162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:26:56,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:26:59,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ee70274, interval=5s}] took [34529ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:27:05,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34529293721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:27:18,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:27:30,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20925883481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:27:44,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:27:55,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25982948796ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:28:05,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [25982ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:28:25,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42s/42004ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:28:43,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42s/42003586922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:29:14,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d0dc153, interval=5s}] took [39145ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:29:09,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:29:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39145680478ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:30:06,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:30:24,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60214436351ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:30:43,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:30:56,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38066066941ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:30:57,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [98280ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:31:11,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:31:23,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27750144722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:31:35,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ee70274, interval=5s}] took [24798ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:31:35,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:31:51,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24798881811ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:32:28,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.8s/51834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:32:42,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@4c83dc91] took [51834ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:32:48,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.8s/51834229097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:31:55,003][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24799ms] which is above the warn threshold of [5s]
[2022-04-27T18:33:16,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.6s/46663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:33:35,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.6s/46663306618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:33:55,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:34:13,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [87245ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:34:18,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40582563744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:34:44,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:35:02,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47640920890ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:35:22,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@407a30b2] took [39905ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:35:25,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:35:42,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39905380497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:36:01,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:36:01,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@26687e6b] took [36335ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:36:16,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36335452162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:36:28,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:36:43,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30150385143ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:36:57,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [58748ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:36:57,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:37:07,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28597774451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:37:18,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:37:28,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21594634855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:37:43,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:38:24,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25077421830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:38:27,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [25077ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:38:32,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:37:52,279][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25078ms] which is above the warn threshold of [5s]
[2022-04-27T18:38:42,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49114276649ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:38:57,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:39:41,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25157756433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:40:02,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:40:13,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64817997520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:40:25,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:40:39,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22404927170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:40:53,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:41:01,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [27481ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:41:06,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27481603066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:41:23,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:41:29,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d0dc153, interval=5s}] took [29908ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:41:40,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29908518408ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:41:55,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:42:06,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31917814183ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:42:22,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:42:35,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27395359803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:42:50,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:42:59,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [55331ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:43:01,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27936584213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:42:46,759][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27395ms] which is above the warn threshold of [5s]
[2022-04-27T18:43:17,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:43:30,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27583006016ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:43:33,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@35fc24ac] took [27583ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:43:49,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:44:07,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28677081020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:44:27,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:44:44,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39490302546ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:44:58,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31327ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:45:07,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [70817ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:45:16,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31327318678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:45:27,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:45:42,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30913556858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:45:54,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:46:01,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@674901d6] took [27096ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:46:14,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27096760460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:46:31,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36547ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:46:50,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36547145942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:47:06,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:47:07,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [69914ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:47:19,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33367772461ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:47:32,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:47:34,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ee70274, interval=5s}] took [26729ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:47:44,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26729890923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:47:47,244][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26730ms] which is above the warn threshold of [5s]
[2022-04-27T18:48:09,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:48:20,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38506824944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:48:29,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:48:38,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18757726084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:48:49,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:48:51,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ee70274, interval=5s}] took [20947ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:48:58,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20947042371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:49:10,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:49:10,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@1359e1b4, interval=1m}] took [20091ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:49:23,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20091087734ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:49:45,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:50:04,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35592815448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:50:17,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [35592ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:50:22,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36328ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:50:43,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36328150986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:50:58,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35653ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:51:27,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35652378145ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:51:46,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.3s/47300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:52:05,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.3s/47300417460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:52:20,336][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [47301ms] which is above the warn threshold of [5s]
[2022-04-27T18:52:24,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [47300ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:52:30,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:52:49,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44343733560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:53:08,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:53:11,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@47d47e37, interval=30s}] took [37828ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:53:27,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37828006722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:54:03,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:54:41,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51118097789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:55:18,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77058ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:55:00,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [51118ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:57:14,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77058062749ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:57:26,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:57:31,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@68bc80aa] took [131316ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:57:39,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131316496045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:57:52,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25601ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:58:05,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25600518709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:58:16,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24993ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:58:27,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24993155263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:58:39,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:58:43,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [46867ms] which is above the warn threshold of [5000ms]
[2022-04-27T18:58:54,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21873872979ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T18:59:07,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T18:58:52,467][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21874ms] which is above the warn threshold of [5s]
[2022-04-27T18:59:50,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28288933423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:00:05,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.7s/57737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:00:15,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [57737ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:00:20,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.7s/57737363261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:00:36,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31510ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:00:37,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@1359e1b4, interval=1m}] took [31509ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:00:50,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31509292186ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:00:59,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ee70274, interval=5s}] took [24062ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:00:59,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:01:17,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24062810266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:01:28,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:01:49,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28829236315ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:01:59,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [28829ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:02:13,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42782ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:02:26,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42782560500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:02:38,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:02:41,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d0dc153, interval=5s}] took [25431ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:03:14,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25431605779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:06:18,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211056ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:09:50,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211055712830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:12:52,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:13:09,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ee70274, interval=5s}] took [386957ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:13:52,544][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [386958ms] which is above the warn threshold of [5s]
[2022-04-27T19:15:31,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386957817711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:18:37,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:18:44,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5097/0x00000008017d6e58@24c681aa] took [334805ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:21:12,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334805469595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:24:02,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/341091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:26:39,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@608d5eaa, interval=1s}] took [341215ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:27:04,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/341215475448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:30:11,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d0dc153, interval=5s}] took [353192ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:30:33,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:36:33,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353192940544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:39:33,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:42:50,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571541408005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:45:20,345][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [571541ms] which is above the warn threshold of [5s]
[2022-04-27T19:46:03,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:46:15,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d0dc153, interval=5s}] took [396632ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:48:45,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396632255550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-27T19:51:53,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@1359e1b4, interval=1m}] took [349154ms] which is above the warn threshold of [5000ms]
[2022-04-27T19:52:08,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-27T19:55:24,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349154970293ns] on relative clock which is above the warn threshold of [5000ms]
