[2022-04-22T17:20:12,753][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-22T17:20:12,765][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-22T17:20:12,766][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-22T17:20:20,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-22T17:20:20,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-22T17:20:20,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-22T17:20:20,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-22T17:20:20,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-22T17:20:20,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-22T17:20:20,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-22T17:20:20,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-22T17:20:20,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-22T17:20:20,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-22T17:20:20,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-22T17:20:20,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-22T17:20:20,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-22T17:20:20,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-22T17:20:20,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-22T17:20:20,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-22T17:20:20,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-22T17:20:20,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-22T17:20:20,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-22T17:20:20,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-22T17:20:20,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-22T17:20:20,892][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-22T17:20:20,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-22T17:20:20,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-22T17:20:20,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-22T17:20:20,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-22T17:20:20,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-22T17:20:20,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-22T17:20:20,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-22T17:20:20,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-22T17:20:20,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-22T17:20:20,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-22T17:20:20,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-22T17:20:20,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-22T17:20:20,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-22T17:20:20,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-22T17:20:20,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-22T17:20:20,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-22T17:20:20,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-22T17:20:20,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-22T17:20:20,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-22T17:20:20,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-22T17:20:20,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-22T17:20:20,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-22T17:20:20,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-22T17:20:20,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-22T17:20:20,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-22T17:20:20,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-22T17:20:20,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-22T17:20:20,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-22T17:20:20,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-22T17:20:20,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-22T17:20:20,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-22T17:20:20,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-22T17:20:20,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-22T17:20:20,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-22T17:20:20,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-22T17:20:20,908][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-22T17:20:20,909][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-22T17:20:21,008][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105gb], net total_space [125.8gb], types [ext4]
[2022-04-22T17:20:21,010][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-22T17:20:21,409][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-22T17:20:33,939][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-22T17:20:33,944][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-22T17:20:34,973][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-22T17:20:35,109][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-22T17:20:35,800][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-22T17:20:36,588][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-22T17:20:36,589][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-22T17:20:36,625][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-22T17:20:36,627][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-22T17:20:36,850][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-22T17:20:39,662][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-22T17:20:39,801][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{RK_BBTyBS5qShaXBunQesA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 294, version: 13240, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{RK_BBTyBS5qShaXBunQesA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-22T17:20:39,994][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{RK_BBTyBS5qShaXBunQesA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 294, version: 13240, reason: Publication{term=294, version=13240}
[2022-04-22T17:20:40,112][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-22T17:20:40,113][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-22T17:20:41,106][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-22T17:20:41,113][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [60] indices into cluster_state
[2022-04-22T17:20:41,952][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-22T17:20:41,953][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-22T17:20:42,721][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-22T17:20:42,907][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-22T17:20:43,181][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-22T17:20:43,184][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-22T17:20:43,185][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-22T17:20:43,364][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-22T17:20:43,676][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-22T17:20:43,867][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-22T17:20:46,775][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-22T17:20:57,504][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-22T17:20:57,771][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-22T17:21:08,052][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.22][0]]]).
[2022-04-22T17:21:43,313][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 761 finished with response BulkByScrollResponse[took=305.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-22T17:21:45,569][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 786 finished with response BulkByScrollResponse[took=2.2s,timed_out=false,sliceId=null,updated=922,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-22T17:21:54,435][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-22T17:22:47,983][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:22:48,181][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:22:48,922][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:28:41,702][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:45:52,199][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:50:17,801][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T17:51:23,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5e345158, interval=1s}] took [8486ms] which is above the warn threshold of [5000ms]
[2022-04-22T17:51:58,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5e345158, interval=1s}] took [15817ms] which is above the warn threshold of [5000ms]
[2022-04-22T17:57:17,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:02:00,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5584213184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:04:51,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/667406ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:10:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/667294017600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:14:35,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8m/589932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:17:34,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8m/590142526535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:23:10,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:26:46,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483870065147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:29:48,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:03,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433995127203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:30,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.8s/48871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:46,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48912996841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:30:50,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@69f89df, interval=5s}] took [48912ms] which is above the warn threshold of [5000ms]
[2022-04-22T18:31:03,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:31:28,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32785878887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:34:19,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:37:08,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/183631877970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:41:03,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353406ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:46:29,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353504062800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T18:53:45,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3m/742984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:53:45,709][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.7s/17757ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e]], which exceeds the warn threshold of [10s]
[2022-04-22T19:09:21,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3m/743353946350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:11:59,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2m/1157210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T18:57:22,350][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [743354ms] which is above the warn threshold of [5s]
[2022-04-22T19:11:15,537][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11046ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T19:15:48,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2m/1157069209024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:16:56,653][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.8s/32867ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T19:20:30,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/516663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T19:23:46,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/516248381896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:27:05,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396684ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T19:32:48,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396538674597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:37:03,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/549532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T19:52:45,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/549996974495ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T19:54:47,557][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [27.8m/1673317ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [31.1m/1869648ms]
[2022-04-22T19:56:59,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6m/1240648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:01:32,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6m/1240317792031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:02:13,752][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3860171ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [1.1h/4180327ms]
[2022-04-22T20:04:51,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/468194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:06:25,819][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.2h/4328766ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [1.2h/4353584ms]
[2022-04-22T20:08:12,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/468595552768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:13:02,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/498741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:12:48,754][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.5s/12550ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:16:25,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/498908334750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:19:29,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:17:36,855][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [38.3s/38353ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:23:18,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387394604175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:23:39,815][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.4h/5215069ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [1h/3691350ms]
[2022-04-22T20:25:25,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:26:51,118][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.5h/5563235ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [1h/3695469ms]
[2022-04-22T20:29:59,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348166371666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:31:58,619][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.4s/24407ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:33:32,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/486983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:36:40,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/487286143110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:37:06,626][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.4s/35404ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf3965f6], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1fd40f2e], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b46cbbd3], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c803c2c1]], which exceeds the warn threshold of [10s]
[2022-04-22T20:40:09,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:40:41,883][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.7h/6448107ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [21.8m/1312406ms]
[2022-04-22T20:43:11,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397585643791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:36:53,184][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:58370}] took [348166ms] which is above the warn threshold of [5000ms]
[2022-04-22T20:46:47,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T20:49:47,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389395437257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T20:54:05,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:04:05,807][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-22T21:04:05,865][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-22T21:04:05,870][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-22T21:04:15,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-22T21:04:15,028][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-22T21:04:15,029][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-22T21:04:15,030][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-22T21:04:15,030][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-22T21:04:15,031][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-22T21:04:15,031][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-22T21:04:15,032][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-22T21:04:15,033][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-22T21:04:15,034][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-22T21:04:15,034][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-22T21:04:15,040][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-22T21:04:15,042][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-22T21:04:15,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-22T21:04:15,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-22T21:04:15,044][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-22T21:04:15,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-22T21:04:15,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-22T21:04:15,050][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-22T21:04:15,051][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-22T21:04:15,051][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-22T21:04:15,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-22T21:04:15,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-22T21:04:15,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-22T21:04:15,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-22T21:04:15,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-22T21:04:15,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-22T21:04:15,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-22T21:04:15,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-22T21:04:15,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-22T21:04:15,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-22T21:04:15,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-22T21:04:15,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-22T21:04:15,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-22T21:04:15,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-22T21:04:15,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-22T21:04:15,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-22T21:04:15,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-22T21:04:15,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-22T21:04:15,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-22T21:04:15,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-22T21:04:15,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-22T21:04:15,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-22T21:04:15,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-22T21:04:15,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-22T21:04:15,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-22T21:04:15,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-22T21:04:15,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-22T21:04:15,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-22T21:04:15,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-22T21:04:15,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-22T21:04:15,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-22T21:04:15,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-22T21:04:15,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-22T21:04:15,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-22T21:04:15,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-22T21:04:15,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-22T21:04:15,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-22T21:04:15,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-22T21:04:15,379][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.9gb], net total_space [125.8gb], types [ext4]
[2022-04-22T21:04:15,387][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-22T21:04:16,433][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-22T21:04:31,657][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-22T21:04:31,665][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-22T21:04:31,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-22T21:04:31,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-22T21:04:31,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-22T21:04:31,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-22T21:04:31,673][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-22T21:04:31,674][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-22T21:04:31,675][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-22T21:04:31,676][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-22T21:04:31,677][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-22T21:04:31,678][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-22T21:04:31,680][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-22T21:04:31,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-22T21:04:31,682][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-22T21:04:33,099][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-22T21:04:33,295][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-22T21:04:34,276][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-22T21:04:35,435][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-22T21:04:35,436][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-22T21:04:35,554][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-22T21:04:35,556][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-22T21:04:35,871][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-22T21:04:39,644][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-22T21:04:39,813][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 295, version: 13327, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-22T21:04:39,985][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 295, version: 13327, reason: Publication{term=295, version=13327}
[2022-04-22T21:04:40,147][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-22T21:04:40,150][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-22T21:04:41,614][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-22T21:04:41,636][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [60] indices into cluster_state
[2022-04-22T21:04:43,032][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-22T21:04:43,036][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-22T21:04:44,104][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-22T21:04:44,328][WARN ][o.e.i.t.Translog         ] [tpotcluster-node-01] [logstash-2022.04.22][0] deleted previously created, but not yet committed, next generation [translog-4.tlog]. This can happen due to a tragic exception when creating a new generation
[2022-04-22T21:04:44,522][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-22T21:04:44,527][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-22T21:04:44,529][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-22T21:04:45,096][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-22T21:04:45,350][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-22T21:04:45,531][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-22T21:04:46,404][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-22T21:04:48,248][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-22T21:05:06,568][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:05:08,346][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:05:58,130][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][61][17] duration [1.2s], collections [1]/[3.6s], total [1.2s]/[2.5s], memory [1.2gb]->[227.3mb]/[2gb], all_pools {[young] [1gb]->[4mb]/[0b]}{[old] [195.3mb]->[195.3mb]/[2gb]}{[survivor] [12mb]->[32mb]/[0b]}
[2022-04-22T21:05:58,452][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][61] overhead, spent [1.2s] collecting in the last [3.6s]
[2022-04-22T21:06:04,996][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][65] overhead, spent [484ms] collecting in the last [1.8s]
[2022-04-22T21:06:18,073][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][67][21] duration [6.8s], collections [1]/[9.1s], total [6.8s]/[10.8s], memory [301.4mb]->[236.1mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [227.3mb]->[227.3mb]/[2gb]}{[survivor] [6.1mb]->[8.8mb]/[0b]}
[2022-04-22T21:06:16,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:19,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][67] overhead, spent [6.8s] collecting in the last [9.1s]
[2022-04-22T21:06:20,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8553337014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:22,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7546ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:23,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7546098261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:23,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [7546ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:06:28,691][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][68][23] duration [2.3s], collections [2]/[13.1s], total [2.3s]/[13.1s], memory [236.1mb]->[246mb]/[2gb], all_pools {[young] [16mb]->[8mb]/[0b]}{[old] [227.3mb]->[234mb]/[2gb]}{[survivor] [8.8mb]->[11.9mb]/[0b]}
[2022-04-22T21:06:45,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:46,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7254099165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:06:46,947][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][72][24] duration [5.3s], collections [1]/[9s], total [5.3s]/[18.4s], memory [290mb]->[252.4mb]/[2gb], all_pools {[young] [48mb]->[20mb]/[0b]}{[old] [234mb]->[242.1mb]/[2gb]}{[survivor] [11.9mb]->[10.3mb]/[0b]}
[2022-04-22T21:06:46,948][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][72] overhead, spent [5.3s] collecting in the last [9s]
[2022-04-22T21:06:46,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [9293ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:07:01,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:07:01,763][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][78][25] duration [4.1s], collections [1]/[7.1s], total [4.1s]/[22.6s], memory [300.4mb]->[254.4mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [242.1mb]->[245mb]/[2gb]}{[survivor] [10.3mb]->[9.4mb]/[0b]}
[2022-04-22T21:07:01,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5908658322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:07:01,818][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][78] overhead, spent [4.1s] collecting in the last [7.1s]
[2022-04-22T21:07:01,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5908ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:07:07,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][79][26] duration [3s], collections [1]/[5.2s], total [3s]/[25.6s], memory [254.4mb]->[254.3mb]/[2gb], all_pools {[young] [8mb]->[4mb]/[0b]}{[old] [245mb]->[249.4mb]/[2gb]}{[survivor] [9.4mb]->[4.8mb]/[0b]}
[2022-04-22T21:07:07,882][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][79] overhead, spent [3s] collecting in the last [5.2s]
[2022-04-22T21:07:11,045][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][27] duration [923ms], collections [1]/[3.7s], total [923ms]/[26.5s], memory [254.3mb]->[254.9mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [249.4mb]->[249.4mb]/[2gb]}{[survivor] [4.8mb]->[5.5mb]/[0b]}
[2022-04-22T21:07:15,113][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][81][28] duration [1s], collections [1]/[3.7s], total [1s]/[27.6s], memory [254.9mb]->[259.1mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [249.4mb]->[249.4mb]/[2gb]}{[survivor] [5.5mb]->[9.6mb]/[0b]}
[2022-04-22T21:07:15,620][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][81] overhead, spent [1s] collecting in the last [3.7s]
[2022-04-22T21:07:20,337][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][82][29] duration [2.1s], collections [1]/[5.6s], total [2.1s]/[29.7s], memory [259.1mb]->[272.1mb]/[2gb], all_pools {[young] [4mb]->[16mb]/[0b]}{[old] [249.4mb]->[250.4mb]/[2gb]}{[survivor] [9.6mb]->[5.7mb]/[0b]}
[2022-04-22T21:07:21,394][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][82] overhead, spent [2.1s] collecting in the last [5.6s]
[2022-04-22T21:07:29,170][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][85][30] duration [1.7s], collections [1]/[4.5s], total [1.7s]/[31.5s], memory [328.1mb]->[257.7mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [250.4mb]->[250.4mb]/[2gb]}{[survivor] [5.7mb]->[7.3mb]/[0b]}
[2022-04-22T21:07:29,581][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][85] overhead, spent [1.7s] collecting in the last [4.5s]
[2022-04-22T21:07:34,953][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][86][31] duration [2.8s], collections [1]/[5.5s], total [2.8s]/[34.3s], memory [257.7mb]->[258.4mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [250.4mb]->[250.4mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-04-22T21:07:35,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][86] overhead, spent [2.8s] collecting in the last [5.5s]
[2022-04-22T21:07:42,657][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][88][32] duration [2.5s], collections [1]/[1.8s], total [2.5s]/[36.8s], memory [290.4mb]->[306.4mb]/[2gb], all_pools {[young] [36mb]->[88mb]/[0b]}{[old] [250.4mb]->[250.4mb]/[2gb]}{[survivor] [8mb]->[8mb]/[0b]}
[2022-04-22T21:07:42,807][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][88] overhead, spent [2.5s] collecting in the last [1.8s]
[2022-04-22T21:07:46,335][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][90] overhead, spent [610ms] collecting in the last [1.5s]
[2022-04-22T21:08:12,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:12,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [19675ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16633023274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59446}] took [17234ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59448}] took [18107ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:13,065][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [18107ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:08:14,559][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][92][35] duration [12.7s], collections [1]/[21.3s], total [12.7s]/[50.5s], memory [335.1mb]->[286.6mb]/[2gb], all_pools {[young] [72mb]->[20mb]/[0b]}{[old] [254.3mb]->[254.6mb]/[2gb]}{[survivor] [8.8mb]->[16mb]/[0b]}
[2022-04-22T21:08:15,480][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][92] overhead, spent [12.7s] collecting in the last [21.3s]
[2022-04-22T21:08:21,542][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][94][36] duration [874ms], collections [1]/[1.9s], total [874ms]/[51.4s], memory [298.6mb]->[302.6mb]/[2gb], all_pools {[young] [28mb]->[48mb]/[0b]}{[old] [254.6mb]->[254.6mb]/[2gb]}{[survivor] [16mb]->[16mb]/[0b]}
[2022-04-22T21:08:21,754][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [874ms] collecting in the last [1.9s]
[2022-04-22T21:08:34,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:38,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10532878643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:41,179][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][95][37] duration [6.5s], collections [1]/[3.9s], total [6.5s]/[57.9s], memory [302.6mb]->[361.4mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [254.6mb]->[276.4mb]/[2gb]}{[survivor] [16mb]->[5.2mb]/[0b]}
[2022-04-22T21:08:41,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:39,159][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.1s/15101ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.16][0]], allocationId [4B6Hamj8QROW8mV5aAUaZQ], primary term [128], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-22T21:08:42,021][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][95] overhead, spent [6.5s] collecting in the last [3.9s]
[2022-04-22T21:08:42,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7490317370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:08:44,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [18423ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:00,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@48d28149] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:09,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:10,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5927734267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:10,325][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][97][38] duration [4.6s], collections [1]/[7.6s], total [4.6s]/[1m], memory [361.6mb]->[283mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [276.4mb]->[276.4mb]/[2gb]}{[survivor] [5.2mb]->[6.5mb]/[0b]}
[2022-04-22T21:09:10,424][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [4.6s] collecting in the last [7.6s]
[2022-04-22T21:09:10,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5927ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:11,482][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [20134ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:09:15,616][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][98][39] duration [2.2s], collections [1]/[2s], total [2.2s]/[1m], memory [283mb]->[371mb]/[2gb], all_pools {[young] [8mb]->[4mb]/[0b]}{[old] [276.4mb]->[276.4mb]/[2gb]}{[survivor] [6.5mb]->[10mb]/[0b]}
[2022-04-22T21:09:15,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [2.2s] collecting in the last [2s]
[2022-04-22T21:09:15,801][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [29s] publication of cluster state version [13386] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:09:30,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e40ccf, interval=30s}] took [10344ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:09:30,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:32,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10344238024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:09:36,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][102][40] duration [7.4s], collections [1]/[12.6s], total [7.4s]/[1.2m], memory [330.5mb]->[292.6mb]/[2gb], all_pools {[young] [52mb]->[24mb]/[0b]}{[old] [276.4mb]->[279.7mb]/[2gb]}{[survivor] [10mb]->[8.8mb]/[0b]}
[2022-04-22T21:09:38,996][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][102] overhead, spent [7.4s] collecting in the last [12.6s]
[2022-04-22T21:09:39,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [9141ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:10:17,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:11:37,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5734783020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:12:01,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:12:12,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111881205229ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:12:42,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:13:05,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34507370978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:13:49,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:14:35,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67879039945ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:14:48,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:14:54,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63485625465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:00,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:03,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882396961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:09,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:14,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389966429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@7cb6889d] took [12290ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,497][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [349124ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:15:21,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12290635105ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:15:21,785][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6m] publication of cluster state version [13387] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:15:24,425][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][103][42] duration [4.8s], collections [2]/[5.8m], total [4.8s]/[1.2m], memory [292.6mb]->[297mb]/[2gb], all_pools {[young] [24mb]->[8mb]/[0b]}{[old] [279.7mb]->[282.3mb]/[2gb]}{[survivor] [8.8mb]->[6.6mb]/[0b]}
[2022-04-22T21:15:24,480][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7m/422380ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [7m/422366ms]
[2022-04-22T21:15:26,584][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][104] overhead, spent [579ms] collecting in the last [1.8s]
[2022-04-22T21:15:32,473][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][107][44] duration [938ms], collections [1]/[1.2s], total [938ms]/[1.3m], memory [320.9mb]->[372.9mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [282.3mb]->[282.3mb]/[2gb]}{[survivor] [6.5mb]->[5.7mb]/[0b]}
[2022-04-22T21:15:32,699][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [938ms] collecting in the last [1.2s]
[2022-04-22T21:15:36,899][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-22T21:15:49,258][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][116][46] duration [2.9s], collections [1]/[6.1s], total [2.9s]/[1.3m], memory [372.3mb]->[292.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [282.3mb]->[284.2mb]/[2gb]}{[survivor] [9.9mb]->[8.3mb]/[0b]}
[2022-04-22T21:15:49,424][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][116] overhead, spent [2.9s] collecting in the last [6.1s]
[2022-04-22T21:15:55,067][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][121] overhead, spent [281ms] collecting in the last [1.1s]
[2022-04-22T21:16:04,858][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][126][48] duration [1s], collections [1]/[1.5s], total [1s]/[1.3m], memory [368.9mb]->[372.9mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [284.2mb]->[284.2mb]/[2gb]}{[survivor] [8.7mb]->[11mb]/[0b]}
[2022-04-22T21:16:05,487][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][126] overhead, spent [1s] collecting in the last [1.5s]
[2022-04-22T21:16:10,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8826ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:16:12,415][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][49] duration [3.1s], collections [1]/[9.9s], total [3.1s]/[1.4m], memory [372.9mb]->[301.3mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [284.2mb]->[286.4mb]/[2gb]}{[survivor] [11mb]->[6.8mb]/[0b]}
[2022-04-22T21:16:13,424][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [3.1s] collecting in the last [9.9s]
[2022-04-22T21:16:28,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:16:29,253][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][130][50] duration [5.2s], collections [1]/[1.4s], total [5.2s]/[1.5m], memory [361.3mb]->[361.3mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [6.8mb]->[7.8mb]/[0b]}
[2022-04-22T21:16:29,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][130] overhead, spent [5.2s] collecting in the last [1.4s]
[2022-04-22T21:16:29,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [9791ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:16:29,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7494580785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:16:35,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [6377ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:16:39,187][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][131][51] duration [3.5s], collections [1]/[19.2s], total [3.5s]/[1.5m], memory [361.3mb]->[297.6mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [7.8mb]->[7.1mb]/[0b]}
[2022-04-22T21:17:11,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:13,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11889830158ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:16,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:16,748][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][139][52] duration [8.2s], collections [1]/[1.4s], total [8.2s]/[1.7m], memory [357.6mb]->[381.6mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [7.1mb]->[8mb]/[0b]}
[2022-04-22T21:17:16,996][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59528}] took [5348ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:17:17,104][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [5348ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:17:17,826][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][139] overhead, spent [8.2s] collecting in the last [1.4s]
[2022-04-22T21:17:17,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5348410293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:17:18,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [17438ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:17:47,174][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [5404ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:07,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:19,705][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [15561ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:22,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7904ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:26,373][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59528}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:18:43,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:18:44,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12793917341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:18:45,712][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][53] duration [7.6s], collections [1]/[29.4s], total [7.6s]/[1.8m], memory [374.4mb]->[299.5mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [286.4mb]->[286.4mb]/[2gb]}{[survivor] [8mb]->[9mb]/[0b]}
[2022-04-22T21:18:47,027][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [7.6s] collecting in the last [29.4s]
[2022-04-22T21:18:49,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5671ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:19:59,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:19:06,736][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59542}] took [5804ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:06,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46297254740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:14,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:19,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15025163576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:18,617][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59542}] took [15025ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:20,087][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][54] duration [36.8s], collections [1]/[6.2s], total [36.8s]/[2.4m], memory [375.5mb]->[375.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [286.4mb]->[288.5mb]/[2gb]}{[survivor] [9mb]->[4.7mb]/[0b]}
[2022-04-22T21:20:26,243][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [36.8s] collecting in the last [6.2s]
[2022-04-22T21:20:26,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:32,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12152534435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:31,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [75476ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:38,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:44,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11948831396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:20:53,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [15203ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:20:53,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:02,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15203747005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:13,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:23,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18441318304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:33,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21408ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:41,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21408427898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:50,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:21:54,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@7613f8af] took [17629ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:21:56,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17629920548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:04,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:09,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12937712056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [12937ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:22:16,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [12456ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:22:16,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:24,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12456840789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:28,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:30,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11710879929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:32,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:44,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5003150501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [12616ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:22:46,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12616509685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:57,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:58,341][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][55] duration [4.2s], collections [1]/[2.4s], total [4.2s]/[2.5m], memory [357.3mb]->[365.3mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [4.7mb]->[4.5mb]/[0b]}
[2022-04-22T21:22:58,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6911796964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:22:58,935][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [4.2s] collecting in the last [2.4s]
[2022-04-22T21:23:00,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [11108ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:23:10,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5930ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:23:42,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7804ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:00,796][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [5259ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:28,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:24:29,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7643360964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:24:29,410][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][170][56] duration [5s], collections [1]/[12.5s], total [5s]/[2.6m], memory [377mb]->[305.3mb]/[2gb], all_pools {[young] [84mb]->[12mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [4.5mb]->[4.8mb]/[0b]}
[2022-04-22T21:24:29,441][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][170] overhead, spent [5s] collecting in the last [12.5s]
[2022-04-22T21:24:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [6201ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:49,298][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59574}] took [14573ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:50,958][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [13830ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:24:51,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@bf4b888] took [5038ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:25:07,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:25:17,651][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][175][57] duration [1.1s], collections [1]/[2.1s], total [1.1s]/[2.6m], memory [353.3mb]->[293.5mb]/[2gb], all_pools {[young] [60mb]->[36mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [4.8mb]->[5mb]/[0b]}
[2022-04-22T21:25:18,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][175] overhead, spent [1.1s] collecting in the last [2.1s]
[2022-04-22T21:25:25,750][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][58] duration [1.5s], collections [1]/[1.5s], total [1.5s]/[2.6m], memory [377.5mb]->[377.5mb]/[2gb], all_pools {[young] [84mb]->[84mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [5mb]->[5mb]/[0b]}
[2022-04-22T21:25:25,996][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.5s] collecting in the last [1.5s]
[2022-04-22T21:25:29,528][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][59] duration [1.8s], collections [1]/[6.9s], total [1.8s]/[2.6m], memory [377.5mb]->[293.2mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [288.5mb]->[288.5mb]/[2gb]}{[survivor] [5mb]->[4.7mb]/[0b]}
[2022-04-22T21:25:29,809][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [1.8s] collecting in the last [6.9s]
[2022-04-22T21:25:40,332][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][185][61] duration [844ms], collections [1]/[1.8s], total [844ms]/[2.7m], memory [332.5mb]->[299.2mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [288.5mb]->[288.6mb]/[2gb]}{[survivor] [8mb]->[6.6mb]/[0b]}
[2022-04-22T21:25:40,486][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][185] overhead, spent [844ms] collecting in the last [1.8s]
[2022-04-22T21:25:42,867][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][186] overhead, spent [644ms] collecting in the last [2.3s]
[2022-04-22T21:26:09,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:26:10,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19169704773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:26:25,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][63] duration [17.6s], collections [1]/[19.7s], total [17.6s]/[3m], memory [355.1mb]->[295mb]/[2gb], all_pools {[young] [68mb]->[60mb]/[0b]}{[old] [288.6mb]->[288.8mb]/[2gb]}{[survivor] [6.4mb]->[6.2mb]/[0b]}
[2022-04-22T21:26:31,393][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [17.6s] collecting in the last [19.7s]
[2022-04-22T21:26:34,353][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [10207ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:26:35,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [25776ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:26:59,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [14095ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:27:33,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:27:34,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9382073244ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:27:34,692][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][64] duration [5s], collections [1]/[18.4s], total [5s]/[3.1m], memory [375mb]->[298.6mb]/[2gb], all_pools {[young] [80mb]->[32mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [6.2mb]->[5.8mb]/[0b]}
[2022-04-22T21:27:36,130][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [5s] collecting in the last [18.4s]
[2022-04-22T21:27:59,479][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][65] duration [3.4s], collections [1]/[1.3s], total [3.4s]/[3.1m], memory [338.6mb]->[382.6mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [5.8mb]->[4.5mb]/[0b]}
[2022-04-22T21:28:00,476][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [3.4s] collecting in the last [1.3s]
[2022-04-22T21:28:00,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [7364ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:28:26,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5471ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:28:45,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:50,052][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][210][66] duration [10.5s], collections [1]/[9.9s], total [10.5s]/[3.3m], memory [345.3mb]->[381.3mb]/[2gb], all_pools {[young] [56mb]->[52mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [4.5mb]->[6.7mb]/[0b]}
[2022-04-22T21:28:49,543][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59444}] took [14748ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:28:48,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13389367126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:50,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:50,910][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][210] overhead, spent [10.5s] collecting in the last [9.9s]
[2022-04-22T21:28:52,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5671071417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:28:52,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [19260ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:05,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5575ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:19,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5080ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:25,522][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [6004ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:29:20,095][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1m/64274ms] ago, timed out [22s/22034ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2180]
[2022-04-22T21:29:31,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:06,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:09,747][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59574}] took [23854ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:08,896][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][215][67] duration [10.9s], collections [1]/[17.2s], total [10.9s]/[3.5m], memory [371.6mb]->[379.6mb]/[2gb], all_pools {[young] [76mb]->[88mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [6.7mb]->[6.7mb]/[0b]}
[2022-04-22T21:30:09,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20251201835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:10,955][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][215] overhead, spent [10.9s] collecting in the last [17.2s]
[2022-04-22T21:30:10,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:13,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [28933ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:12,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59526}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:30:13,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5480669567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:38,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8606ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:39,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8606243055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:30:42,943][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2180] timed out after [42240ms]
[2022-04-22T21:31:24,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:31:25,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13604436676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:31:25,665][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18408ms] which is above the warn threshold of [5s]
[2022-04-22T21:31:25,849][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][222][68] duration [9s], collections [1]/[2.6s], total [9s]/[3.6m], memory [377.6mb]->[377.6mb]/[2gb], all_pools {[young] [80mb]->[84mb]/[0b]}{[old] [288.8mb]->[288.8mb]/[2gb]}{[survivor] [8.8mb]->[8.8mb]/[0b]}
[2022-04-22T21:31:26,518][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][222] overhead, spent [9s] collecting in the last [2.6s]
[2022-04-22T21:31:28,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [20321ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:31:35,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:31:38,421][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [48.9s/48968ms] to compute cluster state update for [put-mapping [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-22T21:32:15,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:32:16,086][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [18958ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:32:16,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17156433578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:32:16,086][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59574}] took [19158ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:32:18,990][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][228][69] duration [13.3s], collections [1]/[3s], total [13.3s]/[3.8m], memory [380.5mb]->[384.5mb]/[2gb], all_pools {[young] [84mb]->[48mb]/[0b]}{[old] [289.6mb]->[289.6mb]/[2gb]}{[survivor] [6.9mb]->[7.8mb]/[0b]}
[2022-04-22T21:32:20,674][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][228] overhead, spent [13.3s] collecting in the last [3s]
[2022-04-22T21:32:23,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [25982ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:32:42,716][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][229][70] duration [2.4s], collections [1]/[37.1s], total [2.4s]/[3.9m], memory [384.5mb]->[313mb]/[2gb], all_pools {[young] [48mb]->[16mb]/[0b]}{[old] [289.6mb]->[289.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-04-22T21:32:48,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [13366ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:33:13,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8405ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:33:57,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:33:20,045][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [90139ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:33:59,997][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [33380ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:33:59,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32779800689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:01,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:04,330][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.3m] publication of cluster state version [13391] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:34:06,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5643979620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:09,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:14,176][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [7490ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:34:13,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7489359548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:18,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9703ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:22,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9703708358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:27,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8058ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:29,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8057337672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:36,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:54,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9661752618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:34:56,807][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][71] duration [24.5s], collections [1]/[1.4m], total [24.5s]/[4.3m], memory [349mb]->[335.5mb]/[2gb], all_pools {[young] [56mb]->[40mb]/[0b]}{[old] [289.6mb]->[289.6mb]/[2gb]}{[survivor] [7.4mb]->[9.8mb]/[0b]}
[2022-04-22T21:34:57,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:00,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20846658493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:00,240][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][233] overhead, spent [24.5s] collecting in the last [1.4m]
[2022-04-22T21:35:03,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:03,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [30508ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:05,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5893377434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:09,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:09,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@26881dac] took [6315ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:12,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6315214266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:20,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:21,923][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [9247ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:23,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9247081750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:12,710][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6316ms] which is above the warn threshold of [5s]
[2022-04-22T21:35:25,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:28,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6780363576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:32,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:33,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [6380ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:35:33,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6380305384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:57,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:35:58,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11380053405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:36:00,052][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][236][72] duration [8.7s], collections [1]/[1.7s], total [8.7s]/[4.4m], memory [363.5mb]->[383.5mb]/[2gb], all_pools {[young] [64mb]->[36mb]/[0b]}{[old] [289.6mb]->[291.2mb]/[2gb]}{[survivor] [9.8mb]->[7.9mb]/[0b]}
[2022-04-22T21:36:01,094][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][236] overhead, spent [8.7s] collecting in the last [1.7s]
[2022-04-22T21:36:01,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [17051ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:01,650][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59628}] took [5271ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:27,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:36:28,316][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [10078ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:28,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9678288212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:36:27,855][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=295, version=13391}] took [1.7m] which is above the warn threshold of [30s]: [running task [Publication{term=295, version=13391}]] took [61ms], [connecting to new nodes] took [263ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2e2c2a19] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@458356df] took [39108ms], [org.elasticsearch.script.ScriptService@4c7fa9bf] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@47310c8c] took [0ms], [org.elasticsearch.snapshots.RestoreService@8029a91] took [0ms], [org.elasticsearch.ingest.IngestService@57e800e0] took [2313ms], [org.elasticsearch.action.ingest.IngestActionForwarder@66eb79f3] took [75ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce420@29715cce] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@35bb9d63] took [83ms], [org.elasticsearch.tasks.TaskManager@22c37434] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1f5aa57e] took [1ms], [org.elasticsearch.cluster.InternalClusterInfoService@b4f4839] took [72ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7c35fe48] took [0ms], [org.elasticsearch.indices.SystemIndexManager@37946f33] took [2801ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@18519c31] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@6089c80c] took [217ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@3605aed] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1af4dbb8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@ed59c61] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@254bf147] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@60f3312a] took [31812ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@221cc90b] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@2e4361af] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@6ad3a1fd] took [5431ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1c37bb0f] took [160ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4823e815] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@7d58b6a7] took [15585ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@47310c8c] took [150ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@af32cb8] took [2145ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@2b3d012a] took [37ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5759296f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@76db8471] took [1900ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@22dcf891] took [545ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7165c736] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@32bab212] took [104ms], [org.elasticsearch.node.ResponseCollectorService@7767dca4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@f54394f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@505703e4] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@379a11de] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@f364cff] took [0ms], [org.elasticsearch.indices.store.IndicesStore@4efac294] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5ed4375d] took [0ms], [org.elasticsearch.license.LicenseService@5878e82a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7d62863a] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@239e70ef] took [0ms], [org.elasticsearch.gateway.GatewayService@4c01fbbb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@5f56632e] took [0ms]
[2022-04-22T21:36:29,005][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][242][73] duration [6s], collections [1]/[5.9s], total [6s]/[4.5m], memory [367.2mb]->[387.2mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [291.2mb]->[291.2mb]/[2gb]}{[survivor] [7.9mb]->[7.6mb]/[0b]}
[2022-04-22T21:36:29,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][242] overhead, spent [6s] collecting in the last [5.9s]
[2022-04-22T21:36:29,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [10078ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:36:43,204][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][244][74] duration [2.7s], collections [1]/[12.2s], total [2.7s]/[4.6m], memory [366.9mb]->[299.6mb]/[2gb], all_pools {[young] [68mb]->[20mb]/[0b]}{[old] [291.2mb]->[291.2mb]/[2gb]}{[survivor] [7.6mb]->[8.3mb]/[0b]}
[2022-04-22T21:37:07,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:08,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15213011219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:13,328][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][247][75] duration [10.8s], collections [1]/[18.6s], total [10.8s]/[4.8m], memory [347.6mb]->[305.9mb]/[2gb], all_pools {[young] [48mb]->[4mb]/[0b]}{[old] [291.2mb]->[291.2mb]/[2gb]}{[survivor] [8.3mb]->[14.6mb]/[0b]}
[2022-04-22T21:37:14,046][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ] update_mapping [_doc]
[2022-04-22T21:37:14,084][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [6883ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:37:15,737][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][247] overhead, spent [10.8s] collecting in the last [18.6s]
[2022-04-22T21:37:18,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [26367ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:37:21,345][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [33s/33050ms] to compute cluster state update for [put-mapping [logstash-2022.04.22/1kJgRKkfSPmRy5pI413_DQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-22T21:37:21,766][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59666}] took [8099ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:37:50,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:53,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17929757908ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:37:56,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@2145458a] took [6769ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:07,297][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][76] duration [10.2s], collections [1]/[50.6s], total [10.2s]/[4.9m], memory [305.9mb]->[327.9mb]/[2gb], all_pools {[young] [4mb]->[28mb]/[0b]}{[old] [291.2mb]->[296.8mb]/[2gb]}{[survivor] [14.6mb]->[7.1mb]/[0b]}
[2022-04-22T21:38:14,266][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [9052ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:15,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [16074ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:38,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [6922ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:41,368][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [8060ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:38:41,063][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10547ms] which is above the warn threshold of [5s]
[2022-04-22T21:38:53,295][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [63283ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [59] unchanged indices
[2022-04-22T21:38:59,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5614ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:01,081][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.6m] publication of cluster state version [13392] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-22T21:39:11,144][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][251][77] duration [2.6s], collections [1]/[1.9s], total [2.6s]/[5m], memory [343.9mb]->[391.9mb]/[2gb], all_pools {[young] [40mb]->[64mb]/[0b]}{[old] [296.8mb]->[296.8mb]/[2gb]}{[survivor] [7.1mb]->[8.9mb]/[0b]}
[2022-04-22T21:39:12,298][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][251] overhead, spent [2.6s] collecting in the last [1.9s]
[2022-04-22T21:39:12,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [6555ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:26,011][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][257][80] duration [2.5s], collections [2]/[1.2s], total [2.5s]/[5m], memory [388.3mb]->[392.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [296.8mb]->[296.8mb]/[2gb]}{[survivor] [7.5mb]->[10.2mb]/[0b]}
[2022-04-22T21:39:26,268][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][257] overhead, spent [2.5s] collecting in the last [1.2s]
[2022-04-22T21:39:26,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5212ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:34,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][261][81] duration [1.7s], collections [1]/[4.2s], total [1.7s]/[5.1m], memory [370.7mb]->[306.9mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [297.9mb]->[297.9mb]/[2gb]}{[survivor] [8.8mb]->[9mb]/[0b]}
[2022-04-22T21:39:34,860][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][261] overhead, spent [1.7s] collecting in the last [4.2s]
[2022-04-22T21:39:39,762][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][263][83] duration [1.3s], collections [1]/[2.9s], total [1.3s]/[5.1m], memory [315.1mb]->[305.9mb]/[2gb], all_pools {[young] [12mb]->[8mb]/[0b]}{[old] [298.1mb]->[299.4mb]/[2gb]}{[survivor] [9mb]->[6.4mb]/[0b]}
[2022-04-22T21:39:39,912][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][263] overhead, spent [1.3s] collecting in the last [2.9s]
[2022-04-22T21:39:52,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8244ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:39:54,363][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [5217ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:40:00,515][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][266][84] duration [2.9s], collections [1]/[4.7s], total [2.9s]/[5.1m], memory [369.9mb]->[307mb]/[2gb], all_pools {[young] [88mb]->[36mb]/[0b]}{[old] [299.4mb]->[299.4mb]/[2gb]}{[survivor] [6.4mb]->[7.6mb]/[0b]}
[2022-04-22T21:40:01,024][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][266] overhead, spent [2.9s] collecting in the last [4.7s]
[2022-04-22T21:40:01,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5877ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:40:15,854][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][275][87] duration [1.2s], collections [1]/[2.3s], total [1.2s]/[5.2m], memory [342.3mb]->[312mb]/[2gb], all_pools {[young] [32mb]->[20mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [7.9mb]->[5.5mb]/[0b]}
[2022-04-22T21:40:16,130][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][275] overhead, spent [1.2s] collecting in the last [2.3s]
[2022-04-22T21:40:42,032][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][279][88] duration [4.4s], collections [1]/[5.2s], total [4.4s]/[5.2m], memory [340mb]->[396mb]/[2gb], all_pools {[young] [32mb]->[24mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [5.5mb]->[7.5mb]/[0b]}
[2022-04-22T21:40:41,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7837ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:40:42,234][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][279] overhead, spent [4.4s] collecting in the last [5.2s]
[2022-04-22T21:40:42,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7836881631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:40:42,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [8893ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:40:49,881][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][283][89] duration [867ms], collections [1]/[2.6s], total [867ms]/[5.3m], memory [394mb]->[309.5mb]/[2gb], all_pools {[young] [84mb]->[28mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [7.5mb]->[7.1mb]/[0b]}
[2022-04-22T21:40:50,331][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][283] overhead, spent [867ms] collecting in the last [2.6s]
[2022-04-22T21:41:01,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][288][91] duration [1.7s], collections [1]/[1.4s], total [1.7s]/[5.3m], memory [345.4mb]->[381.4mb]/[2gb], all_pools {[young] [40mb]->[88mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [6.9mb]->[6.9mb]/[0b]}
[2022-04-22T21:41:01,461][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][288] overhead, spent [1.7s] collecting in the last [1.4s]
[2022-04-22T21:41:23,636][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [8672ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:31,669][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [9337ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:31,669][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59672}] took [9337ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:35,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@3c748761] took [7809ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:41:42,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [5548ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:06,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [17817ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:06,984][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59666}] took [15390ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:21,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:28,615][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59672}] took [21698ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:27,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:30,344][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59662}] took [16311ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:42:32,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14828232802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:26,049][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3191] timed out after [37476ms]
[2022-04-22T21:42:34,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:42:40,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7117887047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:12,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:12,632][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][298][94] duration [18.3s], collections [1]/[50.2s], total [18.3s]/[5.6m], memory [388.8mb]->[392.8mb]/[2gb], all_pools {[young] [80mb]->[88mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-04-22T21:43:17,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38418908084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:19,998][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][298] overhead, spent [18.3s] collecting in the last [50.2s]
[2022-04-22T21:43:34,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:36,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [59107ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:43:38,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20688355335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:38,060][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [20688ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:43:42,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:48,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8894780750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:52,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:43:57,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10125428867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:09,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:16,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14341965519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:18,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@535dd0cb] took [14341ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:44:23,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:33,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16273662083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:42,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:51,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18481455412ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:51,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [18481ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:44:59,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:44:59,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4d672f52, interval=1m}] took [17439ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:45:10,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17439804926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:19,384][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [17440ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:45:20,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20936ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:29,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20936018560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:35,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:41,297][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:59658}] took [14942ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:45:40,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14941627291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:45,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:50,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10041268916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:45:57,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:06,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215155681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:08,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [12215ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:46:18,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:03,825][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3320] timed out after [98114ms]
[2022-04-22T21:46:28,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19815891520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:39,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:46:48,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21671868025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:06,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:18,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25550795039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:35,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30774ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:47:56,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30773638884ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:10,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:22,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33655278840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:37,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:24,114][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [33656ms] which is above the warn threshold of [5s]
[2022-04-22T21:48:50,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27692998356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:53,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [27692ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:48:55,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:07,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19438786452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:14,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:48:58,310][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7m/421381ms] ago, timed out [6.3m/383905ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3191]
[2022-04-22T21:49:23,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18611035433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:40,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:49:55,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25166552192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:07,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:08,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [26933ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:50:16,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26933159738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:23,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:34,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16881224253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:47,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23976ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:50:58,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23976454840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:09,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:17,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21778073173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:26,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:38,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17159655487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:47,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:51:56,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [37412ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:52:04,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20252483940ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:52:18,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:02,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31464053535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:09,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:14,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111104268496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:21,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:27,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12259959893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:34,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:36,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13171340049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T21:54:35,114][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3420] timed out after [237994ms]
[2022-04-22T21:54:41,809][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18973ms] which is above the warn threshold of [5s]
[2022-04-22T21:55:00,725][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][303][95] duration [1.3m], collections [1]/[3.2m], total [1.3m]/[6.9m], memory [372.5mb]->[327.5mb]/[2gb], all_pools {[young] [64mb]->[20mb]/[0b]}{[old] [302.4mb]->[302.4mb]/[2gb]}{[survivor] [6mb]->[5.1mb]/[0b]}
[2022-04-22T21:55:08,214][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][303] overhead, spent [1.3m] collecting in the last [3.2m]
[2022-04-22T21:54:51,361][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [10.6m/639722ms] ago, timed out [9m/541608ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3320]
[2022-04-22T21:55:14,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [35217ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:57:28,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [37998ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:58:28,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5127/0x00000008017e2d88@44ec6075] took [45707ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:58:57,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [8867ms] which is above the warn threshold of [5000ms]
[2022-04-22T21:59:32,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [15343ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:02:58,633][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [22555ms] which is above the warn threshold of [5s]
[2022-04-22T22:04:14,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4f42d76f, interval=1s}] took [59445ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:04:53,359][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.3m/859786ms] ago, timed out [10.3m/621792ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3420]
[2022-04-22T22:08:11,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f9a5405, interval=5s}] took [12727ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:08:44,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@24ca5ab, interval=5s}] took [16088ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:11:20,498][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3564] timed out after [549897ms]
[2022-04-22T22:16:00,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:17:52,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6121166748ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:21:10,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/297573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:25:54,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/297105491761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:29:56,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:30:16,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@112ee234, interval=1m}] took [537380ms] which is above the warn threshold of [5000ms]
[2022-04-22T22:33:34,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537380519471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:36:58,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/423005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:41:12,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422978887084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:44:43,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:49:07,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456880117742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:46:48,988][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10977ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250]], which exceeds the warn threshold of [10s]
[2022-04-22T22:53:15,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/520368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T22:45:36,800][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [879859ms] which is above the warn threshold of [5s]
[2022-04-22T22:57:05,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/520199151694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T22:57:02,547][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.2s/17295ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:01:10,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/462101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:02:00,677][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7.7m/462108ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [7.4m/447750ms]
[2022-04-22T23:05:04,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/462108375094ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:07:09,302][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.2s/10266ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d]], which exceeds the warn threshold of [10s]
[2022-04-22T23:09:22,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/472609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:13:21,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/472579938281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:13:11,868][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [40.1s/40123ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d]], which exceeds the warn threshold of [10s]
[2022-04-22T23:17:35,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/511031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:17:46,397][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [24.1m/1446101ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [16.6m/997107ms]
[2022-04-22T23:21:12,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/511412591251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:22:50,384][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.5s/30523ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:24:19,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:26:12,549][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [31m/1863716ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [12.5m/755373ms]
[2022-04-22T23:27:26,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417615353585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:29:24,910][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4h/5149285ms] ago, timed out [1.2h/4599388ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{U5IzAx2CQMajY6Z5m_lqBg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3564]
[2022-04-22T23:30:26,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:30:41,363][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17s/17027ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:33:28,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367603599968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:34:39,559][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.9s/30972ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:37:29,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:39:55,974][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [44.1m/2651212ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [13.8m/832255ms]
[2022-04-22T23:42:23,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419892485132ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:46:20,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8m/533975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:46:47,174][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.1s/12181ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:50:14,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8m/533965428048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-22T23:51:17,367][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32s/32097ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@283a2250], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78d4cb88], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d6d782d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21047f1b]], which exceeds the warn threshold of [10s]
[2022-04-22T23:54:59,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/509211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-22T23:56:18,647][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3694148ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [14.7m/887702ms]
[2022-04-22T23:59:01,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/508970136969ns] on relative clock which is above the warn threshold of [5000ms]
