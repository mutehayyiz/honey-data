[2022-03-25T17:46:56,505][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T17:46:56,553][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T17:46:56,554][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T17:47:13,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T17:47:13,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T17:47:13,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T17:47:13,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T17:47:13,920][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T17:47:13,921][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T17:47:13,922][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T17:47:13,923][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T17:47:13,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T17:47:13,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T17:47:13,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T17:47:13,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T17:47:13,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T17:47:13,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T17:47:13,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T17:47:13,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T17:47:13,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T17:47:13,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T17:47:13,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T17:47:13,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T17:47:13,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T17:47:13,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T17:47:13,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T17:47:13,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T17:47:13,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T17:47:13,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T17:47:13,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T17:47:13,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T17:47:13,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T17:47:13,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T17:47:13,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T17:47:13,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T17:47:13,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T17:47:13,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T17:47:14,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T17:47:14,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T17:47:14,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T17:47:14,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T17:47:14,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T17:47:14,024][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T17:47:14,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T17:47:14,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T17:47:14,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T17:47:14,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T17:47:14,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T17:47:14,047][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T17:47:14,047][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T17:47:14,048][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T17:47:14,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T17:47:14,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T17:47:14,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T17:47:14,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T17:47:14,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T17:47:14,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T17:47:14,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T17:47:14,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T17:47:14,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T17:47:14,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T17:47:14,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T17:47:14,299][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104gb], net total_space [125.8gb], types [ext4]
[2022-03-25T17:47:14,315][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T17:47:15,097][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T17:47:41,902][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T17:47:41,913][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T17:47:44,130][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T17:47:44,423][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T17:47:46,261][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T17:47:48,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T17:47:48,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T17:47:48,181][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T17:47:48,183][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T17:47:48,623][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T17:47:52,262][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T17:47:52,483][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 117, version: 3307, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T17:47:52,813][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 117, version: 3307, reason: Publication{term=117, version=3307}
[2022-03-25T17:47:53,210][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T17:47:53,211][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T17:47:55,214][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T17:47:55,251][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T17:47:56,841][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T17:47:56,842][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T17:47:58,094][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:58,491][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:59,215][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:59,316][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-25T17:47:59,327][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-25T17:47:59,330][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-25T17:48:01,401][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T17:48:01,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T17:48:06,310][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][18] overhead, spent [300ms] collecting in the last [1s]
[2022-03-25T17:48:08,914][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T17:48:23,704][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.25][0]]]).
[2022-03-25T17:48:24,144][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-25T17:48:24,665][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-25T17:49:29,651][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 504 finished with response BulkByScrollResponse[took=478.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T17:49:32,624][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 525 finished with response BulkByScrollResponse[took=2.9s,timed_out=false,sliceId=null,updated=1039,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T17:49:43,738][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-25T18:40:56,756][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T18:53:43,446][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T18:59:47,606][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T19:00:00,368][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4328] overhead, spent [569ms] collecting in the last [1.1s]
[2022-03-25T19:04:18,185][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4569][98] duration [2.4s], collections [1]/[3.7s], total [2.4s]/[7.6s], memory [1.2gb]->[177.9mb]/[2gb], all_pools {[young] [1gb]->[4mb]/[0b]}{[old] [168.4mb]->[168.4mb]/[2gb]}{[survivor] [8.7mb]->[9.5mb]/[0b]}
[2022-03-25T19:04:19,761][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4569] overhead, spent [2.4s] collecting in the last [3.7s]
[2022-03-25T19:04:25,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [10724ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:04:29,781][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4571][99] duration [1.4s], collections [1]/[2.8s], total [1.4s]/[9s], memory [197.9mb]->[182mb]/[2gb], all_pools {[young] [52mb]->[24mb]/[0b]}{[old] [168.4mb]->[169.2mb]/[2gb]}{[survivor] [9.5mb]->[8.8mb]/[0b]}
[2022-03-25T19:04:30,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4571] overhead, spent [1.4s] collecting in the last [2.8s]
[2022-03-25T19:04:52,801][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4577][100] duration [2s], collections [1]/[3.9s], total [2s]/[11s], memory [214mb]->[242mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [169.2mb]->[169.2mb]/[2gb]}{[survivor] [8.8mb]->[9.5mb]/[0b]}
[2022-03-25T19:04:53,967][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4577] overhead, spent [2s] collecting in the last [3.9s]
[2022-03-25T19:04:55,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [8929ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:05:36,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [21616ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:06:27,390][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:37172}] took [13206ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:06:58,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:11,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17431827175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:15,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [26298ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:07:15,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26298ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:21,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26298054036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:25,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:28,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [10527ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:07:32,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10527761630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:23,991][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26298ms] which is above the warn threshold of [5s]
[2022-03-25T19:07:44,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:50,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18250754263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:08:05,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:08:41,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [20257ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:08:41,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20257936508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:11,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66164ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:17,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66164336526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:31,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12863447260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:48,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24904ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:00,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24904796383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:16,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:34,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28080859831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:44,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:47,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [27758ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:10:49,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758010201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:03,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [17827ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:11:02,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:12,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17827198896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:16,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:21,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14369898133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:24,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:19,357][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14370ms] which is above the warn threshold of [5s]
[2022-03-25T19:11:27,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [7976ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:11:28,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7976224930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:38,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [6092ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:12:25,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:12:26,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43301401616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:12:29,182][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4589][101] duration [30.4s], collections [1]/[55.7s], total [30.4s]/[41.5s], memory [258.8mb]->[193.4mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [169.2mb]->[170.1mb]/[2gb]}{[survivor] [9.5mb]->[7.3mb]/[0b]}
[2022-03-25T19:12:29,065][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [32901] timed out after [71464ms]
[2022-03-25T19:12:30,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4589] overhead, spent [30.4s] collecting in the last [55.7s]
[2022-03-25T19:12:31,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [6174ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:12:34,443][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.5m/451011ms] ago, timed out [6.3m/379547ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [32901]
[2022-03-25T19:12:56,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [5427ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:13:15,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [9659ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:14:27,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [50964ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:18:37,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:19:16,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24575424343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:19:43,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:05,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65148679130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:39,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:48,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [121854ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:21:07,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56706215790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:23:11,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:38,921][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [65149ms] which is above the warn threshold of [5s]
[2022-03-25T19:24:37,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150858076373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:24:49,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:26:40,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98564707333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:02,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/133798ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:19,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/133798205583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:30,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:38,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27380024265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:43,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:30,562][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33066] timed out after [371278ms]
[2022-03-25T19:27:47,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14045012735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:49,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:48,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4599][102] duration [1.4m], collections [1]/[8m], total [1.4m]/[2.1m], memory [225.4mb]->[186.1mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-25T19:27:51,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5339125149ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:51,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [19384ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:27:55,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:59,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6396666769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:03,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:05,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [7923ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:27:58,568][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.3m/558237ms] ago, timed out [3.1m/186959ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33066]
[2022-03-25T19:28:06,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7923648031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:09,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:11,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6506112771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:10,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [6506ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:16,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [6905ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:16,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:27,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6905766542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:37,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:44,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [20409ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:46,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20409206380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:54,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:56,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [16562ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:01,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16562907176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:07,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:14,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13495022644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:22,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:26,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14613956808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:25,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [14613ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:31,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [9301ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:31,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:38,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9301760803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,693][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [37.4s/37410ms] ago, timed out [0s/0ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33126]
[2022-03-25T19:29:46,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [13122ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:50,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13122256963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:02,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,257][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33126] timed out after [37410ms]
[2022-03-25T19:30:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476619774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:26,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:40,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24079280312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:48,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [24079ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:30:54,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:01,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914627885ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:10,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:16,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16962305169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:23,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:32,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13815134368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:32,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [30777ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:31:13,916][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16963ms] which is above the warn threshold of [5s]
[2022-03-25T19:31:44,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:56,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20604565563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:12,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:27,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28235774198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:45,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:04,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32853177729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:17,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:27,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31978845533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:31,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [31978ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:33:41,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:55,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24117731252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:09,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:27,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27865933874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:53,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:15,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38798970596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:34,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:52,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46106475305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:54,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [46106ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:36:10,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:30,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34062310888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:48,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:47,369][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33182] timed out after [117186ms]
[2022-03-25T19:37:01,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39370681162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:37:24,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32318ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:37:44,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32318096264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:50,724][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [39371ms] which is above the warn threshold of [5s]
[2022-03-25T19:38:18,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:38:40,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50525538257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:21,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:17,079][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.5s/20526ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93]], which exceeds the warn threshold of [10s]
[2022-03-25T19:40:10,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67529145259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:52,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [67529ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:40:52,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:26,718][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:37180}] took [150373ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:41:33,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93589702063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:41:59,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:16,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67349234040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:31,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:52,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32204612075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:07,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:28,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35139925862ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:51,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:44:03,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@4c5e33d1] took [44827ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:44:17,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44827664644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:44:41,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:02,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48050334855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:23,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:33,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958836384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:47,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:57,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [28463ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:46:01,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28463489349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:19,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:26,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [32147ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:46:34,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32147720356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:54,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:10,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35438493693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:15,634][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.1m/909933ms] ago, timed out [13.2m/792747ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33182]
[2022-03-25T19:47:24,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:43,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28152052937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:58,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:04,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [35278ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:48:08,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35278367777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:25,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:29,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [27422ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:48:41,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27422457062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:55,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:49:11,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30219065515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:28,342][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33271] timed out after [183059ms]
[2022-03-25T19:49:20,704][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10986ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0]], which exceeds the warn threshold of [10s]
[2022-03-25T19:49:33,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:09,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31659155326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:39,713][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.4s/17407ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@95bd4ef1]], which exceeds the warn threshold of [10s]
[2022-03-25T19:50:41,197][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40706}] took [89301ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:50:45,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:55,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77603991973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:11,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:26,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26030020562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:33,339][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40684}] took [103634ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:51:35,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [26030ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:51:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:54,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30957882736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:56:12,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:56:43,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270571251994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:01,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:13,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48609265786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:26,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:27,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@59cd0a64] took [25428ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:57:39,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25428147920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:58,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:01,978][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.1m/847822ms] ago, timed out [11m/664763ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33271]
[2022-03-25T19:58:05,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831762065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:16,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4613][103] duration [3.6m], collections [1]/[6.6m], total [3.6m]/[5.8m], memory [254.1mb]->[187.6mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [8mb]->[5.4mb]/[0b]}
[2022-03-25T19:58:19,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:22,963][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4613] overhead, spent [3.6m] collecting in the last [6.6m]
[2022-03-25T19:58:25,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19941912137ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:25,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [52773ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:58:39,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:47,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20594361043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:08,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:33,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29109027589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:47,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [37771ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:59:47,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:54,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37771644334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:08,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:34,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21711780943ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:53,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:11,820][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40688}] took [45241ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:01:11,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45240699162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:15,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [45240ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:01:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:47,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37797219032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:02:07,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:02:29,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28051543340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:03:10,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:52,809][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33333] timed out after [140248ms]
[2022-03-25T20:02:13,914][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28052ms] which is above the warn threshold of [5s]
[2022-03-25T20:03:36,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66575536362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:04:06,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:05:42,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58244757285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:05:59,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:06:16,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114668192682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:06:30,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:07:10,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31828887822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:07:11,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [31828ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:07:37,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:02,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66489628213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:44,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45264467266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:08,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:16,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@61661069] took [45422ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:09:28,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45422658334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:53,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:20,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.6s/44639698135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:35,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:11:00,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472507228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:11:32,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58s/58021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:30,626][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44639ms] which is above the warn threshold of [5s]
[2022-03-25T20:11:56,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58203900545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:12:16,907][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14m/845860ms] ago, timed out [11.7m/705612ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33333]
[2022-03-25T20:12:24,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [58203ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:12:25,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:13:33,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53292884968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:14:33,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:14:55,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127417383810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:15:14,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:15:35,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41070333827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:02,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:28,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47649460881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:47,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45844ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:17:30,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45844050512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:01,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:57,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73293848606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:19:11,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:19:29,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69922473716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:49,925][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33409] timed out after [366096ms]
[2022-03-25T20:19:47,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:09,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36110978754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:42,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:42,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [50945ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:21:18,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50945347426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:21:47,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:11,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69820139104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:31,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:23:01,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43113470025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:14,310][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [69820ms] which is above the warn threshold of [5s]
[2022-03-25T20:23:41,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:23:59,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70847132487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:21,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:35,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39639395822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:47,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:55,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [25862ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:24:59,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25862115846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:25:13,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:25:16,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [26521ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:25:35,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26521059095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:01,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:27,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36533791135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:39,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:47,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48377322444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:53:14,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4m/1584266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:55:40,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4m/1584266404672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:56:39,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4619][104] duration [22.1m], collections [1]/[26.7m], total [22.1m]/[27.9m], memory [243.6mb]->[175.6mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [5.4mb]->[5.4mb]/[0b]}
[2022-03-25T20:58:00,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:59:45,529][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4619] overhead, spent [22.1m] collecting in the last [26.7m]
[2022-03-25T21:00:33,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296750798431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:02:37,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [1881017ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:02:53,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:05:25,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292145004438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:08:07,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:10:50,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313933965622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:12:24,446][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [58.9m/3537673ms] ago, timed out [52.8m/3171577ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33409]
[2022-03-25T21:14:02,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:16:46,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307272735975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:19:02,329][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.8s/21840ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93]], which exceeds the warn threshold of [10s]
[2022-03-25T21:19:23,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:21:45,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357627262708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:16:21,329][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [307273ms] which is above the warn threshold of [5s]
[2022-03-25T21:22:24,440][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.8s/32852ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T21:24:13,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:26:58,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301544254369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:28:09,749][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.5s/10582ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T21:30:51,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:33:27,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392548510415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:26:40,608][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33485] timed out after [2572007ms]
[2022-03-25T21:36:13,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:38:40,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313791736947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:41:42,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:44:43,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342577554963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:46:50,278][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.5m/694093ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [12.3m/741466ms]
[2022-03-25T21:48:13,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:50:27,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [391411ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:50:32,899][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [29m/1741873ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [29.1m/1750738ms]
[2022-03-25T21:50:57,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391411321665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:54:11,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [344322ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:54:09,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:57:38,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344322819398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:00:41,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:57:29,252][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/111228ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@95bd4ef1], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T22:04:38,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401884462127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:10:55,140][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T22:10:55,208][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T22:10:55,211][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T22:11:04,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T22:11:04,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T22:11:04,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T22:11:04,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T22:11:04,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T22:11:04,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T22:11:04,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T22:11:04,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T22:11:04,422][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T22:11:04,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T22:11:04,424][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T22:11:04,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T22:11:04,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T22:11:04,430][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T22:11:04,432][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T22:11:04,434][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T22:11:04,436][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T22:11:04,439][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T22:11:04,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T22:11:04,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T22:11:04,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T22:11:04,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T22:11:04,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T22:11:04,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T22:11:04,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T22:11:04,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T22:11:04,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T22:11:04,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T22:11:04,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T22:11:04,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T22:11:04,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T22:11:04,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T22:11:04,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T22:11:04,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T22:11:04,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T22:11:04,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T22:11:04,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T22:11:04,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T22:11:04,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T22:11:04,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T22:11:04,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T22:11:04,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T22:11:04,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T22:11:04,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T22:11:04,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T22:11:04,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T22:11:04,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T22:11:04,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T22:11:04,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T22:11:04,501][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T22:11:04,607][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.8gb], net total_space [125.8gb], types [ext4]
[2022-03-25T22:11:04,609][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T22:11:05,064][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T22:11:19,671][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T22:11:19,679][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T22:11:19,682][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,683][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-25T22:11:19,687][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,689][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T22:11:19,690][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T22:11:19,692][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,693][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T22:11:21,050][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T22:11:21,255][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T22:11:23,329][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T22:11:25,875][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T22:11:25,889][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T22:11:26,092][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T22:11:26,101][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T22:11:26,715][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T22:11:33,330][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T22:11:33,614][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 118, version: 3348, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T22:11:34,113][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 118, version: 3348, reason: Publication{term=118, version=3348}
[2022-03-25T22:11:36,888][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T22:11:36,932][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T22:11:40,913][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T22:12:43,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:12:44,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7509ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:14:21,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21456633825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:14:39,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:14:52,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140076244527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:15:14,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:00,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34176985191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:15,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [174253ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:16:24,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:35,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75259746665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:41,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:46,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16787969799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:52,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:54,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12022878658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:58,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:02,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6424749114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:05,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:09,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7433347976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:11,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:12,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204232007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:12,442][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3349}] took [5.4m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3349}]] took [0ms], [connecting to new nodes] took [20ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [50ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [188ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [1655ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [9ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [0ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [20ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [83ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [18ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [3ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [9ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [34ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [4ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [4ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@21a3c20] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [3ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@2e661a53] took [9ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [0ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [11ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [0ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [2ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [1720ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@38d52050] took [329836ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [42ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [42ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:17:13,046][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T22:17:12,931][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39714}] took [33053ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:17:15,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@51c28ffa] took [52450ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:17:19,748][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-25T22:17:19,746][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-25T22:17:20,500][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-25T22:17:21,154][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,317][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-25T22:17:21,509][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/config%3A7.17.0, params: {index=.kibana_7.17.0, id=config:7.17.0}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][config:7.17.0]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,666][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/space%3Adefault, params: {index=.kibana_7.17.0, id=space:default}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][space:default]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,757][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T22:17:21,857][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T22:19:27,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@634595fc] took [96554ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:19:57,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8017ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:20:23,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [11599ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:20:37,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:20:23,390][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9999ms] which is above the warn threshold of [5s]
[2022-03-25T22:20:40,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8867931415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:20:47,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:00,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862549429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:05,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [18521ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:21:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18521072164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:06,212][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [195505ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [25] unchanged indices
[2022-03-25T22:21:21,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:25,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257943654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:29,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:34,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8121776198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:38,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:40,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [9148ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:21:32,919][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.1m] publication of cluster state version [3352] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-25T22:21:43,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9148234875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:50,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:53,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70a8e64b, interval=5s}] took [11351ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:22:00,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11351952328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:15,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:28,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23617793568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:38,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:44,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23613384484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:29,720][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [56] timed out after [44924ms]
[2022-03-25T22:22:53,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:10,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15476690326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:18,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:18,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [63516ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:25,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24426559439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:30,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:34,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13090294246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:36,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [13090ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:40,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:45,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8938395580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:51,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67d1dedd, interval=1m}] took [12441ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:51,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:54,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12441410473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:59,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:00,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [7896ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:24:04,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7896552751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:10,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:14,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [10791ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:24:20,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10791911221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:28,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:32,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18246965640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:37,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:42,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9256499383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:46,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:50,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9410464457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:54,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7014ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:57,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7013776830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:01,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:05,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517369129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:13,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:19,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8271920785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:22,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:28,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13024824539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:34,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@51a65b55] took [83125ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:33,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:38,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10383722736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:42,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:42,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [9138ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:46,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9138746833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:52,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:55,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9447642534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:56,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [9447ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:58,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:01,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6514831719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:06,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:09,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8568826690ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:05,997][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [60] timed out after [89980ms]
[2022-03-25T22:26:10,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [12599ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:09,382][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [61] timed out after [35485ms]
[2022-03-25T22:26:21,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6054ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:21,783][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6255ms] which is above the warn threshold of [5s]
[2022-03-25T22:26:41,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8818ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:50,015][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3352}] took [4.9m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3352}]] took [340ms], [connecting to new nodes] took [1879ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [332ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [131205ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [0ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [806ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [290ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [1ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [271ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [68ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [339ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [153ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [1711ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [12954ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [1210ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [272ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [333ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [82195ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [133ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [10720ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [576ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [123ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [7784ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [389ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [12567ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [127ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [13073ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [5001ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [753ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [130ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [66ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [0ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:26:55,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7939ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:27:20,048][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.5s/17552ms] to notify listeners on successful publication of cluster state (version: 3352, uuid: Z61V7ePbSGKdeoXbXCUUWw) for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [89], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [89], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [31], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [31], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-25T22:27:26,484][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10m/601335ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [10m/601773ms]
[2022-03-25T22:27:43,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:27:36,434][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3m/184436ms] ago, timed out [1.5m/94456ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [60]
[2022-03-25T22:27:44,298][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.1m/490220ms] ago, timed out [7.4m/445296ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [56]
[2022-03-25T22:27:44,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6392053009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:27:15,189][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketTimeoutException: null
	at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:110) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-25T22:27:44,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@562ad2d6] took [43340ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:27:48,415][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.4m/144721ms] ago, timed out [1.8m/109236ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [61]
[2022-03-25T22:27:58,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6001ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:07,047][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65] timed out after [21902ms]
[2022-03-25T22:28:14,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6831ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:40,761][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5606ms] which is above the warn threshold of [5s]
[2022-03-25T22:28:42,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:50,433][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39714}] took [611367ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:38,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@20891f77] took [47907ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:52,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7827ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:58,768][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [71] timed out after [64493ms]
[2022-03-25T22:30:00,571][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [113565ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [25] unchanged indices
[2022-03-25T22:30:00,849][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [72] timed out after [21642ms]
[2022-03-25T22:30:07,107][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.1m] publication of cluster state version [3353] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-25T22:30:32,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6743ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:30:55,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@33fc8944] took [15168ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:30:56,658][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/191775ms] ago, timed out [2.8m/169873ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65]
[2022-03-25T22:31:04,235][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [80] timed out after [22346ms]
[2022-03-25T22:31:06,257][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/86581ms] ago, timed out [1m/64939ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [72]
[2022-03-25T22:31:06,257][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.1m/131686ms] ago, timed out [1.1m/67193ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [71]
[2022-03-25T22:31:13,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5955ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:21,903][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [39.6s/39676ms] ago, timed out [17.3s/17330ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [80]
[2022-03-25T22:31:21,640][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [81] timed out after [24172ms]
[2022-03-25T22:31:25,660][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [29.4s/29461ms] ago, timed out [5.2s/5289ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [81]
[2022-03-25T22:31:27,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6446ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:39,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6985ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:51,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5651ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:32:01,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6302ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:59,077][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3353}] took [1.7m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3353}]] took [53ms], [connecting to new nodes] took [525ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [7816ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [0ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [316ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [155ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [101ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [51ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [138ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [2085ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [494ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [4ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [342ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [232ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [36644ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [9765ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [791ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [398ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [10487ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [486ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [12232ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [118ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [249ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [8752ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [3795ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [311ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [2054ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [56ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [97ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [48ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [341ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [48ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [0ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:33:47,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@4e214b72] took [99676ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:01,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5032ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:20,143][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2m/120578ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-25T22:34:29,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8639ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:57,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [17408ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:36:54,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:38:02,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5165797971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:37:49,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [85051ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:39:24,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:39:52,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159957984247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:40:33,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:41:36,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61589604827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:43:27,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:45:55,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173885158101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:47:00,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:47:43,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218584186744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:49:11,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:50:46,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127079825867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:52:11,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/186960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:52:58,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/186959830872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:53:40,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:55:22,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89362844379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:56:48,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:57:35,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187493783819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:58:29,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:58:53,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103723539477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:59:27,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@3dca0ae] took [1148678ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:59:38,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:00:25,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69620749208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:01:29,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:02:06,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109267493825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:02:39,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:03:10,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70001213370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:03:48,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:04:24,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70670960726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:03,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:40,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [74295ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:05:30,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74295620605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:06:24,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:07:02,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76597084010ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:35,870][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [74296ms] which is above the warn threshold of [5s]
[2022-03-25T23:07:32,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:08:04,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72619469411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:08:34,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:09:11,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61628805599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:10:18,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:10:31,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [102596ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:10:59,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102596780343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:11:30,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:11:32,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@625fb4bb, interval=30s}] took [73040ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:12:00,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73040553218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:12:35,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:13:15,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65278156830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:13:58,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:14:48,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81538025314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:16:52,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:18:42,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173571902567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:21:19,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:23:50,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263627140079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:22:41,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43280}] took [263627ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:26:11,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:28:25,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292323478499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:33:19,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:38:41,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403969572235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:42:03,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:44:36,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540056550106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:47:22,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:50:39,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324771832285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:53:28,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:53:57,176][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [4696785ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [23] unchanged indices
[2022-03-25T23:56:15,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361734784179ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:59:31,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:42:10,467][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T00:42:10,494][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T00:42:10,497][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T00:42:17,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T00:42:17,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T00:42:17,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T00:42:17,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T00:42:17,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T00:42:17,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T00:42:17,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T00:42:17,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T00:42:17,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T00:42:17,095][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T00:42:17,096][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T00:42:17,096][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T00:42:17,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T00:42:17,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T00:42:17,101][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T00:42:17,102][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T00:42:17,103][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T00:42:17,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T00:42:17,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T00:42:17,107][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T00:42:17,108][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T00:42:17,109][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T00:42:17,110][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T00:42:17,111][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T00:42:17,111][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T00:42:17,112][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T00:42:17,113][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T00:42:17,114][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T00:42:17,116][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T00:42:17,117][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T00:42:17,117][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T00:42:17,118][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T00:42:17,120][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T00:42:17,121][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T00:42:17,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T00:42:17,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T00:42:17,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T00:42:17,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T00:42:17,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T00:42:17,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T00:42:17,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T00:42:17,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T00:42:17,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T00:42:17,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T00:42:17,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T00:42:17,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T00:42:17,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T00:42:17,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T00:42:17,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T00:42:17,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T00:42:17,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T00:42:17,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T00:42:17,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T00:42:17,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T00:42:17,142][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T00:42:17,142][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T00:42:17,143][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T00:42:17,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T00:42:17,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T00:42:17,252][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.9gb], net total_space [125.8gb], types [ext4]
[2022-03-26T00:42:17,253][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T00:42:17,685][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T00:42:37,840][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T00:42:37,848][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T00:42:42,120][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T00:42:42,381][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T00:43:44,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [6330ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:44:31,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:45:33,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15354950944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:45:49,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:46:04,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92927613443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:46:24,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:46:38,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@59f8a4ec, interval=1m}] took [34760ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:46:52,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34760410006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:22,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:29,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [56960ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:47:41,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56960130833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:53,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:56,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6cbb4d8b, interval=30s}] took [33673ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:48:04,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33673548894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:48:18,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:48:22,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [24793ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:48:39,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24793906046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:48:58,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:49:17,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36426514201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:49:45,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:07,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38291216981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:19,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:19,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [45301ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:50:28,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45301823103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:32,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:33,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15007691939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:53:10,315][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T00:53:11,677][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T00:53:11,685][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T00:53:11,778][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T00:53:11,781][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T00:53:12,161][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T00:53:15,507][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T00:53:15,631][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 119, version: 3355, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T00:53:15,850][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 119, version: 3355, reason: Publication{term=119, version=3355}
[2022-03-26T00:53:15,981][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T00:53:15,981][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T00:53:17,286][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T00:53:17,300][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-26T00:53:19,335][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T00:53:19,337][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T00:53:20,885][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-03-26T00:53:22,599][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T00:53:22,615][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T00:53:22,618][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T00:53:26,495][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T00:53:27,967][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T00:53:53,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@73889450] took [7515ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:53:53,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][26][15] duration [2.3s], collections [1]/[8.2s], total [2.3s]/[3.4s], memory [306.4mb]->[134.4mb]/[2gb], all_pools {[young] [184mb]->[4mb]/[0b]}{[old] [111.8mb]->[111.8mb]/[2gb]}{[survivor] [10.5mb]->[18.5mb]/[0b]}
[2022-03-26T00:53:54,250][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][26] overhead, spent [2.3s] collecting in the last [8.2s]
[2022-03-26T00:53:54,701][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48588}] took [21860ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:12,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][34][16] duration [2.2s], collections [1]/[1.5s], total [2.2s]/[5.6s], memory [186.4mb]->[206.4mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [111.8mb]->[126.1mb]/[2gb]}{[survivor] [18.5mb]->[5.2mb]/[0b]}
[2022-03-26T00:54:12,969][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][34] overhead, spent [2.2s] collecting in the last [1.5s]
[2022-03-26T00:54:25,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5324ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:54:25,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5323783973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:54:25,773][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][40][17] duration [2.1s], collections [1]/[6.3s], total [2.1s]/[7.7s], memory [207.3mb]->[134.7mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [126.1mb]->[126.1mb]/[2gb]}{[survivor] [5.2mb]->[8.5mb]/[0b]}
[2022-03-26T00:54:19,140][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43802}] took [6924ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:25,836][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][40] overhead, spent [2.1s] collecting in the last [6.3s]
[2022-03-26T00:54:35,090][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43794}] took [9689ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:35,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@5f784320] took [5416ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:36,142][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][44][18] duration [2.9s], collections [1]/[5.9s], total [2.9s]/[10.7s], memory [194.7mb]->[169mb]/[2gb], all_pools {[young] [60mb]->[48mb]/[0b]}{[old] [126.1mb]->[126.1mb]/[2gb]}{[survivor] [8.5mb]->[14.8mb]/[0b]}
[2022-03-26T00:54:36,548][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][44] overhead, spent [2.9s] collecting in the last [5.9s]
[2022-03-26T00:54:38,307][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T00:54:44,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5465ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:46,018][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10765ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [26] unchanged indices
[2022-03-26T00:54:49,473][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][48][19] duration [841ms], collections [1]/[3.3s], total [841ms]/[11.5s], memory [221mb]->[148.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [126.1mb]->[132.5mb]/[2gb]}{[survivor] [14.8mb]->[16mb]/[0b]}
[2022-03-26T00:54:49,416][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.3s] publication of cluster state version [3376] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T00:54:55,197][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][50][20] duration [1.3s], collections [1]/[1.1s], total [1.3s]/[12.9s], memory [212.5mb]->[228.5mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [132.5mb]->[147.9mb]/[2gb]}{[survivor] [16mb]->[2.8mb]/[0b]}
[2022-03-26T00:54:55,556][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][50] overhead, spent [1.3s] collecting in the last [1.1s]
[2022-03-26T00:54:58,687][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [564ms] collecting in the last [1.4s]
[2022-03-26T00:55:05,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][55][22] duration [1.7s], collections [1]/[3.2s], total [1.7s]/[15.2s], memory [227.5mb]->[153.1mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [147.9mb]->[147.9mb]/[2gb]}{[survivor] [3.5mb]->[5.1mb]/[0b]}
[2022-03-26T00:55:06,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][55] overhead, spent [1.7s] collecting in the last [3.2s]
[2022-03-26T00:55:18,569][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10181ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [27] unchanged indices
[2022-03-26T00:55:23,204][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.5s] publication of cluster state version [3379] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T00:55:34,122][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T00:55:34,143][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][64][23] duration [3.6s], collections [1]/[5.7s], total [3.6s]/[18.9s], memory [233.1mb]->[156.4mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [147.9mb]->[147.9mb]/[2gb]}{[survivor] [5.1mb]->[8.4mb]/[0b]}
[2022-03-26T00:55:34,670][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][64] overhead, spent [3.6s] collecting in the last [5.7s]
[2022-03-26T00:55:38,380][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-03-26T00:55:43,677][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-03-26T00:55:50,607][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40226}] took [27130ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:56:12,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:56:13,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7575783842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:56:13,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [7575ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:56:15,088][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][78][24] duration [4s], collections [1]/[12.8s], total [4s]/[23s], memory [232.4mb]->[162.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [147.9mb]->[149.2mb]/[2gb]}{[survivor] [8.4mb]->[13.4mb]/[0b]}
[2022-03-26T00:56:15,556][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][78] overhead, spent [4s] collecting in the last [12.8s]
[2022-03-26T00:56:17,692][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [35839ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [24] unchanged indices
[2022-03-26T00:56:19,879][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [40.1s] publication of cluster state version [3380] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T00:56:32,732][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T00:56:33,342][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T00:56:34,793][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.26] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-26T00:56:36,924][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [364ms] collecting in the last [1.1s]
[2022-03-26T00:56:46,017][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][26] duration [3.8s], collections [1]/[5.1s], total [3.8s]/[27.2s], memory [231.7mb]->[170.3mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [155.7mb]->[160.4mb]/[2gb]}{[survivor] [12mb]->[9.8mb]/[0b]}
[2022-03-26T00:56:46,367][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [3.8s] collecting in the last [5.1s]
[2022-03-26T00:56:46,426][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-26T00:56:46,659][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.26][0]]]).
[2022-03-26T00:56:50,883][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [400ms] collecting in the last [1.1s]
[2022-03-26T00:56:53,563][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:56:55,112][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:56:59,515][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][102][31] duration [844ms], collections [1]/[1.3s], total [844ms]/[29.1s], memory [225.8mb]->[257.8mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [8mb]->[6.8mb]/[0b]}
[2022-03-26T00:56:59,593][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][102] overhead, spent [844ms] collecting in the last [1.3s]
[2022-03-26T00:56:59,587][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:00,034][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:12,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:12,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7085ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:57:12,654][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:12,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6084493165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:12,861][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.3s/11349ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-26T00:57:13,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][107][32] duration [4.3s], collections [1]/[8.4s], total [4.3s]/[33.5s], memory [252.6mb]->[215.7mb]/[2gb], all_pools {[young] [84mb]->[68mb]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [6.8mb]->[5.9mb]/[0b]}
[2022-03-26T00:57:13,760][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [4.3s] collecting in the last [8.4s]
[2022-03-26T00:57:22,440][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][108][33] duration [2.3s], collections [1]/[1.3s], total [2.3s]/[35.9s], memory [215.7mb]->[251.7mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [5.9mb]->[5.6mb]/[0b]}
[2022-03-26T00:57:22,733][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][108] overhead, spent [2.3s] collecting in the last [1.3s]
[2022-03-26T00:57:22,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7964ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:57:28,095][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][109][34] duration [2.1s], collections [1]/[11.6s], total [2.1s]/[38s], memory [251.7mb]->[167.9mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [5.6mb]->[6.1mb]/[0b]}
[2022-03-26T00:57:29,581][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T00:57:32,747][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-03-26T00:57:33,361][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-03-26T00:57:34,238][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:50,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:51,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8597852961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:54,302][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][117][35] duration [6.8s], collections [1]/[10s], total [6.8s]/[44.8s], memory [235.9mb]->[168.5mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [6.1mb]->[6.7mb]/[0b]}
[2022-03-26T00:57:55,972][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [6.8s] collecting in the last [10s]
[2022-03-26T00:57:56,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [14400ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:04,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6020ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:33,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7259ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:32,106][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3391}] took [34.6s] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3391}]] took [29ms], [connecting to new nodes] took [58ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [659ms], [org.elasticsearch.script.ScriptService@287d7733] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [1079ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [83ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [15640ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [434ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [151ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [1800ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [115ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [8390ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [4ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [3669ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [2047ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [138ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [64ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [0ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [62ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [0ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T00:58:54,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [19517ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:54,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:58:54,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10336637479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:58:57,140][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][121][36] duration [6.8s], collections [1]/[29.1s], total [6.8s]/[51.7s], memory [224.5mb]->[193.6mb]/[2gb], all_pools {[young] [56mb]->[20mb]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [6.7mb]->[11.7mb]/[0b]}
[2022-03-26T00:59:05,299][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][124][37] duration [2.1s], collections [1]/[3.5s], total [2.1s]/[53.8s], memory [249.6mb]->[192.1mb]/[2gb], all_pools {[young] [76mb]->[40mb]/[0b]}{[old] [161.8mb]->[167.1mb]/[2gb]}{[survivor] [11.7mb]->[5mb]/[0b]}
[2022-03-26T00:59:08,291][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][124] overhead, spent [2.1s] collecting in the last [3.5s]
[2022-03-26T00:59:15,693][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][38] duration [2.2s], collections [1]/[4.5s], total [2.2s]/[56s], memory [256.1mb]->[169.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [5mb]->[2.3mb]/[0b]}
[2022-03-26T00:59:15,901][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [2.2s] collecting in the last [4.5s]
[2022-03-26T00:59:15,911][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:20,402][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][129][39] duration [1.3s], collections [1]/[1.3s], total [1.3s]/[57.4s], memory [201.4mb]->[261.4mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [2.3mb]->[3mb]/[0b]}
[2022-03-26T00:59:20,663][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][129] overhead, spent [1.3s] collecting in the last [1.3s]
[2022-03-26T00:59:21,397][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:21,547][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:28,784][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:28,628][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][134][40] duration [993ms], collections [1]/[2.1s], total [993ms]/[58.4s], memory [254.1mb]->[171.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [3mb]->[4mb]/[0b]}
[2022-03-26T00:59:29,036][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][134] overhead, spent [993ms] collecting in the last [2.1s]
[2022-03-26T00:59:34,136][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T00:59:34,523][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T00:59:41,352][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][141][41] duration [1s], collections [1]/[1.3s], total [1s]/[59.4s], memory [251.1mb]->[259.1mb]/[2gb], all_pools {[young] [80mb]->[92mb]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [4mb]->[4mb]/[0b]}
[2022-03-26T00:59:41,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][141] overhead, spent [1s] collecting in the last [1.3s]
[2022-03-26T00:59:43,975][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T00:59:48,891][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-03-26T00:59:53,370][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:59,248][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T00:59:59,739][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T01:00:03,888][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][42] duration [2.2s], collections [1]/[1s], total [2.2s]/[1m], memory [247.4mb]->[174.1mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [167.1mb]->[167.8mb]/[2gb]}{[survivor] [8.3mb]->[6.2mb]/[0b]}
[2022-03-26T01:00:04,216][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [2.2s] collecting in the last [1s]
[2022-03-26T01:00:07,108][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:18,330][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:18,821][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][43] duration [2.3s], collections [1]/[4.1s], total [2.3s]/[1m], memory [242.1mb]->[190.6mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [167.8mb]->[167.8mb]/[2gb]}{[survivor] [6.2mb]->[6.7mb]/[0b]}
[2022-03-26T01:00:18,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [2.3s] collecting in the last [4.1s]
[2022-03-26T01:00:20,330][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [531ms] collecting in the last [1.5s]
[2022-03-26T01:00:30,437][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:40,520][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:48,770][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][184][49] duration [1.5s], collections [1]/[3s], total [1.5s]/[1.1m], memory [198.7mb]->[177.2mb]/[2gb], all_pools {[young] [28mb]->[32mb]/[0b]}{[old] [167.8mb]->[167.8mb]/[2gb]}{[survivor] [6.8mb]->[9.3mb]/[0b]}
[2022-03-26T01:00:49,242][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][184] overhead, spent [1.5s] collecting in the last [3s]
[2022-03-26T01:00:54,364][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [588ms] collecting in the last [1.2s]
[2022-03-26T01:00:58,488][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][190] overhead, spent [380ms] collecting in the last [1.2s]
[2022-03-26T01:01:01,764][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][192] overhead, spent [611ms] collecting in the last [1.4s]
[2022-03-26T01:01:02,506][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:01:05,289][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][193][53] duration [1.1s], collections [1]/[2.9s], total [1.1s]/[1.1m], memory [183.5mb]->[180.1mb]/[2gb], all_pools {[young] [28mb]->[28mb]/[0b]}{[old] [168.6mb]->[171.1mb]/[2gb]}{[survivor] [10.8mb]->[8.9mb]/[0b]}
[2022-03-26T01:01:05,639][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][193] overhead, spent [1.1s] collecting in the last [2.9s]
[2022-03-26T01:01:10,655][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][54] duration [1.7s], collections [1]/[3.3s], total [1.7s]/[1.1m], memory [252.1mb]->[178.5mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [8.9mb]->[7.3mb]/[0b]}
[2022-03-26T01:01:11,199][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [1.7s] collecting in the last [3.3s]
[2022-03-26T01:01:18,070][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][55] duration [3.3s], collections [1]/[2.2s], total [3.3s]/[1.2m], memory [178.5mb]->[266.5mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-26T01:01:18,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [3.3s] collecting in the last [2.2s]
[2022-03-26T01:01:18,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6474ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:01:27,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:01:43,981][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.3s/20397ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc]], which exceeds the warn threshold of [10s]
[2022-03-26T01:01:43,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:01:44,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13008858728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:01:44,023][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][56] duration [10.3s], collections [1]/[1.2s], total [10.3s]/[1.4m], memory [263.1mb]->[263.1mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [8mb]->[5.4mb]/[0b]}
[2022-03-26T01:01:44,133][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [10.3s] collecting in the last [1.2s]
[2022-03-26T01:01:44,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [15361ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:02:03,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:02:03,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3e3bb723, interval=1m}] took [6030ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:02:04,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6030500813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:02:05,342][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][57] duration [4.7s], collections [1]/[7.7s], total [4.7s]/[1.4m], memory [244.6mb]->[198.5mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [5.4mb]->[7.3mb]/[0b]}
[2022-03-26T01:02:06,776][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [4.7s] collecting in the last [7.7s]
[2022-03-26T01:02:32,216][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][219][58] duration [2.7s], collections [1]/[3.9s], total [2.7s]/[1.5m], memory [226.5mb]->[230.5mb]/[2gb], all_pools {[young] [48mb]->[64mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [7.3mb]->[7.3mb]/[0b]}
[2022-03-26T01:02:32,455][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][219] overhead, spent [2.7s] collecting in the last [3.9s]
[2022-03-26T01:02:38,704][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][222][59] duration [1.3s], collections [1]/[3.3s], total [1.3s]/[1.5m], memory [196.4mb]->[180.4mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [171.1mb]->[171.6mb]/[2gb]}{[survivor] [9.2mb]->[8.7mb]/[0b]}
[2022-03-26T01:02:38,872][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][222] overhead, spent [1.3s] collecting in the last [3.3s]
[2022-03-26T01:02:39,048][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:02:40,141][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][223] overhead, spent [410ms] collecting in the last [1.5s]
[2022-03-26T01:02:40,362][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:02:41,045][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:02:53,157][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][231][63] duration [1s], collections [1]/[1.1s], total [1s]/[1.5m], memory [235.3mb]->[267.3mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [171.6mb]->[171.6mb]/[2gb]}{[survivor] [7.6mb]->[6mb]/[0b]}
[2022-03-26T01:02:53,565][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][231] overhead, spent [1s] collecting in the last [1.1s]
[2022-03-26T01:03:01,188][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:03:23,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][242][64] duration [2.7s], collections [1]/[10.1s], total [2.7s]/[1.6m], memory [261.7mb]->[181mb]/[2gb], all_pools {[young] [84mb]->[12mb]/[0b]}{[old] [171.6mb]->[171.7mb]/[2gb]}{[survivor] [6mb]->[9.2mb]/[0b]}
[2022-03-26T01:03:23,602][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][242] overhead, spent [2.7s] collecting in the last [10.1s]
[2022-03-26T01:03:27,027][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [19137ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T01:03:28,209][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.2s] publication of cluster state version [3405] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T01:03:34,148][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][246][65] duration [3.3s], collections [1]/[6s], total [3.3s]/[1.6m], memory [257mb]->[183.5mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [171.7mb]->[172.2mb]/[2gb]}{[survivor] [9.2mb]->[7.3mb]/[0b]}
[2022-03-26T01:03:35,033][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][246] overhead, spent [3.3s] collecting in the last [6s]
[2022-03-26T01:03:44,219][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][249][66] duration [2.9s], collections [1]/[1.4s], total [2.9s]/[1.7m], memory [255.5mb]->[180.2mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-26T01:03:44,602][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][249] overhead, spent [2.9s] collecting in the last [1.4s]
[2022-03-26T01:04:22,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7341ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:04:53,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [11324ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:04:53,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:04:53,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9724037547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:04:54,156][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][262][67] duration [4.2s], collections [1]/[18s], total [4.2s]/[1.8m], memory [264.2mb]->[183mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [8mb]->[10.8mb]/[0b]}
[2022-03-26T01:05:04,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:05:05,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8502447726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:05:05,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][263][68] duration [6.5s], collections [1]/[1.2s], total [6.5s]/[1.9m], memory [183mb]->[259mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [172.2mb]->[174.7mb]/[2gb]}{[survivor] [10.8mb]->[7.7mb]/[0b]}
[2022-03-26T01:05:06,494][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][263] overhead, spent [6.5s] collecting in the last [1.2s]
[2022-03-26T01:05:06,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [11677ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:05:22,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6368ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:05:24,167][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1124] timed out after [16631ms]
[2022-03-26T01:05:39,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:05:57,176][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1126] timed out after [15563ms]
[2022-03-26T01:06:12,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [15268ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:06:12,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:06:13,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11464424683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:06:15,171][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][276][69] duration [7.5s], collections [1]/[17.8s], total [7.5s]/[2m], memory [266.5mb]->[191.5mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [174.7mb]->[174.7mb]/[2gb]}{[survivor] [7.7mb]->[8.7mb]/[0b]}
[2022-03-26T01:06:16,341][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][276] overhead, spent [7.5s] collecting in the last [17.8s]
[2022-03-26T01:07:01,971][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1142] timed out after [15700ms]
[2022-03-26T01:07:01,900][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1141] timed out after [16100ms]
[2022-03-26T01:07:18,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:18,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5886997341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:19,187][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40366}] took [9185ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:07:18,899][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][297][70] duration [4.8s], collections [1]/[1.6s], total [4.8s]/[2.1m], memory [263.5mb]->[267.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [174.7mb]->[176.1mb]/[2gb]}{[survivor] [8.7mb]->[5.6mb]/[0b]}
[2022-03-26T01:07:23,469][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][297] overhead, spent [4.8s] collecting in the last [1.6s]
[2022-03-26T01:07:23,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:24,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5698577093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:24,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [11785ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:07:35,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:07:56,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6774ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:08:06,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:58,420][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1154] timed out after [17327ms]
[2022-03-26T01:07:58,382][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1153] timed out after [20329ms]
[2022-03-26T01:08:08,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5046726208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:08:29,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6574ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:08:42,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:08:43,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9470501883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:08:46,053][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][309][71] duration [5s], collections [1]/[22.1s], total [5s]/[2.2m], memory [257.8mb]->[221.9mb]/[2gb], all_pools {[young] [80mb]->[40mb]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [5.6mb]->[5.8mb]/[0b]}
[2022-03-26T01:09:10,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:09:11,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11511195899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:09:15,008][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][314][72] duration [5.9s], collections [1]/[13.3s], total [5.9s]/[2.3m], memory [229.9mb]->[180.7mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [5.8mb]->[4.5mb]/[0b]}
[2022-03-26T01:09:17,311][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][314] overhead, spent [5.9s] collecting in the last [13.3s]
[2022-03-26T01:09:19,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [21424ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:09:21,883][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1159] timed out after [32293ms]
[2022-03-26T01:09:22,850][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.4m/268713ms] ago, timed out [4.2m/252082ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1124]
[2022-03-26T01:09:22,502][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1160] timed out after [32093ms]
[2022-03-26T01:09:38,479][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [41.2s/41251ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@19e46b8a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9e693ae8]], which exceeds the warn threshold of [10s]
[2022-03-26T01:09:42,834][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.9m/176669ms] ago, timed out [2.6m/160569ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1141]
[2022-03-26T01:09:51,058][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3m/183673ms] ago, timed out [2.7m/167973ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1142]
[2022-03-26T01:10:00,056][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.3m/141329ms] ago, timed out [2m/121000ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1153]
[2022-03-26T01:10:06,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@291f3494] took [5403ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:10:20,630][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1191] timed out after [16410ms]
[2022-03-26T01:10:25,123][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1192] timed out after [17010ms]
[2022-03-26T01:10:30,201][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168746ms] ago, timed out [2.5m/151419ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1154]
[2022-03-26T01:10:40,746][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.7m/345856ms] ago, timed out [5.5m/330293ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1126]
[2022-03-26T01:10:42,955][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.8m/113306ms] ago, timed out [1.3m/81013ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1159]
[2022-03-26T01:10:44,328][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/114707ms] ago, timed out [1.3m/82614ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1160]
[2022-03-26T01:10:50,329][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [45.8s/45830ms] ago, timed out [29.4s/29420ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1191]
[2022-03-26T01:10:57,286][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_monitoring/bulk?system_id=kibana&system_api_version=7&interval=10000ms][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43880}] took [131603ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:10:56,455][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [49s/49026ms] ago, timed out [32s/32016ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1192]
[2022-03-26T01:11:18,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:04,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7787ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:46,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:12:47,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5764ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:47,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5364411326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:12:48,545][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][365][73] duration [3.8s], collections [1]/[7s], total [3.8s]/[2.3m], memory [256.7mb]->[195mb]/[2gb], all_pools {[young] [88mb]->[12mb]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [4.5mb]->[6.8mb]/[0b]}
[2022-03-26T01:12:48,222][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40426}] took [6807ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:48,222][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40432}] took [5364ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:48,671][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][365] overhead, spent [3.8s] collecting in the last [7s]
[2022-03-26T01:13:12,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:12,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5938376342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:13,443][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][372][74] duration [4.7s], collections [1]/[1.5s], total [4.7s]/[2.4m], memory [243mb]->[251mb]/[2gb], all_pools {[young] [64mb]->[68mb]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [6.8mb]->[6.8mb]/[0b]}
[2022-03-26T01:13:14,324][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][372] overhead, spent [4.7s] collecting in the last [1.5s]
[2022-03-26T01:13:14,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9215ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:13:32,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:32,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [5657ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:13:33,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5457810437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:33,399][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][377][75] duration [3.8s], collections [1]/[7.6s], total [3.8s]/[2.5m], memory [245.3mb]->[193.1mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [176.1mb]->[177.1mb]/[2gb]}{[survivor] [9.2mb]->[7.9mb]/[0b]}
[2022-03-26T01:13:33,474][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][377] overhead, spent [3.8s] collecting in the last [7.6s]
[2022-03-26T01:13:53,095][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][384][76] duration [2.6s], collections [1]/[1.6s], total [2.6s]/[2.5m], memory [217.1mb]->[273.1mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [7.9mb]->[9.7mb]/[0b]}
[2022-03-26T01:13:53,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][384] overhead, spent [2.6s] collecting in the last [1.6s]
[2022-03-26T01:13:55,635][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][385][77] duration [1.1s], collections [1]/[6s], total [1.1s]/[2.5m], memory [273.1mb]->[187mb]/[2gb], all_pools {[young] [0b]->[20mb]/[0b]}{[old] [177.1mb]->[178.3mb]/[2gb]}{[survivor] [9.7mb]->[8.6mb]/[0b]}
[2022-03-26T01:14:01,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][387][79] duration [1.6s], collections [1]/[1.1s], total [1.6s]/[2.6m], memory [232.2mb]->[272.2mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [178.8mb]->[180.7mb]/[2gb]}{[survivor] [9.4mb]->[7.9mb]/[0b]}
[2022-03-26T01:14:04,003][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][387] overhead, spent [1.6s] collecting in the last [1.1s]
[2022-03-26T01:14:04,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6526ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:14:11,237][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][390][80] duration [1.8s], collections [1]/[1.1s], total [1.8s]/[2.6m], memory [240.7mb]->[189.4mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [180.7mb]->[180.7mb]/[2gb]}{[survivor] [7.9mb]->[8.6mb]/[0b]}
[2022-03-26T01:14:11,402][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][390] overhead, spent [1.8s] collecting in the last [1.1s]
[2022-03-26T01:14:15,616][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][392][82] duration [834ms], collections [1]/[2.4s], total [834ms]/[2.6m], memory [201.4mb]->[187.7mb]/[2gb], all_pools {[young] [16mb]->[28mb]/[0b]}{[old] [180.7mb]->[180.7mb]/[2gb]}{[survivor] [8.6mb]->[6.9mb]/[0b]}
[2022-03-26T01:14:15,837][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][392] overhead, spent [834ms] collecting in the last [2.4s]
[2022-03-26T01:14:31,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][402][84] duration [1.5s], collections [1]/[3s], total [1.5s]/[2.7m], memory [263.5mb]->[189.8mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [180.7mb]->[180.7mb]/[2gb]}{[survivor] [6.7mb]->[9mb]/[0b]}
[2022-03-26T01:14:31,497][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][402] overhead, spent [1.5s] collecting in the last [3s]
[2022-03-26T01:14:34,438][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][403][85] duration [824ms], collections [1]/[2.9s], total [824ms]/[2.7m], memory [189.8mb]->[189.2mb]/[2gb], all_pools {[young] [16mb]->[24mb]/[0b]}{[old] [180.7mb]->[181.8mb]/[2gb]}{[survivor] [9mb]->[7.3mb]/[0b]}
[2022-03-26T01:14:34,878][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][403] overhead, spent [824ms] collecting in the last [2.9s]
[2022-03-26T01:14:45,977][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][407][86] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[2.7m], memory [221.2mb]->[211.2mb]/[2gb], all_pools {[young] [32mb]->[32mb]/[0b]}{[old] [181.8mb]->[181.8mb]/[2gb]}{[survivor] [7.3mb]->[9.3mb]/[0b]}
[2022-03-26T01:14:46,579][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][407] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-03-26T01:14:51,791][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][408][87] duration [2.7s], collections [1]/[5.8s], total [2.7s]/[2.7m], memory [211.2mb]->[191mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [181.8mb]->[182.3mb]/[2gb]}{[survivor] [9.3mb]->[8.6mb]/[0b]}
[2022-03-26T01:14:52,234][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][408] overhead, spent [2.7s] collecting in the last [5.8s]
[2022-03-26T01:14:59,164][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][410][88] duration [3.4s], collections [1]/[5.3s], total [3.4s]/[2.8m], memory [203mb]->[191.7mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [182.3mb]->[182.3mb]/[2gb]}{[survivor] [8.6mb]->[9.3mb]/[0b]}
[2022-03-26T01:14:59,441][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][410] overhead, spent [3.4s] collecting in the last [5.3s]
[2022-03-26T01:14:59,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:15:33,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [26718ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:16:10,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [9750ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:16:12,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:16:33,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6108390361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:16:48,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:00,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36543086853ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:13,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:26,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25389161832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:40,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:50,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@e119509] took [26389ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:17:55,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26389417383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:07,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:21,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27566198070ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:32,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:47,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24289412872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:52,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [20911ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:18:52,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:59,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20911442409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:04,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:04,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [12642ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:19:09,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12642710552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:16,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:25,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11576660323ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:34,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:24,058][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1550] timed out after [85410ms]
[2022-03-26T01:19:41,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17216895952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:44,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [17216ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:19:49,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:54,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15340862375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:00,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [8723ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:19:57,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:09,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8723930551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:13,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:16,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16426600022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:16,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [16426ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:20:17,072][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [309852ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T01:20:17,164][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.5m/154695ms] ago, timed out [1.1m/69285ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1550]
[2022-03-26T01:20:17,164][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [5.2m] publication of cluster state version [3406] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T01:20:32,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:33,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12470630477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:33,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@33ab4b62] took [13271ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:20:36,227][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][418][89] duration [9.1s], collections [1]/[16.8s], total [9.1s]/[3m], memory [243.7mb]->[232.8mb]/[2gb], all_pools {[young] [56mb]->[40mb]/[0b]}{[old] [182.3mb]->[184.1mb]/[2gb]}{[survivor] [9.3mb]->[8.7mb]/[0b]}
[2022-03-26T01:20:36,958][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][418] overhead, spent [9.1s] collecting in the last [16.8s]
[2022-03-26T01:20:36,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5114ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:20:43,043][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][420][90] duration [2.5s], collections [1]/[4.4s], total [2.5s]/[3m], memory [252.8mb]->[207.9mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [8.7mb]->[7.8mb]/[0b]}
[2022-03-26T01:20:43,388][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][420] overhead, spent [2.5s] collecting in the last [4.4s]
[2022-03-26T01:22:04,626][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40476}] took [5080ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:22:29,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:22:30,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [14000ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:22:30,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13400319638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:22:31,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][462][91] duration [10.2s], collections [1]/[17.3s], total [10.2s]/[3.2m], memory [275.9mb]->[195.9mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [7.8mb]->[7.8mb]/[0b]}
[2022-03-26T01:22:32,481][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][462] overhead, spent [10.2s] collecting in the last [17.3s]
[2022-03-26T01:22:43,829][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43892}] took [6813ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:22:44,770][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:22:47,215][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.6s/12677ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc]], which exceeds the warn threshold of [10s]
[2022-03-26T01:23:05,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:05,708][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][471][92] duration [9s], collections [1]/[2s], total [9s]/[3.3m], memory [251.9mb]->[255.9mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [7.8mb]->[7.3mb]/[0b]}
[2022-03-26T01:23:05,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12410377818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:05,913][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][471] overhead, spent [9s] collecting in the last [2s]
[2022-03-26T01:23:05,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [13611ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:10,746][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [20.5s] publication of cluster state version [3407] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T01:23:24,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [7796ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:24,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:25,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7596601507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:28,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][475][93] duration [5.5s], collections [1]/[12.7s], total [5.5s]/[3.4m], memory [267.4mb]->[197.3mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [7.3mb]->[9.2mb]/[0b]}
[2022-03-26T01:23:29,869][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][475] overhead, spent [5.5s] collecting in the last [12.7s]
[2022-03-26T01:23:27,318][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7796ms] which is above the warn threshold of [5s]
[2022-03-26T01:23:31,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7145ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:48,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7698ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:50,400][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40476}] took [10785ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:24:08,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@20c218a3] took [9831ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:24:35,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [11574ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:25:03,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [12494ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:24:56,126][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1793] timed out after [32919ms]
[2022-03-26T01:25:35,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [17045ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:26:15,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:22,387][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40518}] took [14717ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:26:23,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9842ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:26:30,962][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6760ms] which is above the warn threshold of [5s]
[2022-03-26T01:26:23,842][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3407}] took [2.9m] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3407}]] took [40ms], [connecting to new nodes] took [30ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [14129ms], [org.elasticsearch.script.ScriptService@287d7733] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [212ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [55ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [45ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [2755ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [209ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [276ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [64ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [63ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [71708ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [19413ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [1259ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [236ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [16295ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [2970ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [20573ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [16425ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [14010ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [497ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [2021ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [88ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [157ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [335ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [0ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [89ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [0ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T01:26:29,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5701894187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:38,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:47,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23090063911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:55,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:27:07,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17090242795ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:27:07,046][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40518}] took [17091ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:27:07,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@4a2ad09d] took [17090ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:27:15,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:27:24,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19227867578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:28:58,540][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40518}] took [103653ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:28:58,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103653ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:28:59,172][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][481][94] duration [1.2m], collections [1]/[2.8m], total [1.2m]/[4.7m], memory [261.3mb]->[224mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [184.1mb]->[185mb]/[2gb]}{[survivor] [9.2mb]->[6.9mb]/[0b]}
[2022-03-26T01:28:59,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103653512600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:28:59,239][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][481] overhead, spent [1.2m] collecting in the last [2.8m]
[2022-03-26T01:28:59,529][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.8m/289646ms] ago, timed out [4.2m/256727ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1793]
[2022-03-26T01:29:06,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:29:07,597][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][483][95] duration [4.4s], collections [1]/[6.3s], total [4.4s]/[4.7m], memory [276mb]->[190.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.9mb]->[5.1mb]/[0b]}
[2022-03-26T01:29:07,598][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][483] overhead, spent [4.4s] collecting in the last [6.3s]
[2022-03-26T01:29:07,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5716222756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:29:47,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][501][96] duration [1.4s], collections [1]/[1.3s], total [1.4s]/[4.8m], memory [234.1mb]->[242.1mb]/[2gb], all_pools {[young] [48mb]->[16mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [5.1mb]->[6.7mb]/[0b]}
[2022-03-26T01:29:47,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][501] overhead, spent [1.4s] collecting in the last [1.3s]
[2022-03-26T01:29:48,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:30:07,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5215ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:24,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@27121f33] took [6156ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:34,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6116ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:51,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7967ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:43,370][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1936] timed out after [17192ms]
[2022-03-26T01:31:09,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [14511ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:31:09,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:31:09,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14311280333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:31:09,523][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3408}] took [1.3m] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3408}]] took [0ms], [connecting to new nodes] took [94ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [5601ms], [org.elasticsearch.script.ScriptService@287d7733] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [327ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [46ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [317ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [65ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [34979ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [7831ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [363ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [54ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [4904ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [10162ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [55ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [15517ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [18ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [1ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [0ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [0ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [8ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [0ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T01:31:09,590][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][509][97] duration [2.6s], collections [1]/[26.3s], total [2.6s]/[4.8m], memory [275.8mb]->[216mb]/[2gb], all_pools {[young] [84mb]->[24mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.7mb]->[6.9mb]/[0b]}
[2022-03-26T01:31:09,601][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [44.1s/44162ms] ago, timed out [26.9s/26970ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1936]
[2022-03-26T01:31:09,682][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:31:12,862][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][510][98] duration [1.6s], collections [1]/[3s], total [1.6s]/[4.8m], memory [216mb]->[191.3mb]/[2gb], all_pools {[young] [24mb]->[12mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.9mb]->[6.2mb]/[0b]}
[2022-03-26T01:31:13,048][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][510] overhead, spent [1.6s] collecting in the last [3s]
[2022-03-26T01:31:13,295][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] starting SLM retention snapshot cleanup task
[2022-03-26T01:31:22,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [8313ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:31:40,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9399ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:02,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:07,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11041463308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:26,144][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&_source=alert.executionStatus%2Cnamespace%2Cnamespaces%2Ctype%2Creferences%2CmigrationVersion%2CcoreMigrationVersion%2Cupdated_at%2CoriginId%2CexecutionStatus&rest_total_hits_as_int=true&size=1][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43926}] took [32148ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:25,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@4343a70e] took [12442ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:26,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:35,199][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] there are no repositories to fetch, SLM retention snapshot cleanup task complete
[2022-03-26T01:32:37,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23834216504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:41,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:46,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15448762977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:50,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:56,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [8645ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:54,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8645133203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:00,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:06,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [10003ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:33:08,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10003086411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:14,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:15,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [13681ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:33:21,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13681731824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:28,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:35,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13016546600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:26,477][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1971] timed out after [71613ms]
[2022-03-26T01:33:44,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:47,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [30223ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:33:52,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17207009626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:58,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:59,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [14019ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:34:04,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14019604704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:11,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:24,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12751816419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:45,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:57,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33448426326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:35:11,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:35:34,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26220233185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:35:43,283][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3409}] took [3.8m] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3409}]] took [217ms], [connecting to new nodes] took [1364ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [43318ms], [org.elasticsearch.script.ScriptService@287d7733] took [77ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [2380ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [208ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [165ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [131ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [17631ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [114ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [388ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [62ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [63442ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [62ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [20764ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [820ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [459ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [11538ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [1224ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [20606ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [35698ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [23177ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [212ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [7349ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [290ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [199ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [764ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [677ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [1421ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [493ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [259ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [319ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [175ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [351ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T01:35:51,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:03,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [65726ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:36:11,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39506669544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:20,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:21,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [30081ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:36:29,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30081704563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:37,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:45,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@148cc201] took [17024ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:36:45,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17024468085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:55,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17839ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:04,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17838258425ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:11,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:18,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16812125969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:24,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:28,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779391497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:33,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:35,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9158ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:39:59,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9158504249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:21,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168286ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:25,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [168286ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:40:33,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168286103738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:49,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:27,358][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [168287ms] which is above the warn threshold of [5s]
[2022-03-26T01:40:26,831][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [2005] timed out after [72612ms]
[2022-03-26T01:41:06,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25905306037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:41:20,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:41:40,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32818844919ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:01,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39973ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:24,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39972805885ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:29,208][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][517][99] duration [2m], collections [1]/[3.4m], total [2m]/[6.8m], memory [235.3mb]->[197mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.2mb]->[8mb]/[0b]}
[2022-03-26T01:41:48,381][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2006] timed out after [55588ms]
[2022-03-26T01:42:43,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:45,151][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][517] overhead, spent [2m] collecting in the last [3.4m]
[2022-03-26T01:42:53,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42543146936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:54,977][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/382139ms] ago, timed out [5.1m/309527ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2005]
[2022-03-26T01:43:01,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [115334ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:43:11,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:43:37,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28514868693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:44:02,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:44:37,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47209533913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:44:52,789][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11.9m/715728ms] ago, timed out [10.7m/644115ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1971]
[2022-03-26T01:45:02,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63913ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:45:28,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63912913746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:45:42,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:45:56,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39134622443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:08,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:21,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [66147ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:46:23,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27012950746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:41,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:56,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32032511391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:10,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:14,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@7941dda2] took [30004ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:47:17,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30004647446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:26,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:33,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14153676397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:41,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17364ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:49,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17364031461ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:01,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@59f8a4ec, interval=1m}] took [17536ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:48:00,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:16,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17536736441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:33,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:43,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33617464401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:43,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [33617ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:48:50,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17006ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:57,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17006601294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:04,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:53,985][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [2040] timed out after [129683ms]
[2022-03-26T01:49:09,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [13890ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:49:11,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13890277119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:21,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18080643731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:44,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21855ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:21,099][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2041] timed out after [113568ms]
[2022-03-26T01:49:59,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21854551879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:50:15,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:50:27,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31874467209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:50:26,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [31874ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:50:59,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:52:05,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41160635837ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:53:10,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:55:25,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/128339864802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:57:10,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/243383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:57:14,403][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.4m/384884ms] ago, timed out [4.2m/255201ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2040]
[2022-03-26T01:59:25,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/243382897449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:55:45,756][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [128340ms] which is above the warn threshold of [5s]
[2022-03-26T02:01:41,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:04:23,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269379797247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:07:36,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:10:51,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337773369215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:14:10,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:17:24,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408673857446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:20:28,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:23:38,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378317180888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:26:57,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:30:02,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389492542175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:34:12,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:37:03,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [397915ms] which is above the warn threshold of [5000ms]
[2022-03-26T02:37:29,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397915621451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:41:46,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/491344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:44:53,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/490885532314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:48:19,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:48:14,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@506af249] took [490885ms] which is above the warn threshold of [5000ms]
[2022-03-26T02:51:37,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386980112740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:55:02,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3e3bb723, interval=1m}] took [406378ms] which is above the warn threshold of [5000ms]
[2022-03-26T02:54:59,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:57:10,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406378673554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:00:26,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:03:09,146][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3h/5023343ms] ago, timed out [1.3h/4967755ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2006]
[2022-03-26T03:04:15,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326347398264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:09:49,809][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T03:09:49,823][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T03:09:49,824][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T03:09:56,585][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T03:09:56,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T03:09:56,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T03:09:56,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T03:09:56,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T03:09:56,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T03:09:56,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T03:09:56,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T03:09:56,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T03:09:56,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T03:09:56,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T03:09:56,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T03:09:56,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T03:09:56,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T03:09:56,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T03:09:56,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T03:09:56,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T03:09:56,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T03:09:56,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T03:09:56,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T03:09:56,613][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T03:09:56,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T03:09:56,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T03:09:56,617][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T03:09:56,619][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T03:09:56,619][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T03:09:56,622][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T03:09:56,623][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T03:09:56,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T03:09:56,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T03:09:56,625][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T03:09:56,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T03:09:56,629][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T03:09:56,630][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T03:09:56,631][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T03:09:56,632][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T03:09:56,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T03:09:56,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T03:09:56,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T03:09:56,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T03:09:56,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T03:09:56,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T03:09:56,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T03:09:56,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T03:09:56,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T03:09:56,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T03:09:56,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T03:09:56,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T03:09:56,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T03:09:56,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T03:09:56,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T03:09:56,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T03:09:56,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T03:09:56,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T03:09:56,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T03:09:56,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T03:09:56,660][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T03:09:56,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T03:09:56,662][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T03:09:56,744][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.8gb], net total_space [125.8gb], types [ext4]
[2022-03-26T03:09:56,745][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T03:09:57,178][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T03:10:16,644][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T03:10:16,650][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-26T03:10:16,652][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T03:10:16,656][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-26T03:10:16,658][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T03:10:16,661][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-26T03:10:16,665][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-26T03:10:16,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T03:10:16,669][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-26T03:10:16,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-26T03:10:16,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-26T03:10:16,673][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T03:10:16,676][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T03:10:16,677][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T03:10:16,678][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T03:10:18,115][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T03:10:18,352][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T03:10:19,697][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T03:10:20,817][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T03:10:20,820][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T03:10:20,910][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T03:10:20,913][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T03:10:21,221][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T03:10:24,909][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T03:10:25,261][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 120, version: 3410, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T03:10:25,614][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 120, version: 3410, reason: Publication{term=120, version=3410}
[2022-03-26T03:10:25,781][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T03:10:25,782][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T03:10:27,260][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T03:10:27,288][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [28] indices into cluster_state
[2022-03-26T03:11:36,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:38,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [10254ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:11:45,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6477158569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:45,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [54459ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:11:45,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.4s/54459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:46,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.4s/54459365626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:46,698][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40844}] took [69346ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:11:50,641][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3412}] took [1.3m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3412}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [81492ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [18ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [0ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [39ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [60ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [59ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [452ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [308ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [10ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [17ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [12ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [14ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [43ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [0ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [26ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [0ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:11:54,470][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T03:11:54,659][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T03:11:56,398][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][18] overhead, spent [391ms] collecting in the last [1.5s]
[2022-03-26T03:12:14,619][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T03:12:14,710][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T03:12:14,721][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T03:12:15,943][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-26T03:12:24,617][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][39][14] duration [1.2s], collections [1]/[3s], total [1.2s]/[2.8s], memory [203.9mb]->[101.6mb]/[2gb], all_pools {[young] [116mb]->[8mb]/[0b]}{[old] [79.9mb]->[79.9mb]/[2gb]}{[survivor] [8mb]->[17.6mb]/[0b]}
[2022-03-26T03:12:25,200][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][39] overhead, spent [1.2s] collecting in the last [3s]
[2022-03-26T03:13:09,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@6546140a] took [5701ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:13:20,843][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11263ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [26] unchanged indices
[2022-03-26T03:13:34,334][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [27s] publication of cluster state version [3423] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:13:42,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6205ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:15:21,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@54494e75] took [88960ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:16:02,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [11339ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:16:57,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38367ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:00,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38367255005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:03,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:09,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7947272144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:17,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:22,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [21279ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:17:21,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13332145785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:26,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:30,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8852553451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:59,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:06,035][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][71][15] duration [24.9s], collections [1]/[1.6m], total [24.9s]/[27.7s], memory [169.6mb]->[103.9mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [79.9mb]->[95.9mb]/[2gb]}{[survivor] [17.6mb]->[8mb]/[0b]}
[2022-03-26T03:18:05,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10187464876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:10,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:10,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [10187ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:18:13,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34028675276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:05,014][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10187ms] which is above the warn threshold of [5s]
[2022-03-26T03:18:17,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:22,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7766562110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:27,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [7984ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:18:25,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:30,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7984044116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:33,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:34,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7759ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:18:37,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7759625915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:39,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:41,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6173340813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:54,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:56,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8407116114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:55,074][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3423}] took [4.8m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3423}]] took [256ms], [connecting to new nodes] took [676ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [39344ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [30837ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [277ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [497ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [82ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [235ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [34214ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [150ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [2945ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [79ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [2133ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [487ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [324ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [132055ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [13446ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [668ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [563ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [11268ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [692ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [7332ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [128ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [6906ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [2858ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [437ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [1448ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [135ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [123ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [425ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [144ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [1ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:19:19,098][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.8m/412004ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [6.8m/411569ms]
[2022-03-26T03:19:48,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@409e8065] took [68200ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:19:59,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:20:19,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:22,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [6187ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:20:30,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5987194797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:40,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:42,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595663847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:55,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [5251ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:03,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@2df03855] took [43985ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:35,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:22:36,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [7929ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:35,819][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [152846ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [28] unchanged indices
[2022-03-26T03:22:36,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7929173585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:22:36,467][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40848}] took [585176ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:36,606][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.9m] publication of cluster state version [3424] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:22:41,510][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-26T03:22:46,097][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][87][16] duration [889ms], collections [1]/[2.8s], total [889ms]/[28.6s], memory [171.9mb]->[103.9mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [95.9mb]->[95.9mb]/[2gb]}{[survivor] [8mb]->[8mb]/[0b]}
[2022-03-26T03:22:46,289][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][87] overhead, spent [889ms] collecting in the last [2.8s]
[2022-03-26T03:23:08,651][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][97][17] duration [1.6s], collections [1]/[3.3s], total [1.6s]/[30.3s], memory [155.9mb]->[106.2mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [95.9mb]->[95.9mb]/[2gb]}{[survivor] [8mb]->[10.2mb]/[0b]}
[2022-03-26T03:23:08,789][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [1.6s] collecting in the last [3.3s]
[2022-03-26T03:23:14,740][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][100][18] duration [1.5s], collections [1]/[3s], total [1.5s]/[31.8s], memory [142.2mb]->[112.5mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [95.9mb]->[101.8mb]/[2gb]}{[survivor] [10.2mb]->[10.7mb]/[0b]}
[2022-03-26T03:23:15,363][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][100] overhead, spent [1.5s] collecting in the last [3s]
[2022-03-26T03:23:59,164][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [20393ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:24:01,006][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.9s] publication of cluster state version [3427] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:24:37,209][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][129][19] duration [2.7s], collections [1]/[4.8s], total [2.7s]/[34.6s], memory [192.5mb]->[116.8mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [101.8mb]->[104.8mb]/[2gb]}{[survivor] [10.7mb]->[12mb]/[0b]}
[2022-03-26T03:24:38,068][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][129] overhead, spent [2.7s] collecting in the last [4.8s]
[2022-03-26T03:24:39,320][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [15796ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [28] unchanged indices
[2022-03-26T03:24:40,724][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.1s] publication of cluster state version [3428] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:25:02,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:25:03,573][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][136][20] duration [6.6s], collections [1]/[2.1s], total [6.6s]/[41.2s], memory [192.8mb]->[200.8mb]/[2gb], all_pools {[young] [76mb]->[24mb]/[0b]}{[old] [104.8mb]->[111.9mb]/[2gb]}{[survivor] [12mb]->[5mb]/[0b]}
[2022-03-26T03:25:03,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][136] overhead, spent [6.6s] collecting in the last [2.1s]
[2022-03-26T03:25:03,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8788ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:25:03,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8388547234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:25:03,944][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-26T03:25:19,283][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][143][21] duration [2s], collections [1]/[2.3s], total [2s]/[43.3s], memory [193mb]->[197mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [5mb]->[7.1mb]/[0b]}
[2022-03-26T03:25:19,509][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][143] overhead, spent [2s] collecting in the last [2.3s]
[2022-03-26T03:25:23,277][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][22] duration [1.7s], collections [1]/[6.9s], total [1.7s]/[45s], memory [197mb]->[118.4mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [7.1mb]->[6.5mb]/[0b]}
[2022-03-26T03:25:29,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][146][23] duration [1.3s], collections [1]/[1.7s], total [1.3s]/[46.4s], memory [182.4mb]->[206.4mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [6.5mb]->[8.6mb]/[0b]}
[2022-03-26T03:25:30,162][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][146] overhead, spent [1.3s] collecting in the last [1.7s]
[2022-03-26T03:25:36,823][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][24] duration [3.6s], collections [1]/[4.2s], total [3.6s]/[50s], memory [206.4mb]->[204.5mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [8.6mb]->[10.1mb]/[0b]}
[2022-03-26T03:25:37,723][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [3.6s] collecting in the last [4.2s]
[2022-03-26T03:25:39,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7395ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:25:54,819][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][153][25] duration [1.8s], collections [1]/[3.6s], total [1.8s]/[51.9s], memory [178mb]->[123.7mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [111.9mb]->[113.9mb]/[2gb]}{[survivor] [10.1mb]->[9.8mb]/[0b]}
[2022-03-26T03:25:55,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][153] overhead, spent [1.8s] collecting in the last [3.6s]
[2022-03-26T03:25:59,813][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][26] duration [2s], collections [1]/[4.4s], total [2s]/[53.9s], memory [123.7mb]->[124.5mb]/[2gb], all_pools {[young] [0b]->[12mb]/[0b]}{[old] [113.9mb]->[114.8mb]/[2gb]}{[survivor] [9.8mb]->[9.7mb]/[0b]}
[2022-03-26T03:26:00,341][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [2s] collecting in the last [4.4s]
[2022-03-26T03:26:07,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:07,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [5694ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:26:08,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5494262552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:09,056][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][155][27] duration [4s], collections [1]/[9.5s], total [4s]/[57.9s], memory [124.5mb]->[131.4mb]/[2gb], all_pools {[young] [12mb]->[8mb]/[0b]}{[old] [114.8mb]->[116.8mb]/[2gb]}{[survivor] [9.7mb]->[14.6mb]/[0b]}
[2022-03-26T03:26:10,234][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][155] overhead, spent [4s] collecting in the last [9.5s]
[2022-03-26T03:26:25,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:25,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [7586ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:26:25,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6785646693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:26,824][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][158][28] duration [2.8s], collections [1]/[10.5s], total [2.8s]/[1m], memory [199.4mb]->[143.7mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [116.8mb]->[122.4mb]/[2gb]}{[survivor] [14.6mb]->[13.2mb]/[0b]}
[2022-03-26T03:26:27,537][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][158] overhead, spent [2.8s] collecting in the last [10.5s]
[2022-03-26T03:26:30,044][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16150ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T03:26:36,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:37,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [5279ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:26:37,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5279692189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:36,914][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.8s] publication of cluster state version [3431] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:26:50,123][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40916}] took [5265ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:06,441][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40916}] took [6948ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:20,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:21,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10971574347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:21,577][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][171][29] duration [8.5s], collections [1]/[2.3s], total [8.5s]/[1.1m], memory [207.7mb]->[211.7mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [122.4mb]->[129.3mb]/[2gb]}{[survivor] [13.2mb]->[9.1mb]/[0b]}
[2022-03-26T03:27:22,115][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][171] overhead, spent [8.5s] collecting in the last [2.3s]
[2022-03-26T03:27:21,577][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.9s/12903ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-26T03:27:23,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [14425ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:37,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:38,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9126328511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:39,585][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][174][30] duration [5.5s], collections [1]/[1.4s], total [5.5s]/[1.2m], memory [214.5mb]->[222.5mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [129.3mb]->[134.3mb]/[2gb]}{[survivor] [9.1mb]->[5.7mb]/[0b]}
[2022-03-26T03:27:40,129][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][174] overhead, spent [5.5s] collecting in the last [1.4s]
[2022-03-26T03:27:40,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [13055ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:42,945][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [17058ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:27:45,736][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [21.7s] publication of cluster state version [3432] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:27:59,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6287ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:59,767][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][175][31] duration [4.3s], collections [1]/[31.4s], total [4.3s]/[1.3m], memory [222.5mb]->[140.8mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [5.7mb]->[6.5mb]/[0b]}
[2022-03-26T03:27:59,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6287014349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:32,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:32,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8831ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:28:33,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6230187668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:35,269][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3432}] took [47.3s] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3432}]] took [92ms], [connecting to new nodes] took [32ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [526ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [11197ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [39ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [41ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [69ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [51ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [90ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [13840ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [3848ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [226ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [2358ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [357ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [3888ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [9546ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [882ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [128ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [0ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:28:46,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:47,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@1f238b3a] took [9078ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:28:47,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8678798772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:47,428][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][188][32] duration [5.4s], collections [1]/[10.4s], total [5.4s]/[1.4m], memory [220.8mb]->[176.1mb]/[2gb], all_pools {[young] [84mb]->[36mb]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [6.5mb]->[5.7mb]/[0b]}
[2022-03-26T03:28:47,429][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][188] overhead, spent [5.4s] collecting in the last [10.4s]
[2022-03-26T03:29:04,411][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16794ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [24] unchanged indices
[2022-03-26T03:29:13,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:29:14,510][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][33] duration [5.9s], collections [1]/[3.4s], total [5.9s]/[1.5m], memory [192.1mb]->[196.1mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [5.7mb]->[7.2mb]/[0b]}
[2022-03-26T03:29:14,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [5.9s] collecting in the last [3.4s]
[2022-03-26T03:29:14,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8478422113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:29:14,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9078ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:29:14,433][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.9s] publication of cluster state version [3433] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:30:00,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:30:01,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11290038635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:30:01,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][34] duration [6.3s], collections [1]/[3.4s], total [6.3s]/[1.6m], memory [193.5mb]->[193.5mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-03-26T03:30:02,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [6.3s] collecting in the last [3.4s]
[2022-03-26T03:30:03,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [15513ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:30:10,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@233130dd] took [5340ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:30:45,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6785ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:30:55,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5995ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:31:05,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:09,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6768552268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:09,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@6a8e418c] took [7399ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:31:10,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:10,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5375442321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:11,473][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][212][35] duration [4.4s], collections [1]/[21.5s], total [4.4s]/[1.6m], memory [183.5mb]->[148.5mb]/[2gb], all_pools {[young] [40mb]->[4mb]/[0b]}{[old] [134.3mb]->[136.5mb]/[2gb]}{[survivor] [9.2mb]->[8mb]/[0b]}
[2022-03-26T03:31:42,210][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40950}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:31:52,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@20167a91] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:32:23,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:26,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9492223224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:27,782][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][225][36] duration [5.7s], collections [1]/[11.1s], total [5.7s]/[1.7m], memory [228.5mb]->[145.9mb]/[2gb], all_pools {[young] [83.9mb]->[4mb]/[0b]}{[old] [136.5mb]->[136.5mb]/[2gb]}{[survivor] [8mb]->[9.4mb]/[0b]}
[2022-03-26T03:32:28,029][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][225] overhead, spent [5.7s] collecting in the last [11.1s]
[2022-03-26T03:32:43,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:43,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6500894168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:43,966][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][228][37] duration [4s], collections [1]/[1.3s], total [4s]/[1.8m], memory [209.9mb]->[229.9mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [136.5mb]->[137.8mb]/[2gb]}{[survivor] [9.4mb]->[8.8mb]/[0b]}
[2022-03-26T03:32:43,967][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][228] overhead, spent [4s] collecting in the last [1.3s]
[2022-03-26T03:32:43,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6901ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:32:45,449][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3433}] took [3.5m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3433}]] took [0ms], [connecting to new nodes] took [198ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [164974ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [7637ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [128ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [125ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [720ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [119ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [118ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [29729ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [804ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [130ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [207ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [75ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [35ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [374ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [202ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [155ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [0ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [38ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:32:50,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][230][38] duration [2s], collections [1]/[4.5s], total [2s]/[1.8m], memory [214.7mb]->[150.6mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [137.8mb]->[137.8mb]/[2gb]}{[survivor] [8.8mb]->[12.7mb]/[0b]}
[2022-03-26T03:32:51,119][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][230] overhead, spent [2s] collecting in the last [4.5s]
[2022-03-26T03:32:56,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][231][39] duration [2.2s], collections [1]/[7s], total [2.2s]/[1.9m], memory [150.6mb]->[153.9mb]/[2gb], all_pools {[young] [4mb]->[8mb]/[0b]}{[old] [137.8mb]->[142.5mb]/[2gb]}{[survivor] [12.7mb]->[11.3mb]/[0b]}
[2022-03-26T03:32:57,071][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][231] overhead, spent [2.2s] collecting in the last [7s]
[2022-03-26T03:33:06,614][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][40] duration [3.2s], collections [1]/[1.7s], total [3.2s]/[1.9m], memory [193.9mb]->[237.9mb]/[2gb], all_pools {[young] [68mb]->[32mb]/[0b]}{[old] [142.5mb]->[148.3mb]/[2gb]}{[survivor] [11.3mb]->[10.1mb]/[0b]}
[2022-03-26T03:33:07,211][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][233] overhead, spent [3.2s] collecting in the last [1.7s]
[2022-03-26T03:33:07,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7487ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:33:18,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:18,821][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][234][41] duration [6.7s], collections [1]/[17.8s], total [6.7s]/[2m], memory [237.9mb]->[163.2mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [148.3mb]->[152.4mb]/[2gb]}{[survivor] [10.1mb]->[10.8mb]/[0b]}
[2022-03-26T03:33:19,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478264215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:19,410][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][234] overhead, spent [6.7s] collecting in the last [17.8s]
[2022-03-26T03:33:20,767][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14020ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T03:33:24,028][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.3s] publication of cluster state version [3434] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:33:40,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:40,826][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9434ms] which is above the warn threshold of [5s]
[2022-03-26T03:33:41,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8634198936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:41,898][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][240][42] duration [5.4s], collections [1]/[1.1s], total [5.4s]/[2.1m], memory [215.2mb]->[227.2mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [152.4mb]->[156.3mb]/[2gb]}{[survivor] [10.8mb]->[9.8mb]/[0b]}
[2022-03-26T03:33:41,899][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][240] overhead, spent [5.4s] collecting in the last [1.1s]
[2022-03-26T03:33:41,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9234ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:13,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:34:13,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@2aa8fcab] took [14949ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:13,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7643258429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:34:20,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5937ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:29,638][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3434}] took [1m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3434}]] took [142ms], [connecting to new nodes] took [132ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [2785ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [1551ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [34ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [32ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [484ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [42ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [39ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [23011ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [5425ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [123ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [14719ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [5057ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [5688ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [3954ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [571ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [56ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [58ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:34:42,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:34:43,118][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][43] duration [5.8s], collections [1]/[14.7s], total [5.8s]/[2.2m], memory [218.2mb]->[222.2mb]/[2gb], all_pools {[young] [52mb]->[84mb]/[0b]}{[old] [156.3mb]->[156.3mb]/[2gb]}{[survivor] [9.8mb]->[9.8mb]/[0b]}
[2022-03-26T03:34:43,716][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][248] overhead, spent [5.8s] collecting in the last [14.7s]
[2022-03-26T03:34:43,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [12559ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:43,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10447901434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:35:20,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5594ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:35:34,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@6e314064] took [8625ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:36:09,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [24706ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:36:39,855][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [108110ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:36:44,871][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [3435] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:36:57,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5327ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:37:00,987][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/86102ms] ago, timed out [21.3s/21366ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [748]
[2022-03-26T03:37:12,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6635ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:37:33,527][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [748] timed out after [64736ms]
[2022-03-26T03:37:49,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5547ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:38:07,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5005ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:38:21,282][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3435}] took [1.4m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3435}]] took [229ms], [connecting to new nodes] took [383ms], [applying settings] took [76ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [690ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [7041ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [215ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [161ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [105ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [826ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [163ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [51ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [28505ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [9447ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [413ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [549ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [12639ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [499ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [10380ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [130ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [7925ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [4509ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [1180ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [72ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [87ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [73ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [165ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [277ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [88ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:38:31,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@c72165d] took [11396ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:38:46,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:39:05,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:05,500][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [33.5s/33529ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-26T03:39:11,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10722189675ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:17,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [12648ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:39:17,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:21,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12648037566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:26,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:22,806][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12648ms] which is above the warn threshold of [5s]
[2022-03-26T03:39:26,830][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [829] timed out after [45698ms]
[2022-03-26T03:39:51,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9099ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:39:51,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9099154716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:52,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:53,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26359441725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:57,141][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85064ms] ago, timed out [39.3s/39366ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [829]
[2022-03-26T03:40:07,949][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][275][44] duration [14.2s], collections [1]/[41.6s], total [14.2s]/[2.5m], memory [252.8mb]->[177.8mb]/[2gb], all_pools {[young] [88mb]->[8mb]/[0b]}{[old] [161mb]->[161mb]/[2gb]}{[survivor] [7.7mb]->[8.7mb]/[0b]}
[2022-03-26T03:40:09,423][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][275] overhead, spent [14.2s] collecting in the last [41.6s]
[2022-03-26T03:40:10,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8206ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:40:38,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@53397fc0] took [9104ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:40:56,389][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [877] timed out after [18272ms]
[2022-03-26T03:41:00,282][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [22s/22075ms] ago, timed out [3.8s/3803ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [877]
[2022-03-26T03:40:59,858][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [67099ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:41:05,489][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.7m] publication of cluster state version [3436] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:41:43,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:41:44,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6483ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:41:46,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5682867752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:41:48,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5129ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:42:32,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:42:32,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [13965ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:42:33,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12564020125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:42:35,758][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][300][45] duration [8s], collections [1]/[22.4s], total [8s]/[2.6m], memory [249.8mb]->[171.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [161mb]->[164.4mb]/[2gb]}{[survivor] [8.7mb]->[7.2mb]/[0b]}
[2022-03-26T03:42:37,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][300] overhead, spent [8s] collecting in the last [22.4s]
[2022-03-26T03:42:38,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6256ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:43:10,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7582ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:43:46,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [14371ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:13,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7916ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:39,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:40,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9240ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:41,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5259043507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:40,235][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [993] timed out after [33528ms]
[2022-03-26T03:44:40,818][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10041ms] which is above the warn threshold of [5s]
[2022-03-26T03:44:46,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:51,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6231517658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:56,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:58,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [11542ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:58,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11542605333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:45:10,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8460ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:45:27,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@371bb03b] took [6615ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:45:39,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9607ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:45:40,094][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3436}] took [4.4m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3436}]] took [54ms], [connecting to new nodes] took [57ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [124802ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [16527ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [499ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [392ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [152ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [4ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [3345ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [83ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [589ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [225ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [80174ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [10544ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [411ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [6042ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [517ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [5132ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [47ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [7492ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [4710ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [2138ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [71ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [5ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [149ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [3442ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [1ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [141ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:46:09,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [13733ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:46:24,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6589ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:46:16,907][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1033] timed out after [47693ms]
[2022-03-26T03:46:39,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5692ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:46:47,663][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/79518ms] ago, timed out [31.8s/31825ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1033]
[2022-03-26T03:47:17,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [19099ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:47:17,362][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.2m/197714ms] ago, timed out [2.7m/164186ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [993]
[2022-03-26T03:47:42,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@58f1f34b] took [9859ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:48:09,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [10489ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:53:16,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [61817ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:58:58,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [37383ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:59:33,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:59:31,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@46743e7f] took [12218ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:00:03,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11769551584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:00:35,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:00:54,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64501964388ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:01:17,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:01:35,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42400732615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:01:59,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:02:24,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41400038652ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:02:45,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:03:05,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45467682491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:02:56,187][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1068] timed out after [812715ms]
[2022-03-26T04:03:22,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:03:12,267][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [45468ms] which is above the warn threshold of [5s]
[2022-03-26T04:03:52,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36271261420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:04:17,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:04:40,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56986318331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:05:11,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.9s/54979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:05:44,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [54978ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:05:35,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.9s/54978963340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:02,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:17,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51112353978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:31,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:42,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28746960234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:58,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:13,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27352443020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:29,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:41,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30672725685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:51,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [30672ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:07:56,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:12,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27226799527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:28,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:39,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32770569348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:55,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:09,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26253571806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:24,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:38,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27519849462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:44,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@176bc469] took [27519ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:10:01,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37952ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:10:20,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37951996375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:10:46,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:11:01,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43632951797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:11:21,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36069ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:11:40,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36069059060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:00,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [75503ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:12:00,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:20,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39434711264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:39,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:54,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35571588419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:13:10,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:52,047][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35572ms] which is above the warn threshold of [5s]
[2022-03-26T04:13:25,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34121067275ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:14:22,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:15:31,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68946305444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:16:36,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:16:46,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [134226ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:17:51,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134226472625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:19:22,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:21:04,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166432283899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:23:06,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:25:16,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216618221108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:28:23,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:31:14,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313862476629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:34:00,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338658ms] on absolute clock which is above the warn threshold of [5000ms]
