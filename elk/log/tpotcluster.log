[2022-03-31T17:12:52,786][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T17:12:52,827][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T17:12:52,828][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T17:12:59,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T17:12:59,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T17:12:59,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T17:12:59,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T17:12:59,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T17:12:59,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T17:12:59,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T17:12:59,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T17:12:59,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T17:12:59,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T17:12:59,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T17:12:59,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T17:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T17:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T17:12:59,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T17:12:59,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T17:12:59,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T17:12:59,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T17:12:59,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T17:12:59,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T17:12:59,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T17:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T17:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T17:12:59,409][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T17:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T17:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T17:12:59,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T17:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T17:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T17:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T17:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T17:12:59,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T17:12:59,479][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.2gb], net total_space [125.8gb], types [ext4]
[2022-03-31T17:12:59,480][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T17:12:59,707][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T17:13:09,457][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T17:13:09,459][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T17:13:10,433][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T17:13:10,552][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T17:13:11,275][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T17:13:12,000][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T17:13:12,001][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T17:13:12,024][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T17:13:12,026][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T17:13:12,200][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T17:13:14,014][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T17:13:14,133][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 169, version: 4918, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T17:13:14,304][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 169, version: 4918, reason: Publication{term=169, version=4918}
[2022-03-31T17:13:14,406][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T17:13:14,407][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T17:13:15,087][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T17:13:15,093][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T17:13:15,796][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T17:13:15,797][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T17:13:16,358][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:16,488][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:16,776][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T17:13:16,780][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T17:13:16,783][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T17:13:16,906][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:17,218][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T17:13:17,376][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T17:13:19,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T17:13:20,336][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-31T17:13:39,973][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-31T17:13:40,092][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-31T17:14:19,372][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 527 finished with response BulkByScrollResponse[took=333.6ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-31T17:14:21,900][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 543 finished with response BulkByScrollResponse[took=2.6s,timed_out=false,sliceId=null,updated=1036,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-31T17:14:29,045][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-31T17:14:51,442][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:14:51,569][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:14:51,581][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:10,197][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:11,029][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:11,166][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:12,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:15,029][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:18,202][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:22,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,037][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,116][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,235][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,306][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:07,289][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:16,333][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:16,402][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:36,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:38,285][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:17:44,428][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,358][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,420][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,425][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,759][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:11,391][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:39,477][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:39,537][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:40,472][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:45,769][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:28:36,844][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:34:02,052][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:35:12,088][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:35:12,162][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:41:11,426][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:41:11,503][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:44:26,573][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,585][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,675][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,686][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,790][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:12,577][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,584][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,644][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,650][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:52:51,841][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:56:02,982][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:57:23,034][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:57:23,130][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:04:52,544][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:04:53,475][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:06:00,466][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:06:00,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:12:47,828][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:13:05,830][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:14:24,807][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:31:06,738][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:33:17,155][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:38:29,565][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:47:19,147][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:55:12,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8323ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:55:49,269][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60610}] took [8546ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:56:00,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8662ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:56:26,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:04,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:18,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/469179476688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:33,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:09,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32037024964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:06,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60590}] took [469179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:06,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60612}] took [469179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:24,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:26,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52467198605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:30,819][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.8m/593109ms] ago, timed out [9.3m/558079ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [42739]
[2022-03-31T19:05:41,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6078][104] duration [6.6m], collections [1]/[9.1m], total [6.6m]/[6.6m], memory [1.3gb]->[252.7mb]/[2gb], all_pools {[young] [1.1gb]->[64mb]/[0b]}{[old] [180.1mb]->[180.1mb]/[2gb]}{[survivor] [7.5mb]->[8.6mb]/[0b]}
[2022-03-31T19:05:41,280][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [42739] timed out after [35030ms]
[2022-03-31T19:05:41,834][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6078] overhead, spent [6.6m] collecting in the last [9.1m]
[2022-03-31T19:05:41,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [16994ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:44,299][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6079][105] duration [2.6s], collections [1]/[15.9s], total [2.6s]/[6.6m], memory [252.7mb]->[267.3mb]/[2gb], all_pools {[young] [64mb]->[80mb]/[0b]}{[old] [180.1mb]->[180.3mb]/[2gb]}{[survivor] [8.6mb]->[7mb]/[0b]}
[2022-03-31T19:05:55,479][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6086][106] duration [1s], collections [1]/[2.3s], total [1s]/[6.7m], memory [267.3mb]->[198.6mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [180.3mb]->[180.3mb]/[2gb]}{[survivor] [7mb]->[6.3mb]/[0b]}
[2022-03-31T19:05:55,655][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6086] overhead, spent [1s] collecting in the last [2.3s]
[2022-03-31T19:05:59,540][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T19:06:02,425][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6091][107] duration [742ms], collections [1]/[1.4s], total [742ms]/[6.7m], memory [246.6mb]->[210.3mb]/[2gb], all_pools {[young] [64mb]->[16mb]/[0b]}{[old] [180.3mb]->[180.3mb]/[2gb]}{[survivor] [6.3mb]->[13.9mb]/[0b]}
[2022-03-31T19:06:02,656][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6091] overhead, spent [742ms] collecting in the last [1.4s]
[2022-03-31T19:06:04,387][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T19:06:08,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6094][110] duration [883ms], collections [1]/[2.1s], total [883ms]/[6.7m], memory [196.2mb]->[196.1mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.6mb]->[5.5mb]/[0b]}
[2022-03-31T19:06:08,462][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6094] overhead, spent [883ms] collecting in the last [2.1s]
[2022-03-31T19:06:11,520][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6095][111] duration [1.1s], collections [1]/[3.4s], total [1.1s]/[6.7m], memory [196.1mb]->[195.8mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.5mb]->[5.2mb]/[0b]}
[2022-03-31T19:06:11,722][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6095] overhead, spent [1.1s] collecting in the last [3.4s]
[2022-03-31T19:06:14,195][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6096][112] duration [1.1s], collections [1]/[2.8s], total [1.1s]/[6.7m], memory [195.8mb]->[195.3mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.2mb]->[4.7mb]/[0b]}
[2022-03-31T19:06:14,400][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6096] overhead, spent [1.1s] collecting in the last [2.8s]
[2022-03-31T19:06:21,337][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6101][113] duration [923ms], collections [1]/[1.8s], total [923ms]/[6.8m], memory [243.3mb]->[197.5mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [4.7mb]->[6.9mb]/[0b]}
[2022-03-31T19:06:22,879][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6101] overhead, spent [923ms] collecting in the last [1.8s]
[2022-03-31T19:06:31,947][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6108][115] duration [714ms], collections [1]/[1.2s], total [714ms]/[6.8m], memory [236.5mb]->[194.8mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.9mb]->[4.2mb]/[0b]}
[2022-03-31T19:06:32,291][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6108] overhead, spent [714ms] collecting in the last [1.2s]
[2022-03-31T19:06:42,071][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6116] overhead, spent [538ms] collecting in the last [1.5s]
[2022-03-31T19:06:45,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6118] overhead, spent [578ms] collecting in the last [1.7s]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35248}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,939][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6135][122] duration [3.4s], collections [1]/[5.3s], total [3.4s]/[6.9m], memory [217.3mb]->[202.7mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [3.4mb]->[4.9mb]/[0b]}
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:14,546][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6135] overhead, spent [3.4s] collecting in the last [5.3s]
[2022-03-31T19:07:14,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6259ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35242}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:32,057][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6141][123] duration [2.3s], collections [1]/[5.9s], total [2.3s]/[6.9m], memory [234.7mb]->[200.9mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [4.9mb]->[7.1mb]/[0b]}
[2022-03-31T19:07:32,729][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6141] overhead, spent [2.3s] collecting in the last [5.9s]
[2022-03-31T19:07:42,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:07:42,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [14930ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:43,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389036208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:07:47,726][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6142][124] duration [6.7s], collections [1]/[12.7s], total [6.7s]/[7m], memory [200.9mb]->[224.5mb]/[2gb], all_pools {[young] [8mb]->[24mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [7.1mb]->[6.7mb]/[0b]}
[2022-03-31T19:07:49,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6142] overhead, spent [6.7s] collecting in the last [12.7s]
[2022-03-31T19:07:51,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7308ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:10,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [9177ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:57,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44789ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:08:57,370][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [46990ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:58,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44788663816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:04,942][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5601ms] which is above the warn threshold of [5s]
[2022-03-31T19:09:05,820][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6144][125] duration [38.7s], collections [1]/[59.4s], total [38.7s]/[7.7m], memory [228.5mb]->[222.4mb]/[2gb], all_pools {[young] [32mb]->[24mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [6.7mb]->[8.6mb]/[0b]}
[2022-03-31T19:09:09,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6144] overhead, spent [38.7s] collecting in the last [59.4s]
[2022-03-31T19:09:13,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [13122ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:25,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5802ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,814][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6146][126] duration [10.8s], collections [1]/[6.9s], total [10.8s]/[7.9m], memory [226.4mb]->[286.4mb]/[2gb], all_pools {[young] [24mb]->[32mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [8.6mb]->[8mb]/[0b]}
[2022-03-31T19:09:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15579015529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,958][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6146] overhead, spent [10.8s] collecting in the last [6.9s]
[2022-03-31T19:09:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [16179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:52,011][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6148][127] duration [3.2s], collections [1]/[1.2s], total [3.2s]/[7.9m], memory [249.8mb]->[257.8mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [8mb]->[8.7mb]/[0b]}
[2022-03-31T19:09:52,471][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6148] overhead, spent [3.2s] collecting in the last [1.2s]
[2022-03-31T19:09:52,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,600][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6149][128] duration [4.5s], collections [1]/[12.9s], total [4.5s]/[8m], memory [257.8mb]->[201.8mb]/[2gb], all_pools {[young] [8mb]->[12mb]/[0b]}{[old] [193.8mb]->[194.2mb]/[2gb]}{[survivor] [8.7mb]->[7.6mb]/[0b]}
[2022-03-31T19:09:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6130551581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,730][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6149] overhead, spent [4.5s] collecting in the last [12.9s]
[2022-03-31T19:09:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6130ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:15,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6153][129] duration [5s], collections [1]/[2s], total [5s]/[8.1m], memory [273.8mb]->[289.8mb]/[2gb], all_pools {[young] [72mb]->[52mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.6mb]->[7.5mb]/[0b]}
[2022-03-31T19:10:15,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:10:16,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7102418865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:10:16,106][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6153] overhead, spent [5s] collecting in the last [2s]
[2022-03-31T19:10:16,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7302ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:34,024][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6157][130] duration [3.2s], collections [1]/[2.2s], total [3.2s]/[8.1m], memory [261.8mb]->[289.8mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.5mb]->[7.8mb]/[0b]}
[2022-03-31T19:10:34,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6157] overhead, spent [3.2s] collecting in the last [2.2s]
[2022-03-31T19:10:35,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11017ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:37,386][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6158][131] duration [2.4s], collections [1]/[12s], total [2.4s]/[8.2m], memory [289.8mb]->[209.2mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.8mb]->[7mb]/[0b]}
[2022-03-31T19:10:49,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6162][132] duration [2.1s], collections [1]/[5.4s], total [2.1s]/[8.2m], memory [213.2mb]->[213.5mb]/[2gb], all_pools {[young] [12mb]->[20mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7mb]->[7.2mb]/[0b]}
[2022-03-31T19:10:49,721][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6162] overhead, spent [2.1s] collecting in the last [5.4s]
[2022-03-31T19:11:01,436][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6163][134] duration [6.1s], collections [2]/[7s], total [6.1s]/[8.3m], memory [213.5mb]->[215.8mb]/[2gb], all_pools {[young] [20mb]->[4mb]/[0b]}{[old] [194.2mb]->[195.5mb]/[2gb]}{[survivor] [7.2mb]->[10mb]/[0b]}
[2022-03-31T19:11:02,244][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6163] overhead, spent [6.1s] collecting in the last [7s]
[2022-03-31T19:11:04,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8125ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:07,210][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6165] overhead, spent [582ms] collecting in the last [1.2s]
[2022-03-31T19:11:28,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:29,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6185628504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:29,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6172][136] duration [4.2s], collections [1]/[3.3s], total [4.2s]/[8.4m], memory [282.8mb]->[290.8mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [6.4mb]->[6.5mb]/[0b]}
[2022-03-31T19:11:29,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6172] overhead, spent [4.2s] collecting in the last [3.3s]
[2022-03-31T19:11:29,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6385ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:47,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:48,197][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6178][137] duration [2.6s], collections [1]/[6.8s], total [2.6s]/[8.4m], memory [278.9mb]->[204.3mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [6.5mb]->[8mb]/[0b]}
[2022-03-31T19:11:53,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5424975194ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:54,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:53,920][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6178] overhead, spent [2.6s] collecting in the last [6.8s]
[2022-03-31T19:11:55,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6898208933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:55,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [12323ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:58,820][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6179][140] duration [6.1s], collections [3]/[10.8s], total [6.1s]/[8.5m], memory [204.3mb]->[201.7mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [8mb]->[5.3mb]/[0b]}
[2022-03-31T19:11:59,176][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6179] overhead, spent [6.1s] collecting in the last [10.8s]
[2022-03-31T19:12:11,413][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6184][141] duration [2.4s], collections [1]/[4.6s], total [2.4s]/[8.6m], memory [249.7mb]->[217.8mb]/[2gb], all_pools {[young] [52mb]->[16mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [5.3mb]->[5.4mb]/[0b]}
[2022-03-31T19:12:12,040][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6184] overhead, spent [2.4s] collecting in the last [4.6s]
[2022-03-31T19:12:12,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5124ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:22,888][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6190][142] duration [1.4s], collections [1]/[3.4s], total [1.4s]/[8.6m], memory [237.8mb]->[200.8mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [5.4mb]->[4.4mb]/[0b]}
[2022-03-31T19:12:23,535][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6190] overhead, spent [1.4s] collecting in the last [3.4s]
[2022-03-31T19:12:46,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:46,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6463ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:46,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5663364526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,884][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [9259ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,857][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6200][144] duration [9.1s], collections [2]/[14.9s], total [9.1s]/[8.8m], memory [280.8mb]->[208.4mb]/[2gb], all_pools {[young] [87.9mb]->[36mb]/[0b]}{[old] [196.3mb]->[196.4mb]/[2gb]}{[survivor] [4.4mb]->[8mb]/[0b]}
[2022-03-31T19:12:56,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9258564750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:56,110][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6200] overhead, spent [9.1s] collecting in the last [14.9s]
[2022-03-31T19:12:56,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [9258ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:13:20,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:23,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23855602023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:33,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:40,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13291120682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:46,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:51,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579457648ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:52,686][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6201][145] duration [17.8s], collections [1]/[27.4s], total [17.8s]/[9m], memory [208.4mb]->[210.2mb]/[2gb], all_pools {[young] [36mb]->[12mb]/[0b]}{[old] [196.4mb]->[196.4mb]/[2gb]}{[survivor] [8mb]->[9.7mb]/[0b]}
[2022-03-31T19:13:54,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:59,078][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6201] overhead, spent [17.8s] collecting in the last [27.4s]
[2022-03-31T19:13:59,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8213830658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35242}] took [13734ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35282}] took [13734ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:04,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [34084ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:14,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13734178316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:21,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:28,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13160752307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:40,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:47,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635288522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:56,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:57,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [16505ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:02,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16505714411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:58,707][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16506ms] which is above the warn threshold of [5s]
[2022-03-31T19:15:11,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:12,839][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [14804ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:19,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804030000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:52,236][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6203][146] duration [16.8s], collections [1]/[20.6s], total [16.8s]/[9.3m], memory [222.2mb]->[246.2mb]/[2gb], all_pools {[young] [16mb]->[72mb]/[0b]}{[old] [196.4mb]->[196.4mb]/[2gb]}{[survivor] [9.7mb]->[9.7mb]/[0b]}
[2022-03-31T19:15:52,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6203] overhead, spent [16.8s] collecting in the last [20.6s]
[2022-03-31T19:15:53,614][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [41329ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [56132ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41328448939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:50,319][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [43515] timed out after [113954ms]
[2022-03-31T19:16:10,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [14070ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:16:10,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:10,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14070574936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:14,791][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/208340ms] ago, timed out [1.5m/94386ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [43515]
[2022-03-31T19:16:14,906][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6204][147] duration [9.1s], collections [1]/[1m], total [9.1s]/[9.5m], memory [246.2mb]->[214.3mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [196.4mb]->[198.3mb]/[2gb]}{[survivor] [9.7mb]->[8mb]/[0b]}
[2022-03-31T19:16:26,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:28,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8607236776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:27,905][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6206][148] duration [6.2s], collections [1]/[9.9s], total [6.2s]/[9.6m], memory [246.3mb]->[205.5mb]/[2gb], all_pools {[young] [72mb]->[28mb]/[0b]}{[old] [198.3mb]->[198.3mb]/[2gb]}{[survivor] [8mb]->[7.2mb]/[0b]}
[2022-03-31T19:16:28,567][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6206] overhead, spent [6.2s] collecting in the last [9.9s]
[2022-03-31T19:16:45,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:48,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320004956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:48,744][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6208][149] duration [6.9s], collections [1]/[2.3s], total [6.9s]/[9.7m], memory [281.5mb]->[285.5mb]/[2gb], all_pools {[young] [80mb]->[88mb]/[0b]}{[old] [198.3mb]->[198.3mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-03-31T19:16:51,048][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6208] overhead, spent [6.9s] collecting in the last [2.3s]
[2022-03-31T19:16:50,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:58,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [18679ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:16:58,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158197752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:02,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:03,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [11221ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:06,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11221683414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:12,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:14,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5145/0x00000008017f1d80@6a3a4261] took [10518ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:17,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518083841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:20,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:26,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8425527213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:27,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8425ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:33,641][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35344}] took [10911ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:32,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:36,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910392266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:41,150][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [8859ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [8859ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:43,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859358328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:48,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7245998531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7245ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:18:01,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:11,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13191718366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:19,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18498ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:59,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [18497ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:19:31,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18497778394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:32,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@c12697f, interval=1m}] took [72591ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:19:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72591ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:33,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72591565071ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:40,100][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6213][151] duration [48.4s], collections [2]/[1.3m], total [48.4s]/[10.5m], memory [261.9mb]->[237.8mb]/[2gb], all_pools {[young] [60mb]->[36mb]/[0b]}{[old] [198.3mb]->[200.4mb]/[2gb]}{[survivor] [7.5mb]->[9.3mb]/[0b]}
[2022-03-31T19:19:42,986][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6213] overhead, spent [48.4s] collecting in the last [1.3m]
[2022-03-31T19:19:44,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [12280ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:06,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:08,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10717680333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:08,571][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6218][152] duration [7.8s], collections [1]/[12.6s], total [7.8s]/[10.6m], memory [285.8mb]->[208mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [200.4mb]->[201.5mb]/[2gb]}{[survivor] [9.3mb]->[6.5mb]/[0b]}
[2022-03-31T19:20:15,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:15,838][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6218] overhead, spent [7.8s] collecting in the last [12.6s]
[2022-03-31T19:20:16,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862216387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:16,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [20579ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:18,105][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6219][153] duration [3.5s], collections [1]/[11.3s], total [3.5s]/[10.7m], memory [208mb]->[288.6mb]/[2gb], all_pools {[young] [12mb]->[80mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [6.5mb]->[7mb]/[0b]}
[2022-03-31T19:20:19,279][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6219] overhead, spent [3.5s] collecting in the last [11.3s]
[2022-03-31T19:20:34,982][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6222][154] duration [2.8s], collections [1]/[2.1s], total [2.8s]/[10.7m], memory [288.6mb]->[292.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [7mb]->[3.7mb]/[0b]}
[2022-03-31T19:20:37,254][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6222] overhead, spent [2.8s] collecting in the last [2.1s]
[2022-03-31T19:20:37,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11597ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:57,815][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6232][155] duration [706ms], collections [1]/[1.2s], total [706ms]/[10.8m], memory [225.3mb]->[229.3mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [3.7mb]->[3.1mb]/[0b]}
[2022-03-31T19:20:58,065][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6232] overhead, spent [706ms] collecting in the last [1.2s]
[2022-03-31T19:21:29,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:21:30,421][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6240][156] duration [5.6s], collections [1]/[4.3s], total [5.6s]/[10.9m], memory [276.7mb]->[276.7mb]/[2gb], all_pools {[young] [72mb]->[92mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [3.1mb]->[3.1mb]/[0b]}
[2022-03-31T19:21:30,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9093295596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:21:30,914][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6240] overhead, spent [5.6s] collecting in the last [4.3s]
[2022-03-31T19:21:30,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [10941ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:21:45,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11645ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:16,449][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35376}] took [24149ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:16,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:22:17,297][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6245][157] duration [13.8s], collections [1]/[2.9s], total [13.8s]/[11.1m], memory [250.6mb]->[262.6mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [5mb]->[6.5mb]/[0b]}
[2022-03-31T19:22:17,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19858473482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:22:18,357][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6245] overhead, spent [13.8s] collecting in the last [2.9s]
[2022-03-31T19:22:20,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [27101ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:32,370][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6249][158] duration [1s], collections [1]/[3.1s], total [1s]/[11.1m], memory [224.1mb]->[210.9mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [6.5mb]->[9.3mb]/[0b]}
[2022-03-31T19:22:32,863][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6249] overhead, spent [1s] collecting in the last [3.1s]
[2022-03-31T19:22:35,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5117ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:37,551][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6250][160] duration [1.6s], collections [2]/[4.5s], total [1.6s]/[11.1m], memory [210.9mb]->[295.7mb]/[2gb], all_pools {[young] [0b]->[28mb]/[0b]}{[old] [201.5mb]->[203.9mb]/[2gb]}{[survivor] [9.3mb]->[4.7mb]/[0b]}
[2022-03-31T19:22:38,087][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6250] overhead, spent [1.6s] collecting in the last [4.5s]
[2022-03-31T19:26:17,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/185094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:32:32,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/185435813173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:34:51,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [208966ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:35:13,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:38:30,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540616712550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:42:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:43:56,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [427142ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:45:52,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427142976773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:48:28,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:47:11,064][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.2m/76359ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26]], which exceeds the warn threshold of [10s]
[2022-03-31T19:50:51,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016408265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:53:50,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:53:57,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5c0a69ee, interval=5s}] took [679083ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:56:38,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312067572289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:59:07,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:01:51,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327075915271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:04:47,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:06:39,686][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15526ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7]], which exceeds the warn threshold of [10s]
[2022-03-31T20:07:27,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339996989053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:10:13,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:12:23,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325021196471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:11:38,325][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.5s/11517ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:14:24,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:16:36,737][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.3s/26315ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:17:04,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251476839909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:20:01,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:20:31,204][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.5s/11501ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:22:58,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328540951672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:23:41,486][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.9s/24908ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:20:45,644][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [905039ms] which is above the warn threshold of [5s]
[2022-03-31T20:25:54,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:38:57,673][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T20:38:57,686][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T20:38:57,687][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T20:39:03,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T20:39:03,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T20:39:03,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T20:39:03,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T20:39:03,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T20:39:03,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T20:39:03,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T20:39:03,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T20:39:03,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T20:39:03,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T20:39:03,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T20:39:03,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T20:39:03,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T20:39:03,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T20:39:03,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T20:39:03,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T20:39:03,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T20:39:03,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T20:39:03,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T20:39:03,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T20:39:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T20:39:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T20:39:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T20:39:03,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T20:39:03,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T20:39:03,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T20:39:03,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T20:39:03,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T20:39:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T20:39:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T20:39:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T20:39:03,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T20:39:03,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T20:39:03,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T20:39:03,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T20:39:03,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T20:39:03,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T20:39:03,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T20:39:03,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T20:39:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T20:39:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T20:39:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T20:39:03,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T20:39:03,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T20:39:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T20:39:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T20:39:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T20:39:03,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T20:39:03,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T20:39:03,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T20:39:03,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T20:39:03,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T20:39:03,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T20:39:03,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T20:39:03,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T20:39:03,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T20:39:03,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T20:39:03,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T20:39:03,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T20:39:04,064][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103gb], net total_space [125.8gb], types [ext4]
[2022-03-31T20:39:04,064][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T20:39:04,628][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T20:39:20,103][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T20:39:20,111][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-31T20:39:20,113][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T20:39:20,115][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-31T20:39:20,116][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T20:39:20,117][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-31T20:39:20,117][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-31T20:39:20,118][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T20:39:20,119][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-31T20:39:20,119][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-31T20:39:20,120][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-31T20:39:20,120][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T20:39:20,122][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T20:39:20,123][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T20:39:20,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T20:39:21,862][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T20:39:22,037][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T20:39:23,280][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T20:39:24,464][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T20:39:24,465][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T20:39:24,493][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T20:39:24,496][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T20:39:24,751][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T20:39:28,194][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T20:39:28,364][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nQU3MU6hSMmnvawcghyAiA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 170, version: 5020, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nQU3MU6hSMmnvawcghyAiA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T20:39:28,617][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nQU3MU6hSMmnvawcghyAiA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 170, version: 5020, reason: Publication{term=170, version=5020}
[2022-03-31T20:39:28,781][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T20:39:28,784][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T20:39:30,141][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T20:39:30,154][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T20:39:31,430][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T20:39:31,431][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T20:39:32,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-31T20:39:33,084][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-31T20:39:33,468][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T20:39:33,472][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T20:39:33,483][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T20:39:33,662][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-31T20:39:34,453][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T20:39:34,748][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T20:39:39,003][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T20:39:50,973][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.31][0]]]).
[2022-03-31T20:40:14,661][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:14,833][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:15,557][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:15,834][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:16,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:16,736][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T21:04:27,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73a14b10, interval=1s}] took [31766ms] which is above the warn threshold of [5000ms]
[2022-03-31T21:16:53,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5612ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:32:16,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5320754331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:35:57,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8m/1850539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:33:19,407][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.5s/22579ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5e1b599d]], which exceeds the warn threshold of [10s]
[2022-03-31T21:36:33,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6e830b69, interval=5s}] took [1850361ms] which is above the warn threshold of [5000ms]
[2022-03-31T21:38:13,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8m/1850361593952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:42:51,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:40:53,448][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.1s/16101ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@12ade8e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@85afadec]], which exceeds the warn threshold of [10s]
[2022-03-31T21:45:23,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288962423394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:47:57,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:49:53,758][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.3m/82659ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@12ade8e]], which exceeds the warn threshold of [10s]
[2022-03-31T21:51:50,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431519045870ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:51:51,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017eebe8@4c5666e5] took [720481ms] which is above the warn threshold of [5000ms]
[2022-03-31T21:56:32,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:56:27,279][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.8s/10895ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@85afadec], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5e1b599d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@be6ac145]], which exceeds the warn threshold of [10s]
[2022-03-31T22:08:35,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463420157077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:10:13,935][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.5s/32560ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@85afadec], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5e1b599d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@be6ac145]], which exceeds the warn threshold of [10s]
[2022-03-31T22:14:21,382][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T22:14:21,417][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T22:14:21,418][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T22:14:25,822][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T22:14:25,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T22:14:25,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T22:14:25,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T22:14:25,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T22:14:25,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T22:14:25,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T22:14:25,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T22:14:25,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T22:14:25,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T22:14:25,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T22:14:25,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T22:14:25,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T22:14:25,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T22:14:25,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T22:14:25,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T22:14:25,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T22:14:25,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T22:14:25,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T22:14:25,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T22:14:25,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T22:14:25,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T22:14:25,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T22:14:25,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T22:14:25,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T22:14:25,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T22:14:25,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T22:14:25,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T22:14:25,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T22:14:25,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T22:14:25,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T22:14:25,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T22:14:25,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T22:14:25,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T22:14:25,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T22:14:25,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T22:14:25,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T22:14:25,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T22:14:25,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T22:14:25,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T22:14:25,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T22:14:25,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T22:14:25,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T22:14:25,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T22:14:25,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T22:14:25,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T22:14:25,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T22:14:25,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T22:14:25,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T22:14:25,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T22:14:25,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T22:14:25,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T22:14:25,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T22:14:25,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T22:14:25,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T22:14:25,903][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103gb], net total_space [125.8gb], types [ext4]
[2022-03-31T22:14:25,904][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T22:14:26,208][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T22:14:35,115][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T22:14:35,120][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-31T22:14:35,121][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T22:14:35,122][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-31T22:14:35,123][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T22:14:35,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-31T22:14:35,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-31T22:14:35,125][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T22:14:35,125][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-31T22:14:35,126][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-31T22:14:35,126][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-31T22:14:35,127][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T22:14:35,128][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T22:14:35,128][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T22:14:35,129][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T22:14:36,150][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T22:14:36,278][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T22:14:36,975][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T22:14:37,661][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T22:14:37,662][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T22:14:37,731][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T22:14:37,733][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T22:14:37,937][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T22:14:39,861][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T22:14:39,974][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 171, version: 5071, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T22:14:40,155][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 171, version: 5071, reason: Publication{term=171, version=5071}
[2022-03-31T22:14:40,320][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T22:14:40,323][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T22:14:43,931][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:929) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-31T22:14:44,594][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T22:14:44,619][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T22:14:47,176][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T22:14:47,793][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T22:15:05,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:06,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5474565092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:15:21,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5474ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:16:49,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:52,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112170499736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:55,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:57,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6893162897ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:17:08,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@5bb125ee] took [13697ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:17:09,302][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36512}] took [21192ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:17:46,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:17:56,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][27][9] duration [1.6s], collections [1]/[16.4s], total [1.6s]/[1.8s], memory [530.5mb]->[102.2mb]/[2gb], all_pools {[young] [452mb]->[8mb]/[0b]}{[old] [54.5mb]->[54.5mb]/[2gb]}{[survivor] [23.9mb]->[39.6mb]/[0b]}
[2022-03-31T22:17:55,953][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [25795ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:17:56,285][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.4s] publication of cluster state version [5081] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:17:57,064][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T22:17:57,663][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T22:17:57,813][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T22:17:59,000][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][29] overhead, spent [307ms] collecting in the last [1s]
[2022-03-31T22:18:12,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:19,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5576226086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:21,112][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][32][11] duration [2.5s], collections [1]/[10.6s], total [2.5s]/[4.7s], memory [186mb]->[99mb]/[2gb], all_pools {[young] [88mb]->[4mb]/[0b]}{[old] [91mb]->[91mb]/[2gb]}{[survivor] [7mb]->[8mb]/[0b]}
[2022-03-31T22:18:21,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [9162ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:18:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9162420389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:39,524][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][39][12] duration [1.4s], collections [1]/[2.7s], total [1.4s]/[6.1s], memory [163mb]->[101.1mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [91mb]->[91mb]/[2gb]}{[survivor] [8mb]->[10.1mb]/[0b]}
[2022-03-31T22:18:39,751][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][39] overhead, spent [1.4s] collecting in the last [2.7s]
[2022-03-31T22:18:44,730][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][41][13] duration [1.9s], collections [1]/[3.5s], total [1.9s]/[8.1s], memory [157.1mb]->[100.6mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [91mb]->[92.8mb]/[2gb]}{[survivor] [10.1mb]->[7.7mb]/[0b]}
[2022-03-31T22:18:40,504][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.flush(SSLSocketOutputRecord.java:268) ~[?:?]
	at sun.security.ssl.HandshakeOutStream.flush(HandshakeOutStream.java:89) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.onProduceFinished(Finished.java:404) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.produce(Finished.java:379) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.ServerHelloDone$ServerHelloDoneConsumer.consume(ServerHelloDone.java:182) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-31T22:18:44,852][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][41] overhead, spent [1.9s] collecting in the last [3.5s]
[2022-03-31T22:18:46,677][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T22:18:46,971][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T22:18:48,467][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][44] overhead, spent [300ms] collecting in the last [1.1s]
[2022-03-31T22:18:52,475][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][46][15] duration [1s], collections [1]/[2.4s], total [1s]/[9.4s], memory [166.8mb]->[106.9mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [92.8mb]->[94.9mb]/[2gb]}{[survivor] [9.9mb]->[12mb]/[0b]}
[2022-03-31T22:18:52,765][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][46] overhead, spent [1s] collecting in the last [2.4s]
[2022-03-31T22:19:06,181][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12232ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:19:12,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@63692784] took [10664ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:19:36,325][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][64][17] duration [778ms], collections [1]/[2.2s], total [778ms]/[10.4s], memory [118.3mb]->[112mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [98.5mb]->[105.4mb]/[2gb]}{[survivor] [15.8mb]->[6.5mb]/[0b]}
[2022-03-31T22:19:36,859][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T22:19:36,964][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][64] overhead, spent [778ms] collecting in the last [2.2s]
[2022-03-31T22:19:52,180][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][70][18] duration [2.1s], collections [1]/[4.6s], total [2.1s]/[12.5s], memory [180mb]->[121.4mb]/[2gb], all_pools {[young] [72mb]->[8mb]/[0b]}{[old] [105.4mb]->[105.4mb]/[2gb]}{[survivor] [6.5mb]->[11.9mb]/[0b]}
[2022-03-31T22:19:52,927][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][70] overhead, spent [2.1s] collecting in the last [4.6s]
[2022-03-31T22:20:06,340][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.7s] publication of cluster state version [5088] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:20:33,173][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][19] duration [1s], collections [1]/[2s], total [1s]/[13.6s], memory [197.4mb]->[123.7mb]/[2gb], all_pools {[young] [79.9mb]->[0b]/[0b]}{[old] [105.4mb]->[109.7mb]/[2gb]}{[survivor] [11.9mb]->[14mb]/[0b]}
[2022-03-31T22:20:33,353][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [1s] collecting in the last [2s]
[2022-03-31T22:20:37,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [635ms] collecting in the last [1.8s]
[2022-03-31T22:20:39,620][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [447ms] collecting in the last [1.6s]
[2022-03-31T22:20:41,509][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [568ms] collecting in the last [2s]
[2022-03-31T22:20:42,852][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [334ms] collecting in the last [1.2s]
[2022-03-31T22:20:44,121][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][100] overhead, spent [378ms] collecting in the last [1.4s]
[2022-03-31T22:20:49,347][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][104] overhead, spent [364ms] collecting in the last [1.1s]
[2022-03-31T22:21:03,443][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][111] overhead, spent [514ms] collecting in the last [1.2s]
[2022-03-31T22:21:05,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][29] duration [1.1s], collections [1]/[2.6s], total [1.1s]/[18.4s], memory [148.5mb]->[147.8mb]/[2gb], all_pools {[young] [40mb]->[8mb]/[0b]}{[old] [141.1mb]->[141.1mb]/[2gb]}{[survivor] [7.3mb]->[6.7mb]/[0b]}
[2022-03-31T22:21:06,193][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [1.1s] collecting in the last [2.6s]
[2022-03-31T22:21:15,542][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][118][30] duration [851ms], collections [1]/[1.5s], total [851ms]/[19.3s], memory [207.8mb]->[235.8mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [141.1mb]->[141.1mb]/[2gb]}{[survivor] [6.7mb]->[8.6mb]/[0b]}
[2022-03-31T22:21:15,627][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][118] overhead, spent [851ms] collecting in the last [1.5s]
[2022-03-31T22:21:16,865][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T22:21:29,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][32] duration [1.2s], collections [1]/[2.4s], total [1.2s]/[20.8s], memory [186.6mb]->[153.9mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [141.1mb]->[142.6mb]/[2gb]}{[survivor] [9.4mb]->[11.2mb]/[0b]}
[2022-03-31T22:21:29,503][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [1.2s] collecting in the last [2.4s]
[2022-03-31T22:21:37,177][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][131] overhead, spent [689ms] collecting in the last [1.9s]
[2022-03-31T22:22:00,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:22:00,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@12a49828] took [8208ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7207761193ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:22:03,865][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][136][34] duration [5.8s], collections [1]/[10.9s], total [5.8s]/[27.4s], memory [237.8mb]->[162.7mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [147.8mb]->[147.8mb]/[2gb]}{[survivor] [5.9mb]->[6.8mb]/[0b]}
[2022-03-31T22:22:04,965][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][136] overhead, spent [5.8s] collecting in the last [10.9s]
[2022-03-31T22:22:06,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6932ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:19,844][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36590}] took [7060ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:32,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5943ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:51,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [5118ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:10,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:12,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [7323ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:17,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5122172389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:25,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:32,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14656245187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:37,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [14656ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:40,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:47,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14929921909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:52,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:57,114][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36590}] took [12806ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:57,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12805636523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:59,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:04,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7682695925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:14,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:19,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14020763027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:28,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:36,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14210825197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:50,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:03,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20586438221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:04,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@7def8108] took [69306ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:25:13,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:25,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23673372005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:36,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:49,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22716366811ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:14,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:14,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [39522ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:26:15,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39522345403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:14,660][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36594}] took [120710ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:26:16,148][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][140][35] duration [11.2s], collections [1]/[2.9m], total [11.2s]/[38.6s], memory [182.7mb]->[160.9mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [147.8mb]->[147.8mb]/[2gb]}{[survivor] [6.8mb]->[9mb]/[0b]}
[2022-03-31T22:26:31,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5860ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:32,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5859994752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:32,749][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][145][36] duration [3s], collections [1]/[7.5s], total [3s]/[41.7s], memory [232.9mb]->[156.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [147.8mb]->[150.2mb]/[2gb]}{[survivor] [9mb]->[6.3mb]/[0b]}
[2022-03-31T22:26:32,750][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][145] overhead, spent [3s] collecting in the last [7.5s]
[2022-03-31T22:26:37,850][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=171, version=5098}] took [4.9m] which is above the warn threshold of [30s]: [running task [Publication{term=171, version=5098}]] took [0ms], [connecting to new nodes] took [71ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@60ac379] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@fee596e] took [294786ms], [org.elasticsearch.script.ScriptService@14b0327] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [0ms], [org.elasticsearch.snapshots.RestoreService@62fc05e5] took [0ms], [org.elasticsearch.ingest.IngestService@1fa415fd] took [598ms], [org.elasticsearch.action.ingest.IngestActionForwarder@46af6f15] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d4a60@1c861d89] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e57e0d1] took [0ms], [org.elasticsearch.tasks.TaskManager@21e3151b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1c8911f5] took [113ms], [org.elasticsearch.cluster.InternalClusterInfoService@bf2b5a6] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@730a58ab] took [0ms], [org.elasticsearch.indices.SystemIndexManager@3b3cfe2f] took [672ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2760f7ba] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@75ac1e03] took [85ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2cf24896] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4f271213] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@2ee5bd05] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@72eb7556] took [45ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@44cb5db6] took [1022ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@5526550] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5615ceb4] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@407b49c3] took [123ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d51aa11] took [99ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d3b3d1d] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@55233660] took [47ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [75ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4f0c5856] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f0d7284] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5a15f4de] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@742f02f7] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@78365094] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a089e17] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@73f2938a] took [19ms], [org.elasticsearch.node.ResponseCollectorService@390acf90] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5b84015b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@55f2c41a] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@32fb63ed] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5b5f5edf] took [45ms], [org.elasticsearch.indices.store.IndicesStore@345db5b8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@19bffc81] took [0ms], [org.elasticsearch.license.LicenseService@6e5a3edb] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@16fbb05e] took [47ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@32500c10] took [0ms], [org.elasticsearch.gateway.GatewayService@368df3b1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7b4bbe88] took [0ms]
[2022-03-31T22:26:49,797][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][150][37] duration [3.9s], collections [1]/[1.4s], total [3.9s]/[45.6s], memory [212.6mb]->[232.6mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [150.2mb]->[150.2mb]/[2gb]}{[survivor] [6.3mb]->[6.3mb]/[0b]}
[2022-03-31T22:26:49,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:50,410][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][150] overhead, spent [3.9s] collecting in the last [1.4s]
[2022-03-31T22:26:50,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8179788205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:51,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [8580ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:00,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:27:00,905][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][38] duration [3.3s], collections [1]/[13.3s], total [3.3s]/[48.9s], memory [232.6mb]->[206.5mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [150.2mb]->[150.4mb]/[2gb]}{[survivor] [6.3mb]->[14.9mb]/[0b]}
[2022-03-31T22:27:00,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5192491815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:27:00,984][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [3.3s] collecting in the last [13.3s]
[2022-03-31T22:27:00,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6647ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:15,118][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36638}] took [5619ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:15,161][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][156][39] duration [3.6s], collections [1]/[1.5s], total [3.6s]/[52.6s], memory [229.4mb]->[245.4mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [150.4mb]->[157.8mb]/[2gb]}{[survivor] [14.9mb]->[15mb]/[0b]}
[2022-03-31T22:27:15,659][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][156] overhead, spent [3.6s] collecting in the last [1.5s]
[2022-03-31T22:27:19,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [10315ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:26,355][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][157][41] duration [4.8s], collections [2]/[11.3s], total [4.8s]/[57.4s], memory [245.4mb]->[267mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [157.8mb]->[172.2mb]/[2gb]}{[survivor] [15mb]->[4.8mb]/[0b]}
[2022-03-31T22:27:27,017][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][157] overhead, spent [4.8s] collecting in the last [11.3s]
[2022-03-31T22:27:28,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7430ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:35,929][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [634ms] collecting in the last [2.1s]
[2022-03-31T22:27:37,811][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][162] overhead, spent [688ms] collecting in the last [1.9s]
[2022-03-31T22:27:50,460][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][168][45] duration [1s], collections [1]/[2.2s], total [1s]/[1m], memory [178.7mb]->[179.7mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [6.4mb]->[7.4mb]/[0b]}
[2022-03-31T22:27:51,079][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][168] overhead, spent [1s] collecting in the last [2.2s]
[2022-03-31T22:27:56,371][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][171][46] duration [908ms], collections [1]/[2s], total [908ms]/[1m], memory [199.7mb]->[177.2mb]/[2gb], all_pools {[young] [64mb]->[16mb]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [7.4mb]->[4.9mb]/[0b]}
[2022-03-31T22:27:56,590][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][171] overhead, spent [908ms] collecting in the last [2s]
[2022-03-31T22:28:07,381][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][177][47] duration [1.2s], collections [1]/[3.1s], total [1.2s]/[1m], memory [245.2mb]->[182.9mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [4.9mb]->[10.6mb]/[0b]}
[2022-03-31T22:28:07,514][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][177] overhead, spent [1.2s] collecting in the last [3.1s]
[2022-03-31T22:28:10,796][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][48] duration [1.1s], collections [1]/[1.2s], total [1.1s]/[1m], memory [182.9mb]->[266.9mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [172.2mb]->[174.3mb]/[2gb]}{[survivor] [10.6mb]->[9.7mb]/[0b]}
[2022-03-31T22:28:11,199][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.1s] collecting in the last [1.2s]
[2022-03-31T22:28:16,315][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][181][49] duration [917ms], collections [1]/[2.4s], total [917ms]/[1m], memory [240mb]->[192.9mb]/[2gb], all_pools {[young] [56mb]->[12mb]/[0b]}{[old] [174.3mb]->[176.9mb]/[2gb]}{[survivor] [9.7mb]->[16mb]/[0b]}
[2022-03-31T22:28:16,511][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][181] overhead, spent [917ms] collecting in the last [2.4s]
[2022-03-31T22:28:19,052][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][182][50] duration [1.2s], collections [1]/[2.8s], total [1.2s]/[1m], memory [192.9mb]->[199.5mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [176.9mb]->[192mb]/[2gb]}{[survivor] [16mb]->[7.4mb]/[0b]}
[2022-03-31T22:28:19,247][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][182] overhead, spent [1.2s] collecting in the last [2.8s]
[2022-03-31T22:28:22,406][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][183][51] duration [1.6s], collections [1]/[3s], total [1.6s]/[1.1m], memory [199.5mb]->[201.3mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [192mb]->[192mb]/[2gb]}{[survivor] [7.4mb]->[9.2mb]/[0b]}
[2022-03-31T22:28:22,622][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][183] overhead, spent [1.6s] collecting in the last [3s]
[2022-03-31T22:28:22,851][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A31032022%3Acount%3Adelay_threshold_exceeded?refresh=wait_for&require_alias=true&_source=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57996}] took [6624ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:24,185][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57998}] took [7923ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:24,382][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57994}] took [8123ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:28,545][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][186][53] duration [1.3s], collections [1]/[2.9s], total [1.3s]/[1.1m], memory [259mb]->[205.5mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [198mb]->[198mb]/[2gb]}{[survivor] [4.9mb]->[7.4mb]/[0b]}
[2022-03-31T22:28:29,194][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][186] overhead, spent [1.3s] collecting in the last [2.9s]
[2022-03-31T22:28:31,962][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][187][54] duration [920ms], collections [1]/[3.5s], total [920ms]/[1.1m], memory [205.5mb]->[204.5mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [198mb]->[198mb]/[2gb]}{[survivor] [7.4mb]->[6.5mb]/[0b]}
[2022-03-31T22:28:32,243][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [920ms] collecting in the last [3.5s]
[2022-03-31T22:28:33,092][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T22:28:44,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [8556ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:46,835][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12431ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:28:47,364][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [13.6s] publication of cluster state version [5105] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:28:54,774][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][56] duration [1s], collections [1]/[3s], total [1s]/[1.1m], memory [285.7mb]->[243.9mb]/[2gb], all_pools {[young] [88mb]->[36mb]/[0b]}{[old] [198mb]->[198mb]/[2gb]}{[survivor] [7.6mb]->[9.9mb]/[0b]}
[2022-03-31T22:28:55,492][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [1s] collecting in the last [3s]
[2022-03-31T22:29:03,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:03,853][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][197][57] duration [3s], collections [1]/[8.3s], total [3s]/[1.2m], memory [243.9mb]->[208.1mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [198mb]->[200.7mb]/[2gb]}{[survivor] [9.9mb]->[7.4mb]/[0b]}
[2022-03-31T22:29:03,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5057021013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:05,441][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][197] overhead, spent [3s] collecting in the last [8.3s]
[2022-03-31T22:29:06,000][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14902ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [31] unchanged indices
[2022-03-31T22:29:06,699][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.8s] publication of cluster state version [5106] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_APPLY_COMMIT]
[2022-03-31T22:29:40,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@d5e287e] took [21100ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:29:51,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:56,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8337628009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:57,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36638}] took [10599ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:29:57,444][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36562}] took [10599ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:01,871][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36642}] took [12414ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:01,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:00,155][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36636}] took [10599ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:05,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9573923997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:05,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [9573ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:14,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@49e48ccf, interval=1m}] took [10354ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:12,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:18,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10354622820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:43,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32364ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:46,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32363772050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:03,661][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57960}] took [32364ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:04,784][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36666}] took [20717ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:04,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:07,990][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][59] duration [20.2s], collections [2]/[49.8s], total [20.2s]/[1.5m], memory [280.1mb]->[294.4mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [200.7mb]->[202.5mb]/[2gb]}{[survivor] [7.4mb]->[9.2mb]/[0b]}
[2022-03-31T22:31:07,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20717763522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:08,959][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [20.2s] collecting in the last [49.8s]
[2022-03-31T22:31:14,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [30488ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:15,878][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36666}] took [10572ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:18,962][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:36208}] took [34891ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:26,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:26,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [6602ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:26,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6202037542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:38,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:42,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5015098686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:47,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:53,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8624080770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:05,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13941473349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:05,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@6cf9f0d4] took [22565ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:08,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:11,500][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36666}] took [21524ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:11,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [7582ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:12,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7582428329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:15,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:18,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6744ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:19,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6744026290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:22,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:24,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7031943061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:27,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:09,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5223177857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:09,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5223ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:10,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:11,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [43723ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:12,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43723025621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:15,156][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][60] duration [16.2s], collections [1]/[46.6s], total [16.2s]/[1.8m], memory [263.8mb]->[225.2mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [202.5mb]->[202.8mb]/[2gb]}{[survivor] [9.2mb]->[6.4mb]/[0b]}
[2022-03-31T22:33:16,293][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [16.2s] collecting in the last [46.6s]
[2022-03-31T22:33:17,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6802ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:30,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:40,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5402ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:47,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:56,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5148599771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:57,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:57,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@63cdcee2] took [10116ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:58,535][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36676}] took [10116ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:58,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10116343776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:03,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5249ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:03,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:04,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249554061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:18,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:18,488][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36684}] took [14691ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:18,388][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=171, version=5106}] took [5m] which is above the warn threshold of [30s]: [running task [Publication{term=171, version=5106}]] took [44ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@60ac379] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@fee596e] took [242771ms], [org.elasticsearch.script.ScriptService@14b0327] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [0ms], [org.elasticsearch.snapshots.RestoreService@62fc05e5] took [0ms], [org.elasticsearch.ingest.IngestService@1fa415fd] took [495ms], [org.elasticsearch.action.ingest.IngestActionForwarder@46af6f15] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d4a60@1c861d89] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e57e0d1] took [72ms], [org.elasticsearch.tasks.TaskManager@21e3151b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1c8911f5] took [121ms], [org.elasticsearch.cluster.InternalClusterInfoService@bf2b5a6] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@730a58ab] took [0ms], [org.elasticsearch.indices.SystemIndexManager@3b3cfe2f] took [1551ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2760f7ba] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@75ac1e03] took [201ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2cf24896] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4f271213] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@2ee5bd05] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@72eb7556] took [140ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@44cb5db6] took [14203ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@5526550] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5615ceb4] took [1ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@407b49c3] took [8598ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d51aa11] took [364ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d3b3d1d] took [456ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@55233660] took [20070ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [926ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4f0c5856] took [5715ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f0d7284] took [64ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5a15f4de] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@742f02f7] took [1665ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@78365094] took [3003ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a089e17] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@73f2938a] took [445ms], [org.elasticsearch.node.ResponseCollectorService@390acf90] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5b84015b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@55f2c41a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@32fb63ed] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5b5f5edf] took [75ms], [org.elasticsearch.indices.store.IndicesStore@345db5b8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@19bffc81] took [0ms], [org.elasticsearch.license.LicenseService@6e5a3edb] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@16fbb05e] took [75ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@32500c10] took [0ms], [org.elasticsearch.gateway.GatewayService@368df3b1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7b4bbe88] took [0ms]
[2022-03-31T22:34:18,488][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36682}] took [14691ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:24,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859332292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:27,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:30,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [9655ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:30,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9655016283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:33,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:37,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5106444865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5106ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:46,747][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [5107ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:47,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:47,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@49e48ccf, interval=1m}] took [14440ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:48,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14440779903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:59,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:02,260][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11005ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [95], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [95], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-31T22:35:01,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7565499869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:04,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:06,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6039480341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:07,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6039ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:08,045][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [6039ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:08,045][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36684}] took [6039ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:10,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36682}] took [6153ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:18,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:19,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379298810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:19,482][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][221][61] duration [6s], collections [1]/[6.6s], total [6s]/[1.9m], memory [289.2mb]->[297.2mb]/[2gb], all_pools {[young] [84mb]->[36mb]/[0b]}{[old] [202.8mb]->[202.8mb]/[2gb]}{[survivor] [6.4mb]->[8.6mb]/[0b]}
[2022-03-31T22:35:19,482][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][221] overhead, spent [6s] collecting in the last [6.6s]
[2022-03-31T22:35:19,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7779ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:37,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:35,225][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [23189ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:35:42,612][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36676}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:36:10,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:17,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26185324199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:20,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:10,903][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1m] publication of cluster state version [5107] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:36:24,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9804795250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:26,040][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][226][62] duration [19.5s], collections [1]/[29.9s], total [19.5s]/[2.2m], memory [291.4mb]->[216.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [202.8mb]->[203.4mb]/[2gb]}{[survivor] [8.6mb]->[13.5mb]/[0b]}
[2022-03-31T22:36:27,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:28,642][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][226] overhead, spent [19.5s] collecting in the last [29.9s]
[2022-03-31T22:36:30,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [16760ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:36:32,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6955452073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:42,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:48,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15080530050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:54,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [15080ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:36:57,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:05,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14532852790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:12,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@34ef3bd3] took [14373ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:37:13,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:23,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14373768709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:31,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:38,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19029893595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:50,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:57,917][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [37755ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:37:58,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18725628155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:03,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:27,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13024603440ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:29,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [13024ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:38:34,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:36,546][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [31906ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:38:39,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31906568087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:44,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9878ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:45,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [9877ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:39:00,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9877385527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:10,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:23,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26176951528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:41,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:45,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16451187642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:50,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:50,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [23622ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:39:51,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23622479178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:40:01,286][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168555ms] ago, timed out [1.2m/75991ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [991]
[2022-03-31T22:40:08,109][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=171, version=5107}] took [3.6m] which is above the warn threshold of [30s]: [running task [Publication{term=171, version=5107}]] took [223ms], [connecting to new nodes] took [154ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@60ac379] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@fee596e] took [68090ms], [org.elasticsearch.script.ScriptService@14b0327] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [0ms], [org.elasticsearch.snapshots.RestoreService@62fc05e5] took [0ms], [org.elasticsearch.ingest.IngestService@1fa415fd] took [8247ms], [org.elasticsearch.action.ingest.IngestActionForwarder@46af6f15] took [203ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d4a60@1c861d89] took [297ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e57e0d1] took [619ms], [org.elasticsearch.tasks.TaskManager@21e3151b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1c8911f5] took [376ms], [org.elasticsearch.cluster.InternalClusterInfoService@bf2b5a6] took [285ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@730a58ab] took [696ms], [org.elasticsearch.indices.SystemIndexManager@3b3cfe2f] took [8097ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2760f7ba] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@75ac1e03] took [656ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2cf24896] took [104ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4f271213] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@2ee5bd05] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@72eb7556] took [190ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@44cb5db6] took [112446ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@5526550] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5615ceb4] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@407b49c3] took [3784ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d51aa11] took [239ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d3b3d1d] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@55233660] took [2202ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [712ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4f0c5856] took [4028ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f0d7284] took [59ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5a15f4de] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@742f02f7] took [4655ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@78365094] took [1352ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a089e17] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@73f2938a] took [51ms], [org.elasticsearch.node.ResponseCollectorService@390acf90] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5b84015b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@55f2c41a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@32fb63ed] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5b5f5edf] took [48ms], [org.elasticsearch.indices.store.IndicesStore@345db5b8] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@19bffc81] took [0ms], [org.elasticsearch.license.LicenseService@6e5a3edb] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@16fbb05e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@32500c10] took [0ms], [org.elasticsearch.gateway.GatewayService@368df3b1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7b4bbe88] took [0ms]
[2022-03-31T22:40:15,089][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [991] timed out after [92564ms]
[2022-03-31T22:40:27,306][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-03-31T22:40:32,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7003ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:42:46,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/115109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:43:18,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/115108995049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:43:53,138][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][244][63] duration [1.5m], collections [1]/[2.8s], total [1.5m]/[3.8m], memory [288.9mb]->[288.9mb]/[2gb], all_pools {[young] [72mb]->[80mb]/[0b]}{[old] [203.4mb]->[203.4mb]/[2gb]}{[survivor] [13.5mb]->[13.5mb]/[0b]}
[2022-03-31T22:43:54,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:44:25,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][244] overhead, spent [1.5m] collecting in the last [2.8s]
[2022-03-31T22:44:25,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70485299967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:45:02,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [188675ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:45:09,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:45:43,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71446029910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:46:14,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:46:40,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69097238819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:47:10,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:47:49,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50481832516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:48:26,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:48:53,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81610176926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:49:32,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:50:12,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64715290468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:51:06,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:51:48,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92506551096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:52:10,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:52:20,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [66806ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:52:30,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66806178633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:01:25,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/527785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:04:21,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/527784638637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:07:27,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:10:43,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379339498970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:14:07,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:17:12,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367369404147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:20:06,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:23:01,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392432971198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:26:18,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:29:50,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372939158069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:33:29,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:36:32,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417069742454ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:37:33,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@56ecfdd0] took [417069ms] which is above the warn threshold of [5000ms]
[2022-03-31T23:39:45,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:42:42,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382910240146ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:46:29,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/393107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:49:42,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392574643480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:53:50,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/452897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:57:18,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/452967504255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:14:17,394][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-01T00:14:17,406][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-01T00:14:17,407][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-01T00:14:22,261][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-01T00:14:22,263][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-01T00:14:22,263][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-01T00:14:22,264][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-01T00:14:22,264][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-01T00:14:22,264][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-01T00:14:22,265][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-01T00:14:22,265][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-01T00:14:22,266][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-01T00:14:22,266][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-01T00:14:22,266][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-01T00:14:22,267][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-01T00:14:22,267][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-01T00:14:22,267][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-01T00:14:22,268][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-01T00:14:22,268][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-01T00:14:22,269][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-01T00:14:22,269][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-01T00:14:22,269][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-01T00:14:22,270][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-01T00:14:22,270][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-01T00:14:22,270][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-01T00:14:22,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-01T00:14:22,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-01T00:14:22,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-01T00:14:22,272][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-01T00:14:22,272][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-01T00:14:22,272][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-01T00:14:22,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-01T00:14:22,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-01T00:14:22,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-01T00:14:22,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-01T00:14:22,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-01T00:14:22,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-01T00:14:22,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-01T00:14:22,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-01T00:14:22,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-01T00:14:22,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-01T00:14:22,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-01T00:14:22,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-01T00:14:22,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-01T00:14:22,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-01T00:14:22,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-01T00:14:22,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-01T00:14:22,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-01T00:14:22,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-01T00:14:22,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-01T00:14:22,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-01T00:14:22,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-01T00:14:22,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-01T00:14:22,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-01T00:14:22,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-01T00:14:22,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-01T00:14:22,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-01T00:14:22,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-01T00:14:22,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-01T00:14:22,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-01T00:14:22,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-01T00:14:22,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-01T00:14:22,349][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103gb], net total_space [125.8gb], types [ext4]
[2022-04-01T00:14:22,350][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-01T00:14:22,724][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-01T00:16:18,702][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-01T00:16:18,707][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-01T00:16:18,708][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-01T00:16:18,709][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-01T00:16:18,710][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-01T00:16:18,710][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-01T00:16:18,711][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-01T00:16:18,713][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-01T00:16:18,713][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-01T00:16:18,714][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-01T00:16:18,715][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-01T00:16:18,715][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-01T00:16:18,717][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-01T00:16:18,717][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-01T00:16:18,718][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-01T00:16:19,882][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-01T00:16:20,091][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-01T00:16:20,916][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-01T00:16:21,946][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-01T00:16:21,948][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-01T00:16:22,025][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-01T00:16:22,027][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-01T00:16:22,249][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-01T00:16:23,987][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-01T00:16:24,120][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 172, version: 5108, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-01T00:16:24,247][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 172, version: 5108, reason: Publication{term=172, version=5108}
[2022-04-01T00:16:24,351][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-01T00:16:24,352][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-01T00:16:24,997][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-01T00:16:25,008][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-04-01T00:16:25,620][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-01T00:16:25,622][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-01T00:16:26,332][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-01T00:16:26,732][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-01T00:16:26,735][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-01T00:16:26,736][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-01T00:16:27,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-01T00:16:27,763][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-01T00:22:42,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:10,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5322296797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:19,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/457523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:24,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/457657347156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:28,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:32,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9731886190ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:37,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8720ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:39,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8719787518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:42,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5014ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:43,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5014150519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:24:42,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [487786ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:24:46,400][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-01T00:24:47,157][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5123}] took [7.9m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5123}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [836ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [22ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [1ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [5ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [64ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [8ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [16ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [6ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [291035ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [151695ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [466ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [268ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [29962ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [6036ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [414ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [86ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [156ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [277ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [84ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:24:52,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [5802ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:25:04,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:07,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8245919117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:09,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5154ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:15,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5153869313ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:39,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:45,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29028329642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:46,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:50,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8514337852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:25:46,698][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37072}] took [549400ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:24:47,738][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] error updating geoip database [GeoLite2-ASN.mmdb]
java.net.SocketException: Connection reset
	at sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323) ~[?:?]
	at sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350) ~[?:?]
	at sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803) ~[?:?]
	at java.net.Socket$SocketInputStream.read(Socket.java:976) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:478) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readFully(SSLSocketInputRecord.java:461) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:243) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:181) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1509) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1476) ~[?:?]
	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:1060) ~[?:?]
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:282) ~[?:?]
	at java.io.BufferedInputStream.read(BufferedInputStream.java:343) ~[?:?]
	at sun.net.www.MeteredStream.read(MeteredStream.java:141) ~[?:?]
	at java.io.FilterInputStream.read(FilterInputStream.java:132) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3648) ~[?:?]
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:282) ~[?:?]
	at java.io.BufferedInputStream.read(BufferedInputStream.java:343) ~[?:?]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.getChunk(GeoIpDownloader.java:239) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.indexChunks(GeoIpDownloader.java:210) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.processDatabase(GeoIpDownloader.java:162) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:126) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-01T00:26:08,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@3e90af3b] took [64562ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:26:08,424][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [30180ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-04-01T00:26:08,773][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-01T00:26:10,428][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][8][11] duration [3.8s], collections [1]/[9.6m], total [3.8s]/[4.3s], memory [148.4mb]->[148.4mb]/[2gb], all_pools {[young] [44mb]->[32mb]/[0b]}{[old] [98.5mb]->[98.5mb]/[2gb]}{[survivor] [17.9mb]->[17.8mb]/[0b]}
[2022-04-01T00:26:24,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:26:25,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6588241414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:26:29,092][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37080}] took [31228ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:26:38,591][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12525ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [32] unchanged indices
[2022-04-01T00:26:39,536][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.4s] publication of cluster state version [5125] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:27:17,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:27:21,097][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][22][12] duration [14s], collections [1]/[4s], total [14s]/[18.3s], memory [188.4mb]->[192.4mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [98.5mb]->[108.8mb]/[2gb]}{[survivor] [17.8mb]->[14.5mb]/[0b]}
[2022-04-01T00:27:21,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25422380254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:27:23,046][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][22] overhead, spent [14s] collecting in the last [4s]
[2022-04-01T00:27:23,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:27:25,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [35023ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:27:25,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6432736674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:27:34,039][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5125}] took [53.7s] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5125}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [193ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [28ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [134ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [1479ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [480ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [1373ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [273ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [4098ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [32497ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [3582ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [55ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [7203ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [1934ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [123ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [1ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [137ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [48ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [44ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:27:48,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5120ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:28:01,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:03,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7006ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:28:03,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5205285855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:35,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:38,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13431613979ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:43,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:47,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8302173181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:45,134][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37098}] took [19035ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:28:50,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:28:53,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7147874447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:29:03,605][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37098}] took [8205ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:29:11,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@274f2d0b] took [28027ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:29:14,059][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [91721ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [31] unchanged indices
[2022-04-01T00:29:17,742][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.6m] publication of cluster state version [5126] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:29:26,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5533ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:29:30,142][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][33][13] duration [1.8s], collections [1]/[9s], total [1.8s]/[20.2s], memory [199.3mb]->[123mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [108.8mb]->[112.9mb]/[2gb]}{[survivor] [14.5mb]->[10.1mb]/[0b]}
[2022-04-01T00:30:05,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:30:06,042][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][44][14] duration [5.1s], collections [1]/[10.7s], total [5.1s]/[25.3s], memory [203mb]->[126.4mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [112.9mb]->[117.9mb]/[2gb]}{[survivor] [10.1mb]->[8.5mb]/[0b]}
[2022-04-01T00:30:06,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8034572024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:30:06,903][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][44] overhead, spent [5.1s] collecting in the last [10.7s]
[2022-04-01T00:30:30,581][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5126}] took [1.1m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5126}]] took [94ms], [connecting to new nodes] took [110ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [45461ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [183ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [39ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [428ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [1ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [60ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [2269ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [3890ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [87ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [5196ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [120ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [2400ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [4247ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [3530ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [486ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [124ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [67ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [62ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [72ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:30:40,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@64572e6c] took [20543ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:30:50,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:30:51,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6551988913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:30:50,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7952ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:31:00,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7298ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:31:01,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7298338217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:31:08,813][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [29.2s/29293ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fcaaac06], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@812f7b64], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@599b2715], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b9ea8ebd]], which exceeds the warn threshold of [10s]
[2022-04-01T00:31:18,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:31:19,213][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][48][15] duration [5.5s], collections [1]/[24.3s], total [5.5s]/[30.9s], memory [190.4mb]->[127.5mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [117.9mb]->[117.9mb]/[2gb]}{[survivor] [8.5mb]->[9.6mb]/[0b]}
[2022-04-01T00:31:19,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5246522357ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:31:19,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11649ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:31:35,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:31:36,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7229113977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:31:37,433][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][52][16] duration [4.9s], collections [1]/[2.1s], total [4.9s]/[35.9s], memory [199.5mb]->[211.5mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [117.9mb]->[119.6mb]/[2gb]}{[survivor] [9.6mb]->[14.8mb]/[0b]}
[2022-04-01T00:31:37,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [4.9s] collecting in the last [2.1s]
[2022-04-01T00:31:37,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [10038ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:31:39,187][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14075ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-04-01T00:31:40,381][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.9s] publication of cluster state version [5127] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:31:58,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:00,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11126773352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:01,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][57][17] duration [5.5s], collections [1]/[1.4s], total [5.5s]/[41.4s], memory [206.5mb]->[210.5mb]/[2gb], all_pools {[young] [72mb]->[80mb]/[0b]}{[old] [119.6mb]->[119.6mb]/[2gb]}{[survivor] [14.8mb]->[10.1mb]/[0b]}
[2022-04-01T00:32:04,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][57] overhead, spent [5.5s] collecting in the last [1.4s]
[2022-04-01T00:32:06,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [20182ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:32:17,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:20,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7162056348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:21,085][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37118}] took [7162ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:32:25,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:32,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8182526556ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:44,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:45,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [34333ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:32:49,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18589132233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:32:54,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10657ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:00,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10656695121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:00,458][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37118}] took [29246ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:33:00,458][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5127}] took [1.2m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5127}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [2267ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [217ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [1ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [476ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [132ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [1140ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [13841ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [300ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [56ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [6964ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [771ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [11564ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [64ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [24567ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [12122ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [371ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [1508ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [287ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [1326ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:33:08,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:14,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13203831820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:19,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:22,448][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37118}] took [11388ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:33:24,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11387761280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:32,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:39,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13288763335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:45,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:50,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12564582156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:33:55,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:03,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10253966825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:06,766][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.2s/24227ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.23][0]], allocationId [7OUa4pOzQ92zZWRp15bVow], primary term [50], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.23][0]], allocationId [7OUa4pOzQ92zZWRp15bVow], primary term [50], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.23][0]], allocationId [7OUa4pOzQ92zZWRp15bVow], primary term [50], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.23][0]], allocationId [7OUa4pOzQ92zZWRp15bVow], primary term [50], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.25][0]], allocationId [xnje6XRQTqa1Tm982Wk3UQ], primary term [42], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.25][0]], allocationId [xnje6XRQTqa1Tm982Wk3UQ], primary term [42], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-01T00:34:09,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:14,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13636457029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:18,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:22,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9643385608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:29,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10540ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:36,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10539591951ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:38,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@1872edfe] took [69926ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:34:45,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:51,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15007601566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:34:58,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14596ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:08,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14596601501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:11,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [14596ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:35:14,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:18,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15792747141ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:28,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:36,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13488219756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:44,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:50,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16757596773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:35:52,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [16757ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:35:56,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:04,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11315229383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:12,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:19,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15289214756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:25,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13868ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:33,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13867694315ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:44,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:53,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17998477418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:36:30,974][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13868ms] which is above the warn threshold of [5s]
[2022-04-01T00:37:01,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17965ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:17,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17964827509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:26,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:38,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25019822937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:41,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:46,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15629906543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:49,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:37:53,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7419587542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:38:00,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:38:09,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [18729ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:38:09,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11310056458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:38:35,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34466ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:39:27,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34465044427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:39:33,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.5s/57578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:39:38,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.5s/57578062427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:39:46,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:39:49,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14419697617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:39:50,410][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.2m/317920ms] ago, timed out [2.7m/165842ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [377]
[2022-04-01T00:39:52,145][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [293222ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [32] unchanged indices
[2022-04-01T00:39:56,685][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [5.4m] publication of cluster state version [5128] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:40:02,604][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [377] timed out after [152078ms]
[2022-04-01T00:40:18,617][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.9m/417866ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [8.5m/514271ms]
[2022-04-01T00:40:31,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:40:40,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@7ed256c9] took [5910ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:40:42,652][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][76][18] duration [3.2s], collections [1]/[6.5s], total [3.2s]/[44.6s], memory [215.2mb]->[137.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [125.1mb]->[130.1mb]/[2gb]}{[survivor] [10.1mb]->[7.4mb]/[0b]}
[2022-04-01T00:40:43,555][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][76] overhead, spent [3.2s] collecting in the last [6.5s]
[2022-04-01T00:40:44,706][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [18053ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [31] unchanged indices
[2022-04-01T00:40:47,931][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24s] publication of cluster state version [5129] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:41:00,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6131ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:41:03,003][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37118}] took [61737ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:41:03,003][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37134}] took [30084ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:41:12,983][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37148}] took [19352ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:41:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:41:13,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5909762125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:41:29,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5252ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:41:34,679][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37146}] took [19860ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:41:52,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [17516ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:43:04,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@21da17a1] took [51814ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:43:36,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6403ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:43:29,357][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6803ms] which is above the warn threshold of [5s]
[2022-04-01T00:43:58,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [14387ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:44:15,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@4adea572] took [5998ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:44:18,220][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5129}] took [3.4m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5129}]] took [34ms], [connecting to new nodes] took [34ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [195649ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [5341ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [42ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [77ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [46ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [670ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [226ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [102ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [5770ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [102ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [21ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [81ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [22ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [24ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [32ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [0ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [31ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:44:22,343][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [494ms] collecting in the last [1.5s]
[2022-04-01T00:44:22,403][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-01T00:44:23,744][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-01T00:44:31,304][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][99][20] duration [1.6s], collections [1]/[3.7s], total [1.6s]/[46.8s], memory [214.9mb]->[148.8mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [130.1mb]->[135.9mb]/[2gb]}{[survivor] [12.8mb]->[12.9mb]/[0b]}
[2022-04-01T00:44:31,709][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [1.6s] collecting in the last [3.7s]
[2022-04-01T00:44:32,404][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-01T00:44:49,488][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13279ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [31] unchanged indices
[2022-04-01T00:44:50,247][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.8s] publication of cluster state version [5133] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:45:10,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:45:25,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11515279283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:45:29,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:45:35,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19655294605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:45:49,018][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][21] duration [6.8s], collections [1]/[13.5s], total [6.8s]/[53.7s], memory [220.8mb]->[153.6mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [135.9mb]->[142.1mb]/[2gb]}{[survivor] [12.9mb]->[11.5mb]/[0b]}
[2022-04-01T00:45:50,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:45:56,661][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [6.8s] collecting in the last [13.5s]
[2022-04-01T00:45:57,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21268165329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:00,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [40923ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:46:05,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:12,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14105251070ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:22,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:33,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16617594576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:35,866][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37174}] took [30723ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:46:40,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18480ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:43,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18479894483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:46:44,215][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37162}] took [90127ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:46:47,040][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37172}] took [49203ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:46:47,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:47:01,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7873271742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:47:04,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:47:06,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16568145379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:47:17,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@6da89a0] took [10205ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:47:27,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5371ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:47:39,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:48:00,926][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37188}] took [6803ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:48:21,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@55f15466] took [26148ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:48:41,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [5085ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:48:49,722][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37188}] took [8851ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:48:58,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [13018ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:49:30,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:49:27,011][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [655] timed out after [61475ms]
[2022-04-01T00:49:42,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:49:51,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:50:00,891][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.6m/99095ms] ago, timed out [37.6s/37620ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [655]
[2022-04-01T00:50:05,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:50:18,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:50:19,126][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][122][22] duration [3.6s], collections [1]/[9.6s], total [3.6s]/[57.4s], memory [229.6mb]->[233.6mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [142.1mb]->[146.8mb]/[2gb]}{[survivor] [11.5mb]->[5.6mb]/[0b]}
[2022-04-01T00:50:19,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227434316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:50:19,235][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][122] overhead, spent [3.6s] collecting in the last [9.6s]
[2022-04-01T00:50:19,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8828ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:50:19,527][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5133}] took [5.4m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5133}]] took [0ms], [connecting to new nodes] took [30ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [173ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [269015ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [3344ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [83ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [81ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [1825ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [545ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [21ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [155ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [31028ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [4436ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [508ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [229ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [5378ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [66ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [10356ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [28ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [70ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [51ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [0ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:50:28,119][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-01T00:50:30,977][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][128] overhead, spent [685ms] collecting in the last [2.1s]
[2022-04-01T00:50:32,225][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-01T00:50:33,449][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-01T00:50:33,441][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-01T00:50:33,717][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-01T00:50:33,717][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-01T00:50:38,609][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][132][24] duration [1.7s], collections [1]/[3.3s], total [1.7s]/[59.8s], memory [233.5mb]->[163.3mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [146.8mb]->[148.7mb]/[2gb]}{[survivor] [10.6mb]->[14.5mb]/[0b]}
[2022-04-01T00:50:38,860][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][132] overhead, spent [1.7s] collecting in the last [3.3s]
[2022-04-01T00:51:46,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@7ababf71] took [21749ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:52:06,131][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37216}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:52:06,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5267ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:52:30,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@72269c47] took [8495ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:52:41,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5065ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:52:47,651][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5137}] took [1.7m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5137}]] took [33ms], [connecting to new nodes] took [69ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [69860ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [334ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [49ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [55ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [97ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [54ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [1593ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [161ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [106ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [18915ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [3790ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [160ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [5316ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [41ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [1306ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [3054ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [923ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [225ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [44ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [82ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T00:53:03,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8497ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:04,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8497015181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:04,441][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][25] duration [5s], collections [1]/[1.3s], total [5s]/[1m], memory [239.3mb]->[239.3mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [148.7mb]->[156.3mb]/[2gb]}{[survivor] [14.5mb]->[11.7mb]/[0b]}
[2022-04-01T00:53:05,051][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [5s] collecting in the last [1.3s]
[2022-04-01T00:53:05,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [13448ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:53:17,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:18,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8777461439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:18,862][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][26] duration [6.3s], collections [1]/[16s], total [6.3s]/[1.1m], memory [239.3mb]->[248.1mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [156.3mb]->[162.8mb]/[2gb]}{[survivor] [11.7mb]->[12.5mb]/[0b]}
[2022-04-01T00:53:19,186][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37216}] took [9178ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:53:19,402][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [6.3s] collecting in the last [16s]
[2022-04-01T00:53:19,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9377ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:53:28,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@6e69df] took [9298ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:53:41,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:42,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5494662209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:46,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:47,009][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37216}] took [5009ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:53:47,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5008ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:53:52,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5008362363ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:53,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T00:53:55,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7494569686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T00:54:05,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7718ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:54:13,248][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [19205ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-04-01T00:54:17,247][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.9s] publication of cluster state version [5138] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:54:52,941][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [17976ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [32] unchanged indices
[2022-04-01T00:54:53,736][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [19.9s] publication of cluster state version [5139] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T00:55:06,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5270ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:55:20,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5849ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:55:30,974][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37236}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:55:32,783][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.8s/15864ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-01T00:55:40,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@10ebdec1] took [8404ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:55:56,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7403ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:56:10,393][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37236}] took [14561ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:56:18,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [12628ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:57:11,132][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13911ms] which is above the warn threshold of [5s]
[2022-04-01T00:57:49,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@5a1b603d] took [33488ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:58:19,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [21011ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:59:02,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5849ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:59:02,166][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1027] timed out after [68391ms]
[2022-04-01T00:59:17,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11406ms] which is above the warn threshold of [5000ms]
[2022-04-01T00:59:48,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8604ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:00:01,544][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.2m/133353ms] ago, timed out [1m/64962ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1027]
[2022-04-01T01:00:17,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@42a66a8] took [16809ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:00:31,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [10605ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:01:08,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6537ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:01:13,012][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [331313ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [31] unchanged indices
[2022-04-01T01:01:12,967][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1085] timed out after [54312ms]
[2022-04-01T01:01:20,229][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [5.6m] publication of cluster state version [5140] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T01:01:24,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5804ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:01:40,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:01:47,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [12109ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:01:47,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10508105738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:01:48,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:01:54,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8447387034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:01:54,658][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.5m/91855ms] ago, timed out [37.5s/37543ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1085]
[2022-04-01T01:02:00,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:01,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [12118ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:02:04,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12118420409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:07,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6415ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:11,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6415ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:02:10,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6415020715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:13,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:16,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6501196468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:19,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:25,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5827497692ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:30,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:30,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@fb146a2] took [12328ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:02:33,809][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37244}] took [82753ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:02:34,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11146490309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:02:38,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7169ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:02:36,617][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5370ms] which is above the warn threshold of [5s]
[2022-04-01T01:02:57,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8175ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:02:58,448][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][210][27] duration [2.6s], collections [1]/[9.2s], total [2.6s]/[1.2m], memory [251.4mb]->[180.2mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [162.8mb]->[167.1mb]/[2gb]}{[survivor] [12.5mb]->[13.1mb]/[0b]}
[2022-04-01T01:02:59,294][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][210] overhead, spent [2.6s] collecting in the last [9.2s]
[2022-04-01T01:03:06,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:03:22,422][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5140}] took [1.9m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5140}]] took [194ms], [connecting to new nodes] took [355ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [112470ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [126ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [23ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [222ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [608ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [480ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [67ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [48ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [19ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [19ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [1ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [20ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T01:04:00,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:04:01,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9176730892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:04:13,594][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][225][28] duration [7.3s], collections [1]/[10s], total [7.3s]/[1.3m], memory [256.2mb]->[186.7mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [167.1mb]->[174.7mb]/[2gb]}{[survivor] [13.1mb]->[12mb]/[0b]}
[2022-04-01T01:04:24,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:04:28,689][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][225] overhead, spent [7.3s] collecting in the last [10s]
[2022-04-01T01:04:31,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8290130149ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:04:36,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [23705ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:04:42,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:04:49,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17778713438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:04:58,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:05:06,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15534189781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:05:13,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:05:34,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15481358256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:05:40,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27454ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:05:45,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27454112753ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:05:37,868][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1233] timed out after [66996ms]
[2022-04-01T01:05:51,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:01,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9704589458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:08,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:13,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18368203664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:19,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:18,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [28209ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:06:21,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9841370689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:27,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [9730ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:06:28,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:36,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9730109120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:50,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:02,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22099232095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:06:44,628][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9730ms] which is above the warn threshold of [5s]
[2022-04-01T01:07:09,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:16,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19664416360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:21,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:24,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11469ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:07:26,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11469772681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:33,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [11041ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:07:32,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:29,835][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.5m/210810ms] ago, timed out [2.3m/143814ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1233]
[2022-04-01T01:07:37,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11041089823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:46,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14387ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:07:53,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14387686540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:08:05,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@7741c399] took [14387ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:08:06,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:08:18,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19125844900ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:08:28,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:08:34,487][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:58496}] took [18482ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:08:36,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18482241617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:08:50,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:09:00,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24474429449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:09:07,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [24474ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:09:11,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22684ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:09:24,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22684457190ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:09:37,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:09:48,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25945013997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:09:51,730][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [279954ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-04-01T01:09:58,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [20344ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:09:58,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:08,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20344692700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:20,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:07,793][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [5.6m] publication of cluster state version [5141] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T01:10:31,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925062740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:37,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:49,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18265366995ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:37,471][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1276] timed out after [152982ms]
[2022-04-01T01:10:52,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:10:57,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14593405771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:01,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:03,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8784472362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:01,588][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23378ms] which is above the warn threshold of [5s]
[2022-04-01T01:11:07,199][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37288}] took [5964ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:11:07,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:10,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5963713506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:12,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5963ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:11:15,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8104ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:18,307][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.3m/200589ms] ago, timed out [47.6s/47607ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1276]
[2022-04-01T01:11:19,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8104237924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:29,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14069ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:39,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14068560050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:43,921][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37288}] took [14069ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:11:48,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [32745ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:11:48,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:51,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18676462242ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:11:59,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:12:00,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6252240968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:12:02,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:12:03,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@3bbd7f34] took [8277ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:12:04,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8277502778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:12:34,612][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5141}] took [1.9m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5141}]] took [227ms], [connecting to new nodes] took [3353ms], [applying settings] took [161ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [9ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [33852ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [743ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [74ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [71ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [13950ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [596ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [131ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [40574ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [1870ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [406ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [31ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [2809ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [187ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [2273ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [3763ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [3986ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [3535ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [42ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [47ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [44ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T01:12:56,572][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37302}] took [6767ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:12:56,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:12:56,572][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37300}] took [6767ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:12:57,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6167175987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:13:00,058][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][242][29] duration [4.3s], collections [1]/[1.2s], total [4.3s]/[1.4m], memory [238.7mb]->[270.7mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [174.7mb]->[179.7mb]/[2gb]}{[survivor] [12mb]->[10.7mb]/[0b]}
[2022-04-01T01:13:03,152][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][242] overhead, spent [4.3s] collecting in the last [1.2s]
[2022-04-01T01:13:08,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [18418ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:13:29,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:13:42,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@1e5ffec6] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:13:53,089][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37310}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:14:05,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9205ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:15:12,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [54541ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:15:20,926][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37312}] took [34787ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:15:13,162][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [141027ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [32] unchanged indices
[2022-04-01T01:15:55,709][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.1m] publication of cluster state version [5142] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T01:17:24,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@375098d2] took [22640ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:17:33,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:17:41,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5049540184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:17:49,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:06,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15943523447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:19,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:27,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30284920735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:34,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:37,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [45071ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:18:39,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14786584055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:42,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:45,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8815488979ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:45,883][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8815ms] which is above the warn threshold of [5s]
[2022-04-01T01:18:48,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:49,194][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/78758ms] ago, timed out [0s/0ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1486]
[2022-04-01T01:18:53,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5846116358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:19:02,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12323ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:18:47,378][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1486] timed out after [78758ms]
[2022-04-01T01:19:09,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12322610359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:19:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:19:23,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [31875ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:19:31,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19552606819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:19:41,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:19:45,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [20031ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:19:50,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20031505991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:19:58,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:07,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15787178998ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:14,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:18,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246862732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:25,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:32,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12421147001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:34,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [12421ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:20:37,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11836ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:42,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11836147954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:44,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:49,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@66557197] took [7902ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:20:48,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7902342963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:51,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:53,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [6981ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:20:55,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6981458974ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:20:58,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6775ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:02,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6774754935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:08,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9022ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:19,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9022367614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:21,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9022ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:21:21,711][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5142}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5142}]] took [296ms], [connecting to new nodes] took [1188ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [83767ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [5350ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [227ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [138ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [121ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [425ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [4510ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [185ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [1240ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [151ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [101059ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [20227ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [1130ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [1143ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [8707ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [173ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [9474ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [8159ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [14062ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [112ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [853ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [175ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [69ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [163ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [63ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [285ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T01:21:22,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:30,202][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.7m/524844ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [14.1m/848562ms]
[2022-04-01T01:21:30,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15382822662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:31,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [9254ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:21:31,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9255ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:32,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9254596552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:21:41,380][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37328}] took [5100ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:21:44,368][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7302ms] which is above the warn threshold of [5s]
[2022-04-01T01:21:46,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6701ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:21:59,386][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [25.7s/25750ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-01T01:22:00,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8804ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:22:08,844][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37328}] took [16409ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:22:16,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6803ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:22:30,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@77bbadc9] took [9890ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:22:53,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11934ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:23:06,641][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37328}] took [14808ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:23:12,849][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1622] timed out after [41110ms]
[2022-04-01T01:23:25,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5222ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:23:48,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [12439ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:23:54,210][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/81099ms] ago, timed out [39.9s/39989ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1622]
[2022-04-01T01:24:41,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@4f9a6f1f] took [12654ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:24:55,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7875ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:25:01,061][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37338}] took [7274ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:25:07,904][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5288ms] which is above the warn threshold of [5s]
[2022-04-01T01:25:24,763][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37338}] took [11606ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:25:31,170][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1684] timed out after [44649ms]
[2022-04-01T01:25:40,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [15115ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:25:54,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:00,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11087780042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:25:57,721][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [193638ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [32] unchanged indices
[2022-04-01T01:26:06,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12973ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:11,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12973125182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:20,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:27,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14406937464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:12,947][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4m] publication of cluster state version [5143] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T01:26:36,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:41,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14118073367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:47,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:51,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12147205769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:26:58,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11009ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:26:57,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:04,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11009755121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:11,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:19,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13221495672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:18,259][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.4m/149930ms] ago, timed out [1.7m/105281ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1684]
[2022-04-01T01:27:20,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@12382bf2] took [13221ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:27:23,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:31,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12063680982ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:37,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:41,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726267015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:49,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [24384ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:27:48,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:53,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9658286254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:27:56,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:05,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8646404289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:08,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:13,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12558455957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:21,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:15,351][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1738] timed out after [57653ms]
[2022-04-01T01:28:26,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12622420906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:26,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [12622ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:28:31,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:25,782][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12622ms] which is above the warn threshold of [5s]
[2022-04-01T01:28:35,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9266956289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:42,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:49,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320240481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:56,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:28:56,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11320ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:29:04,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14780117353ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:13,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:21,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16321797832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:33,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:45,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20456912389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:47,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [20456ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:29:46,216][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.3m/142421ms] ago, timed out [1.4m/84768ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1738]
[2022-04-01T01:29:48,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:53,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14597125327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:29:59,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:06,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11156097171ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:12,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@22b50078] took [11156ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:30:12,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:17,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13440244799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:20,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8354ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:23,500][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37352}] took [8354ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:30:23,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8353959838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:27,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:31,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5726333913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:36,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:40,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9858885578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:42,472][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37352}] took [9859ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:30:41,057][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1805] timed out after [37379ms]
[2022-04-01T01:30:44,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:47,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7688ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:30:49,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7688253268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:53,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:30:57,409][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37352}] took [9363ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:30:58,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9362693403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:06,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:10,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13240263239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:05,596][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9363ms] which is above the warn threshold of [5s]
[2022-04-01T01:31:13,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:16,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6693702218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:20,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6693ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:31:22,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:28,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8833892222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:36,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:42,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [13201ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:31:42,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13201091699ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:44,674][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37352}] took [13201ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:31:47,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:31:51,114][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.7m/107425ms] ago, timed out [1.1m/70046ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1805]
[2022-04-01T01:31:54,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11026040064ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:02,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:11,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15566009473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:12,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@4b4e5b78] took [26592ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:32:17,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:21,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14761020603ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:23,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6865ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:29,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6864756025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:36,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:42,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11699ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:32:44,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11699781931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:51,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [15207ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:32:50,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:54,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15207282781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:32:59,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:04,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8785625879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:07,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:07,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8785ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:33:10,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8306806574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:13,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:16,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5746318675ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:18,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@1cf27cc9] took [5746ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:33:20,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:22,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6308246998ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:33:19,156][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] starting SLM retention snapshot cleanup task
[2022-04-01T01:33:27,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7195ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:34:00,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T01:34:01,203][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][275][30] duration [15.8s], collections [1]/[12.3s], total [15.8s]/[1.6m], memory [270.5mb]->[270.5mb]/[2gb], all_pools {[young] [80mb]->[80mb]/[0b]}{[old] [179.7mb]->[179.7mb]/[2gb]}{[survivor] [10.7mb]->[10.7mb]/[0b]}
[2022-04-01T01:34:02,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23038647414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T01:34:02,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][275] overhead, spent [15.8s] collecting in the last [12.3s]
[2022-04-01T01:34:00,867][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26040ms] which is above the warn threshold of [5s]
[2022-04-01T01:34:04,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [29485ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:34:03,850][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] there are no repositories to fetch, SLM retention snapshot cleanup task complete
[2022-04-01T01:34:21,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:34:38,454][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37364}] took [15408ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:34:46,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [19079ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:34:54,459][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1953] timed out after [38290ms]
[2022-04-01T01:35:32,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [18609ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:35:43,507][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37364}] took [14008ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:35:49,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9004ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:35:53,376][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5143}] took [9.2m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5143}]] took [245ms], [connecting to new nodes] took [341ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [419358ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [1ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [215ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [53ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [0ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [116ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [55ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [33ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [24000ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [531ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [51019ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [66ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [10969ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [154ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [14247ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [4994ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [14163ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [7484ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [5027ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [1539ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [53ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [59ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [51ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [230ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [226ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [115ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T01:35:54,878][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.6m/100904ms] ago, timed out [1m/62614ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1953]
[2022-04-01T01:36:12,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9987ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:36:26,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6674ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:36:31,366][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.9s/17909ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.31][0]], allocationId [cKNYYuYDR5iIRRfI45y0Jg], primary term [5], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.31][0]], allocationId [cKNYYuYDR5iIRRfI45y0Jg], primary term [5], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.31][0]], allocationId [cKNYYuYDR5iIRRfI45y0Jg], primary term [5], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.31][0]], allocationId [cKNYYuYDR5iIRRfI45y0Jg], primary term [5], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-01T01:36:33,677][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2012] timed out after [32651ms]
[2022-04-01T01:36:27,505][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8876ms] which is above the warn threshold of [5s]
[2022-04-01T01:36:45,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7899ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:37:08,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [14166ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:37:13,048][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.2m/73588ms] ago, timed out [40.9s/40937ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2012]
[2022-04-01T01:37:21,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@a606292] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:37:34,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9558ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:37:47,242][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [52608ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-04-01T01:38:02,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [13443ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:37:59,253][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.3m] publication of cluster state version [5144] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T01:38:22,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [7343ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:38:49,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [21859ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:39:48,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@4f6c2ce8] took [35835ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:40:44,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [18889ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:40:59,934][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7004ms] which is above the warn threshold of [5s]
[2022-04-01T01:41:27,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [34909ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:41:59,584][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2115] timed out after [126183ms]
[2022-04-01T01:42:27,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [14584ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:42:38,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [5354ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:42:56,151][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/187319ms] ago, timed out [1m/61136ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2115]
[2022-04-01T01:43:29,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [18306ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:43:54,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@1a5f3309] took [10444ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:44:35,940][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5144}] took [6.1m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5144}]] took [285ms], [connecting to new nodes] took [1942ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [123944ms], [org.elasticsearch.script.ScriptService@35070132] took [1ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [4954ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [748ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [215ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [78ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [161ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [81ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [291ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [5280ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [366ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [135ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [77ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [113435ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [111ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [12566ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [381ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [7503ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [765ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [30447ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [283ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [29323ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [18224ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [886ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [3539ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [1430ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [283ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [470ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [484ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [347ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [1ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [3178ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [335ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [521ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [269ms]
[2022-04-01T01:45:05,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [59079ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:45:21,145][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [9.2m/555944ms] which is longer than the warn threshold of [300000ms]; there are currently [10] pending tasks, the oldest of which has age [18.5m/1113200ms]
[2022-04-01T01:45:55,352][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23052ms] which is above the warn threshold of [5s]
[2022-04-01T01:46:46,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [12819ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:47:19,494][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-04-01T01:47:22,520][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2206] timed out after [203203ms]
[2022-04-01T01:47:30,376][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.1m/66238ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [96], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [96], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [96], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [96], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [96], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [96], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [96], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [96], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-01T01:47:50,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [25059ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:48:34,341][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.6m/276795ms] ago, timed out [1.2m/73592ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2206]
[2022-04-01T01:48:46,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [14721ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:49:04,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@62b0f4df] took [10243ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:49:20,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [9111ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:49:58,052][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10585ms] which is above the warn threshold of [5s]
[2022-04-01T01:50:17,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [19787ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:50:40,418][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2257] timed out after [95031ms]
[2022-04-01T01:50:52,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9960ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:51:12,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9233ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:51:59,894][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/171466ms] ago, timed out [1.2m/76435ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2257]
[2022-04-01T01:51:56,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [26416ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:52:26,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@575d6a19] took [10737ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:52:57,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11406ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:53:09,218][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [249246ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [32] unchanged indices
[2022-04-01T01:53:18,350][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2333] timed out after [49125ms]
[2022-04-01T01:53:23,396][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10926ms] which is above the warn threshold of [5s]
[2022-04-01T01:53:31,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11348ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:53:26,126][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.9m] publication of cluster state version [5145] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-01T01:53:48,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8404ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:54:09,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11163ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:54:40,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@1b723d6a] took [8179ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:54:40,392][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.2m/132632ms] ago, timed out [1.3m/83507ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2333]
[2022-04-01T01:55:01,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11878ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:55:49,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [13563ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:55:40,110][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2404] timed out after [54586ms]
[2022-04-01T01:56:15,406][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10475ms] which is above the warn threshold of [5s]
[2022-04-01T01:56:28,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [23722ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:56:53,850][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=172, version=5145}] took [3.2m] which is above the warn threshold of [30s]: [running task [Publication{term=172, version=5145}]] took [50ms], [connecting to new nodes] took [735ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54f4e060] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@531276d4] took [5013ms], [org.elasticsearch.script.ScriptService@35070132] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [0ms], [org.elasticsearch.snapshots.RestoreService@6dcccefb] took [0ms], [org.elasticsearch.ingest.IngestService@2d28a140] took [1955ms], [org.elasticsearch.action.ingest.IngestActionForwarder@20ab2b46] took [369ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc660@786e68bb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@735eda5b] took [187ms], [org.elasticsearch.tasks.TaskManager@1b9457d1] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@621caefe] took [122ms], [org.elasticsearch.cluster.InternalClusterInfoService@3feb5036] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d55b3ad] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66906d79] took [3299ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@16c401a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@5ced7030] took [351ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4b69b3f9] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@478b89b2] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@2aca9154] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@304e2f07] took [113ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215e59a9] took [77231ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3db98fe9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@35117027] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@44a677ab] took [29641ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@554cc7cd] took [614ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3c06a877] took [630ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2287798] took [17687ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1e48cab2] took [4031ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@75445359] took [21426ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@46059093] took [383ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5915a3f3] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@25b2c607] took [16699ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@657f81b2] took [10225ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@15295c33] took [301ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3010fca8] took [310ms], [org.elasticsearch.node.ResponseCollectorService@297b8909] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@81b1c03] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@570c01f7] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@60ce4202] took [133ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1938a7c9] took [186ms], [org.elasticsearch.indices.store.IndicesStore@7f4520] took [66ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7c922c36] took [0ms], [org.elasticsearch.license.LicenseService@7dabcef6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5bdd2f2e] took [65ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@73898526] took [0ms], [org.elasticsearch.gateway.GatewayService@3c0bc912] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@550eb9aa] took [0ms]
[2022-04-01T01:56:58,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [15560ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:57:13,249][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.4m/149644ms] ago, timed out [1.5m/95058ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2404]
[2022-04-01T01:57:11,817][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [21.1m/1269004ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [30.4m/1826294ms]
[2022-04-01T01:57:19,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@284f9078] took [13281ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:58:00,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [12057ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:58:11,557][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [28.3s/28340ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-01T01:58:27,470][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2482] timed out after [65197ms]
[2022-04-01T01:58:40,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [14120ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:58:45,594][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/86498ms] ago, timed out [21.3s/21301ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2482]
[2022-04-01T01:59:06,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [16118ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:59:29,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@195efcbe] took [12737ms] which is above the warn threshold of [5000ms]
[2022-04-01T01:59:51,056][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37414}] took [14808ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:00:04,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [14608ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:00:18,217][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [48.7s/48744ms] ago, timed out [801ms/801ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2552]
[2022-04-01T02:00:17,487][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2552] timed out after [47943ms]
[2022-04-01T02:00:28,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:00:37,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6300ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:00:54,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [5352ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:00,106][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37424}] took [20961ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:00,106][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37414}] took [37067ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:15,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@4772d4ea] took [7653ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:17,029][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11055ms] which is above the warn threshold of [5s]
[2022-04-01T02:01:37,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [16444ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:42,962][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37426}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:59,304][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37434}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:01:58,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6803ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:02:09,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7420ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:02:13,420][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37434}] took [6115ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:02:21,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7798ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:02:50,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@5c886c88] took [8508ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:03:02,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5402ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:03:16,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7208ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:03:28,202][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2697] timed out after [33023ms]
[2022-04-01T02:03:40,632][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37434}] took [12293ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:03:50,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [13704ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:04:06,084][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7604ms] which is above the warn threshold of [5s]
[2022-04-01T02:04:15,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9404ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:04:37,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8253ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:04:51,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6803ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:04:59,026][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2768] timed out after [27916ms]
[2022-04-01T02:05:05,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7905ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:05:35,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [22489ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:05:35,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:05:36,374][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1m/65643ms] ago, timed out [37.7s/37727ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2768]
[2022-04-01T02:05:37,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21688775343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:05:58,263][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][331][31] duration [12.7s], collections [1]/[39.5s], total [12.7s]/[1.9m], memory [275.7mb]->[198.5mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [186.2mb]->[186.2mb]/[2gb]}{[survivor] [5.4mb]->[8.2mb]/[0b]}
[2022-04-01T02:06:00,580][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][331] overhead, spent [12.7s] collecting in the last [39.5s]
[2022-04-01T02:06:00,008][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37448}] took [9832ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:06:05,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [30587ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:06:44,615][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37448}] took [27734ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:06:53,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@3330ea6c] took [8344ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:07:08,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [10384ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:07:20,033][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.4m/268907ms] ago, timed out [3.9m/235884ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2697]
[2022-04-01T02:07:34,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [8011ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:08:06,730][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37448}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:08:06,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5927ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:08:09,158][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][340][32] duration [4.1s], collections [1]/[5.1s], total [4.1s]/[1.9m], memory [274.5mb]->[278.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [186.2mb]->[186.2mb]/[2gb]}{[survivor] [8.2mb]->[9.4mb]/[0b]}
[2022-04-01T02:08:08,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5927594490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:08:09,514][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][340] overhead, spent [4.1s] collecting in the last [5.1s]
[2022-04-01T02:08:09,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [6327ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:08:53,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [42447ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:09:03,948][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37456}] took [32035ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:09:05,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [5076ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:09:00,067][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37458}] took [26109ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:09:27,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [5069ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:09:43,046][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [18.8s/18876ms] ago, timed out [400ms/400ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2929]
[2022-04-01T02:09:43,817][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2929] timed out after [18476ms]
[2022-04-01T02:09:44,809][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37462}] took [9097ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:10:09,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [10639ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:10:26,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9030ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:10:40,255][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37464}] took [8742ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:10:53,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [7874ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:11:04,881][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2989] timed out after [23149ms]
[2022-04-01T02:11:12,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9405ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:11:37,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [17465ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:11:58,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:11:58,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13921786989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:11:59,021][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/79150ms] ago, timed out [56s/56001ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2989]
[2022-04-01T02:11:59,057][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][353][33] duration [9.2s], collections [1]/[23s], total [9.2s]/[2.1m], memory [271.6mb]->[275.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [186.2mb]->[189.1mb]/[2gb]}{[survivor] [9.4mb]->[7.6mb]/[0b]}
[2022-04-01T02:11:59,067][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][353] overhead, spent [9.2s] collecting in the last [23s]
[2022-04-01T02:11:59,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [16122ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:11:58,948][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14723ms] which is above the warn threshold of [5s]
[2022-04-01T02:12:12,790][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][361] overhead, spent [420ms] collecting in the last [1.3s]
[2022-04-01T02:12:20,618][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][365][37] duration [1.5s], collections [1]/[3s], total [1.5s]/[2.1m], memory [235.5mb]->[199.9mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [190mb]->[190mb]/[2gb]}{[survivor] [5.4mb]->[9.9mb]/[0b]}
[2022-04-01T02:12:21,079][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][365] overhead, spent [1.5s] collecting in the last [3s]
[2022-04-01T02:12:34,383][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][368][38] duration [2.2s], collections [1]/[3.2s], total [2.2s]/[2.2m], memory [211.9mb]->[279.9mb]/[2gb], all_pools {[young] [16mb]->[52mb]/[0b]}{[old] [190mb]->[192.5mb]/[2gb]}{[survivor] [9.9mb]->[6.5mb]/[0b]}
[2022-04-01T02:12:35,211][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][368] overhead, spent [2.2s] collecting in the last [3.2s]
[2022-04-01T02:12:37,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9104ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:12:47,616][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][370][39] duration [1.6s], collections [1]/[1.5s], total [1.6s]/[2.2m], memory [279mb]->[283mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [192.5mb]->[192.5mb]/[2gb]}{[survivor] [6.5mb]->[5.2mb]/[0b]}
[2022-04-01T02:12:49,983][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][370] overhead, spent [1.6s] collecting in the last [1.5s]
[2022-04-01T02:12:52,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [11359ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:13:13,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [9770ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:13:33,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@30a9c35b] took [8920ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:13:35,551][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37490}] took [73204ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:13:46,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [7263ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:14:08,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:13,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14186703540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:19,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:25,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12083784024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:32,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:34,543][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][375][40] duration [10.4s], collections [1]/[32.7s], total [10.4s]/[2.4m], memory [281.7mb]->[281.7mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [192.5mb]->[192.5mb]/[2gb]}{[survivor] [5.2mb]->[5.2mb]/[0b]}
[2022-04-01T02:14:41,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][375] overhead, spent [10.4s] collecting in the last [32.7s]
[2022-04-01T02:14:41,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12512417939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:44,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:14:44,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [39783ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:14:56,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12747136308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:15:03,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:15:11,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18680110578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:15:38,255][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37510}] took [70811ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:15:43,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:15:56,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36757495173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:16:17,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:16:27,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37574768813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:16:47,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:16:53,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [28832ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:17:00,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28832047203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:17:17,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:17:31,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30015643343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:17:42,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:18:22,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26265867251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:18:29,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:18:43,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46061286161ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:19:00,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:18:54,209][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [46061ms] which is above the warn threshold of [5s]
[2022-04-01T02:19:10,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30318675781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:19:21,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:19:24,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@2200843a] took [50478ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:19:35,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20159459308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:19:50,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:20:04,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29271553771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:20:20,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:20:38,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [59803ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:20:38,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30531974867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:20:56,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:21:27,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36697665358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:21:57,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:22:27,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60655458109ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:23:04,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [58220ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:23:00,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:23:31,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58220845522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:23:59,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62854ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:24:21,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62854586687ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:24:45,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45123ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:25:12,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45122746519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:25:47,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63719ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:26:02,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [63719ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:26:21,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63719484920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:26:49,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:27:21,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58384708369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:29:22,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/153530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:29:46,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/153529689290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:30:21,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59s/59017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:33:24,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59s/59016970651ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:33:29,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [212546ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:33:44,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/205934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:34:17,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/205934153549ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:34:46,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:34:58,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61487131955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:34:25,831][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3181] timed out after [445459ms]
[2022-04-01T02:35:21,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:35:47,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32448131914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:35:58,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:35:23,817][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [32448ms] which is above the warn threshold of [5s]
[2022-04-01T02:36:09,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [39717ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:36:18,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39717616047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:36:57,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.8s/59805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:37:14,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.8s/59805059330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:37:28,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:38:01,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29728154834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:38:39,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:39:05,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70269279876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:39:42,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:40:16,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61675293035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:41:05,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:41:13,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [82176ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:42:07,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82176651906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:42:24,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:42:47,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82953930054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:43:11,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45689ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:44:30,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45688566117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:44:48,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:44:49,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@753de048, interval=1m}] took [98698ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:45:05,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98698387881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:45:18,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:45:31,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28806121385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:45:49,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:45:59,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31550854938ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:46:10,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [60356ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:46:25,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:46:44,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34186322124ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:47:00,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:47:15,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38047978092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:47:29,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28142ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:47:41,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28141163020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:47:48,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@3cae7020] took [28141ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:47:59,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:48:12,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29951325105ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:48:23,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:48:35,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25192525510ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:48:44,154][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [29m/1744464ms] ago, timed out [21.6m/1299005ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3181]
[2022-04-01T02:48:50,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:49:09,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25970087222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:49:25,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:49:25,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@2412555f] took [35242ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:49:32,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35242624114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:49:27,101][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35243ms] which is above the warn threshold of [5s]
[2022-04-01T02:49:38,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13689ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:49:45,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13688481871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:49:53,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:50:05,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14760451584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:50:08,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [28448ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:50:20,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:50:32,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26826415089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:50:40,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:50:53,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20067579223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:51:12,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:51:23,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30969306958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:51:30,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:51:23,033][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3264] timed out after [191699ms]
[2022-04-01T02:51:35,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18025861800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:51:47,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:51:50,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [17717ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:51:54,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17717608693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:52:09,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19877ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:52:22,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19877506548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:52:34,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:52:48,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26663733460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:53:00,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:53:03,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [72057ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:53:16,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25515849032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:53:40,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:53:42,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [36575ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:54:00,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36575636042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:54:20,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:54:41,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43898017550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:55:08,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.6s/46676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:55:36,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.6s/46676705522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:56:19,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:56:35,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [69412ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:56:48,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69412250982ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:57:35,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72292ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:58:07,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72292718875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:58:30,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T02:58:59,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60762590557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T02:59:28,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@6b05f44a] took [60762ms] which is above the warn threshold of [5000ms]
[2022-04-01T02:59:35,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:00:05,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65593483565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:00:52,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:01:20,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72258345774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:01:47,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:02:30,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58668407325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:02:45,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:03:04,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59156074173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:02:50,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [190082ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:03:32,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:03:48,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45316418191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:03:58,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:04:01,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c7a8ed5, interval=5s}] took [28449ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:04:11,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28449765083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:04:29,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:04:50,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28748529333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:05:00,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:05:07,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32679776981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:05:16,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:05:32,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15899612251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:05:44,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:05:48,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [28492ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:05:58,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28492975955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:06:10,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:06:18,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25900038640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:06:31,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:06:45,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19380417468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:06:31,643][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3340] timed out after [461163ms]
[2022-04-01T03:06:54,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24990ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:06:42,100][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19381ms] which is above the warn threshold of [5s]
[2022-04-01T03:07:08,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24990670270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:07:20,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:07:36,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25114572721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:07:56,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:07:56,408][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.8m/530649ms] ago, timed out [1.1m/69486ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3340]
[2022-04-01T03:08:13,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36080792300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:08:28,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [61195ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:08:31,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:08:47,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34724562810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:09:10,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:09:23,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36209403340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:09:38,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:09:59,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31258439276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:10:18,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:10:25,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@33198041, interval=5s}] took [39863ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:10:35,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39863635835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:10:55,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:11:11,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37800342404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:11:34,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:12:04,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33863370126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:12:29,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:13:19,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56985245050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:13:33,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e1200@5182f2e9] took [56985ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:13:44,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:14:27,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71370674722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:14:50,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:15:45,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71449918919ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:16:20,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:16:36,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6528aa51, interval=1s}] took [159831ms] which is above the warn threshold of [5000ms]
[2022-04-01T03:17:10,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88381428991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:18:03,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:19:39,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102063071070ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:20:41,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/160664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:21:39,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/160664239571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:23:00,504][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [33.1m/1991451ms] ago, timed out [29.9m/1799752ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jIxraf_UTnK7fIAmFt2kCw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3264]
[2022-04-01T03:23:07,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/144719ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:25:12,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/144719065577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:22:47,522][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [160664ms] which is above the warn threshold of [5s]
[2022-04-01T03:28:24,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/300072ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:30:40,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/299923369090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:31:47,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/193232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:33:20,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/193380645773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:35:00,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:37:12,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/217888380471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:38:17,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/190819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:40:36,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/190747287368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:42:50,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/281655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:44:40,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282067842897ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:45:46,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:46:26,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173410624093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:47:52,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126339ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:49:47,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126138484883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:51:55,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232386ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T03:54:29,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232300685181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T03:56:38,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287053ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T05:20:25,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287139912128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T05:24:07,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4h/5242403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-01T05:21:05,815][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3419] timed out after [1996066ms]
[2022-04-01T05:27:58,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4h/5242339248565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-01T05:31:06,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431903ms] on absolute clock which is above the warn threshold of [5000ms]
