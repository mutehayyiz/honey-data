[2022-03-29T00:00:04,448][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.29] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-29T00:00:05,289][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.29][0]]]).
[2022-03-29T00:00:06,951][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:13,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:21,225][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:31,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:35,801][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:39,859][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:41,252][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:42,864][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:43,895][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:44,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:45,414][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:46,334][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:46,922][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:49,079][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:53,957][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:58,787][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:03,534][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:10,514][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:15,193][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:23,095][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:34,583][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:50,986][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11.4s] publication of cluster state version [4215] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:02:35,529][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:02:42,148][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.8s/16817ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:02:59,005][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12002ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:03:03,982][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.6s] publication of cluster state version [4216] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:03:19,281][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:31,797][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:45,375][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:46,554][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:07,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:09,457][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:09,458][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][820][93] duration [1.1s], collections [1]/[2.7s], total [1.1s]/[3.1m], memory [1.3gb]->[240.6mb]/[2gb], all_pools {[young] [1.1gb]->[24mb]/[0b]}{[old] [203.6mb]->[204.6mb]/[2gb]}{[survivor] [10.9mb]->[12mb]/[0b]}
[2022-03-29T00:04:09,649][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][820] overhead, spent [1.1s] collecting in the last [2.7s]
[2022-03-29T00:04:18,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:22,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][828][94] duration [1.4s], collections [1]/[1.1s], total [1.4s]/[3.1m], memory [276.6mb]->[280.6mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [204.6mb]->[204.6mb]/[2gb]}{[survivor] [12mb]->[12mb]/[0b]}
[2022-03-29T00:04:22,985][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][828] overhead, spent [1.4s] collecting in the last [1.1s]
[2022-03-29T00:04:25,718][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][829][95] duration [1.1s], collections [1]/[5.2s], total [1.1s]/[3.2m], memory [280.6mb]->[216.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [204.6mb]->[211.9mb]/[2gb]}{[survivor] [12mb]->[4.9mb]/[0b]}
[2022-03-29T00:04:30,878][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:40,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][836][96] duration [5s], collections [1]/[6.1s], total [5s]/[3.2m], memory [280.9mb]->[217.5mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [4.9mb]->[5.5mb]/[0b]}
[2022-03-29T00:04:40,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:40,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5825300232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:40,434][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][836] overhead, spent [5s] collecting in the last [6.1s]
[2022-03-29T00:04:52,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,399][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][837][97] duration [7s], collections [1]/[1.9s], total [7s]/[3.4m], memory [217.5mb]->[297.5mb]/[2gb], all_pools {[young] [0b]->[20mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [5.5mb]->[6.5mb]/[0b]}
[2022-03-29T00:04:53,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][837] overhead, spent [7s] collecting in the last [1.9s]
[2022-03-29T00:04:53,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [10206ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9542982608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,933][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [21199ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:04:54,131][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [22.4s] publication of cluster state version [4222] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:04:59,748][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:02,960][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][842][98] duration [1.6s], collections [1]/[3.8s], total [1.6s]/[3.4m], memory [274.5mb]->[222.9mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.5mb]->[6.9mb]/[0b]}
[2022-03-29T00:05:03,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][842] overhead, spent [1.6s] collecting in the last [3.8s]
[2022-03-29T00:05:10,710][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][848][99] duration [738ms], collections [1]/[1.7s], total [738ms]/[3.4m], memory [278.9mb]->[276.5mb]/[2gb], all_pools {[young] [64mb]->[56mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.9mb]->[8.5mb]/[0b]}
[2022-03-29T00:05:10,869][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][848] overhead, spent [738ms] collecting in the last [1.7s]
[2022-03-29T00:05:15,066][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][849][100] duration [2.1s], collections [1]/[4.3s], total [2.1s]/[3.4m], memory [276.5mb]->[286.1mb]/[2gb], all_pools {[young] [56mb]->[72mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [8.5mb]->[6.1mb]/[0b]}
[2022-03-29T00:05:15,560][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][849] overhead, spent [2.1s] collecting in the last [4.3s]
[2022-03-29T00:05:15,406][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:23,360][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][853][101] duration [1.5s], collections [1]/[1.3s], total [1.5s]/[3.5m], memory [302.1mb]->[306.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.1mb]->[5.7mb]/[0b]}
[2022-03-29T00:05:23,553][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][853] overhead, spent [1.5s] collecting in the last [1.3s]
[2022-03-29T00:05:24,498][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:38,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][862][102] duration [1.9s], collections [1]/[3.6s], total [1.9s]/[3.5m], memory [297.7mb]->[226.6mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [5.7mb]->[6.6mb]/[0b]}
[2022-03-29T00:05:39,351][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][862] overhead, spent [1.9s] collecting in the last [3.6s]
[2022-03-29T00:05:49,638][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:50,083][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][867][103] duration [3.2s], collections [1]/[4.9s], total [3.2s]/[3.5m], memory [274.6mb]->[219.9mb]/[2gb], all_pools {[young] [60mb]->[48mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.6mb]->[7.9mb]/[0b]}
[2022-03-29T00:05:50,196][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][867] overhead, spent [3.2s] collecting in the last [4.9s]
[2022-03-29T00:05:50,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5207ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:05:50,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:01,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][873][104] duration [1.9s], collections [1]/[3.1s], total [1.9s]/[3.6m], memory [303.9mb]->[218mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [7.9mb]->[6mb]/[0b]}
[2022-03-29T00:06:01,826][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][873] overhead, spent [1.9s] collecting in the last [3.1s]
[2022-03-29T00:06:03,738][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:07,177][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][875][105] duration [2s], collections [1]/[1.2s], total [2s]/[3.6m], memory [270mb]->[306mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6mb]->[9.8mb]/[0b]}
[2022-03-29T00:06:07,378][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][875] overhead, spent [2s] collecting in the last [1.2s]
[2022-03-29T00:06:09,191][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][876][106] duration [861ms], collections [1]/[4.9s], total [861ms]/[3.6m], memory [306mb]->[222mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [211.9mb]->[213.2mb]/[2gb]}{[survivor] [9.8mb]->[8.8mb]/[0b]}
[2022-03-29T00:06:16,352][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][882] overhead, spent [263ms] collecting in the last [1s]
[2022-03-29T00:06:24,023][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:39,759][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][901] overhead, spent [370ms] collecting in the last [1.2s]
[2022-03-29T00:06:56,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][910][112] duration [3.6s], collections [1]/[5.6s], total [3.6s]/[3.7m], memory [295.6mb]->[221.1mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [213.2mb]->[213.2mb]/[2gb]}{[survivor] [6.3mb]->[7.8mb]/[0b]}
[2022-03-29T00:07:00,366][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][910] overhead, spent [3.6s] collecting in the last [5.6s]
[2022-03-29T00:07:01,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5455ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:07:09,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:09,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][911][113] duration [2.2s], collections [1]/[7.8s], total [2.2s]/[3.7m], memory [221.1mb]->[293.2mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [213.2mb]->[213.2mb]/[2gb]}{[survivor] [7.8mb]->[7.9mb]/[0b]}
[2022-03-29T00:07:10,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5274535049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:10,293][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][911] overhead, spent [2.2s] collecting in the last [7.8s]
[2022-03-29T00:07:10,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5674ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:07:12,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][912][114] duration [3.8s], collections [1]/[7.6s], total [3.8s]/[3.8m], memory [293.2mb]->[256.9mb]/[2gb], all_pools {[young] [80mb]->[36mb]/[0b]}{[old] [213.2mb]->[214mb]/[2gb]}{[survivor] [7.9mb]->[6.9mb]/[0b]}
[2022-03-29T00:07:12,844][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][912] overhead, spent [3.8s] collecting in the last [7.6s]
[2022-03-29T00:07:45,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:45,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11568672739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:46,839][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][920][115] duration [8.1s], collections [1]/[2.9s], total [8.1s]/[3.9m], memory [304.9mb]->[304.9mb]/[2gb], all_pools {[young] [84mb]->[84mb]/[0b]}{[old] [214mb]->[214mb]/[2gb]}{[survivor] [6.9mb]->[6.9mb]/[0b]}
[2022-03-29T00:07:51,154][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][920] overhead, spent [8.1s] collecting in the last [2.9s]
[2022-03-29T00:07:53,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [24156ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:25,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:26,172][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][922][116] duration [10.7s], collections [1]/[17.3s], total [10.7s]/[4.1m], memory [263.4mb]->[219.2mb]/[2gb], all_pools {[young] [68mb]->[28mb]/[0b]}{[old] [214mb]->[214.2mb]/[2gb]}{[survivor] [5.3mb]->[4.9mb]/[0b]}
[2022-03-29T00:08:26,563][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][922] overhead, spent [10.7s] collecting in the last [17.3s]
[2022-03-29T00:08:26,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [14086ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:26,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14086186061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:27,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:08:31,917][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24s/24014ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:08:51,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,543][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][930][117] duration [6.4s], collections [1]/[1.6s], total [6.4s]/[4.2m], memory [287.2mb]->[287.2mb]/[2gb], all_pools {[young] [68mb]->[72mb]/[0b]}{[old] [214.2mb]->[214.2mb]/[2gb]}{[survivor] [4.9mb]->[4.9mb]/[0b]}
[2022-03-29T00:08:51,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8110480180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][930] overhead, spent [6.4s] collecting in the last [1.6s]
[2022-03-29T00:08:51,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [10915ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,585][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38574}] took [8511ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:59,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][932][118] duration [3.1s], collections [1]/[1.4s], total [3.1s]/[4.3m], memory [248.1mb]->[252.1mb]/[2gb], all_pools {[young] [28mb]->[88mb]/[0b]}{[old] [214.2mb]->[214.2mb]/[2gb]}{[survivor] [5.8mb]->[5.8mb]/[0b]}
[2022-03-29T00:08:59,515][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][932] overhead, spent [3.1s] collecting in the last [1.4s]
[2022-03-29T00:08:59,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5359ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:58,854][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [15160ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:09:07,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:09:07,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][933][119] duration [5.2s], collections [1]/[6.3s], total [5.2s]/[4.4m], memory [252.1mb]->[219.6mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [214.2mb]->[214.4mb]/[2gb]}{[survivor] [5.8mb]->[5.1mb]/[0b]}
[2022-03-29T00:09:08,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6468673732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:09:08,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][933] overhead, spent [5.2s] collecting in the last [6.3s]
[2022-03-29T00:09:08,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6468ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:09:00,546][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.1s] publication of cluster state version [4229] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:09:17,459][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][939][120] duration [1s], collections [1]/[1.8s], total [1s]/[4.4m], memory [299.6mb]->[241mb]/[2gb], all_pools {[young] [80mb]->[44mb]/[0b]}{[old] [214.4mb]->[214.4mb]/[2gb]}{[survivor] [5.1mb]->[6.6mb]/[0b]}
[2022-03-29T00:09:18,067][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][939] overhead, spent [1s] collecting in the last [1.8s]
[2022-03-29T00:09:25,905][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][944] overhead, spent [550ms] collecting in the last [1.4s]
[2022-03-29T00:09:33,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][949][122] duration [1s], collections [1]/[2s], total [1s]/[4.4m], memory [297.3mb]->[220.8mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [214.4mb]->[214.4mb]/[2gb]}{[survivor] [6.9mb]->[6.3mb]/[0b]}
[2022-03-29T00:09:33,523][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][949] overhead, spent [1s] collecting in the last [2s]
[2022-03-29T00:09:41,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][955] overhead, spent [421ms] collecting in the last [1.1s]
[2022-03-29T00:09:50,430][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][962][124] duration [727ms], collections [1]/[1.7s], total [727ms]/[4.4m], memory [280.7mb]->[221.8mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [214.5mb]->[214.5mb]/[2gb]}{[survivor] [6.1mb]->[7.2mb]/[0b]}
[2022-03-29T00:09:50,735][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][962] overhead, spent [727ms] collecting in the last [1.7s]
[2022-03-29T00:09:58,941][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][968] overhead, spent [394ms] collecting in the last [1.2s]
[2022-03-29T00:10:06,183][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][974] overhead, spent [314ms] collecting in the last [1.1s]
[2022-03-29T00:10:34,686][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][998] overhead, spent [301ms] collecting in the last [1s]
[2022-03-29T00:19:22,359][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [7559] timed out after [15963ms]
[2022-03-29T00:19:29,227][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [25.7s/25720ms] ago, timed out [9.7s/9757ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [7559]
[2022-03-29T00:20:43,320][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1532][144] duration [3s], collections [1]/[5.4s], total [3s]/[4.5m], memory [1.3gb]->[224.8mb]/[2gb], all_pools {[young] [1gb]->[0b]/[0b]}{[old] [216.7mb]->[216.8mb]/[2gb]}{[survivor] [6.4mb]->[8mb]/[0b]}
[2022-03-29T00:20:46,638][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1532] overhead, spent [3s] collecting in the last [5.4s]
[2022-03-29T00:20:48,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1533][145] duration [1.8s], collections [1]/[5.6s], total [1.8s]/[4.6m], memory [224.8mb]->[260.3mb]/[2gb], all_pools {[young] [0b]->[36mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [8mb]->[7.4mb]/[0b]}
[2022-03-29T00:20:49,249][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1533] overhead, spent [1.8s] collecting in the last [5.6s]
[2022-03-29T00:20:55,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1536][146] duration [1.4s], collections [1]/[2.6s], total [1.4s]/[4.6m], memory [264.3mb]->[232.2mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [7.4mb]->[7.3mb]/[0b]}
[2022-03-29T00:20:55,988][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1536] overhead, spent [1.4s] collecting in the last [2.6s]
[2022-03-29T00:21:07,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1540][147] duration [2.6s], collections [1]/[5.4s], total [2.6s]/[4.6m], memory [248.2mb]->[226.1mb]/[2gb], all_pools {[young] [24mb]->[8mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [7.3mb]->[5.2mb]/[0b]}
[2022-03-29T00:21:08,578][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1540] overhead, spent [2.6s] collecting in the last [5.4s]
[2022-03-29T00:21:26,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:21:27,423][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1547][148] duration [3.9s], collections [1]/[6.3s], total [3.9s]/[4.7m], memory [290.1mb]->[223.5mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [5.2mb]->[6.7mb]/[0b]}
[2022-03-29T00:21:27,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1547] overhead, spent [3.9s] collecting in the last [6.3s]
[2022-03-29T00:21:27,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5198938076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:21:27,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5198ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:22:10,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:22:11,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740151619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:22:12,039][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1559][149] duration [9.9s], collections [1]/[14.7s], total [9.9s]/[4.8m], memory [279.5mb]->[224.3mb]/[2gb], all_pools {[young] [56mb]->[36mb]/[0b]}{[old] [216.8mb]->[216.9mb]/[2gb]}{[survivor] [6.7mb]->[7.4mb]/[0b]}
[2022-03-29T00:22:13,598][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1559] overhead, spent [9.9s] collecting in the last [14.7s]
[2022-03-29T00:22:19,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [21678ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:22:32,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [7550ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:22,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:22,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@b7390be] took [22529ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:23,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20728762092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:24,139][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1561][150] duration [15.1s], collections [1]/[31.2s], total [15.1s]/[5.1m], memory [268.3mb]->[233.5mb]/[2gb], all_pools {[young] [44mb]->[52mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[8.6mb]/[0b]}
[2022-03-29T00:23:24,901][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1561] overhead, spent [15.1s] collecting in the last [31.2s]
[2022-03-29T00:23:37,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,393][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38574}] took [10536ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735577427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1562][151] duration [6.3s], collections [1]/[15.2s], total [6.3s]/[5.2m], memory [233.5mb]->[268.6mb]/[2gb], all_pools {[young] [52mb]->[48mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [8.6mb]->[7.6mb]/[0b]}
[2022-03-29T00:23:38,801][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1562] overhead, spent [6.3s] collecting in the last [15.2s]
[2022-03-29T00:23:56,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77bf7db2, interval=5s}] took [7379ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:56,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:57,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379302816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:57,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1568][152] duration [5.9s], collections [1]/[8.6s], total [5.9s]/[5.3m], memory [284.6mb]->[244.3mb]/[2gb], all_pools {[young] [88mb]->[28mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.6mb]->[7.4mb]/[0b]}
[2022-03-29T00:23:57,275][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1568] overhead, spent [5.9s] collecting in the last [8.6s]
[2022-03-29T00:23:57,544][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:23:57,721][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.7s/13783ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:24:05,783][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1573][153] duration [1.2s], collections [1]/[2.7s], total [1.2s]/[5.3m], memory [272.3mb]->[232.4mb]/[2gb], all_pools {[young] [48mb]->[32mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-29T00:24:06,128][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1573] overhead, spent [1.2s] collecting in the last [2.7s]
[2022-03-29T00:24:34,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7917ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:24:34,574][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1585][154] duration [5.7s], collections [1]/[2.4s], total [5.7s]/[5.4m], memory [308.4mb]->[312.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[6.4mb]/[0b]}
[2022-03-29T00:24:34,793][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1585] overhead, spent [5.7s] collecting in the last [2.4s]
[2022-03-29T00:24:34,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7916834588ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:24:34,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [9009ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:24:41,062][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=146, version=4230}] took [33.6s] which is above the warn threshold of [30s]: [running task [Publication{term=146, version=4230}]] took [103ms], [connecting to new nodes] took [26ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [146ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [1409ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [137ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [24ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016a7960@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [29ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [144ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801316e58@7c05388] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013da000@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [34ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [12953ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [13048ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [83ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [1166ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [855ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [2138ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [44ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [492ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [42ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [232ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [24ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [127ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-03-29T00:24:52,201][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1592][155] duration [2.8s], collections [1]/[1.6s], total [2.8s]/[5.5m], memory [283.4mb]->[311.4mb]/[2gb], all_pools {[young] [60mb]->[20mb]/[0b]}{[old] [216.9mb]->[217.2mb]/[2gb]}{[survivor] [6.4mb]->[6.3mb]/[0b]}
[2022-03-29T00:24:54,240][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1592] overhead, spent [2.8s] collecting in the last [1.6s]
[2022-03-29T00:24:54,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7540ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:43,612][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1598][156] duration [21.2s], collections [1]/[2.9s], total [21.2s]/[5.8m], memory [279.5mb]->[287.5mb]/[2gb], all_pools {[young] [60mb]->[12mb]/[0b]}{[old] [217.2mb]->[217.3mb]/[2gb]}{[survivor] [6.3mb]->[6.9mb]/[0b]}
[2022-03-29T00:25:42,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:43,298][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38900}] took [26419ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:44,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25522368770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:44,144][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1598] overhead, spent [21.2s] collecting in the last [2.9s]
[2022-03-29T00:25:47,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [25722ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:47,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:54,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5748842916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:00,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:06,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13093898888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:10,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [9183ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:26:09,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:04,483][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13094ms] which is above the warn threshold of [5s]
[2022-03-29T00:26:13,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9183731479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:15,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:17,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738345569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:18,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5738ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:26:38,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:39,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18586912199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:40,206][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1601][157] duration [14.6s], collections [1]/[24.5s], total [14.6s]/[6.1m], memory [248.3mb]->[230.6mb]/[2gb], all_pools {[young] [32mb]->[8mb]/[0b]}{[old] [217.3mb]->[217.4mb]/[2gb]}{[survivor] [6.9mb]->[9.2mb]/[0b]}
[2022-03-29T00:26:43,668][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1601] overhead, spent [14.6s] collecting in the last [24.5s]
[2022-03-29T00:26:45,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7764ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:00,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:27:01,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5198711914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:27:01,190][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1605][158] duration [3.6s], collections [1]/[1.4s], total [3.6s]/[6.1m], memory [302.6mb]->[310.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [217.4mb]->[218.5mb]/[2gb]}{[survivor] [9.2mb]->[8.8mb]/[0b]}
[2022-03-29T00:27:01,507][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1605] overhead, spent [3.6s] collecting in the last [1.4s]
[2022-03-29T00:27:01,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:03,156][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:27:08,612][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1610][160] duration [722ms], collections [1]/[2s], total [722ms]/[6.2m], memory [300.1mb]->[229.8mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [218.5mb]->[219.3mb]/[2gb]}{[survivor] [9.5mb]->[6.4mb]/[0b]}
[2022-03-29T00:27:08,995][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1610] overhead, spent [722ms] collecting in the last [2s]
[2022-03-29T00:27:09,602][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:27:17,569][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1615][161] duration [1.8s], collections [1]/[3.5s], total [1.8s]/[6.2m], memory [281.8mb]->[224.6mb]/[2gb], all_pools {[young] [56mb]->[60mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [6.4mb]->[5.2mb]/[0b]}
[2022-03-29T00:27:17,782][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1615] overhead, spent [1.8s] collecting in the last [3.5s]
[2022-03-29T00:27:28,364][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1620][162] duration [2.1s], collections [1]/[3.9s], total [2.1s]/[6.2m], memory [304.6mb]->[225.8mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [5.2mb]->[6.4mb]/[0b]}
[2022-03-29T00:27:30,705][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1620] overhead, spent [2.1s] collecting in the last [3.9s]
[2022-03-29T00:27:32,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7268ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:34,658][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1621][163] duration [1.1s], collections [1]/[6.4s], total [1.1s]/[6.2m], memory [225.8mb]->[242.1mb]/[2gb], all_pools {[young] [28mb]->[16mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [6.4mb]->[6.7mb]/[0b]}
[2022-03-29T00:27:43,997][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1624][164] duration [2.5s], collections [1]/[4.9s], total [2.5s]/[6.3m], memory [246.1mb]->[268.8mb]/[2gb], all_pools {[young] [24mb]->[44mb]/[0b]}{[old] [219.3mb]->[219.5mb]/[2gb]}{[survivor] [6.7mb]->[5.3mb]/[0b]}
[2022-03-29T00:27:44,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1624] overhead, spent [2.5s] collecting in the last [4.9s]
[2022-03-29T00:27:46,126][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:28:25,542][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=146, version=4233}] took [33.9s] which is above the warn threshold of [30s]: [running task [Publication{term=146, version=4233}]] took [19ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [4283ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [154ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016a7960@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [104ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801316e58@7c05388] took [1ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013da000@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [22ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [12294ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [2013ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [70ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [2565ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [128ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [4484ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [59ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [4367ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [2060ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [635ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [64ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [22ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [55ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [0ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.bulk.TransportShardBulkAction$1@5b3a0f6a]]]] took [209ms]
[2022-03-29T00:28:36,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:28:36,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@48bc072c] took [8851ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:28:37,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8451787139ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:28:38,232][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1645][165] duration [5.2s], collections [1]/[10.6s], total [5.2s]/[6.4m], memory [304.8mb]->[249mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.3mb]->[5.5mb]/[0b]}
[2022-03-29T00:28:42,212][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1645] overhead, spent [5.2s] collecting in the last [10.6s]
[2022-03-29T00:28:44,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7916ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:13,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:15,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17099018287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:16,693][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1649][166] duration [12.3s], collections [1]/[2.7s], total [12.3s]/[6.6m], memory [265mb]->[313mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.5mb]->[8.4mb]/[0b]}
[2022-03-29T00:29:18,361][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1649] overhead, spent [12.3s] collecting in the last [2.7s]
[2022-03-29T00:29:26,870][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38922}] took [9030ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:26,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [31292ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:26,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:27,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7628433739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:28,582][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1650][167] duration [4.9s], collections [1]/[32.6s], total [4.9s]/[6.7m], memory [313mb]->[283.5mb]/[2gb], all_pools {[young] [12mb]->[56mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [8.4mb]->[8mb]/[0b]}
[2022-03-29T00:29:36,134][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1653][168] duration [2s], collections [1]/[4s], total [2s]/[6.7m], memory [311.5mb]->[227.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [8mb]->[7.5mb]/[0b]}
[2022-03-29T00:29:36,607][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1653] overhead, spent [2s] collecting in the last [4s]
[2022-03-29T00:29:58,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:59,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388181396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:59,987][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1659][169] duration [9.4s], collections [1]/[1.5s], total [9.4s]/[6.8m], memory [291.1mb]->[315.1mb]/[2gb], all_pools {[young] [64mb]->[36mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [7.5mb]->[5.5mb]/[0b]}
[2022-03-29T00:30:00,470][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1659] overhead, spent [9.4s] collecting in the last [1.5s]
[2022-03-29T00:30:00,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [12825ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:30:02,668][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:04,681][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.5s/22508ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:30:12,225][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1664][170] duration [2.5s], collections [1]/[1.3s], total [2.5s]/[6.9m], memory [281mb]->[313mb]/[2gb], all_pools {[young] [56mb]->[32mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.5mb]->[6.3mb]/[0b]}
[2022-03-29T00:30:13,033][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1664] overhead, spent [2.5s] collecting in the last [1.3s]
[2022-03-29T00:30:13,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5250ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:30:22,342][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1669] overhead, spent [1.5s] collecting in the last [1.3s]
[2022-03-29T00:30:19,539][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:34,416][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10007ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:30:35,603][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.3s] publication of cluster state version [4235] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:30:54,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:58,155][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.9s/12912ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:31:05,528][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1694][172] duration [2.2s], collections [1]/[4.3s], total [2.2s]/[7m], memory [313.8mb]->[224.6mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [6.2mb]->[5.1mb]/[0b]}
[2022-03-29T00:31:05,999][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1694] overhead, spent [2.2s] collecting in the last [4.3s]
[2022-03-29T00:31:11,293][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:31:32,738][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1710][173] duration [2.7s], collections [1]/[5.2s], total [2.7s]/[7m], memory [304.6mb]->[225.7mb]/[2gb], all_pools {[young] [80mb]->[68mb]/[0b]}{[old] [219.5mb]->[219.7mb]/[2gb]}{[survivor] [5.1mb]->[5.9mb]/[0b]}
[2022-03-29T00:31:34,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1710] overhead, spent [2.7s] collecting in the last [5.2s]
[2022-03-29T00:31:49,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:31:49,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1714][174] duration [4.5s], collections [1]/[7.5s], total [4.5s]/[7.1m], memory [305.7mb]->[230.6mb]/[2gb], all_pools {[young] [80mb]->[56mb]/[0b]}{[old] [219.7mb]->[219.8mb]/[2gb]}{[survivor] [5.9mb]->[6.7mb]/[0b]}
[2022-03-29T00:31:49,947][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1714] overhead, spent [4.5s] collecting in the last [7.5s]
[2022-03-29T00:31:49,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6081ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:31:49,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6081482670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:31:58,829][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1717][175] duration [2.7s], collections [1]/[5.2s], total [2.7s]/[7.1m], memory [294.6mb]->[229.1mb]/[2gb], all_pools {[young] [68mb]->[20mb]/[0b]}{[old] [219.8mb]->[219.8mb]/[2gb]}{[survivor] [6.7mb]->[9.2mb]/[0b]}
[2022-03-29T00:31:59,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1717] overhead, spent [2.7s] collecting in the last [5.2s]
[2022-03-29T00:32:09,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:10,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7104933778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:09,944][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1718][176] duration [5.5s], collections [1]/[4.3s], total [5.5s]/[7.2m], memory [229.1mb]->[313.1mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [219.8mb]->[220.5mb]/[2gb]}{[survivor] [9.2mb]->[7.8mb]/[0b]}
[2022-03-29T00:32:10,292][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1718] overhead, spent [5.5s] collecting in the last [4.3s]
[2022-03-29T00:32:10,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7305ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:19,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:20,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8130486128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:24,608][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1719][177] duration [6.2s], collections [1]/[19.7s], total [6.2s]/[7.3m], memory [313.1mb]->[235.5mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [220.5mb]->[220.6mb]/[2gb]}{[survivor] [7.8mb]->[6.9mb]/[0b]}
[2022-03-29T00:32:25,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1719] overhead, spent [6.2s] collecting in the last [19.7s]
[2022-03-29T00:32:28,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:51,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@2a5a3cfa] took [13915ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:51,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:52,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13715138373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:53,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1722][178] duration [10.5s], collections [1]/[18.9s], total [10.5s]/[7.5m], memory [239.5mb]->[260.5mb]/[2gb], all_pools {[young] [16mb]->[32mb]/[0b]}{[old] [220.6mb]->[220.6mb]/[2gb]}{[survivor] [6.9mb]->[7.8mb]/[0b]}
[2022-03-29T00:32:54,063][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1722] overhead, spent [10.5s] collecting in the last [18.9s]
[2022-03-29T00:33:05,135][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1726][179] duration [2.5s], collections [1]/[1.4s], total [2.5s]/[7.5m], memory [280.5mb]->[284.5mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [220.6mb]->[220.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-03-29T00:33:05,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1726] overhead, spent [2.5s] collecting in the last [1.4s]
[2022-03-29T00:33:05,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5580ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:33:15,928][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1732][180] duration [2.2s], collections [1]/[3.5s], total [2.2s]/[7.6m], memory [280.1mb]->[230.6mb]/[2gb], all_pools {[young] [52mb]->[28mb]/[0b]}{[old] [220.6mb]->[220.7mb]/[2gb]}{[survivor] [7.4mb]->[5.9mb]/[0b]}
[2022-03-29T00:33:16,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1732] overhead, spent [2.2s] collecting in the last [3.5s]
[2022-03-29T00:33:25,614][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1737][181] duration [1.5s], collections [1]/[2.5s], total [1.5s]/[7.6m], memory [262.6mb]->[225.9mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [5.9mb]->[5.2mb]/[0b]}
[2022-03-29T00:33:26,286][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1737] overhead, spent [1.5s] collecting in the last [2.5s]
[2022-03-29T00:33:32,869][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1740][182] duration [1.4s], collections [1]/[1.1s], total [1.4s]/[7.6m], memory [249.9mb]->[313.9mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [5.2mb]->[7.7mb]/[0b]}
[2022-03-29T00:33:33,862][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1740] overhead, spent [1.4s] collecting in the last [1.1s]
[2022-03-29T00:33:39,606][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1741][183] duration [3.3s], collections [1]/[8.8s], total [3.3s]/[7.7m], memory [313.9mb]->[244mb]/[2gb], all_pools {[young] [0b]->[36mb]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [7.7mb]->[7.2mb]/[0b]}
[2022-03-29T00:33:40,175][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1741] overhead, spent [3.3s] collecting in the last [8.8s]
[2022-03-29T00:33:40,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5913ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:33:54,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:33:55,548][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1745][184] duration [5.4s], collections [1]/[1.9s], total [5.4s]/[7.8m], memory [300mb]->[316mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [220.7mb]->[221.4mb]/[2gb]}{[survivor] [7.2mb]->[6.5mb]/[0b]}
[2022-03-29T00:33:55,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251125898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:33:57,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1745] overhead, spent [5.4s] collecting in the last [1.9s]
[2022-03-29T00:33:58,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [12188ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:36:54,576][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38940}] took [157718ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:36:58,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:36:58,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1751] overhead, spent [2.4m] collecting in the last [4.2s]
[2022-03-29T00:37:13,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157518145053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:16,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [158118ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:37:23,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:30,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32403ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:37:32,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32403076861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:41,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:53,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719061400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:03,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:11,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21871101760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:28,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:42,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22754831399ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:52,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:59,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@59dfe26f] took [25444ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:39:07,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25444858617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:29,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:47,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38253402042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:05,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:19,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [72483ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:40:30,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34229894778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:48,169][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38254ms] which is above the warn threshold of [5s]
[2022-03-29T00:40:55,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50282ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:59,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [50282ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:41:10,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50282026189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:24,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:37,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29904802093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:54,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:10,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30366249200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:38,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44361ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:58,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44360774509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:13,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:25,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35052698535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:42,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28188267875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:29,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:48,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45659958352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:09,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:22,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41202298523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:36,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:45,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222358581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:56,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20596ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:54,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [27222ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:45:31,110][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8761] timed out after [262450ms]
[2022-03-29T00:46:04,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20596147625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:19,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:28,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22772399224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:39,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:48,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18872320898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:57,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:10,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19129165867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:36,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:50,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37193522889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:48:10,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:48:18,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [73354ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:48:26,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36160644844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:58,594][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [37194ms] which is above the warn threshold of [5s]
[2022-03-29T00:48:50,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32263ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:48:53,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:08,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32263282354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:23,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:42,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40920058378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:58,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:14,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34033118200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:34,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:59,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35330950039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:19,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:34,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [40554ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:51:33,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40554729451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:53,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32970ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:51:50,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:09,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32970699836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:24,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:47,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37233563416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:07,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:46,571][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [13.5m/812754ms] ago, timed out [9.1m/550304ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8761]
[2022-03-29T00:53:12,636][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [43430ms] which is above the warn threshold of [5s]
[2022-03-29T00:53:17,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43430766005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:30,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:40,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21443317887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:53,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [21443ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:53:53,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:31,131][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8831] timed out after [183810ms]
[2022-03-29T00:54:04,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24962833494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:16,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:24,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21769557309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:34,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:34,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6229f0b7, interval=1m}] took [19207ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:54:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19207660074ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:51,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:05,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17114582095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:15,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:23,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22786695025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:34,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19992ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:44,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19991202445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:47,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [19991ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:55:54,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:03,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18543956361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:12,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:20,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [18888ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:56:22,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18888706251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:34,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:43,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21960425086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:53,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:04,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19812415737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:15,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:31,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21524731309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:45,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:55,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508088434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:10,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:22,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25139815057ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:31,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20703ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:37,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20702881384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [9734ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.7m/587831ms] ago, timed out [6.7m/404021ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8831]
[2022-03-29T00:58:38,416][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20703ms] which is above the warn threshold of [5s]
[2022-03-29T00:58:42,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9734592903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:42,965][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8893] timed out after [106610ms]
[2022-03-29T00:59:11,399][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:53020}] took [9368ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:59:11,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6766ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:59:29,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [8541ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:00:59,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [50720ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:02:54,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77bf7db2, interval=5s}] took [6563ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:02:59,669][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15514ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@53fb3fe1]], which exceeds the warn threshold of [10s]
[2022-03-29T01:06:06,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [109173ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:06:19,123][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [19.7s/19726ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf767083], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8cb6533a]], which exceeds the warn threshold of [10s]
[2022-03-29T01:12:21,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:14:53,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/147912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:17:02,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/148304069485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:19:42,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:22:24,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292470109777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:25:31,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:28:32,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343476050295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:31:44,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:33:17,109][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [34.8m/2089711ms] ago, timed out [33m/1983101ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8893]
[2022-03-29T01:34:55,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379004857617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:38:21,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:38:57,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@40d0cf29] took [396462ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:41:47,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396462211638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:45:41,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:48:34,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398362757695ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:04:55,636][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T03:04:55,649][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T03:04:55,650][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T03:05:01,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T03:05:01,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T03:05:01,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T03:05:01,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T03:05:01,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T03:05:01,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T03:05:01,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T03:05:01,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T03:05:01,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T03:05:01,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T03:05:01,945][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T03:05:01,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T03:05:01,947][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T03:05:01,949][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T03:05:01,950][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T03:05:01,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T03:05:01,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T03:05:01,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T03:05:01,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T03:05:01,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T03:05:01,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T03:05:01,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T03:05:01,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T03:05:01,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T03:05:01,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T03:05:01,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T03:05:01,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T03:05:01,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T03:05:01,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T03:05:01,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T03:05:01,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T03:05:01,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T03:05:01,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T03:05:01,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T03:05:01,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T03:05:01,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T03:05:01,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T03:05:01,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T03:05:01,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T03:05:01,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T03:05:01,988][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T03:05:01,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T03:05:01,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T03:05:01,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T03:05:01,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T03:05:01,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T03:05:01,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T03:05:01,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T03:05:01,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T03:05:01,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T03:05:01,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T03:05:01,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T03:05:01,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T03:05:01,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T03:05:02,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T03:05:02,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T03:05:02,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T03:05:02,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T03:05:02,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T03:05:02,098][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T03:05:02,099][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T03:05:02,589][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T03:05:16,597][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T03:05:16,604][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,606][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T03:05:16,608][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,609][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,610][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,611][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,613][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T03:05:16,614][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,615][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,616][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T03:05:16,617][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T03:05:16,619][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,620][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T03:05:17,853][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T03:05:18,122][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T03:05:19,094][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T03:05:20,124][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T03:05:20,132][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T03:05:20,273][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T03:05:20,276][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T03:05:20,608][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T03:05:23,623][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T03:05:23,826][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 147, version: 4238, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T03:05:24,110][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 147, version: 4238, reason: Publication{term=147, version=4238}
[2022-03-29T03:05:24,350][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T03:05:24,354][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T03:07:09,953][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T03:07:10,008][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T03:07:10,009][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T03:07:16,336][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T03:07:16,339][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T03:07:16,340][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T03:07:16,341][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T03:07:16,343][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T03:07:16,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T03:07:16,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T03:07:16,346][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T03:07:16,347][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T03:07:16,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T03:07:16,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T03:07:16,349][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T03:07:16,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T03:07:16,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T03:07:16,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T03:07:16,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T03:07:16,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T03:07:16,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T03:07:16,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T03:07:16,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T03:07:16,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T03:07:16,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T03:07:16,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T03:07:16,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T03:07:16,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T03:07:16,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T03:07:16,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T03:07:16,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T03:07:16,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T03:07:16,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T03:07:16,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T03:07:16,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T03:07:16,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T03:07:16,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T03:07:16,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T03:07:16,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T03:07:16,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T03:07:16,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T03:07:16,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T03:07:16,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T03:07:16,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T03:07:16,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T03:07:16,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T03:07:16,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T03:07:16,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T03:07:16,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T03:07:16,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T03:07:16,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T03:07:16,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T03:07:16,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T03:07:16,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T03:07:16,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T03:07:16,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T03:07:16,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T03:07:16,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T03:07:16,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T03:07:16,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T03:07:16,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T03:07:16,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T03:07:16,489][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T03:07:16,490][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T03:07:16,875][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T03:07:31,606][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T03:07:31,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T03:07:34,015][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T03:07:36,314][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T03:07:38,729][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T03:07:43,741][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T03:07:43,748][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T03:07:43,885][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T03:07:43,887][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T03:07:44,537][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T03:08:02,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5859ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:08:57,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@398115be] took [11029ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:09:21,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [7242ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:09:42,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [10806ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:05,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:28,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [6149ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:25,638][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9597ms] which is above the warn threshold of [5s]
[2022-03-29T03:10:45,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [9606ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:01,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5484ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:12,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:26,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5808ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:13:01,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [38032ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:13:38,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [5961ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:16:14,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [40546ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:19:59,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [83485ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:19:50,145][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [94481ms] which is above the warn threshold of [5s]
[2022-03-29T03:20:37,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [6396ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:21:37,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [42602ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:22:18,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [12325ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:23:06,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [30931ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:24:32,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [23687ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:24:09,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:24:05,332][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26489ms] which is above the warn threshold of [5s]
[2022-03-29T03:25:03,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11161483891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:17,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79343124036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:42,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:38,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [79343ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:25:50,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24763936888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:02,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:02,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [19946ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:26:12,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19946505707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:25,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:35,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22695058496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:44,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:50,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [43491ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:26:53,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20796434931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:01,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [16083ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:00,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:10,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16083290578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:20,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:23,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [19569ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:31,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19569527119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:47,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18675149083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:57,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:59,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [36440ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:07,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17765484221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:18,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:18,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [20201ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:27,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20201923492ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:31,665][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20202ms] which is above the warn threshold of [5s]
[2022-03-29T03:28:37,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:48,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20314416752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:50,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [20314ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:56,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:04,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19405727056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:13,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:20,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [16199ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:29:18,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199006411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:27,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:34,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14607565815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:40,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:44,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [13883ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:29:49,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13883279594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:56,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:02,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713177217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:07,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:07,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [11672ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:13,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11672257278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:22,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [13445ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:21,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:25,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13445878624ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:31,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:35,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [10105ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:35,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105106284ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:50,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:58,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031601477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:10,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:20,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20637983234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:22,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [20637ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:31:38,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:00,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26049946844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:21,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:39,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472240626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:54,990][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [41472ms] which is above the warn threshold of [5s]
[2022-03-29T03:33:01,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:16,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [82028ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:33:27,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40556092909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:38,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:52,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39247869018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:05,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:16,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25870121889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:29,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:39,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [49280ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:34:39,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23409927622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:53,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:55,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [24766ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:35:04,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24766154007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:18,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:29,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25961384899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:38,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:47,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [45986ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:35:46,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20024870484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:01,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:03,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@398115be] took [22584ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:36:16,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22584537964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:33,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:46,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30082484704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:01,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:19,364][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28500ms] which is above the warn threshold of [5s]
[2022-03-29T03:37:19,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28500744792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:37,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:47,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [65236ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:37:59,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36735996836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:17,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:19,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [38450ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:38:39,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38450590497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:53,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:10,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39023656366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:19,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [39023ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:39:33,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:51,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37382014529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:16,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:30,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43710863222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:47,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:59,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [73192ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:41:04,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29482028140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:28,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:40,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40986691765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:53,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:53,294][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40987ms] which is above the warn threshold of [5s]
[2022-03-29T03:42:07,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27148630597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:23,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:42,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [56030ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:42:37,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28882264745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:57,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:43:12,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35067403308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:43:29,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:00,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30558250915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:14,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [30558ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:44:22,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:40,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53907127544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:00,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:16,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38762859207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:35,151][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27834ms] which is above the warn threshold of [5s]
[2022-03-29T03:45:41,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [66597ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:45:42,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27834696628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:58,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [29621ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:45:57,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:11,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29621062502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:25,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:27,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [27311ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:46:38,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27311254743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:51,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:05,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26629393927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:18,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [26629ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:47:21,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:37,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29213070582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [31681ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:47:53,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:09,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31681459836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:27,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:40,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32890472264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:54,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:33,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [61472ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:50:38,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28582525458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:52,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:57,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [119075ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:51:04,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119075138476ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:51:16,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:51:16,565][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23023ms] which is above the warn threshold of [5s]
[2022-03-29T03:51:50,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23022818511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:54:30,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:56:03,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [182378ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:57:21,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182378665365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:00:40,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [352800ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:00:48,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:05:07,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352800395322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:08:50,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/484518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:11:04,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [484849ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:12:54,422][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [484849ms] which is above the warn threshold of [5s]
[2022-03-29T04:13:03,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/484849088500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:17:13,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:21:08,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525459272172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:25:37,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:25:39,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [503200ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:21,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T04:28:43,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503200862111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:32:14,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:35:19,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399200492532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:38:29,619][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [399200ms] which is above the warn threshold of [5s]
[2022-03-29T04:38:36,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:40:41,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [382448ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:42:09,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382448855253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:45:20,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:45:53,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [404738ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:48:05,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404738509552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:50:47,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:53:34,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326522723966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:56:50,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:57:45,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [361595ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:59:53,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361595940180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:02:55,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367708ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:03:00,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@731a4124, interval=1m}] took [367838ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:05:58,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367838217160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:09:01,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:10:42,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [357487ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:12:11,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357487506339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:04:45,476][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [367839ms] which is above the warn threshold of [5s]
[2022-03-29T05:16:57,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:20:08,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481305887369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:23:11,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:24:54,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [372270ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:26:18,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372270076985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:29:40,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/385176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:30:10,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [384905ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:59:28,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T05:32:36,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384905563778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:35:27,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:35:36,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [354537ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:38:14,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354537806701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:41:14,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:44:38,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342536657470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:44:54,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [342536ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:46:07,973][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [342537ms] which is above the warn threshold of [5s]
[2022-03-29T05:48:18,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:51:23,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426527140739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:55:22,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:58:11,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [415449ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:59:17,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415449313320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:02:01,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:05:41,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407743001912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:10:41,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:16:37,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [481674ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:19:00,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481674227636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:19:24,092][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [481674ms] which is above the warn threshold of [5s]
[2022-03-29T06:23:08,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7m/764779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:23:52,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [764703ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:26:48,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7m/764703805274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:32:18,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/551987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:35:40,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/551848255984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:39:10,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:05:56,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T06:43:06,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416961644651ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:47:47,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:47:51,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [514544ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:49:52,733][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [514545ms] which is above the warn threshold of [5s]
[2022-03-29T06:52:49,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514544217224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:05:32,659][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T07:05:32,709][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T07:05:32,710][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T07:05:39,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T07:05:39,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T07:05:39,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T07:05:39,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T07:05:39,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T07:05:39,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T07:05:39,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T07:05:39,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T07:05:39,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T07:05:39,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T07:05:39,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T07:05:39,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T07:05:39,748][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T07:05:39,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T07:05:39,751][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T07:05:39,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T07:05:39,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T07:05:39,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T07:05:39,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T07:05:39,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T07:05:39,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T07:05:39,759][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T07:05:39,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T07:05:39,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T07:05:39,762][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T07:05:39,763][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T07:05:39,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T07:05:39,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T07:05:39,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T07:05:39,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T07:05:39,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T07:05:39,770][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T07:05:39,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T07:05:39,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T07:05:39,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T07:05:39,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T07:05:39,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T07:05:39,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T07:05:39,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T07:05:39,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T07:05:39,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T07:05:39,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T07:05:39,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T07:05:39,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T07:05:39,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T07:05:39,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T07:05:39,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T07:05:39,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T07:05:39,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T07:05:39,788][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T07:05:39,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T07:05:39,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T07:05:39,790][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T07:05:39,791][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T07:05:39,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T07:05:39,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T07:05:39,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T07:05:39,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T07:05:39,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T07:05:39,888][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T07:05:39,889][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T07:05:40,424][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T07:05:53,019][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T07:05:53,023][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T07:05:54,795][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T07:05:55,031][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T07:05:56,442][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T07:05:57,819][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T07:05:57,820][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T07:05:57,947][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T07:05:57,949][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T07:05:58,262][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T07:06:00,569][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T07:06:00,732][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 148, version: 4240, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T07:06:00,943][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 148, version: 4240, reason: Publication{term=148, version=4240}
[2022-03-29T07:06:01,069][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T07:06:01,069][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T07:06:02,599][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T07:06:02,618][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T07:06:04,154][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T07:06:04,162][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T07:07:15,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [47233ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:07:34,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:07:44,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7505454611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:07:48,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:07:51,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17331856262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:00,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:02,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12032651436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:05,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:08,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5187547000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:12,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:18,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7335549680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:22,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:25,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9262079576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:28,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:30,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6140802228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:08:43,009][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=148, version=4246}] took [2.6m] which is above the warn threshold of [30s]: [running task [Publication{term=148, version=4246}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [2663ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [12ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [29ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1980@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [24ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@7c05388] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bec08@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [82ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [78ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [20ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [107ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [23ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [23ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [1ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [9ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [2ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [1ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [154752ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-03-29T07:08:43,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@4c8ef8e9] took [72894ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:08:50,300][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.encodeChangeCipherSpec(SSLSocketOutputRecord.java:233) ~[?:?]
	at sun.security.ssl.OutputRecord.changeWriteCiphers(OutputRecord.java:182) ~[?:?]
	at sun.security.ssl.ChangeCipherSpec$T10ChangeCipherSpecProducer.produce(ChangeCipherSpec.java:118) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.onProduceFinished(Finished.java:395) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.produce(Finished.java:379) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.ServerHelloDone$ServerHelloDoneConsumer.consume(ServerHelloDone.java:182) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T07:09:02,279][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][30] overhead, spent [693ms] collecting in the last [2.3s]
[2022-03-29T07:09:13,864][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T07:09:16,214][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T07:09:16,496][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T07:09:24,842][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T07:09:24,842][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T07:09:31,233][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T07:09:45,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5377ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:09:46,254][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38468}] took [11184ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:09:46,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][55][15] duration [3.1s], collections [1]/[6.4s], total [3.1s]/[5.1s], memory [191.7mb]->[132.1mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [101.9mb]->[106.4mb]/[2gb]}{[survivor] [9.7mb]->[9.6mb]/[0b]}
[2022-03-29T07:09:46,750][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][55] overhead, spent [3.1s] collecting in the last [6.4s]
[2022-03-29T07:10:02,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:10:02,497][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][61][16] duration [4.1s], collections [1]/[1.3s], total [4.1s]/[9.2s], memory [160.1mb]->[184.1mb]/[2gb], all_pools {[young] [52mb]->[80mb]/[0b]}{[old] [106.4mb]->[106.4mb]/[2gb]}{[survivor] [9.6mb]->[9.6mb]/[0b]}
[2022-03-29T07:10:03,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5751711551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:10:03,630][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][61] overhead, spent [4.1s] collecting in the last [1.3s]
[2022-03-29T07:10:04,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6151ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:10:20,723][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38464}] took [32821ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:10:20,560][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38466}] took [45292ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:10:54,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5627ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:11:11,831][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [43112ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [30] unchanged indices
[2022-03-29T07:11:16,711][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [50.3s] publication of cluster state version [4257] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T07:11:19,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@73030288] took [18185ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:11:43,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7511735217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,508][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][87][17] duration [4.5s], collections [1]/[9.1s], total [4.5s]/[13.7s], memory [202.2mb]->[128.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [111.6mb]->[115.1mb]/[2gb]}{[survivor] [10.5mb]->[13.7mb]/[0b]}
[2022-03-29T07:11:44,642][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][87] overhead, spent [4.5s] collecting in the last [9.1s]
[2022-03-29T07:11:44,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7261323415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:11:44,935][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39942}] took [7261ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:11:46,759][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16464ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T07:11:46,961][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.6s] publication of cluster state version [4258] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T07:11:52,251][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [540ms] collecting in the last [1.5s]
[2022-03-29T07:11:57,352][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [996ms] collecting in the last [1s]
[2022-03-29T07:11:59,776][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [285ms] collecting in the last [1s]
[2022-03-29T07:12:01,447][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [491ms] collecting in the last [1.4s]
[2022-03-29T07:12:16,082][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][110][25] duration [747ms], collections [1]/[1.8s], total [747ms]/[17.2s], memory [233mb]->[170.3mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [151mb]->[158.6mb]/[2gb]}{[survivor] [14mb]->[11.7mb]/[0b]}
[2022-03-29T07:12:16,526][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][110] overhead, spent [747ms] collecting in the last [1.8s]
[2022-03-29T07:12:20,480][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][26] duration [899ms], collections [1]/[2.2s], total [899ms]/[18.1s], memory [222.3mb]->[173.2mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [158.6mb]->[165.2mb]/[2gb]}{[survivor] [11.7mb]->[8mb]/[0b]}
[2022-03-29T07:12:21,925][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [899ms] collecting in the last [2.2s]
[2022-03-29T07:12:25,384][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][114][27] duration [802ms], collections [1]/[1.7s], total [802ms]/[18.9s], memory [233.2mb]->[174.7mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [165.2mb]->[165.2mb]/[2gb]}{[survivor] [8mb]->[9.5mb]/[0b]}
[2022-03-29T07:12:25,567][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][114] overhead, spent [802ms] collecting in the last [1.7s]
[2022-03-29T07:12:25,752][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [.kibana-event-log-7.16.2-000001][0], [logstash-2022.03.13][0]]]).
[2022-03-29T07:12:37,833][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:12:39,689][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:12:42,302][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [481ms] collecting in the last [1.4s]
[2022-03-29T07:12:43,814][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:12:45,484][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][129] overhead, spent [374ms] collecting in the last [1s]
[2022-03-29T07:12:52,334][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][133] overhead, spent [671ms] collecting in the last [1.1s]
[2022-03-29T07:12:56,227][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][134][34] duration [1.6s], collections [1]/[6s], total [1.6s]/[22.6s], memory [266mb]->[231.9mb]/[2gb], all_pools {[young] [0b]->[52mb]/[0b]}{[old] [172.1mb]->[172.1mb]/[2gb]}{[survivor] [6.7mb]->[7.7mb]/[0b]}
[2022-03-29T07:12:56,431][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][134] overhead, spent [1.6s] collecting in the last [6s]
[2022-03-29T07:13:12,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][36] duration [1.4s], collections [1]/[2.7s], total [1.4s]/[24.2s], memory [255.6mb]->[192.7mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [172.1mb]->[172.1mb]/[2gb]}{[survivor] [7.4mb]->[8.6mb]/[0b]}
[2022-03-29T07:13:12,722][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [1.4s] collecting in the last [2.7s]
[2022-03-29T07:13:36,310][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][156][38] duration [1.1s], collections [1]/[2.5s], total [1.1s]/[25.9s], memory [247.8mb]->[182.4mb]/[2gb], all_pools {[young] [68mb]->[16mb]/[0b]}{[old] [172.1mb]->[172.1mb]/[2gb]}{[survivor] [7.6mb]->[10.3mb]/[0b]}
[2022-03-29T07:13:36,580][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][156] overhead, spent [1.1s] collecting in the last [2.5s]
[2022-03-29T07:13:38,111][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A29032022%3Acount%3Adelay_threshold_exceeded?refresh=wait_for&require_alias=true&_source=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38520}] took [7056ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:13:44,389][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][159][39] duration [2.9s], collections [1]/[5s], total [2.9s]/[28.8s], memory [250.4mb]->[183.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [172.1mb]->[173.2mb]/[2gb]}{[survivor] [10.3mb]->[10.4mb]/[0b]}
[2022-03-29T07:13:44,732][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][159] overhead, spent [2.9s] collecting in the last [5s]
[2022-03-29T07:13:44,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5230ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:13:54,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [5120ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:13:55,773][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:13:55,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][40] duration [2.4s], collections [1]/[6.5s], total [2.4s]/[31.3s], memory [235.6mb]->[203.4mb]/[2gb], all_pools {[young] [56mb]->[24mb]/[0b]}{[old] [173.2mb]->[175.4mb]/[2gb]}{[survivor] [10.4mb]->[8mb]/[0b]}
[2022-03-29T07:13:55,923][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [2.4s] collecting in the last [6.5s]
[2022-03-29T07:14:01,683][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:14:01,935][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][41] duration [3.7s], collections [1]/[5.8s], total [3.7s]/[35s], memory [203.4mb]->[183mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [175.4mb]->[175.4mb]/[2gb]}{[survivor] [8mb]->[7.5mb]/[0b]}
[2022-03-29T07:14:02,318][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [3.7s] collecting in the last [5.8s]
[2022-03-29T07:14:03,186][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.6s/14683ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T07:14:14,886][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.4s] publication of cluster state version [4272] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T07:14:18,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][173][42] duration [1.5s], collections [1]/[2.8s], total [1.5s]/[36.6s], memory [235mb]->[182.2mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [175.4mb]->[175.4mb]/[2gb]}{[survivor] [7.5mb]->[6.8mb]/[0b]}
[2022-03-29T07:14:18,522][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][173] overhead, spent [1.5s] collecting in the last [2.8s]
[2022-03-29T07:14:21,189][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][174][43] duration [889ms], collections [1]/[3.1s], total [889ms]/[37.5s], memory [182.2mb]->[182.6mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [175.4mb]->[175.4mb]/[2gb]}{[survivor] [6.8mb]->[7.1mb]/[0b]}
[2022-03-29T07:14:21,391][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][174] overhead, spent [889ms] collecting in the last [3.1s]
[2022-03-29T07:14:30,639][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:14:45,349][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][191] overhead, spent [659ms] collecting in the last [1s]
[2022-03-29T07:14:55,625][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][48] duration [1.1s], collections [1]/[3.2s], total [1.1s]/[39.8s], memory [234.4mb]->[185.6mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [5.3mb]->[8.5mb]/[0b]}
[2022-03-29T07:14:55,866][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [1.1s] collecting in the last [3.2s]
[2022-03-29T07:15:06,400][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][203][49] duration [1.8s], collections [1]/[3.1s], total [1.8s]/[41.7s], memory [265.6mb]->[185.1mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [8.5mb]->[8mb]/[0b]}
[2022-03-29T07:15:06,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][203] overhead, spent [1.8s] collecting in the last [3.1s]
[2022-03-29T07:15:14,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:15,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5827491514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:15,104][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][50] duration [4.6s], collections [1]/[2s], total [4.6s]/[46.3s], memory [185.1mb]->[273.1mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [8mb]->[9.5mb]/[0b]}
[2022-03-29T07:15:15,801][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [4.6s] collecting in the last [2s]
[2022-03-29T07:15:18,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [10351ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:15:32,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:32,803][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][207][51] duration [3.7s], collections [1]/[1.6s], total [3.7s]/[50s], memory [190.6mb]->[206.6mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [9.5mb]->[9.5mb]/[0b]}
[2022-03-29T07:15:32,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5628212958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:15:32,827][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][207] overhead, spent [3.7s] collecting in the last [1.6s]
[2022-03-29T07:15:32,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [8036ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:15:47,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][214][52] duration [2.8s], collections [1]/[4.7s], total [2.8s]/[52.9s], memory [249.9mb]->[204.9mb]/[2gb], all_pools {[young] [88mb]->[20mb]/[0b]}{[old] [177.9mb]->[177.9mb]/[2gb]}{[survivor] [8mb]->[10.9mb]/[0b]}
[2022-03-29T07:15:48,101][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][214] overhead, spent [2.8s] collecting in the last [4.7s]
[2022-03-29T07:15:48,635][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:15:48,820][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.1s/13197ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T07:15:59,278][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][217][53] duration [2.8s], collections [1]/[4.1s], total [2.8s]/[55.7s], memory [268.9mb]->[268.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [177.9mb]->[180.8mb]/[2gb]}{[survivor] [10.9mb]->[4.9mb]/[0b]}
[2022-03-29T07:15:59,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][217] overhead, spent [2.8s] collecting in the last [4.1s]
[2022-03-29T07:15:59,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5137ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:16:18,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6101ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:16:28,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:16:28,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [6960ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:16:29,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5759648029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:16:29,609][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][224][54] duration [3.4s], collections [1]/[17.5s], total [3.4s]/[59.2s], memory [253.8mb]->[226.4mb]/[2gb], all_pools {[young] [84mb]->[40mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [4.9mb]->[5.6mb]/[0b]}
[2022-03-29T07:16:45,613][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][230][55] duration [3.3s], collections [1]/[6.3s], total [3.3s]/[1m], memory [246.4mb]->[186.9mb]/[2gb], all_pools {[young] [60mb]->[28mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [5.6mb]->[6.1mb]/[0b]}
[2022-03-29T07:16:46,319][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][230] overhead, spent [3.3s] collecting in the last [6.3s]
[2022-03-29T07:16:49,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9672ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:17:09,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:17:10,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][236][56] duration [3.3s], collections [1]/[6.3s], total [3.3s]/[1m], memory [246.9mb]->[188.1mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [6.1mb]->[7.2mb]/[0b]}
[2022-03-29T07:17:10,260][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][236] overhead, spent [3.3s] collecting in the last [6.3s]
[2022-03-29T07:17:10,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5515632988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:17:10,218][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39956}] took [5916ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:17:12,851][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][237][57] duration [1.2s], collections [1]/[2.7s], total [1.2s]/[1.1m], memory [188.1mb]->[189.4mb]/[2gb], all_pools {[young] [4mb]->[4mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [7.2mb]->[8.6mb]/[0b]}
[2022-03-29T07:17:12,998][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][237] overhead, spent [1.2s] collecting in the last [2.7s]
[2022-03-29T07:17:17,750][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][238][58] duration [1.9s], collections [1]/[1.4s], total [1.9s]/[1.1m], memory [189.4mb]->[265.4mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [180.8mb]->[180.8mb]/[2gb]}{[survivor] [8.6mb]->[8.6mb]/[0b]}
[2022-03-29T07:17:18,333][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][238] overhead, spent [1.9s] collecting in the last [1.4s]
[2022-03-29T07:17:22,651][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][240][60] duration [1s], collections [1]/[1.2s], total [1s]/[1.1m], memory [190.9mb]->[189.1mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [8.8mb]->[7mb]/[0b]}
[2022-03-29T07:17:22,993][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][240] overhead, spent [1s] collecting in the last [1.2s]
[2022-03-29T07:17:30,588][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][245][61] duration [1.2s], collections [1]/[2.5s], total [1.2s]/[1.1m], memory [273.1mb]->[191.3mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7mb]->[5.2mb]/[0b]}
[2022-03-29T07:17:31,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][245] overhead, spent [1.2s] collecting in the last [2.5s]
[2022-03-29T07:17:34,090][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:17:41,181][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][252][62] duration [889ms], collections [1]/[2s], total [889ms]/[1.2m], memory [247.3mb]->[189mb]/[2gb], all_pools {[young] [60mb]->[20mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [5.2mb]->[6.8mb]/[0b]}
[2022-03-29T07:17:41,427][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][252] overhead, spent [889ms] collecting in the last [2s]
[2022-03-29T07:17:45,824][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][253][63] duration [1.6s], collections [1]/[4.2s], total [1.6s]/[1.2m], memory [189mb]->[210.1mb]/[2gb], all_pools {[young] [20mb]->[20mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [6.8mb]->[8mb]/[0b]}
[2022-03-29T07:17:46,473][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][253] overhead, spent [1.6s] collecting in the last [4.2s]
[2022-03-29T07:17:53,565][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][256][64] duration [1.7s], collections [1]/[3.3s], total [1.7s]/[1.2m], memory [214.1mb]->[188.1mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [8mb]->[6mb]/[0b]}
[2022-03-29T07:17:53,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][256] overhead, spent [1.7s] collecting in the last [3.3s]
[2022-03-29T07:18:01,130][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][260][66] duration [797ms], collections [1]/[2.5s], total [797ms]/[1.2m], memory [264.1mb]->[190.1mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7.9mb]->[8mb]/[0b]}
[2022-03-29T07:18:01,304][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][260] overhead, spent [797ms] collecting in the last [2.5s]
[2022-03-29T07:18:09,312][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][264][67] duration [1.4s], collections [1]/[3.2s], total [1.4s]/[1.3m], memory [226.1mb]->[189.9mb]/[2gb], all_pools {[young] [40mb]->[8mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [8mb]->[7.8mb]/[0b]}
[2022-03-29T07:18:09,504][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][264] overhead, spent [1.4s] collecting in the last [3.2s]
[2022-03-29T07:18:36,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6198ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:19:02,600][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39956}] took [32318ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:19:15,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [14263ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:19:39,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [11044ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:20:24,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@639ce543] took [6470ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:20:39,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [11019ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:21:21,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:21,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [30915ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:21:23,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29915261054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:25,885][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][277][70] duration [23s], collections [1]/[57.4s], total [23s]/[1.7m], memory [205.2mb]->[209.9mb]/[2gb], all_pools {[young] [20mb]->[24mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7.1mb]->[7.8mb]/[0b]}
[2022-03-29T07:21:27,127][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][277] overhead, spent [23s] collecting in the last [57.4s]
[2022-03-29T07:21:37,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][282][71] duration [2s], collections [1]/[4s], total [2s]/[1.7m], memory [233.9mb]->[191.5mb]/[2gb], all_pools {[young] [44mb]->[24mb]/[0b]}{[old] [182.1mb]->[182.1mb]/[2gb]}{[survivor] [7.8mb]->[9.3mb]/[0b]}
[2022-03-29T07:21:38,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][282] overhead, spent [2s] collecting in the last [4s]
[2022-03-29T07:21:47,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:48,149][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][284][72] duration [4.2s], collections [1]/[7.6s], total [4.2s]/[1.8m], memory [231.5mb]->[190mb]/[2gb], all_pools {[young] [40mb]->[20mb]/[0b]}{[old] [182.1mb]->[183.5mb]/[2gb]}{[survivor] [9.3mb]->[6.5mb]/[0b]}
[2022-03-29T07:21:48,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6679219441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:21:49,123][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][284] overhead, spent [4.2s] collecting in the last [7.6s]
[2022-03-29T07:21:56,819][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][287][73] duration [2.9s], collections [1]/[4.4s], total [2.9s]/[1.8m], memory [270mb]->[189.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.5mb]->[5.8mb]/[0b]}
[2022-03-29T07:21:57,191][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][287] overhead, spent [2.9s] collecting in the last [4.4s]
[2022-03-29T07:22:07,526][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][291][74] duration [2.8s], collections [1]/[1.4s], total [2.8s]/[1.9m], memory [265.4mb]->[277.4mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [5.8mb]->[5mb]/[0b]}
[2022-03-29T07:22:08,892][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][291] overhead, spent [2.8s] collecting in the last [1.4s]
[2022-03-29T07:22:09,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7315ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:22:18,018][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][297] overhead, spent [336ms] collecting in the last [1.1s]
[2022-03-29T07:22:22,401][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][298][76] duration [1.6s], collections [1]/[3.1s], total [1.6s]/[1.9m], memory [266.4mb]->[190.1mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.8mb]->[6.5mb]/[0b]}
[2022-03-29T07:22:22,589][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][298] overhead, spent [1.6s] collecting in the last [3.1s]
[2022-03-29T07:22:36,285][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][305][77] duration [2.1s], collections [1]/[1.5s], total [2.1s]/[1.9m], memory [246.1mb]->[188.7mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.5mb]->[5.1mb]/[0b]}
[2022-03-29T07:22:36,562][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][305] overhead, spent [2.1s] collecting in the last [1.5s]
[2022-03-29T07:22:47,255][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][310][78] duration [759ms], collections [1]/[2.6s], total [759ms]/[1.9m], memory [244.7mb]->[206mb]/[2gb], all_pools {[young] [56mb]->[24mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [5.1mb]->[6.4mb]/[0b]}
[2022-03-29T07:22:47,444][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][310] overhead, spent [759ms] collecting in the last [2.6s]
[2022-03-29T07:23:02,749][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][316][79] duration [2.9s], collections [1]/[1.5s], total [2.9s]/[2m], memory [222mb]->[226mb]/[2gb], all_pools {[young] [32mb]->[40mb]/[0b]}{[old] [183.5mb]->[183.5mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-03-29T07:23:03,177][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][316] overhead, spent [2.9s] collecting in the last [1.5s]
[2022-03-29T07:23:03,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7216ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:23:25,992][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][322][80] duration [1.6s], collections [1]/[1.5s], total [1.6s]/[2m], memory [271mb]->[279mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [183.5mb]->[183.6mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-29T07:23:26,079][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][322] overhead, spent [1.6s] collecting in the last [1.5s]
[2022-03-29T07:23:31,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][325][82] duration [1.8s], collections [1]/[3.1s], total [1.8s]/[2.1m], memory [263.4mb]->[191.1mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [183.6mb]->[183.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-03-29T07:23:32,105][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][325] overhead, spent [1.8s] collecting in the last [3.1s]
[2022-03-29T07:24:21,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][359][86] duration [1.9s], collections [1]/[3.3s], total [1.9s]/[2.1m], memory [253.9mb]->[194.4mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [183.7mb]->[183.8mb]/[2gb]}{[survivor] [6.1mb]->[6.6mb]/[0b]}
[2022-03-29T07:24:21,186][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][359] overhead, spent [1.9s] collecting in the last [3.3s]
[2022-03-29T07:24:32,214][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][364][87] duration [2s], collections [1]/[3.8s], total [2s]/[2.1m], memory [258.4mb]->[191.8mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [183.8mb]->[183.8mb]/[2gb]}{[survivor] [6.6mb]->[8mb]/[0b]}
[2022-03-29T07:24:32,698][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][364] overhead, spent [2s] collecting in the last [3.8s]
[2022-03-29T07:24:41,475][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][367][88] duration [2.9s], collections [1]/[4.7s], total [2.9s]/[2.2m], memory [271.8mb]->[190.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [183.8mb]->[183.8mb]/[2gb]}{[survivor] [8mb]->[6.7mb]/[0b]}
[2022-03-29T07:24:41,908][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][367] overhead, spent [2.9s] collecting in the last [4.7s]
[2022-03-29T07:25:07,108][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][381][89] duration [1.8s], collections [1]/[1.9s], total [1.8s]/[2.2m], memory [274.6mb]->[274.6mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [183.8mb]->[183.9mb]/[2gb]}{[survivor] [6.7mb]->[4.6mb]/[0b]}
[2022-03-29T07:25:07,239][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][381] overhead, spent [1.8s] collecting in the last [1.9s]
[2022-03-29T07:25:07,539][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_transform/endpoint.metadata_*/_stats][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38466}] took [63568ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:25:09,430][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T07:25:13,206][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][386] overhead, spent [264ms] collecting in the last [1s]
[2022-03-29T07:25:29,546][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][395][91] duration [1.3s], collections [1]/[2.6s], total [1.3s]/[2.2m], memory [269.8mb]->[229.7mb]/[2gb], all_pools {[young] [80mb]->[40mb]/[0b]}{[old] [183.9mb]->[184mb]/[2gb]}{[survivor] [5.9mb]->[5.7mb]/[0b]}
[2022-03-29T07:25:29,747][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][395] overhead, spent [1.3s] collecting in the last [2.6s]
[2022-03-29T07:25:35,631][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][398][92] duration [1.5s], collections [1]/[2.7s], total [1.5s]/[2.3m], memory [241.7mb]->[192.6mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [184mb]->[184mb]/[2gb]}{[survivor] [5.7mb]->[8.5mb]/[0b]}
[2022-03-29T07:25:36,212][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][398] overhead, spent [1.5s] collecting in the last [2.7s]
[2022-03-29T07:25:41,870][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][399][93] duration [2.3s], collections [1]/[6.7s], total [2.3s]/[2.3m], memory [192.6mb]->[191.5mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [184mb]->[184mb]/[2gb]}{[survivor] [8.5mb]->[7.4mb]/[0b]}
[2022-03-29T07:25:42,135][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][399] overhead, spent [2.3s] collecting in the last [6.7s]
[2022-03-29T07:25:50,954][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][405][94] duration [1s], collections [1]/[2.6s], total [1s]/[2.3m], memory [259.5mb]->[193.2mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [184mb]->[184mb]/[2gb]}{[survivor] [7.4mb]->[9.1mb]/[0b]}
[2022-03-29T07:25:51,233][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][405] overhead, spent [1s] collecting in the last [2.6s]
[2022-03-29T07:26:38,325][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][440][100] duration [1.6s], collections [1]/[3.2s], total [1.6s]/[2.4m], memory [275.2mb]->[192mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [185.2mb]->[185.5mb]/[2gb]}{[survivor] [5.9mb]->[6.4mb]/[0b]}
[2022-03-29T07:26:38,823][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][440] overhead, spent [1.6s] collecting in the last [3.2s]
[2022-03-29T07:26:56,201][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][451][101] duration [1.6s], collections [1]/[2.8s], total [1.6s]/[2.4m], memory [276mb]->[193.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [185.5mb]->[185.5mb]/[2gb]}{[survivor] [6.4mb]->[7.8mb]/[0b]}
[2022-03-29T07:26:56,351][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][451] overhead, spent [1.6s] collecting in the last [2.8s]
[2022-03-29T07:27:00,101][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][453][102] duration [982ms], collections [1]/[2.3s], total [982ms]/[2.4m], memory [201.4mb]->[195.8mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [185.5mb]->[185.6mb]/[2gb]}{[survivor] [7.8mb]->[10.2mb]/[0b]}
[2022-03-29T07:27:00,367][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][453] overhead, spent [982ms] collecting in the last [2.3s]
[2022-03-29T07:27:02,180][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][454] overhead, spent [698ms] collecting in the last [2.2s]
[2022-03-29T07:27:10,679][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][460][104] duration [780ms], collections [1]/[1.5s], total [780ms]/[2.4m], memory [278mb]->[194.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [186.4mb]->[186.4mb]/[2gb]}{[survivor] [7.5mb]->[7.9mb]/[0b]}
[2022-03-29T07:27:10,798][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][460] overhead, spent [780ms] collecting in the last [1.5s]
[2022-03-29T07:27:48,710][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:54228}] took [5852ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:27:53,526][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][484][107] duration [2.5s], collections [1]/[5.5s], total [2.5s]/[2.5m], memory [278.4mb]->[192.8mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [186.9mb]->[187.9mb]/[2gb]}{[survivor] [7.4mb]->[4.8mb]/[0b]}
[2022-03-29T07:27:53,663][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][484] overhead, spent [2.5s] collecting in the last [5.5s]
[2022-03-29T07:28:11,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][492][108] duration [3.8s], collections [1]/[6.3s], total [3.8s]/[2.6m], memory [236.8mb]->[194.6mb]/[2gb], all_pools {[young] [44mb]->[4mb]/[0b]}{[old] [187.9mb]->[187.9mb]/[2gb]}{[survivor] [4.8mb]->[6.6mb]/[0b]}
[2022-03-29T07:28:12,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][492] overhead, spent [3.8s] collecting in the last [6.3s]
[2022-03-29T07:28:14,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7712ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:28:31,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:28:31,900][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40062}] took [7938ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:28:32,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7137852621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:28:32,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][498][109] duration [5.7s], collections [1]/[7.7s], total [5.7s]/[2.7m], memory [266.6mb]->[197.6mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [187.9mb]->[188mb]/[2gb]}{[survivor] [6.6mb]->[5.6mb]/[0b]}
[2022-03-29T07:28:32,380][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][498] overhead, spent [5.7s] collecting in the last [7.7s]
[2022-03-29T07:28:32,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [7137ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:28:49,670][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][510] overhead, spent [625ms] collecting in the last [1.6s]
[2022-03-29T07:29:29,563][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][541][113] duration [1.3s], collections [1]/[2.6s], total [1.3s]/[2.7m], memory [274.2mb]->[194.8mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [188mb]->[188mb]/[2gb]}{[survivor] [6.1mb]->[6.7mb]/[0b]}
[2022-03-29T07:29:30,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][541] overhead, spent [1.3s] collecting in the last [2.6s]
[2022-03-29T07:29:36,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:29:37,272][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40062}] took [6069ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:29:37,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5269062366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:29:39,012][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][542][114] duration [3.9s], collections [1]/[2.8s], total [3.9s]/[2.8m], memory [194.8mb]->[282.8mb]/[2gb], all_pools {[young] [16mb]->[12mb]/[0b]}{[old] [188mb]->[188mb]/[2gb]}{[survivor] [6.7mb]->[6.4mb]/[0b]}
[2022-03-29T07:29:39,746][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][542] overhead, spent [3.9s] collecting in the last [2.8s]
[2022-03-29T07:29:39,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [8705ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:29:38,976][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6270ms] which is above the warn threshold of [5s]
[2022-03-29T07:30:06,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@59aaf69e] took [11406ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:30:18,265][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][553][115] duration [2.8s], collections [1]/[5.1s], total [2.8s]/[2.8m], memory [262.5mb]->[210.7mb]/[2gb], all_pools {[young] [84mb]->[36mb]/[0b]}{[old] [188mb]->[188.3mb]/[2gb]}{[survivor] [6.4mb]->[6.4mb]/[0b]}
[2022-03-29T07:30:18,296][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:54250}] took [6118ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:30:18,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][553] overhead, spent [2.8s] collecting in the last [5.1s]
[2022-03-29T07:30:28,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][558][116] duration [2.2s], collections [1]/[3.6s], total [2.2s]/[2.8m], memory [238.7mb]->[198.6mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [6.4mb]->[6.2mb]/[0b]}
[2022-03-29T07:30:29,229][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][558] overhead, spent [2.2s] collecting in the last [3.6s]
[2022-03-29T07:30:51,447][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40110}] took [5577ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:31:09,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [5816ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:31:19,019][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40110}] took [7875ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:31:31,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [12813ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:09,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:25,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9521ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:41,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9169ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:32:51,793][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40062}] took [5477ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:33:35,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:33:35,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [39876ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:33:37,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39026060035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:33:47,976][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][573][117] duration [30s], collections [1]/[1m], total [30s]/[3.3m], memory [274.6mb]->[222.6mb]/[2gb], all_pools {[young] [80mb]->[56mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [6.2mb]->[6.2mb]/[0b]}
[2022-03-29T07:33:55,024][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][573] overhead, spent [30s] collecting in the last [1m]
[2022-03-29T07:34:02,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [27460ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:34:31,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:34:38,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7704770514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:34:43,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:34:51,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [11942ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:34:52,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11942608871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:01,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:11,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18325469405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:14,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [18325ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:35:37,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:42,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35742185185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:47,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9875ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:35:56,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9875306828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:02,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:04,161][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38628}] took [25042ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:36:17,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15166584671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:22,799][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=100][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38700}] took [25042ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:36:31,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [28310ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:36:33,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:44,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28310510904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:36:56,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:07,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25840587903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:31,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:43,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34728158666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:37:57,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:38:08,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25714824231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:38:37,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [25714ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:38:47,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:38:59,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48398127368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:39:26,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:39:38,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40898842282ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:39:56,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29169ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:07,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29169325018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:24,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:36,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28261342791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:47,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23441ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:01,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23440873809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:40:37,992][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28261ms] which is above the warn threshold of [5s]
[2022-03-29T07:41:17,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:27,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [52738ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:41:27,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29297210155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:40,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23218ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:41:43,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [23217ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:41:51,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23217537548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:42:02,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:42:21,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21340543286ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:19,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:24,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138418817371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:28,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:22,130][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2156] timed out after [99941ms]
[2022-03-29T07:44:34,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8602048987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:36,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:36,680][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][578][118] duration [1.3m], collections [1]/[3.8m], total [1.3m]/[4.7m], memory [262.6mb]->[212.3mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [6.2mb]->[8mb]/[0b]}
[2022-03-29T07:44:41,409][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][578] overhead, spent [1.3m] collecting in the last [3.8m]
[2022-03-29T07:44:40,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8979344890ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:44,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:44,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [8979ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:44:35,931][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11.6m/696632ms] ago, timed out [9.9m/596691ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2156]
[2022-03-29T07:44:48,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7792705073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:53,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:44:56,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7757070612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:00,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [7731ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:00,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:05,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7731644947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:09,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:16,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9271146047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:08,057][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7732ms] which is above the warn threshold of [5s]
[2022-03-29T07:45:21,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:21,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [11364ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:25,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11364707484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:30,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:35,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8936818156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:36,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@7297343e] took [8936ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:39,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:43,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421263256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:44,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@69cd45e3, interval=1m}] took [9421ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:45:47,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:52,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8243183524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:45:59,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:01,392][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_monitoring/bulk?system_id=kibana&system_api_version=7&interval=10000ms][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38694}] took [11293ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:03,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11293535437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:05,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [6810ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:05,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:08,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6810526296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:13,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:16,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7270372278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:08,046][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2285] timed out after [35769ms]
[2022-03-29T07:46:20,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [14149ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:19,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:24,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6879048073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:30,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:33,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9671282857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:35,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9671ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:46:36,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:38,169][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/67131ms] ago, timed out [31.3s/31362ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2285]
[2022-03-29T07:46:39,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7541861527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:44,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:49,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7507477226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:46:55,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:01,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10533671631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:07,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [10533ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:47:07,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:15,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12240162256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:20,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:26,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@61d90192] took [13336ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:47:25,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13336896820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:47:30,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:36,204][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][586][119] duration [53.3s], collections [1]/[43.9s], total [53.3s]/[5.6m], memory [256.3mb]->[280.3mb]/[2gb], all_pools {[young] [60mb]->[8mb]/[0b]}{[old] [188.3mb]->[188.3mb]/[2gb]}{[survivor] [8mb]->[6.8mb]/[0b]}
[2022-03-29T07:48:35,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9670705647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:37,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:38,332][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][586] overhead, spent [53.3s] collecting in the last [43.9s]
[2022-03-29T07:48:43,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67205262355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:44,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [76875ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:48:50,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:48:57,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572251730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:02,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:07,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11995468373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:16,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:18,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [13909ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:49:24,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13909033218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:32,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:36,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [14686ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:49:43,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14686141112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:51,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:49:57,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@692a4f58] took [20886ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:49:59,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20886574803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:04,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:11,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12735738521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:18,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:25,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891789933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:30,576][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38696}] took [26628ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:50:32,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:38,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14033471915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:48,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:50:56,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12725474984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:00,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [26758ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:51:05,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19974ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:05,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [19974ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:51:16,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19974013283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:23,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:31,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17898220188ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:41,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:48,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18037105202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:51:33,690][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2385] timed out after [91259ms]
[2022-03-29T07:51:55,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:06,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14178356488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:16,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:21,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [33415ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:52:23,141][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38708}] took [33415ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:52:22,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19236858736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:32,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:39,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17631929253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:50,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:52:57,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18027436508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:04,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:12,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14655507473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:21,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:27,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15968847691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:33,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:35,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [28885ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:53:40,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12916261873ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:48,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:50,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@44375fb2] took [14477ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:53:53,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14477496695ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:53:59,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:02,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fdfa4b9, interval=5s}] took [11320ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:54:07,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320224802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:11,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:14,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11890051372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:17,693][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38684}] took [23210ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:54:20,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:25,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [9350ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:54:29,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9350397501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:15,457][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.3m/259599ms] ago, timed out [2.8m/168340ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2385]
[2022-03-29T07:54:33,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:41,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13059267599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:48,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:54:54,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15063832894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:05,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [30753ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:55:05,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:11,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15689946309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:18,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:25,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@7ad0ca0a] took [13604ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:55:24,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13604585799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:30,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:34,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12208729163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:39,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:43,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7746847389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:40,393][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7746ms] which is above the warn threshold of [5s]
[2022-03-29T07:55:50,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:54,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12325257436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:55:57,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [12325ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:55:58,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:05,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8159140907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:13,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:20,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14371681794ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:32,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18447191323ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:56:48,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [18447ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:56:51,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:00,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20121145144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:07,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:10,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2d987ae3, interval=5s}] took [16278ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:57:14,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16278975176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:22,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:02,710][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2493] timed out after [40440ms]
[2022-03-29T07:57:29,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13745966369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:37,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:46,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15787310759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:57:55,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:58:06,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17855445456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T07:58:10,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [33642ms] which is above the warn threshold of [5000ms]
[2022-03-29T07:58:18,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T07:59:27,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23568470925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:01:10,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/170420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:03:46,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/170076598377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:06:32,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:09:12,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321529498715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:12:59,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:16:28,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386036560177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:20:06,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:23:06,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422426985185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:26:53,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/412337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:30:33,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/411911925578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:30:10,458][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.5s/21570ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2bd125f7]], which exceeds the warn threshold of [10s]
[2022-03-29T08:33:41,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:37:31,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/395053021612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:37:03,735][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.5s/10530ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2bd125f7]], which exceeds the warn threshold of [10s]
[2022-03-29T08:32:22,854][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [411912ms] which is above the warn threshold of [5s]
[2022-03-29T08:40:58,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/450979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:42:46,017][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.1m/66576ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@43cd1a8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@648c3950]], which exceeds the warn threshold of [10s]
[2022-03-29T08:44:28,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/450974927741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:47:53,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/414313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:46:09,397][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.3s/17332ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@43cd1a8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@648c3950]], which exceeds the warn threshold of [10s]
[2022-03-29T08:52:20,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/414329269587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T08:53:26,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16188247, interval=1s}] took [414329ms] which is above the warn threshold of [5000ms]
[2022-03-29T08:53:12,056][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38530}] took [865304ms] which is above the warn threshold of [5000ms]
[2022-03-29T08:55:22,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T08:55:06,056][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [52.5m/3152955ms] ago, timed out [51.8m/3112515ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wiyuKYKUQWOg2mMJwrEYeQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2493]
[2022-03-29T08:58:27,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449701722167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T09:01:36,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372684ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T09:02:35,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e5978@436b2eef] took [372685ms] which is above the warn threshold of [5000ms]
[2022-03-29T09:04:55,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372685985099ns] on relative clock which is above the warn threshold of [5000ms]
