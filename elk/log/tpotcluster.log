[2022-04-18T15:38:27,313][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-18T15:38:27,345][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-18T15:38:27,362][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-18T15:38:35,121][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-18T15:38:35,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-18T15:38:35,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-18T15:38:35,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-18T15:38:35,124][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-18T15:38:35,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-18T15:38:35,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-18T15:38:35,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-18T15:38:35,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-18T15:38:35,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-18T15:38:35,129][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-18T15:38:35,129][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-18T15:38:35,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-18T15:38:35,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-18T15:38:35,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-18T15:38:35,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-18T15:38:35,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-18T15:38:35,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-18T15:38:35,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-18T15:38:35,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-18T15:38:35,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-18T15:38:35,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-18T15:38:35,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-18T15:38:35,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-18T15:38:35,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-18T15:38:35,141][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-18T15:38:35,141][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-18T15:38:35,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-18T15:38:35,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-18T15:38:35,150][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-18T15:38:35,159][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-18T15:38:35,160][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-18T15:38:35,161][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-18T15:38:35,169][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-18T15:38:35,181][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-18T15:38:35,182][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-18T15:38:35,183][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-18T15:38:35,183][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-18T15:38:35,186][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-18T15:38:35,188][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-18T15:38:35,193][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-18T15:38:35,194][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-18T15:38:35,194][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-18T15:38:35,194][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-18T15:38:35,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-18T15:38:35,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-18T15:38:35,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-18T15:38:35,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-18T15:38:35,196][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-18T15:38:35,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-18T15:38:35,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-18T15:38:35,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-18T15:38:35,198][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-18T15:38:35,198][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-18T15:38:35,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-18T15:38:35,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-18T15:38:35,203][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-18T15:38:35,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-18T15:38:35,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-18T15:38:35,336][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-18T15:38:35,337][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-18T15:38:35,962][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-18T15:38:46,559][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-18T15:38:46,562][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-18T15:38:47,691][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-18T15:38:47,820][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-18T15:38:48,550][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-18T15:38:49,280][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-18T15:38:49,282][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-18T15:38:49,314][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-18T15:38:49,316][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-18T15:38:49,526][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-18T15:38:52,455][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-18T15:38:52,568][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HX9i-oz3S7-yVUBtKnpxEQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 266, version: 12135, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HX9i-oz3S7-yVUBtKnpxEQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-18T15:38:52,792][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HX9i-oz3S7-yVUBtKnpxEQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 266, version: 12135, reason: Publication{term=266, version=12135}
[2022-04-18T15:38:52,900][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-18T15:38:52,901][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-18T15:38:53,727][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-18T15:38:53,734][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [54] indices into cluster_state
[2022-04-18T15:38:54,600][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-18T15:38:54,601][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-18T15:38:55,210][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-18T15:38:55,393][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-18T15:38:55,712][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-18T15:38:55,715][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-18T15:38:55,715][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-18T15:38:56,019][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-18T15:38:56,099][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-18T15:38:56,178][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-18T15:38:58,508][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-18T15:39:02,502][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-18T15:39:12,182][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-18T15:39:12,338][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-18T15:39:58,894][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 733 finished with response BulkByScrollResponse[took=648.2ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-18T15:40:01,680][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 744 finished with response BulkByScrollResponse[took=3.1s,timed_out=false,sliceId=null,updated=923,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-18T15:40:08,651][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-18T15:40:55,462][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-18T15:40:55,497][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-18T15:40:55,500][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-18T15:41:03,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-18T15:41:03,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-18T15:41:03,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-18T15:41:03,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-18T15:41:03,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-18T15:41:03,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-18T15:41:03,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-18T15:41:03,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-18T15:41:03,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-18T15:41:03,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-18T15:41:03,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-18T15:41:03,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-18T15:41:03,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-18T15:41:03,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-18T15:41:03,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-18T15:41:03,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-18T15:41:03,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-18T15:41:03,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-18T15:41:03,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-18T15:41:03,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-18T15:41:03,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-18T15:41:03,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-18T15:41:03,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-18T15:41:03,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-18T15:41:03,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-18T15:41:03,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-18T15:41:03,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-18T15:41:03,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-18T15:41:03,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-18T15:41:03,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-18T15:41:03,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-18T15:41:03,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-18T15:41:03,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-18T15:41:03,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-18T15:41:03,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-18T15:41:03,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-18T15:41:03,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-18T15:41:03,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-18T15:41:03,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-18T15:41:03,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-18T15:41:03,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-18T15:41:03,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-18T15:41:03,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-18T15:41:03,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-18T15:41:03,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-18T15:41:03,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-18T15:41:03,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-18T15:41:03,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-18T15:41:03,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-18T15:41:03,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-18T15:41:03,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-18T15:41:03,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-18T15:41:03,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-18T15:41:03,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-18T15:41:03,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-18T15:41:03,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-18T15:41:03,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-18T15:41:03,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-18T15:41:03,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-18T15:41:03,520][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.2gb], net total_space [125.8gb], types [ext4]
[2022-04-18T15:41:03,522][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-18T15:41:04,110][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-18T15:41:20,561][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-18T15:41:20,565][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-18T15:41:20,567][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-18T15:41:20,569][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-18T15:41:20,571][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T15:41:20,572][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-18T15:41:20,573][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-18T15:41:20,574][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T15:41:20,575][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-18T15:41:20,576][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-18T15:41:20,577][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-18T15:41:20,578][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-18T15:41:20,579][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-18T15:41:20,580][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T15:41:20,581][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-18T15:41:22,396][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-18T15:41:22,577][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-18T15:41:23,965][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-18T15:41:25,656][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-18T15:41:25,657][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-18T15:41:25,712][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-18T15:41:25,717][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-18T15:41:26,211][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-18T15:41:30,502][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-18T15:41:30,757][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{D5QMiC6pQqe45DJkZm0Ejw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 267, version: 12200, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{D5QMiC6pQqe45DJkZm0Ejw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-18T15:41:31,106][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{D5QMiC6pQqe45DJkZm0Ejw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 267, version: 12200, reason: Publication{term=267, version=12200}
[2022-04-18T15:41:31,361][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-18T15:41:31,363][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-18T15:41:33,184][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-18T15:41:33,199][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [54] indices into cluster_state
[2022-04-18T15:41:35,073][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-18T15:41:35,075][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-18T15:41:36,589][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-18T15:41:36,702][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-18T15:41:36,716][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-18T15:41:36,729][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-18T15:41:36,936][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-18T15:41:37,359][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-18T15:41:37,999][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-18T15:41:38,014][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-18T15:41:38,017][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-18T15:41:38,357][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-18T15:41:38,548][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-18T15:41:38,798][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-18T15:41:43,119][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-18T15:41:50,083][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][24] overhead, spent [265ms] collecting in the last [1s]
[2022-04-18T15:41:50,857][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-18T15:42:20,737][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.18] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-18T15:42:20,898][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.18][0]]]).
[2022-04-18T15:42:21,223][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:21,234][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:21,320][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:21,549][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:25,485][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:26,258][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:26,349][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:26,378][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:27,244][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:30,275][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:31,317][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:31,416][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:42:36,289][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:43:05,367][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:43:08,342][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:43:36,633][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:43:44,630][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:44:07,657][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:44:22,452][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:44:22,692][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:44:22,765][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:44:22,875][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:45:06,740][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:45:10,461][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:45:11,456][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:45:35,804][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:45:35,868][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:03,514][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:03,583][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:04,842][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:04,923][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:20,535][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:20,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:20,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:20,610][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:20,853][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:43,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:43,668][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:46:43,674][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:48:23,004][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:48:45,018][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:52:07,790][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:55:10,847][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:55:10,935][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:55:11,413][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:57:22,435][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:57:22,516][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:57:43,115][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:57:44,035][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:59:39,588][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T15:59:39,674][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T16:00:17,180][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T16:03:45,961][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T16:04:11,246][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T16:04:11,936][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T16:05:18,979][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T16:06:09,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@352f65f9, interval=1s}] took [5856ms] which is above the warn threshold of [5000ms]
[2022-04-18T16:06:30,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5119ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:11:52,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5118415384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:11:37,481][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/94006ms] to compute cluster state update for [put-mapping [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg][_doc]], which exceeds the warn threshold of [10s]
[2022-04-18T16:12:39,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:13:22,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379797373563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:15:00,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:20:21,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149621858333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:20:52,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:28:04,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363851742281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:29:54,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/541981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:30:55,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/541981233678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:34:04,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/233063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:36:56,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232907962805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:40:49,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:42:55,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404285486166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:46:08,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:49:59,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312256070206ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:53:10,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/436158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T16:56:31,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/435960248531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T16:59:47,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396825ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:03:06,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397184899834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T17:07:38,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9m/476082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:08:07,219][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [55.2m/3313707ms] to notify listeners on unchanged cluster state for [put-mapping [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg][_doc]], which exceeds the warn threshold of [10s]
[2022-04-18T17:11:38,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9m/476082060627ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T17:14:44,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/402333ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:13:07,699][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12s/12034ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T17:19:47,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/402333323857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T17:25:14,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3m/618739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:25:22,532][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.1m/69591ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T17:31:19,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3m/618015005191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T17:41:20,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/998179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:45:34,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/998347888820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T17:45:32,480][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [17m/1020348ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [1.3h/4751201ms]
[2022-04-18T17:49:21,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:49:44,475][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [41.4m/2486228ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [1.5h/5636745ms]
[2022-04-18T17:52:24,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467531442382ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T17:53:31,934][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14s/14048ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T17:55:48,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T17:58:19,627][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1m/62916ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T17:59:08,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396010425931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:02:03,590][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [48m/2882238ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [35.4m/2128337ms]
[2022-04-18T18:02:07,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/373900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T18:05:40,334][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [54.2m/3256062ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [37.5m/2253574ms]
[2022-04-18T18:18:58,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/373824024217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:03:03,158][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [396010ms] which is above the warn threshold of [5s]
[2022-04-18T18:21:19,180][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.8s/10846ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T18:22:24,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20m/1204056ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T18:26:06,210][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [40.9s/40913ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T18:27:22,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20m/1204493251994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:30:29,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T18:30:35,605][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.2h/4460555ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [24.8m/1490044ms]
[2022-04-18T18:33:31,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/502714272727ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:34:30,994][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.3h/4963270ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [28.3m/1703523ms]
[2022-04-18T18:36:54,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T18:38:59,852][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.4h/5350526ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [22.1m/1328385ms]
[2022-04-18T18:41:43,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387256497783ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:46:29,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/558882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T18:46:55,975][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.8s/17803ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8c51e9c6]], which exceeds the warn threshold of [10s]
[2022-04-18T18:49:52,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/559011549400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:51:25,691][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.6h/5909538ms] which is longer than the warn threshold of [300000ms]; there are currently [1] pending tasks, the oldest of which has age [23.6m/1421458ms]
[2022-04-18T18:45:13,142][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.4h/8677536ms] ago, timed out [1.3h/4963270ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{D5QMiC6pQqe45DJkZm0Ejw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [12679]
[2022-04-18T18:55:28,611][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.7h/6363440ms] which is longer than the warn threshold of [300000ms]; there are currently [1] pending tasks, the oldest of which has age [1.8m/112574ms]
[2022-04-18T18:54:18,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T18:59:25,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453901902116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T18:59:30,239][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.5s/10544ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2b5e462]], which exceeds the warn threshold of [10s]
[2022-04-18T19:02:41,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/535165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:04:05,673][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.3s/21352ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ed828e85], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3e1d37bd]], which exceeds the warn threshold of [10s]
[2022-04-18T19:07:14,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/535236384899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:11:11,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:17:09,512][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-18T19:17:09,543][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-18T19:17:09,544][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-18T19:17:14,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-18T19:17:14,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-18T19:17:14,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-18T19:17:14,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-18T19:17:14,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-18T19:17:14,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-18T19:17:14,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-18T19:17:14,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-18T19:17:14,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-18T19:17:14,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-18T19:17:14,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-18T19:17:14,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-18T19:17:14,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-18T19:17:14,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-18T19:17:14,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-18T19:17:14,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-18T19:17:14,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-18T19:17:14,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-18T19:17:14,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-18T19:17:14,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-18T19:17:14,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-18T19:17:14,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-18T19:17:14,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-18T19:17:14,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-18T19:17:14,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-18T19:17:14,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-18T19:17:14,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-18T19:17:15,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-18T19:17:15,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-18T19:17:15,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-18T19:17:15,001][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-18T19:17:15,001][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-18T19:17:15,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-18T19:17:15,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-18T19:17:15,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-18T19:17:15,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-18T19:17:15,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-18T19:17:15,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-18T19:17:15,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-18T19:17:15,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-18T19:17:15,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-18T19:17:15,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-18T19:17:15,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-18T19:17:15,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-18T19:17:15,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-18T19:17:15,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-18T19:17:15,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-18T19:17:15,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-18T19:17:15,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-18T19:17:15,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-18T19:17:15,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-18T19:17:15,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-18T19:17:15,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-18T19:17:15,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-18T19:17:15,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-18T19:17:15,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-18T19:17:15,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-18T19:17:15,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-18T19:17:15,012][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-18T19:17:15,091][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.2gb], net total_space [125.8gb], types [ext4]
[2022-04-18T19:17:15,092][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-18T19:17:15,602][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-18T19:17:26,571][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-18T19:17:26,597][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-18T19:17:26,605][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-18T19:17:26,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-18T19:17:26,618][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T19:17:26,624][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-18T19:17:26,635][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-18T19:17:26,638][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T19:17:26,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-18T19:17:26,640][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-18T19:17:26,640][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-18T19:17:26,647][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-18T19:17:26,649][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-18T19:17:26,650][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T19:17:26,651][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-18T19:17:28,496][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-18T19:17:28,717][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-18T19:17:29,879][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-18T19:17:31,050][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-18T19:17:31,057][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-18T19:17:31,201][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-18T19:17:31,204][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-18T19:17:31,544][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-18T19:17:34,488][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-18T19:17:34,627][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 268, version: 12315, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-18T19:17:34,820][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 268, version: 12315, reason: Publication{term=268, version=12315}
[2022-04-18T19:17:34,953][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-18T19:17:34,956][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-18T19:17:35,876][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-18T19:17:35,891][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [55] indices into cluster_state
[2022-04-18T19:17:36,851][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-18T19:17:36,858][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-18T19:17:37,978][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-18T19:17:38,142][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-18T19:17:38,600][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-18T19:17:38,668][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-18T19:17:38,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-18T19:17:38,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-18T19:17:39,340][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-18T19:17:39,668][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-18T19:17:41,876][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-18T19:18:08,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [5703ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:18:16,913][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [17953ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [51] unchanged indices
[2022-04-18T19:18:25,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@6d225137] took [12058ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:18:25,418][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35638}] took [23523ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:18:25,418][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35668}] took [11258ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:18:25,418][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35664}] took [11258ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:18:46,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7fb7af6c] took [6122ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:19:02,725][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [36167ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [53] unchanged indices
[2022-04-18T19:19:06,047][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [39.1s] publication of cluster state version [12374] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-18T19:19:18,410][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [353] timed out after [35138ms]
[2022-04-18T19:19:31,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:19:32,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7715007221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:19:35,125][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][39][15] duration [5.1s], collections [1]/[8.6s], total [5.1s]/[6s], memory [570.8mb]->[251.1mb]/[2gb], all_pools {[young] [340mb]->[0b]/[0b]}{[old] [151.3mb]->[227.1mb]/[2gb]}{[survivor] [79.5mb]->[24mb]/[0b]}
[2022-04-18T19:19:36,166][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][39] overhead, spent [5.1s] collecting in the last [8.6s]
[2022-04-18T19:19:39,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [7569ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:19:52,955][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][43][16] duration [1.8s], collections [1]/[1.3s], total [1.8s]/[7.8s], memory [267.1mb]->[323.1mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [227.1mb]->[247.9mb]/[2gb]}{[survivor] [24mb]->[5.5mb]/[0b]}
[2022-04-18T19:19:53,022][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=268, version=12374}] took [41.5s] which is above the warn threshold of [30s]: [running task [Publication{term=268, version=12374}]] took [0ms], [connecting to new nodes] took [42ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@7125b17e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@53568c8d] took [34232ms], [org.elasticsearch.script.ScriptService@77d7f3bd] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [0ms], [org.elasticsearch.snapshots.RestoreService@545e2b62] took [0ms], [org.elasticsearch.ingest.IngestService@330bf780] took [788ms], [org.elasticsearch.action.ingest.IngestActionForwarder@7b89a047] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1dc0@1561bcfb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@30c1900e] took [49ms], [org.elasticsearch.tasks.TaskManager@5c1ad661] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6a27ce3d] took [93ms], [org.elasticsearch.cluster.InternalClusterInfoService@6cddeebb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@5717550f] took [93ms], [org.elasticsearch.indices.SystemIndexManager@4b13f745] took [551ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6c5e84a2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@7b10308c] took [56ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7f52d7e5] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1c1a9882] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@79833adb] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@c509a28] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1c740da] took [5366ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4180bb24] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@2b40f112] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@6f5d26c5] took [76ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@683c6f37] took [24ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d5180b5] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1352c6c] took [27ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@7585981] took [28ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@c96db8a] took [1ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@40cc7c7a] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48465cc1] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@64fb9a68] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4c5c1d52] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@4542b026] took [27ms], [org.elasticsearch.node.ResponseCollectorService@7dd25a3c] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@68f51596] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@44ef2357] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@14fc786c] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@4846aada] took [0ms], [org.elasticsearch.indices.store.IndicesStore@3c4d0c98] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6e57d221] took [0ms], [org.elasticsearch.license.LicenseService@38d65a50] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1146ca19] took [23ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@2449beb0] took [0ms], [org.elasticsearch.gateway.GatewayService@2cee6e3d] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2b7acc5f] took [0ms]
[2022-04-18T19:19:53,054][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54320}] took [21530ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:19:53,269][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][43] overhead, spent [1.8s] collecting in the last [1.3s]
[2022-04-18T19:19:55,164][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/89177ms] ago, timed out [54s/54039ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [353]
[2022-04-18T19:20:04,822][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][49][17] duration [972ms], collections [1]/[2.6s], total [972ms]/[8.8s], memory [333.5mb]->[254.5mb]/[2gb], all_pools {[young] [80mb]->[44mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [5.5mb]->[6.5mb]/[0b]}
[2022-04-18T19:20:07,837][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][49] overhead, spent [972ms] collecting in the last [2.6s]
[2022-04-18T19:20:10,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [6120ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:20:15,943][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12738ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [54] unchanged indices
[2022-04-18T19:20:19,371][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][50][18] duration [1.6s], collections [1]/[11.6s], total [1.6s]/[10.4s], memory [254.5mb]->[295.7mb]/[2gb], all_pools {[young] [44mb]->[44mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [6.5mb]->[7.7mb]/[0b]}
[2022-04-18T19:20:22,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [5560ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:20:22,329][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54316}] took [74162ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:20:30,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:38,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5794628990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:41,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:45,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12321216845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:45,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [12321ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:20:47,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:52,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5606821013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:56,393][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35664}] took [17928ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:20:56,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:20:59,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9570756261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:21:04,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:21:04,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [16779ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:21:08,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7209076207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:21:07,968][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54308}] took [7209ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:21:09,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@720b929a, interval=5s}] took [5064ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:21:28,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [17615ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:21:49,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@720b929a, interval=5s}] took [14767ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:21:49,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:21:50,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12306552926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:29,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:36,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36106126587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:40,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:44,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10934384078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:48,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:51,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7731079699ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:22:51,379][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=268, version=12375}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=268, version=12375}]] took [124ms], [connecting to new nodes] took [693ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@7125b17e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@53568c8d] took [9927ms], [org.elasticsearch.script.ScriptService@77d7f3bd] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [0ms], [org.elasticsearch.snapshots.RestoreService@545e2b62] took [0ms], [org.elasticsearch.ingest.IngestService@330bf780] took [647ms], [org.elasticsearch.action.ingest.IngestActionForwarder@7b89a047] took [120ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1dc0@1561bcfb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@30c1900e] took [76ms], [org.elasticsearch.tasks.TaskManager@5c1ad661] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6a27ce3d] took [153ms], [org.elasticsearch.cluster.InternalClusterInfoService@6cddeebb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@5717550f] took [155ms], [org.elasticsearch.indices.SystemIndexManager@4b13f745] took [5373ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6c5e84a2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@7b10308c] took [432ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7f52d7e5] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1c1a9882] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@79833adb] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@c509a28] took [148ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1c740da] took [38331ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4180bb24] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@2b40f112] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@6f5d26c5] took [16558ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@683c6f37] took [184ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d5180b5] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1352c6c] took [18398ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [879ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@7585981] took [40084ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@c96db8a] took [187ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@40cc7c7a] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48465cc1] took [10792ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@64fb9a68] took [4212ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4c5c1d52] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@4542b026] took [948ms], [org.elasticsearch.node.ResponseCollectorService@7dd25a3c] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@68f51596] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@44ef2357] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@14fc786c] took [85ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@4846aada] took [218ms], [org.elasticsearch.indices.store.IndicesStore@3c4d0c98] took [446ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6e57d221] took [0ms], [org.elasticsearch.license.LicenseService@38d65a50] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1146ca19] took [171ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@2449beb0] took [0ms], [org.elasticsearch.gateway.GatewayService@2cee6e3d] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2b7acc5f] took [0ms]
[2022-04-18T19:22:56,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:00,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7555969924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:05,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:10,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10142653599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:24,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:27,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18304909613ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:27,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][54][19] duration [23.8s], collections [1]/[1.6m], total [23.8s]/[34.3s], memory [319.7mb]->[312.3mb]/[2gb], all_pools {[young] [64mb]->[68mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [7.7mb]->[8.3mb]/[0b]}
[2022-04-18T19:23:32,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [36003ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:23:32,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:41,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8944224396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:44,843][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-04-18T19:23:46,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:49,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12881573703ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:23:48,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@720b929a, interval=5s}] took [12881ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:23:49,129][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [37.5s/37584ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [141], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [141], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [141], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [141], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-18T19:23:51,806][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12881ms] which is above the warn threshold of [5s]
[2022-04-18T19:24:11,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:15,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10709830792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:20,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:24,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8831943666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:25,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72b6389d, interval=5s}] took [28593ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:24:29,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:35,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9069615215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:38,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:43,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9487706007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:53,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:24:57,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@226b7db7] took [22066ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:24:58,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579194991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:16,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:18,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25441183545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:23,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:27,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6543880926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:28,950][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][55][20] duration [11s], collections [1]/[2.6m], total [11s]/[45.4s], memory [312.3mb]->[260.3mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [8.3mb]->[8.3mb]/[0b]}
[2022-04-18T19:25:28,318][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35686}] took [5288ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:25:30,849][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [79552ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [54] unchanged indices
[2022-04-18T19:25:36,176][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.6m] publication of cluster state version [12376] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-18T19:25:44,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:45,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5913916975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:25:48,245][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][59][21] duration [4.3s], collections [1]/[7.4s], total [4.3s]/[49.8s], memory [316.3mb]->[254.6mb]/[2gb], all_pools {[young] [60mb]->[20mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [8.3mb]->[6.6mb]/[0b]}
[2022-04-18T19:25:50,294][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][59] overhead, spent [4.3s] collecting in the last [7.4s]
[2022-04-18T19:25:50,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [6557ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:26:15,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:26:16,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11200019700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:26:17,247][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][63][22] duration [7.7s], collections [1]/[19.5s], total [7.7s]/[57.5s], memory [298.6mb]->[267.8mb]/[2gb], all_pools {[young] [44mb]->[16mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [6.6mb]->[7.8mb]/[0b]}
[2022-04-18T19:26:17,899][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][63] overhead, spent [7.7s] collecting in the last [19.5s]
[2022-04-18T19:26:27,405][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=268, version=12376}] took [48.6s] which is above the warn threshold of [30s]: [running task [Publication{term=268, version=12376}]] took [0ms], [connecting to new nodes] took [6117ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@7125b17e] took [44ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@53568c8d] took [587ms], [org.elasticsearch.script.ScriptService@77d7f3bd] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [0ms], [org.elasticsearch.snapshots.RestoreService@545e2b62] took [0ms], [org.elasticsearch.ingest.IngestService@330bf780] took [232ms], [org.elasticsearch.action.ingest.IngestActionForwarder@7b89a047] took [60ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1dc0@1561bcfb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@30c1900e] took [0ms], [org.elasticsearch.tasks.TaskManager@5c1ad661] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6a27ce3d] took [1365ms], [org.elasticsearch.cluster.InternalClusterInfoService@6cddeebb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@5717550f] took [147ms], [org.elasticsearch.indices.SystemIndexManager@4b13f745] took [333ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6c5e84a2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@7b10308c] took [47ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7f52d7e5] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1c1a9882] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@79833adb] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@c509a28] took [48ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1c740da] took [12949ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4180bb24] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@2b40f112] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@6f5d26c5] took [17195ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@683c6f37] took [53ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d5180b5] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1352c6c] took [1676ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [311ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@7585981] took [2388ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@c96db8a] took [41ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@40cc7c7a] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48465cc1] took [2456ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@64fb9a68] took [1810ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4c5c1d52] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@4542b026] took [295ms], [org.elasticsearch.node.ResponseCollectorService@7dd25a3c] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@68f51596] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@44ef2357] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@14fc786c] took [34ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@4846aada] took [1ms], [org.elasticsearch.indices.store.IndicesStore@3c4d0c98] took [40ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6e57d221] took [0ms], [org.elasticsearch.license.LicenseService@38d65a50] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1146ca19] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@2449beb0] took [0ms], [org.elasticsearch.gateway.GatewayService@2cee6e3d] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2b7acc5f] took [0ms]
[2022-04-18T19:26:39,064][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35686}] took [7942ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:26:43,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [8176ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:26:53,842][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35702}] took [6697ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:26:59,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@2877110b] took [5739ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:27:19,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9913ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:27:08,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [6124ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:27:19,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9912627981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:27:28,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][71][23] duration [6.7s], collections [1]/[18.5s], total [6.7s]/[1m], memory [339.8mb]->[286.6mb]/[2gb], all_pools {[young] [84mb]->[28mb]/[0b]}{[old] [247.9mb]->[247.9mb]/[2gb]}{[survivor] [7.8mb]->[10.6mb]/[0b]}
[2022-04-18T19:27:29,833][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][71] overhead, spent [6.7s] collecting in the last [18.5s]
[2022-04-18T19:27:30,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [9715ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:27:38,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:27:48,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [6004ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:27:57,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:28:10,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@10e3a10b] took [7805ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:28:29,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:29:07,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:09,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19360310192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:13,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:09,088][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21762ms] which is above the warn threshold of [5s]
[2022-04-18T19:29:16,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6530610859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:10,179][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35710}] took [25164ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:29:21,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:24,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8178790204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:31,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:33,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10087446655ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:32,944][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [939] timed out after [70532ms]
[2022-04-18T19:29:43,664][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35710}] took [12296ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:29:43,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:53,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12296394018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:54,589][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][76][24] duration [12.8s], collections [1]/[1.1m], total [12.8s]/[1.2m], memory [326.6mb]->[307.6mb]/[2gb], all_pools {[young] [68mb]->[72mb]/[0b]}{[old] [247.9mb]->[250.2mb]/[2gb]}{[survivor] [10.6mb]->[9.3mb]/[0b]}
[2022-04-18T19:29:43,216][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/80620ms] ago, timed out [10s/10088ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [939]
[2022-04-18T19:29:55,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:29:57,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [35047ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:29:58,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12663356904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:04,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:08,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7631347782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:15,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:22,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11391794031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:29,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:39,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13255428187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:49,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:30:57,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20204548631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:05,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:11,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16505652033ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:19,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13547ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:26,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13546980273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:40,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20838ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:47,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20837810338ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:50,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [34384ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:31:54,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:31:52,230][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35710}] took [71095ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:32:22,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13819947794ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:32:29,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:32:29,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@2e4a65dc] took [36674ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:32:52,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36674243523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:32:53,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:32:58,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24102499125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:32:58,890][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][78][25] duration [16.3s], collections [1]/[1.8m], total [16.3s]/[1.5m], memory [339.6mb]->[274.6mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [250.2mb]->[251.3mb]/[2gb]}{[survivor] [9.3mb]->[7.3mb]/[0b]}
[2022-04-18T19:32:58,927][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T19:32:58,985][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T19:32:59,013][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [4.7m/282085ms] to compute cluster state update for [put-mapping [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg][_doc, _doc, _doc, _doc]], which exceeds the warn threshold of [10s]
[2022-04-18T19:32:59,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:32:59,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5222831550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:01,928][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][26] duration [927ms], collections [1]/[1.8s], total [927ms]/[1.5m], memory [330.6mb]->[259mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [251.3mb]->[251.3mb]/[2gb]}{[survivor] [7.3mb]->[7.7mb]/[0b]}
[2022-04-18T19:33:02,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [927ms] collecting in the last [1.8s]
[2022-04-18T19:33:20,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72b6389d, interval=5s}] took [5458ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:33:20,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:26,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5458426076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:29,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8952ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:29,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@720b929a, interval=5s}] took [8951ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:33:33,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8951907278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:36,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:40,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8042342883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:42,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5950ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:45,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5949320151ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:45,635][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][90][31] duration [3.9s], collections [1]/[19.3s], total [3.9s]/[1.6m], memory [323.5mb]->[262.9mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [255.5mb]->[255.5mb]/[2gb]}{[survivor] [4mb]->[7.3mb]/[0b]}
[2022-04-18T19:33:50,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:50,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [13991ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:33:54,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7027299064ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:56,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6646ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:33:58,470][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35722}] took [19623ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:33:58,470][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35724}] took [19623ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:34:01,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6646334839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:03,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:07,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7137580583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:11,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@78475b20] took [14394ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:34:11,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:14,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7256989671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:18,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:25,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7340283868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:30,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11603ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:35,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11603137843ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:34:38,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:35:02,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8409945807ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:35:05,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:35:08,176][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][91][32] duration [16s], collections [1]/[1m], total [16s]/[1.9m], memory [262.9mb]->[342.9mb]/[2gb], all_pools {[young] [0b]->[28mb]/[0b]}{[old] [255.5mb]->[255.5mb]/[2gb]}{[survivor] [7.3mb]->[9.9mb]/[0b]}
[2022-04-18T19:35:08,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26982152121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:35:10,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:35:09,944][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][91] overhead, spent [16s] collecting in the last [1m]
[2022-04-18T19:35:11,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5004528108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:35:11,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [40396ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:35:35,252][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35748}] took [12144ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:35:49,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [6878ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:35:47,466][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6107ms] which is above the warn threshold of [5s]
[2022-04-18T19:36:05,684][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-18T19:36:06,695][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][97][33] duration [2.7s], collections [1]/[5.5s], total [2.7s]/[1.9m], memory [333.5mb]->[266.7mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [255.5mb]->[257.4mb]/[2gb]}{[survivor] [9.9mb]->[9.2mb]/[0b]}
[2022-04-18T19:36:07,329][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [2.7s] collecting in the last [5.5s]
[2022-04-18T19:36:07,355][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.8s/10867ms] to compute cluster state update for [put-mapping [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg][_doc]], which exceeds the warn threshold of [10s]
[2022-04-18T19:36:13,806][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35752}] took [6285ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:36:15,838][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][98][34] duration [1.9s], collections [1]/[9.4s], total [1.9s]/[1.9m], memory [266.7mb]->[271.9mb]/[2gb], all_pools {[young] [4mb]->[8mb]/[0b]}{[old] [257.4mb]->[258.7mb]/[2gb]}{[survivor] [9.2mb]->[5.2mb]/[0b]}
[2022-04-18T19:36:18,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [5014ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:36:28,331][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][99][35] duration [1.7s], collections [1]/[6s], total [1.7s]/[2m], memory [271.9mb]->[336mb]/[2gb], all_pools {[young] [8mb]->[80mb]/[0b]}{[old] [258.7mb]->[258.7mb]/[2gb]}{[survivor] [5.2mb]->[5.2mb]/[0b]}
[2022-04-18T19:36:36,653][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [1.7s] collecting in the last [6s]
[2022-04-18T19:36:42,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [22389ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:37:20,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:37:21,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72b6389d, interval=5s}] took [27566ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:37:27,141][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/config%3A7.17.0][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54312}] took [27316ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:37:26,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26714762721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:37:28,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:37:32,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8761659258ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:37:32,859][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35752}] took [8761ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:37:32,859][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35754}] took [8761ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:37:36,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:37:45,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7762272177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:37:46,956][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35748}] took [7763ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:37:36,484][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [72761ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [54] unchanged indices
[2022-04-18T19:37:53,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:08,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17247476020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:19,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:32,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26102564965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:10,640][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [12378] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WFoSrOobSymyd8m524OVEA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-18T19:38:36,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:40,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16932245066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:44,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:46,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][100][36] duration [20s], collections [1]/[1.8m], total [20s]/[2.3m], memory [336mb]->[272.6mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [258.7mb]->[258.7mb]/[2gb]}{[survivor] [5.2mb]->[9.9mb]/[0b]}
[2022-04-18T19:38:48,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7865858841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:38:53,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [50900ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:39:03,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:39:20,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17382566848ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:39:36,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:39:57,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33376304180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:40:34,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58s/58093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:40:57,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58s/58093173672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:41:23,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:41:47,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48998231924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:42:07,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44s/44006ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:42:42,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44s/44006043804ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:43:09,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:43:29,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58660264100ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:43:48,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41991ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:44:11,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41990766644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:44:41,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:44:58,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49934601816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:45:25,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:45:56,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47247082986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:46:31,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:46:01,467][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [47247ms] which is above the warn threshold of [5s]
[2022-04-18T19:47:07,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52494281675ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:47:29,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:47:53,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71437061747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:48:10,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41231ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:48:31,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [112668ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:48:34,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41231731898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:49:03,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:49:21,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53221329759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:49:41,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:49:54,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38151619096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:50:24,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:50:53,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31132547119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:51:14,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:51:38,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e0fb8@6b9090e] took [131560ms] which is above the warn threshold of [5000ms]
[2022-04-18T19:51:40,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62276267163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:52:11,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.9s/55906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:52:30,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.9s/55905497511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:52:48,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:53:08,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36584080033ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:53:35,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:53:52,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46740878933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:54:16,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:54:41,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34665279171ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:57:22,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/191292ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:58:02,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/191292124692ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:58:32,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:58:55,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71162934376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T19:59:20,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49092ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T19:59:45,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49091651787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:00:05,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44772ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:00:27,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44772863830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:00:46,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41168ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:01:09,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41167377383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:01:31,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:01:50,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cfef32d, interval=1s}] took [86238ms] which is above the warn threshold of [5000ms]
[2022-04-18T20:01:51,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45070735516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:02:08,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:02:20,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35081822304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:02:37,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:02:37,673][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=268, version=12378}] took [23.3m] which is above the warn threshold of [30s]: [running task [Publication{term=268, version=12378}]] took [133ms], [connecting to new nodes] took [200ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@7125b17e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@53568c8d] took [442664ms], [org.elasticsearch.script.ScriptService@77d7f3bd] took [106ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [0ms], [org.elasticsearch.snapshots.RestoreService@545e2b62] took [0ms], [org.elasticsearch.ingest.IngestService@330bf780] took [27797ms], [org.elasticsearch.action.ingest.IngestActionForwarder@7b89a047] took [1732ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1dc0@1561bcfb] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@30c1900e] took [796ms], [org.elasticsearch.tasks.TaskManager@5c1ad661] took [217ms], [org.elasticsearch.snapshots.SnapshotsService@6a27ce3d] took [324ms], [org.elasticsearch.cluster.InternalClusterInfoService@6cddeebb] took [746ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@5717550f] took [1809ms], [org.elasticsearch.indices.SystemIndexManager@4b13f745] took [17710ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6c5e84a2] took [434ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@7b10308c] took [2696ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7f52d7e5] took [1108ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1c1a9882] took [134ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@79833adb] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@c509a28] took [235ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1c740da] took [505674ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4180bb24] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@2b40f112] took [106ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@6f5d26c5] took [192973ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@683c6f37] took [3085ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d5180b5] took [1031ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1352c6c] took [58598ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@8acc548] took [8797ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@7585981] took [58252ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@c96db8a] took [207ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@40cc7c7a] took [1355ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48465cc1] took [51431ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@64fb9a68] took [23329ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4c5c1d52] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@4542b026] took [4588ms], [org.elasticsearch.node.ResponseCollectorService@7dd25a3c] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@68f51596] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@44ef2357] took [253ms], [org.elasticsearch.shutdown.PluginShutdownService@14fc786c] took [893ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@4846aada] took [805ms], [org.elasticsearch.indices.store.IndicesStore@3c4d0c98] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6e57d221] took [0ms], [org.elasticsearch.license.LicenseService@38d65a50] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1146ca19] took [171ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@2449beb0] took [0ms], [org.elasticsearch.gateway.GatewayService@2cee6e3d] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2b7acc5f] took [0ms]
[2022-04-18T20:02:54,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30470118894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:03:16,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:02:41,010][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30470ms] which is above the warn threshold of [5s]
[2022-04-18T20:03:40,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38730753361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:15:45,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.8s/49810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:04:05,475][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.6s/11618ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2c06434], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@535b0d6c], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e7f3ba11], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a18fbf75]], which exceeds the warn threshold of [10s]
[2022-04-18T20:16:22,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72b6389d, interval=5s}] took [49810ms] which is above the warn threshold of [5000ms]
[2022-04-18T20:16:25,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.8s/49810026292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:17:02,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9m/776306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:17:12,360][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35748}] took [776305ms] which is above the warn threshold of [5000ms]
[2022-04-18T20:17:30,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9m/776305528874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:18:12,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:18:30,032][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54310}] took [841389ms] which is above the warn threshold of [5000ms]
[2022-04-18T20:18:54,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65084089983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:19:45,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:20:42,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95810131583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:21:33,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:20:52,891][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1391] timed out after [682005ms]
[2022-04-18T20:22:13,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108957555384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:22:56,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:23:41,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82758923252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:24:24,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:24:56,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86391949190ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T20:25:45,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T20:27:08,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76252490017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T21:39:09,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2h/4387161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T21:43:56,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2h/4387009982516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T22:56:08,708][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-18T22:56:08,721][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-18T22:56:08,722][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-18T22:56:13,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-18T22:56:13,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-18T22:56:13,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-18T22:56:13,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-18T22:56:13,892][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-18T22:56:13,892][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-18T22:56:13,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-18T22:56:13,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-18T22:56:13,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-18T22:56:13,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-18T22:56:13,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-18T22:56:13,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-18T22:56:13,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-18T22:56:13,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-18T22:56:13,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-18T22:56:13,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-18T22:56:13,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-18T22:56:13,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-18T22:56:13,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-18T22:56:13,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-18T22:56:13,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-18T22:56:13,900][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-18T22:56:13,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-18T22:56:13,901][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-18T22:56:13,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-18T22:56:13,902][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-18T22:56:13,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-18T22:56:13,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-18T22:56:13,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-18T22:56:13,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-18T22:56:13,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-18T22:56:13,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-18T22:56:13,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-18T22:56:13,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-18T22:56:13,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-18T22:56:13,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-18T22:56:13,906][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-18T22:56:13,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-18T22:56:13,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-18T22:56:13,907][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-18T22:56:13,908][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-18T22:56:13,909][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-18T22:56:13,909][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-18T22:56:13,909][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-18T22:56:13,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-18T22:56:13,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-18T22:56:13,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-18T22:56:13,911][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-18T22:56:13,911][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-18T22:56:13,912][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-18T22:56:13,912][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-18T22:56:13,912][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-18T22:56:13,913][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-18T22:56:13,913][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-18T22:56:13,914][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-18T22:56:13,914][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-18T22:56:13,915][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-18T22:56:13,915][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-18T22:56:13,916][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-18T22:56:14,001][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.2gb], net total_space [125.8gb], types [ext4]
[2022-04-18T22:56:14,002][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-18T22:56:14,605][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-18T22:57:50,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T22:58:07,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55a8f6e7, interval=5s}] took [55006ms] which is above the warn threshold of [5000ms]
[2022-04-18T22:58:08,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13316886562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T22:58:09,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T22:58:09,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40939867608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T22:58:09,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [40939ms] which is above the warn threshold of [5000ms]
[2022-04-18T22:59:08,079][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-18T22:59:08,190][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-18T22:59:08,194][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-18T22:59:08,198][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-18T22:59:08,199][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T22:59:08,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-18T22:59:08,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-18T22:59:08,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T22:59:08,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-18T22:59:08,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-18T22:59:08,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-18T22:59:08,203][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-18T22:59:08,218][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-18T22:59:08,220][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-18T22:59:08,221][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-18T22:59:09,636][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-18T22:59:09,851][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-18T22:59:11,163][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-18T22:59:50,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [5957ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:00:16,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55a8f6e7, interval=5s}] took [5962ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:00:40,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@4ebf1430, interval=1m}] took [11103ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:01:09,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55a8f6e7, interval=5s}] took [5051ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:01:29,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [5672ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:01:47,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55a8f6e7, interval=5s}] took [5618ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:02:12,557][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-18T23:02:12,563][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-18T23:02:12,709][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-18T23:02:12,714][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-18T23:02:12,999][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-18T23:02:28,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5111ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:03:12,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:04:07,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [11655ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:04:36,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [17626ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:05:10,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [21418ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:05:26,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [7531ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:05:26,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:33,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6930783613ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:40,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:44,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14289995700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:46,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [14289ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:05:43,324][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30351ms] which is above the warn threshold of [5s]
[2022-04-18T23:05:48,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:50,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8400061969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:54,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:05:54,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [5430ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:05:55,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5430019142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:06:03,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [9056ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:07:43,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [12085ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:08:02,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [10103ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:08:26,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:08:32,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5678787773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:08:40,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14119ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:08:41,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [14119ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:08:43,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14119466199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:08:48,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:08:43,202][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14119ms] which is above the warn threshold of [5s]
[2022-04-18T23:08:52,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8102327996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:08:54,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [8102ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:08:57,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:04,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9149998355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:15,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:20,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [15979ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:09:22,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15979938814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:27,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:33,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14066619345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:40,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:46,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13010816800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:48,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [13010ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:09:51,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:09:56,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11110790971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:03,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:17,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [10553ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:10:12,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10553225131ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:22,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:23,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [19943ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:10:29,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19943805905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:37,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:45,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15697368722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:51,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [15697ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:10:52,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:10:57,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14667611658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:02,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:09,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9572030561ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:17,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [9572ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:11:17,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:21,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15846889219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:28,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9642ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:35,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9641070857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:39,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [9641ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:11:43,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:47,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257638263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:56,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:59,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55a8f6e7, interval=5s}] took [10869ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:12:08,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10869357126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:11:45,784][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24899ms] which is above the warn threshold of [5s]
[2022-04-18T23:12:26,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:12:46,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30607081329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:02,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:12:54,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [30607ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:13:15,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37607134391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:19,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:25,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17902995815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:35,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:36,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [16473ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:13:37,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16473845515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:43,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7176ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:14:39,925][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8006ms] which is above the warn threshold of [5s]
[2022-04-18T23:14:49,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [6204ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:15:04,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [6791ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:15:38,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [20362ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:16:59,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [53718ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:17:33,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [11242ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:17:39,478][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14490ms] which is above the warn threshold of [5s]
[2022-04-18T23:18:00,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [11703ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:18:32,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [13532ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:18:55,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:19:19,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [12107ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:19:41,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9510ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:19:41,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [5950ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:19:47,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9509802893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:19:52,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:19:53,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [10256ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:19:58,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10256044956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:07,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:08,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [14270ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:20:15,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14270901008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:13:43,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@32ab0711] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-18T23:20:21,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:23,115][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14395ms] which is above the warn threshold of [5s]
[2022-04-18T23:20:28,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14395089551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:28,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [14395ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:20:35,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:42,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13800159635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:49,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:20:56,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [14094ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:20:56,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14094935247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:04,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:11,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15390717683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:17,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:19,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [13200ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:21:24,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13200922279ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:29,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:32,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2acd55dc, interval=5s}] took [11515ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:21:33,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11515577918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:39,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:43,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10214202024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:43,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [10214ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:21:45,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:47,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7054935328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-18T23:21:56,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [6218ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:22:10,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5293ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:22:39,891][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7654ms] which is above the warn threshold of [5s]
[2022-04-18T23:22:47,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [8304ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:23:02,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:23:26,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7708ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:23:39,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [6063ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:24:03,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [9006ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:24:25,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7531ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:24:55,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [12249ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:25:15,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [6499ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:25:21,839][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10263ms] which is above the warn threshold of [5s]
[2022-04-18T23:25:36,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:26:01,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [13009ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:26:19,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:26:37,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5777ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:26:53,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7484ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:27:04,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:27:22,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [7306ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:27:45,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [10098ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:28:08,330][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11359ms] which is above the warn threshold of [5s]
[2022-04-18T23:28:48,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [13474ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:29:09,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [8806ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:29:38,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [10419ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:30:28,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:30:40,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [5288ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:30:57,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@17a9184b, interval=1s}] took [6123ms] which is above the warn threshold of [5000ms]
[2022-04-18T23:59:53,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4m/1709576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T01:18:11,329][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-19T01:18:11,367][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-19T01:18:11,368][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-19T01:18:15,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-19T01:18:15,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-19T01:18:15,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-19T01:18:15,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-19T01:18:15,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-19T01:18:15,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-19T01:18:15,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-19T01:18:15,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-19T01:18:15,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-19T01:18:15,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-19T01:18:16,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-19T01:18:16,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-19T01:18:16,001][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-19T01:18:16,001][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-19T01:18:16,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-19T01:18:16,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-19T01:18:16,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-19T01:18:16,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-19T01:18:16,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-19T01:18:16,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-19T01:18:16,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-19T01:18:16,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-19T01:18:16,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-19T01:18:16,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-19T01:18:16,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-19T01:18:16,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-19T01:18:16,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-19T01:18:16,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-19T01:18:16,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-19T01:18:16,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-19T01:18:16,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-19T01:18:16,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-19T01:18:16,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-19T01:18:16,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-19T01:18:16,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-19T01:18:16,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-19T01:18:16,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-19T01:18:16,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-19T01:18:16,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-19T01:18:16,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-19T01:18:16,012][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-19T01:18:16,012][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-19T01:18:16,013][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-19T01:18:16,013][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-19T01:18:16,013][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-19T01:18:16,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-19T01:18:16,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-19T01:18:16,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-19T01:18:16,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-19T01:18:16,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-19T01:18:16,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-19T01:18:16,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-19T01:18:16,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-19T01:18:16,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-19T01:18:16,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-19T01:18:16,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-19T01:18:16,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-19T01:18:16,018][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-19T01:18:16,018][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-19T01:18:16,076][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-19T01:18:16,077][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-19T01:18:16,603][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-19T01:18:25,805][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-19T01:18:25,813][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-19T01:18:27,835][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-19T01:18:28,048][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-19T01:18:29,101][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-19T01:18:30,006][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-19T01:18:30,007][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-19T01:18:30,134][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-19T01:18:30,137][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-19T01:18:30,428][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-19T01:18:33,351][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-19T01:18:33,562][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hBv9-yy7RQWYtUlLCj5I_Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 269, version: 12379, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hBv9-yy7RQWYtUlLCj5I_Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-19T01:18:33,725][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hBv9-yy7RQWYtUlLCj5I_Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 269, version: 12379, reason: Publication{term=269, version=12379}
[2022-04-19T01:18:33,884][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-19T01:18:33,885][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-19T01:18:34,756][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-19T01:18:34,767][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [55] indices into cluster_state
[2022-04-19T01:18:35,846][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-19T01:18:35,848][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-19T01:18:36,623][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-19T01:18:37,140][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-19T01:18:37,167][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-19T01:18:37,170][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-19T01:18:37,951][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-19T01:18:38,096][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-19T01:18:40,794][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-19T01:18:41,055][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-19T01:18:41,100][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-19T01:18:41,115][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-19T01:40:18,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T01:42:19,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5938918548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T01:42:48,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7m/1425893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T01:43:07,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7m/1426022782078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T01:42:11,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a682d1f, interval=1s}] took [14617ms] which is above the warn threshold of [5000ms]
[2022-04-19T01:43:27,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T01:43:43,597][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1483915ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [54] unchanged indices
[2022-04-19T01:43:57,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43274390281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T01:44:31,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T02:01:25,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63166636288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T02:04:04,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5m/1170026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T02:06:52,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5m/1170025632293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T02:09:41,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T02:12:13,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338118841509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T02:14:40,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T02:45:16,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298050641447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T02:54:19,913][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-19T02:54:20,015][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-19T02:54:20,016][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-19T02:54:26,583][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-19T02:54:26,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-19T02:54:26,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-19T02:54:26,589][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-19T02:54:26,589][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-19T02:54:26,589][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-19T02:54:26,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-19T02:54:26,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-19T02:54:26,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-19T02:54:26,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-19T02:54:26,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-19T02:54:26,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-19T02:54:26,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-19T02:54:26,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-19T02:54:26,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-19T02:54:26,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-19T02:54:26,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-19T02:54:26,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-19T02:54:26,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-19T02:54:26,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-19T02:54:26,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-19T02:54:26,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-19T02:54:26,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-19T02:54:26,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-19T02:54:26,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-19T02:54:26,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-19T02:54:26,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-19T02:54:26,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-19T02:54:26,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-19T02:54:26,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-19T02:54:26,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-19T02:54:26,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-19T02:54:26,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-19T02:54:26,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-19T02:54:26,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-19T02:54:26,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-19T02:54:26,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-19T02:54:26,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-19T02:54:26,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-19T02:54:26,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-19T02:54:26,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-19T02:54:26,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-19T02:54:26,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-19T02:54:26,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-19T02:54:26,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-19T02:54:26,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-19T02:54:26,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-19T02:54:26,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-19T02:54:26,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-19T02:54:26,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-19T02:54:26,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-19T02:54:26,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-19T02:54:26,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-19T02:54:26,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-19T02:54:26,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-19T02:54:26,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-19T02:54:26,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-19T02:54:26,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-19T02:54:26,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-19T02:54:26,698][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.2gb], net total_space [125.8gb], types [ext4]
[2022-04-19T02:54:26,699][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-19T02:54:27,195][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-19T02:54:43,000][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-19T02:54:43,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-19T02:54:43,055][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-19T02:54:43,076][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-19T02:54:43,077][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-19T02:54:43,078][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-19T02:54:43,079][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-19T02:54:43,080][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-19T02:54:43,080][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-19T02:54:43,081][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-19T02:54:43,082][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-19T02:54:43,082][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-19T02:54:43,088][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp]
[2022-04-19T02:54:43,088][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-19T02:54:43,089][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-19T02:54:43,090][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-19T02:54:43,095][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-19T02:54:44,610][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-19T02:54:44,787][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-19T02:54:45,851][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-19T02:58:05,188][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-19T02:58:05,196][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-19T02:58:05,357][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-19T02:58:05,359][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-19T02:58:05,656][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-19T02:58:08,824][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-19T02:58:30,844][WARN ][o.e.c.c.ClusterFormationFailureHelper] [tpotcluster-node-01] master not discovered or elected yet, an election requires a node with id [t9hfPgy_RyC9LOJUxQUrSQ], have discovered possible quorum [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]; discovery will continue using [] from hosts providers and [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}] from last-known cluster state; node term 273, last-accepted version 12401 in term 269
[2022-04-19T02:58:47,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [7014ms] which is above the warn threshold of [5000ms]
[2022-04-19T02:58:48,874][WARN ][o.e.n.Node               ] [tpotcluster-node-01] timed out while waiting for initial discovery state - timeout: 30s
[2022-04-19T02:58:57,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-19T02:59:13,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [10320ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:04:27,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/254900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:05:45,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/254899701150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:05:59,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/141930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:06:13,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/141930096058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:06:26,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26373ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:06:36,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26373358822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:06:48,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22303ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:07:01,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22302889299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:07:14,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:07:16,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@77d94982] took [475648ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:07:18,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27340315177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:07:23,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:07:24,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9392425371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:07:24,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7ea60bd7, interval=5s}] took [9392ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:07:29,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [5886ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:08:01,010][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-19T03:08:03,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [7276ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:08:04,523][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-19T03:08:17,324][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [9.6m/580009ms] to compute cluster state update for [elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_]], which exceeds the warn threshold of [10s]
[2022-04-19T03:08:19,786][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 273, version: 12402, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-19T03:08:34,103][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 273, version: 12402, reason: Publication{term=273, version=12402}
[2022-04-19T03:08:42,207][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.checkGlobalBlock(TransportSingleShardAction.java:112) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.<init>(TransportSingleShardAction.java:146) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.<init>(TransportSingleShardAction.java:130) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-19T03:08:51,436][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-19T03:08:51,466][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [55] indices into cluster_state
[2022-04-19T03:08:52,909][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-19T03:08:52,913][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-19T03:09:08,388][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-19T03:09:08,669][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-19T03:09:08,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-19T03:09:08,918][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-19T03:09:11,960][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-19T03:09:11,910][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-19T03:09:11,910][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-19T03:09:20,368][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-19T03:09:31,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:09:32,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5412917565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:09:38,502][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][84][10] duration [1s], collections [1]/[1.1s], total [1s]/[1.6s], memory [307.9mb]->[327.9mb]/[2gb], all_pools {[young] [212mb]->[0b]/[0b]}{[old] [64mb]->[94.3mb]/[2gb]}{[survivor] [31.9mb]->[11.2mb]/[0b]}
[2022-04-19T03:09:39,343][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][84] overhead, spent [1s] collecting in the last [1.1s]
[2022-04-19T03:09:40,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [18675ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:09:45,458][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] error updating geoip database [GeoLite2-City.mmdb]
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.flush(SSLSocketOutputRecord.java:268) ~[?:?]
	at sun.security.ssl.HandshakeOutStream.flush(HandshakeOutStream.java:89) ~[?:?]
	at sun.security.ssl.Finished$T13FinishedProducer.onProduceFinished(Finished.java:695) ~[?:?]
	at sun.security.ssl.Finished$T13FinishedProducer.produce(Finished.java:674) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.Finished$T13FinishedConsumer.onConsumeFinished(Finished.java:1032) ~[?:?]
	at sun.security.ssl.Finished$T13FinishedConsumer.consume(Finished.java:895) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.processDatabase(GeoIpDownloader.java:160) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:126) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-19T03:09:45,906][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-19T03:09:58,758][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][95][12] duration [2.2s], collections [1]/[3s], total [2.2s]/[4.1s], memory [192.4mb]->[117.3mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [106mb]->[106mb]/[2gb]}{[survivor] [6.4mb]->[11.3mb]/[0b]}
[2022-04-19T03:09:59,519][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][95] overhead, spent [2.2s] collecting in the last [3s]
[2022-04-19T03:10:04,350][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10160ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [54] unchanged indices
[2022-04-19T03:10:05,218][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.7s] publication of cluster state version [12419] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-19T03:10:18,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5164/0x0000000801815200@40d5d42b] took [5139ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:10:46,798][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][116][13] duration [1s], collections [1]/[2.5s], total [1s]/[5.2s], memory [193.3mb]->[120.8mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [106mb]->[110.7mb]/[2gb]}{[survivor] [11.3mb]->[10mb]/[0b]}
[2022-04-19T03:10:47,077][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][116] overhead, spent [1s] collecting in the last [2.5s]
[2022-04-19T03:11:49,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [51857ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:12:53,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:13:22,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27963574479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:13:51,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:14:16,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62741739936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:15:47,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116441ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:16:14,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116441265696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:16:33,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:16:57,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45828439226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:17:18,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:17:34,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43820226394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:17:58,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:18:11,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35448378756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:18:30,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:18:44,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36752250317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:19:02,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:19:18,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32615871532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:19:25,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@668a62aa, interval=5s}] took [194465ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:19:34,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:19:46,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32362875679ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:21:09,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:22:50,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90986459918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:19,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/136386ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:24,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/136386200221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:29,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8950ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:39,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8950060299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:40,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:39,792][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37344}] took [463150ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:23:45,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11762795039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:23:47,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:02,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7222292881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:05,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:07,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17418434150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:18,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:08,740][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [792140ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [54] unchanged indices
[2022-04-19T03:24:21,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13119664015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:23,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:39,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5565111033ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:23,444][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [13.5m] publication of cluster state version [12421] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2tPz16x1TxSDQ22wOkKQ1g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-19T03:24:39,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5164/0x0000000801815200@77b86edc] took [55088ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:24:39,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:39,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15908819374ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:24:40,007][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][122][14] duration [3.7s], collections [1]/[13.7m], total [3.7s]/[8.9s], memory [148.8mb]->[138.5mb]/[2gb], all_pools {[young] [28mb]->[12mb]/[0b]}{[old] [110.7mb]->[114.1mb]/[2gb]}{[survivor] [10mb]->[12.4mb]/[0b]}
[2022-04-19T03:24:46,200][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][123][15] duration [3.4s], collections [1]/[1s], total [3.4s]/[12.4s], memory [138.5mb]->[206.5mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [114.1mb]->[118.9mb]/[2gb]}{[survivor] [12.4mb]->[8.2mb]/[0b]}
[2022-04-19T03:24:46,494][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][123] overhead, spent [3.4s] collecting in the last [1s]
[2022-04-19T03:24:46,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [5567ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:25:06,359][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][133][16] duration [1.9s], collections [1]/[3.9s], total [1.9s]/[14.3s], memory [203.2mb]->[130.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [118.9mb]->[118.9mb]/[2gb]}{[survivor] [8.2mb]->[11.7mb]/[0b]}
[2022-04-19T03:25:06,604][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][133] overhead, spent [1.9s] collecting in the last [3.9s]
[2022-04-19T03:25:16,167][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/config%3A7.17.0][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55970}] took [5758ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:25:16,172][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/space%3Adefault][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55968}] took [5758ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:25:16,640][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][136][17] duration [1.8s], collections [1]/[3.2s], total [1.8s]/[16.1s], memory [186.6mb]->[210.6mb]/[2gb], all_pools {[young] [56mb]->[4mb]/[0b]}{[old] [118.9mb]->[124.5mb]/[2gb]}{[survivor] [11.7mb]->[10mb]/[0b]}
[2022-04-19T03:25:16,644][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][136] overhead, spent [1.8s] collecting in the last [3.2s]
[2022-04-19T03:25:16,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [7180ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:25:22,825][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][141] overhead, spent [541ms] collecting in the last [1.2s]
[2022-04-19T03:25:24,732][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][142] overhead, spent [666ms] collecting in the last [1.5s]
[2022-04-19T03:25:26,307][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-19T03:25:46,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:25:47,012][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][148][21] duration [4.2s], collections [1]/[1.5s], total [4.2s]/[21.9s], memory [223.3mb]->[231.3mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [142mb]->[142mb]/[2gb]}{[survivor] [1.2mb]->[7.2mb]/[0b]}
[2022-04-19T03:25:47,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5291584542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:25:47,235][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][148] overhead, spent [4.2s] collecting in the last [1.5s]
[2022-04-19T03:25:47,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [5891ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:25:48,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:25:48,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8900336925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:25:52,513][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-19T03:25:53,783][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-19T03:25:55,822][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-19T03:25:55,944][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-19T03:25:59,300][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][157][25] duration [816ms], collections [1]/[2.1s], total [816ms]/[23.5s], memory [160.4mb]->[150.2mb]/[2gb], all_pools {[young] [12mb]->[36mb]/[0b]}{[old] [142mb]->[144.4mb]/[2gb]}{[survivor] [10.4mb]->[5.7mb]/[0b]}
[2022-04-19T03:25:59,571][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][157] overhead, spent [816ms] collecting in the last [2.1s]
[2022-04-19T03:26:00,221][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.19] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-19T03:26:03,040][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][159] overhead, spent [425ms] collecting in the last [1.5s]
[2022-04-19T03:26:05,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [567ms] collecting in the last [1s]
[2022-04-19T03:26:50,977][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][36] duration [2.6s], collections [1]/[4.7s], total [2.6s]/[28.4s], memory [369mb]->[227.4mb]/[2gb], all_pools {[young] [160mb]->[4mb]/[0b]}{[old] [185.3mb]->[211.9mb]/[2gb]}{[survivor] [27.7mb]->[15.5mb]/[0b]}
[2022-04-19T03:26:51,422][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [2.6s] collecting in the last [4.7s]
[2022-04-19T03:26:52,762][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][190] overhead, spent [539ms] collecting in the last [1.9s]
[2022-04-19T03:27:08,778][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][197] overhead, spent [568ms] collecting in the last [1.9s]
[2022-04-19T03:27:16,622][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][201][40] duration [2.3s], collections [1]/[3.2s], total [2.3s]/[32.3s], memory [320.4mb]->[235.7mb]/[2gb], all_pools {[young] [88mb]->[8mb]/[0b]}{[old] [226mb]->[226mb]/[2gb]}{[survivor] [6.4mb]->[9.7mb]/[0b]}
[2022-04-19T03:27:16,769][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.18/_WNs-RdNTEa2JXDB9qD5fg] update_mapping [_doc]
[2022-04-19T03:27:16,872][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][201] overhead, spent [2.3s] collecting in the last [3.2s]
[2022-04-19T03:27:19,528][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][41] duration [1.3s], collections [1]/[2.9s], total [1.3s]/[33.6s], memory [235.7mb]->[235.2mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [226mb]->[228.8mb]/[2gb]}{[survivor] [9.7mb]->[6.4mb]/[0b]}
[2022-04-19T03:27:19,641][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [1.3s] collecting in the last [2.9s]
[2022-04-19T03:27:20,206][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:58904}] took [41098ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:27:25,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][42] duration [1.5s], collections [1]/[3.6s], total [1.5s]/[35.2s], memory [315.2mb]->[233.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [228.8mb]->[228.8mb]/[2gb]}{[survivor] [6.4mb]->[4.9mb]/[0b]}
[2022-04-19T03:27:25,862][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [1.5s] collecting in the last [3.6s]
[2022-04-19T03:27:32,857][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][208][43] duration [1.1s], collections [1]/[2.7s], total [1.1s]/[36.3s], memory [261.7mb]->[235.9mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [228.8mb]->[228.8mb]/[2gb]}{[survivor] [4.9mb]->[7.1mb]/[0b]}
[2022-04-19T03:27:33,241][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][208] overhead, spent [1.1s] collecting in the last [2.7s]
[2022-04-19T03:27:35,699][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][209][44] duration [1s], collections [1]/[2.9s], total [1s]/[37.4s], memory [235.9mb]->[269.4mb]/[2gb], all_pools {[young] [0b]->[32mb]/[0b]}{[old] [228.8mb]->[228.8mb]/[2gb]}{[survivor] [7.1mb]->[8.5mb]/[0b]}
[2022-04-19T03:27:35,895][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][209] overhead, spent [1s] collecting in the last [2.9s]
[2022-04-19T03:27:38,667][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [622ms] collecting in the last [1.3s]
[2022-04-19T03:28:00,347][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][219][49] duration [1.9s], collections [1]/[1.1s], total [1.9s]/[40.6s], memory [310.5mb]->[326.5mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [235.1mb]->[242.6mb]/[2gb]}{[survivor] [15.3mb]->[12.6mb]/[0b]}
[2022-04-19T03:28:01,575][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][219] overhead, spent [1.9s] collecting in the last [1.1s]
[2022-04-19T03:28:01,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [14102ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:28:06,257][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][221] overhead, spent [625ms] collecting in the last [1.5s]
[2022-04-19T03:28:27,468][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [.kibana-event-log-7.16.2-000001][0]]]).
[2022-04-19T03:28:31,804][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.19][0]]]).
[2022-04-19T03:28:42,783][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][52] duration [1.5s], collections [1]/[3.4s], total [1.5s]/[43.1s], memory [346.5mb]->[320.6mb]/[2gb], all_pools {[young] [76mb]->[68mb]/[0b]}{[old] [255.1mb]->[262.5mb]/[2gb]}{[survivor] [15.4mb]->[14mb]/[0b]}
[2022-04-19T03:28:42,997][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][248] overhead, spent [1.5s] collecting in the last [3.4s]
[2022-04-19T03:28:50,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:28:51,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@668a62aa, interval=5s}] took [6714ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:28:51,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6512773090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:28:52,761][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][249][53] duration [4.9s], collections [1]/[9.5s], total [4.9s]/[48.1s], memory [320.6mb]->[295.5mb]/[2gb], all_pools {[young] [68mb]->[16mb]/[0b]}{[old] [262.5mb]->[270.2mb]/[2gb]}{[survivor] [14mb]->[9.2mb]/[0b]}
[2022-04-19T03:28:59,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:29:00,558][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][249] overhead, spent [4.9s] collecting in the last [9.5s]
[2022-04-19T03:29:11,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5609470604ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:29:12,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [8559ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:29:29,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:29:43,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28214246559ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:30:25,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.2s/54204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:31:06,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.2s/54203637600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:31:25,362][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:58950}] took [90977ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:31:41,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:33:09,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@668a62aa, interval=5s}] took [76564ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:33:14,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76564904757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:34:52,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:35:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7ea60bd7, interval=5s}] took [189968ms] which is above the warn threshold of [5000ms]
[2022-04-19T03:35:51,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189968684245ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:36:43,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112753ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:37:46,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112753678548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:38:23,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:40:22,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99475524392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:42:49,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:44:34,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263402501845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:45:43,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/174288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:47:34,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/174436931051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:49:59,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/259327ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:50:39,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/259327102893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:50:03,618][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [42.1s/42123ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1c913de5]], which exceeds the warn threshold of [10s]
[2022-04-19T03:51:21,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:52:11,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79985127070ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:53:39,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:54:52,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138164198565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T03:55:41,957][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.7m/107041ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cbf694ad], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ac5efee]], which exceeds the warn threshold of [10s]
[2022-04-19T03:57:20,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T03:58:49,386][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10965ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cbf694ad], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ac5efee]], which exceeds the warn threshold of [10s]
[2022-04-19T03:59:28,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/206743938135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:02:10,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/285854ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:04:57,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/285801477035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:04:55,769][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.2m/136612ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b129ea8a]], which exceeds the warn threshold of [10s]
[2022-04-19T04:07:19,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5164/0x0000000801815200@53837c99] took [285801ms] which is above the warn threshold of [5000ms]
[2022-04-19T04:07:55,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:10:56,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352857410896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:10:23,371][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.4m/84693ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1c913de5]], which exceeds the warn threshold of [10s]
[2022-04-19T04:13:55,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368147ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:16:55,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367834399047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:16:20,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@66800b4d, interval=1s}] took [367834ms] which is above the warn threshold of [5000ms]
[2022-04-19T04:16:04,413][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.2m/75372ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cbf694ad]], which exceeds the warn threshold of [10s]
[2022-04-19T04:20:08,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/359732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:23:07,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/359899722647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:22:49,500][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.3m/82391ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ac5efee]], which exceeds the warn threshold of [10s]
[2022-04-19T04:26:18,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:28:41,480][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1m/62667ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b129ea8a]], which exceeds the warn threshold of [10s]
[2022-04-19T04:32:06,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382961243915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:36:02,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7m/583397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:38:38,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7m/582950724637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:38:48,702][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/109802ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1c913de5]], which exceeds the warn threshold of [10s]
[2022-04-19T04:42:48,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:46:27,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406468577009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:47:52,907][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [4.2m/252349ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cbf694ad], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ac5efee]], which exceeds the warn threshold of [10s]
[2022-04-19T04:50:54,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/487135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:52:26,291][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.1s/15127ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cbf694ad], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ac5efee]], which exceeds the warn threshold of [10s]
[2022-04-19T04:54:08,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/487521910336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T04:57:04,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T04:50:24,535][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [406469ms] which is above the warn threshold of [5s]
[2022-04-19T05:00:57,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368092843077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T05:01:45,588][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.1m/487521ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [15.6m/936761ms]
[2022-04-19T05:01:42,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@668a62aa, interval=5s}] took [855614ms] which is above the warn threshold of [5000ms]
[2022-04-19T05:04:26,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/442382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T05:05:52,611][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [21.6m/1297716ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [16.5m/993845ms]
[2022-04-19T05:07:43,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/442102061418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T05:10:45,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T05:13:24,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370666445249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-19T05:12:50,655][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [3.1m/191980ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1c913de5], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b129ea8a]], which exceeds the warn threshold of [10s]
[2022-04-19T05:16:23,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-19T05:17:00,968][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.7s/15785ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1c913de5], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b129ea8a]], which exceeds the warn threshold of [10s]
[2022-04-19T05:19:21,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347772790066ns] on relative clock which is above the warn threshold of [5000ms]
