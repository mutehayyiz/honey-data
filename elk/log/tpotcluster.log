[2022-04-15T16:28:58,859][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T16:28:58,879][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T16:28:58,880][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T16:29:06,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T16:29:06,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T16:29:06,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T16:29:06,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T16:29:06,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T16:29:06,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T16:29:06,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T16:29:06,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T16:29:06,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T16:29:06,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T16:29:06,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T16:29:06,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T16:29:06,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T16:29:06,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T16:29:06,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T16:29:06,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T16:29:06,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T16:29:06,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T16:29:06,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T16:29:06,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T16:29:06,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T16:29:06,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T16:29:06,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T16:29:06,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T16:29:06,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T16:29:06,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T16:29:06,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T16:29:06,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T16:29:06,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T16:29:06,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T16:29:06,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T16:29:06,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T16:29:06,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T16:29:06,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T16:29:06,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T16:29:06,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T16:29:06,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T16:29:06,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T16:29:06,571][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.4gb], net total_space [125.8gb], types [ext4]
[2022-04-15T16:29:06,572][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T16:29:06,938][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T16:29:19,789][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T16:29:19,792][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T16:29:20,870][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T16:29:20,996][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T16:29:21,710][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T16:29:22,464][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T16:29:22,465][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T16:29:22,496][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T16:29:22,497][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T16:29:22,683][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T16:29:25,138][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T16:29:25,256][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 249, version: 9743, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T16:29:25,475][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 249, version: 9743, reason: Publication{term=249, version=9743}
[2022-04-15T16:29:25,590][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T16:29:25,590][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T16:29:26,281][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T16:29:26,287][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [51] indices into cluster_state
[2022-04-15T16:29:26,993][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T16:29:26,995][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T16:29:27,876][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-15T16:29:28,196][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T16:29:28,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T16:29:28,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T16:29:29,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:29,149][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:31,540][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T16:29:31,592][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-15T16:29:31,614][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-15T16:29:32,208][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:32,212][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:32,409][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:39,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T16:29:39,712][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-15T16:29:39,715][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,523][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T16:29:40,533][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,734][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,740][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:42,103][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:42,103][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:43,761][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.14][0]]]).
[2022-04-15T16:29:46,738][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-15T16:29:46,966][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-15T16:30:31,251][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 1033 finished with response BulkByScrollResponse[took=180.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-15T16:30:33,246][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 1062 finished with response BulkByScrollResponse[took=1.8s,timed_out=false,sliceId=null,updated=923,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-15T16:30:41,334][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-15T16:31:02,632][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.15] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-15T16:31:02,832][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.15][0]]]).
[2022-04-15T16:31:02,961][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,059][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,075][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,164][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,265][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,561][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,713][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:14,461][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:18,315][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:25,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:25,624][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:58,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [21021ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:31:58,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:56200}] took [17702ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:32:45,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [13619ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:39,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [50698ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:44,857][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:39,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5052873314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,007][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/110289ms] to compute cluster state update for [put-mapping [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-15T16:34:45,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59935170929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7c2b5886, interval=5s}] took [59935ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,370][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [60136ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T16:34:45,577][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][15] duration [2.5s], collections [1]/[1.8m], total [2.5s]/[3.4s], memory [621.2mb]->[289.2mb]/[2gb], all_pools {[young] [372mb]->[32mb]/[0b]}{[old] [179.2mb]->[223.2mb]/[2gb]}{[survivor] [74mb]->[34mb]/[0b]}
[2022-04-15T16:34:45,715][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,050][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,320][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,471][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,661][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,748][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,845][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,947][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:51,028][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:51,977][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:54,992][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:55,066][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:55,710][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:02,996][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:21,593][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:29,601][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:45,108][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:37:02,824][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/BByN9-ZNSay2Jx7Y4INJ4w] deleting index
[2022-04-15T16:37:57,040][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:40:57,767][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-1970.01.01] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-15T16:40:57,861][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-1970.01.01][0]]]).
[2022-04-15T16:40:57,918][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-1970.01.01/768VpBTdRaq7sY8TL1VTBA] update_mapping [_doc]
[2022-04-15T16:41:16,407][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:41:16,889][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:41:17,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:45:09,590][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:46:27,647][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:47:18,011][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:48:39,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:49:53,858][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:49:54,805][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:50:55,859][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:50:55,920][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:51:04,952][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:53:35,049][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:54:20,231][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:57:28,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [39778ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:02:44,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T16:57:27,241][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:47982}] took [20072ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:09:27,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738986263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:11:57,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7m/827710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:14:44,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7m/827351886048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:17:02,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:19:35,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292872190294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:23:19,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:26:33,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320186177290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:27:16,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7c2b5886, interval=5s}] took [320186ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:29:25,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:32:46,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433852585741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:35:27,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:38:45,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361533922365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:39:35,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017e7548@229537ec] took [361533ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:41:57,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:45:58,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376234576774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:49:08,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:52:33,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445510161957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:55:28,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:57:59,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368063366226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:00:20,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:01:13,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@150a185c, interval=1m}] took [305035ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:04:30,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305035025168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:03:41,331][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48024}] took [305035ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:07:19,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:05:25,987][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/90898ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6e931853]], which exceeds the warn threshold of [10s]
[2022-04-15T18:11:59,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417581340987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:11:59,139][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48014}] took [417581ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:16:16,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/534783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:14:49,712][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.2s/14203ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cfc3bd12], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@205e664a], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b4f712ef]], which exceeds the warn threshold of [10s]
[2022-04-15T18:20:41,175][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T18:20:41,199][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T18:20:41,200][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T18:20:47,916][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T18:20:47,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T18:20:47,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T18:20:47,926][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T18:20:47,926][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T18:20:47,928][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T18:20:47,928][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T18:20:47,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T18:20:47,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T18:20:47,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T18:20:47,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T18:20:47,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T18:20:48,043][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.3gb], net total_space [125.8gb], types [ext4]
[2022-04-15T18:20:48,044][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T18:20:48,638][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T18:22:26,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:33,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9465130028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [46991ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38261975048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:58,278][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T18:22:58,287][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,288][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T18:22:58,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,291][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,291][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,292][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-15T18:22:58,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T18:22:58,296][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T18:22:58,298][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,299][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T18:22:59,379][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T18:22:59,547][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T18:23:00,595][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T18:23:01,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T18:23:01,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T18:23:01,598][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T18:23:01,599][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T18:23:01,847][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T18:23:04,347][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T18:23:04,521][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 250, version: 9869, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T18:23:04,739][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 250, version: 9869, reason: Publication{term=250, version=9869}
[2022-04-15T18:23:04,887][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T18:23:04,888][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T18:24:31,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:47,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54877333755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:49,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [70411ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:24:52,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:56,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34627385527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:00,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:03,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852133259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:24,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:28,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24030552232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:32,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:36,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7580037029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:42,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:46,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10234621280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:49,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:56,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517664436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:02,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:06,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11698226405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:10,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:13,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8868062263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:16,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:18,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5874287011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:21,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:24,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5295815051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:28,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:32,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7061536935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:36,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7240051537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249757195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:41,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57126}] took [5946ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:26:43,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@653741e5] took [111244ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:26:45,862][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T18:26:49,469][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9870}] took [3.7m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9870}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [1ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [87ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [458ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [3ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [9ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [5ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [217896ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [49ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [35ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [22ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [145ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [58ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [13ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@6b5d5b43] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [1ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@70204568] took [129ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [1ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [77ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [3355ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@7de096e0] took [1574ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [10ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [15ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:26:50,074][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [52] indices into cluster_state
[2022-04-15T18:27:13,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13620ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-15T18:27:14,749][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.4s] publication of cluster state version [9871] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57126}] took [23626ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57130}] took [17718ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57132}] took [17518ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57128}] took [17718ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:41,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:39092}] took [22653ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:54,414][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-15T18:27:55,246][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:55,246][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:55,881][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:57,188][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9871}] took [42.3s] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9871}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [41356ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [20ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [0ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [36ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [66ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [367ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [106ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [24ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [44ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [42ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [19ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [34ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [17ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:28:16,943][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T18:28:19,464][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T18:28:21,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4614a747] took [5424ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:28:30,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [6795ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:28:47,787][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [74] timed out after [24152ms]
[2022-04-15T18:28:49,647][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [28.3s/28353ms] ago, timed out [4.2s/4201ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [74]
[2022-04-15T18:29:05,683][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-15T18:29:16,880][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T18:29:16,922][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T18:29:16,922][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T18:29:23,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@7a91bbf] took [5644ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:29:30,794][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T18:29:32,406][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T18:29:32,717][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [468ms] collecting in the last [1.7s]
[2022-04-15T18:29:36,258][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][91] overhead, spent [378ms] collecting in the last [1.2s]
[2022-04-15T18:30:29,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:30:49,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11549193224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:12,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:17,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30167218663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:26,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:31,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27689965628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:49,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:06,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24274841745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:14,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:24,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25384450876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:29,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:25,767][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9885}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9885}]] took [0ms], [connecting to new nodes] took [69ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [1004ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [4235ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [30ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [33ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [65ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [26ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [50ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [501ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [1894ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [229ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [3127ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1491ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [12504ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [53ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [37883ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [23721ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [227ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [1527ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [78ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [15ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [71ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [201ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [67ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@5e0c4e03]]]] took [15732ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@ba0f0a3]]]] took [30743ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@25ef4db1]]]] took [9197ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4f53ad21]]]] took [12567ms]
[2022-04-15T18:32:32,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15200112406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:44,630][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57180}] took [15200ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:45,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:48,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15395924278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:51,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:51,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4f53c0da] took [176426ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:53,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6411901117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:56,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:58,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5356ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:59,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5356698394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:02,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [5097ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:33:31,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:32,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12031283867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:39,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:41,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7726ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:33:42,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6129855847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:46,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:44,602][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7726ms] which is above the warn threshold of [5s]
[2022-04-15T18:33:49,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7094112922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:53,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:58,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6574704227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:05,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:06,119][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57180}] took [17970ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:12,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396172032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:18,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:23,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [38309ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:23,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13244869723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:26,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:31,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984618601ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:30,835][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57182}] took [8985ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:34,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:35,287][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57186}] took [6925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:34,867][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57184}] took [6925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:43,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925091854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:50,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:53,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16603400115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:55,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:00,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5144155346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:07,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:23,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11639440320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:27,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:32,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20085199770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:27,442][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [124325ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T18:35:38,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:41,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11019135935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:44,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:43,687][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.7m] publication of cluster state version [9886] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:35:55,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6688525846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:00,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:03,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015661204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:07,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:12,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517402059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:18,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:21,794][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [10658ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:22,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10657671230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:26,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:30,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8146308736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:35,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:40,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8726906670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:44,790][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [8727ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:44,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:49,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@63b2653e] took [109275ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:48,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9779743896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [6562ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,938][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [6562ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:55,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6562058405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:01,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:02,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9538ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:05,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9538709861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:10,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:15,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9634308706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:20,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9806416089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:30,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:35,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9963917148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:36,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9963ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:38,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:37,083][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9964ms] which is above the warn threshold of [5s]
[2022-04-15T18:37:44,395][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [8413ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8412625501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:47,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:49,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9245ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:50,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9245455814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:53,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:55,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5225695569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:00,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:01,135][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9886}] took [2.2m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9886}]] took [176ms], [connecting to new nodes] took [944ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [14923ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [9153ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [215ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [205ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [58ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [19698ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [589ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [1164ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [334ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [51ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [110ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [46156ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [13477ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [438ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [110ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [3657ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [267ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [2972ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [42ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [4365ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [1210ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [73ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:38:01,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6886616489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:01,761][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [5.2m/314904ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [8.2m/494217ms]
[2022-04-15T18:38:19,951][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.4s] publication of cluster state version [9887] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:38:32,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12604ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:38:49,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:55,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15457270051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:09,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12376792147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:10,636][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57200}] took [12377ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:39:19,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:28,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17235685524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:45,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:47,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [25031ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:39:55,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25031605108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:06,908][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57200}] took [25031ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:40:07,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:20,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21805840195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:32,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:46,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25775216831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:56,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:07,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23451873749ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:17,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22350245512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:39,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:49,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20873953033ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:59,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:08,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20152186343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:17,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:24,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18514968381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:30,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:36,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13330442511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:42,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:46,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11212412231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:50,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:54,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8319361007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:57,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:59,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@7c3c46bd] took [192585ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:00,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6798756459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:02,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:03,928][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9887}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9887}]] took [98ms], [connecting to new nodes] took [1023ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [7798ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [14005ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [484ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [282ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [170ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [6036ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [2157ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [499ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [182ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [155547ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [79ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [22266ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [711ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [402ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [13093ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [479ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [7929ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [54ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [4823ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [2525ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [959ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [114ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [227ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [62ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4b0a729d]]]] took [3028ms]
[2022-04-15T18:43:06,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6057664874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:09,092][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10.2m/617952ms] which is longer than the warn threshold of [300000ms]; there are currently [1] pending tasks, the oldest of which has age [10.3m/621014ms]
[2022-04-15T18:43:09,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:09,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256501427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:23,590][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11760ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T18:43:24,239][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.1s] publication of cluster state version [9888] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:43:30,648][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:49028}] took [17801ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:45,853][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][137][11] duration [2.1s], collections [1]/[1.1s], total [2.1s]/[3.2s], memory [261.5mb]->[305.5mb]/[2gb], all_pools {[young] [156mb]->[0b]/[0b]}{[old] [98.7mb]->[98.7mb]/[2gb]}{[survivor] [6.7mb]->[13.4mb]/[0b]}
[2022-04-15T18:43:47,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][137] overhead, spent [2.1s] collecting in the last [1.1s]
[2022-04-15T18:43:50,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10589ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:53,552][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.8s/13857ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1377531d], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6411fc55], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f8aaa8fa], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b246ae5e]], which exceeds the warn threshold of [10s]
[2022-04-15T18:44:20,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@5db5e0c6] took [5066ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:44:34,388][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][12] duration [3s], collections [1]/[5.3s], total [3s]/[6.3s], memory [180.2mb]->[116.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [98.7mb]->[110.9mb]/[2gb]}{[survivor] [13.4mb]->[5.6mb]/[0b]}
[2022-04-15T18:44:34,962][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [3s] collecting in the last [5.3s]
[2022-04-15T18:44:34,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:44:55,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6439ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:45:05,803][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57202}] took [10839ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:46:35,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:43,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5075893122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:51,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:56,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16896414874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:03,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:07,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@6fe235c0] took [121063ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:09,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12575315599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:14,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:15,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [10349ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:22,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349790648ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:30,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:35,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [13470ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:36,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13470504449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:42,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:49,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13704101318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:55,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:01,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13685309744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:09,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:15,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14702389375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:22,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:26,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572455168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:20,176][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14703ms] which is above the warn threshold of [5s]
[2022-04-15T18:48:32,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:43,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10348976756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:52,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:57,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [30296ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:49:00,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19947052169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:07,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14155048846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:26,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:30,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [19807ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:49:34,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19807432467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:41,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:49,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15297418544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:54,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:59,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12712ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:50:02,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12712676850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:10,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:17,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16113125027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:22,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:29,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12019842305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:34,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11888ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:41,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11887725293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:49,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:54,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14096523545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:01,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:52,465][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [343332ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T18:51:08,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12596536300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:19,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:28,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18748848676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:38,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:18,488][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6.4m] publication of cluster state version [9891] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:51:45,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033063319ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:53,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:00,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14649192228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:07,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:15,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14145840497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:24,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:31,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17189876446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:34,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4b275c7e] took [149480ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:52:37,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13822ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:45,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13822520526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:50,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:58,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12624921036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:02,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12624ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:53:07,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:15,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16106886622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:21,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:24,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260558073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:29,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:34,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945581248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:39,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9540ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:33,785][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [408] timed out after [57815ms]
[2022-04-15T18:53:44,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9539554154ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:41,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9539ms] which is above the warn threshold of [5s]
[2022-04-15T18:53:50,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [10266ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:53:49,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:55,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10266620571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:02,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:09,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11925978531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:58,359][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85567ms] ago, timed out [27.7s/27752ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [408]
[2022-04-15T18:54:15,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:54:17,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:21,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15507620960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:29,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:36,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11744891203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:41,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:47,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [24407ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:54:49,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12662264407ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:56,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:02,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15164775618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:07,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:15,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10610435308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:22,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:30,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14974763719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:35,592][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9891}] took [3.4m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9891}]] took [276ms], [connecting to new nodes] took [1356ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [79ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [41378ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [10686ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [86ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [74ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [3116ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [1104ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [428ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [1ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [253ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [68747ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [219ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [19224ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [899ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [242ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [18427ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1774ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [18873ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [69ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [15673ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [12170ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [2030ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [79ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [232ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [756ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [65ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [147ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:55:40,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:49,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17131500762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:58,768][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.9m/719303ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [12.5m/750655ms]
[2022-04-15T18:55:58,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:04,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19581077344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:12,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:19,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13331061506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:28,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:36,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726649479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:47,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:53,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21029125089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:58,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:00,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@6537fcd8] took [136493ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:00,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9944582355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:03,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:06,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6003225212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:06,725][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [59s/59034ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-15T18:57:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:14,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7390ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:16,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7390154306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:25,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:34,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13336370355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:41,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:41,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [16304ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:43,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16304809725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:46,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:49,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139961081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:52,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6139ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:54,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:48,490][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6140ms] which is above the warn threshold of [5s]
[2022-04-15T18:57:59,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6895165533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:03,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:09,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9864708841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:16,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [22536ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:16,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:20,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12671448706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:26,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:28,051][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [10404ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:34,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404046759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:40,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:42,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [13419ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:55,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13419878775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:59,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:04,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19296255383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:09,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [19297ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:59:09,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:16,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10289723068ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:15,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10289ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:59:22,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:25,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12044892518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:28,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:41,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959486565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:43,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:45,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144118498ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:55,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:03,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11607929826ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:07,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:11,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779268274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:15,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:18,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8149491377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:22,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:24,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6750758018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:39,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:41,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9613468829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:50,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11253467808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,522][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [11254ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,970][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [206081ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T19:01:01,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@5115e147] took [94017ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:00:55,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,384][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57206}] took [5268ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5268313169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:05,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,896][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.7m] publication of cluster state version [9892] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:01:05,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9329138051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:05,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9954ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:01:08,178][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [17.4m/1048501ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [14m/845109ms]
[2022-04-15T19:01:12,678][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][176][13] duration [1.9s], collections [1]/[1s], total [1.9s]/[8.3s], memory [184.6mb]->[204.6mb]/[2gb], all_pools {[young] [68mb]->[88mb]/[0b]}{[old] [110.9mb]->[110.9mb]/[2gb]}{[survivor] [5.6mb]->[5.6mb]/[0b]}
[2022-04-15T19:01:13,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][176] overhead, spent [1.9s] collecting in the last [1s]
[2022-04-15T19:01:18,707][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][14] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[9.9s], memory [137.4mb]->[124.8mb]/[2gb], all_pools {[young] [16mb]->[12mb]/[0b]}{[old] [110.9mb]->[115.7mb]/[2gb]}{[survivor] [10.5mb]->[9mb]/[0b]}
[2022-04-15T19:01:19,216][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-04-15T19:01:24,513][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][15] duration [2s], collections [1]/[2.5s], total [2s]/[12s], memory [124.8mb]->[208.8mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [115.7mb]->[115.7mb]/[2gb]}{[survivor] [9mb]->[13.7mb]/[0b]}
[2022-04-15T19:01:24,854][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [2s] collecting in the last [2.5s]
[2022-04-15T19:01:30,141][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11143ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T19:01:39,291][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.7s] publication of cluster state version [9893] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:01:45,419][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57206}] took [5595ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:00,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@24d5ea58] took [21424ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:01,973][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57210}] took [6651ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:23,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9066ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:28,308][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57214}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:33,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:34,151][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [535] timed out after [26140ms]
[2022-04-15T19:03:03,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9352ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:03:20,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7200ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:03:40,431][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9893}] took [1.8m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9893}]] took [0ms], [connecting to new nodes] took [262ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [8105ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [6883ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [96ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [65ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [64ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [217ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [373ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [1973ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [530ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [79ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [41144ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [10684ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [590ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [521ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [14320ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [811ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [10619ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [6480ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [3959ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [938ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [142ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [140ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [476ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T19:04:56,890][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.2s/32202ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.15][0]], allocationId [GekoCVcoSgiAIPPGQR_JzQ], primary term [2], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.15][0]], allocationId [GekoCVcoSgiAIPPGQR_JzQ], primary term [2], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-15T19:04:51,259][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168599ms] ago, timed out [2.3m/142459ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [535]
[2022-04-15T19:06:41,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@1f7a1064] took [177710ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:06:27,864][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T19:07:02,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [10928ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:07:31,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12240ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:05,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:08:05,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11067ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:09,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9066619793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:08:09,270][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10267ms] which is above the warn threshold of [5s]
[2022-04-15T19:08:05,594][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [586] timed out after [74520ms]
[2022-04-15T19:08:06,737][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85988ms] ago, timed out [11.4s/11468ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [586]
[2022-04-15T19:08:10,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:27,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9205ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:48,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9915ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:20,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57216}] took [11607ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:41,788][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [187703ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T19:09:47,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@49c9a0fe] took [54470ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:47,558][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.4m] publication of cluster state version [9894] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:10:07,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [8430ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:10:34,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11292ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:10:54,810][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8521ms] which is above the warn threshold of [5s]
[2022-04-15T19:12:23,911][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9894}] took [2.5m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9894}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [125ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [199ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [36ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [0ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [73ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [331ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [69634ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [22783ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [715ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [758ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [12041ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1544ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [14591ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [14301ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [9414ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [3863ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [136ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [59ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [183ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [60ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T19:12:41,543][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.6m/520271ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [11.1m/670692ms]
[2022-04-15T19:12:57,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4d812620] took [135688ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:05,050][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.7s/15753ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-15T19:13:08,075][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57216}] took [164408ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:25,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6670ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:38,272][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57218}] took [5720ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:38,143][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [28234ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-15T19:13:43,850][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [35.1s] publication of cluster state version [9895] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:14:27,749][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57224}] took [9406ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:14:45,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@3988865f] took [59579ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:18,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:15:19,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20524726052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:15:21,432][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][16] duration [13.7s], collections [1]/[30.1s], total [13.7s]/[25.7s], memory [205.4mb]->[136.2mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [115.7mb]->[120.9mb]/[2gb]}{[survivor] [13.7mb]->[11.2mb]/[0b]}
[2022-04-15T19:15:25,831][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [13.7s] collecting in the last [30.1s]
[2022-04-15T19:15:28,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10409ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:40,251][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57226}] took [12262ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:41,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7259ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:42,743][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6649ms] which is above the warn threshold of [5s]
[2022-04-15T19:18:50,880][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57226}] took [48923ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:19:06,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@35354b1f] took [188017ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:20:37,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:10,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32063153765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:24,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:27,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [91539ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:21:41,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47542955654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:55,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:03,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31013618907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:19,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:30,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23604145509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:41,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:55,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21819102996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:08,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:39,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25564011493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:45,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:52,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39665831381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:53,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [39665ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:23:38,179][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [753] timed out after [216443ms]
[2022-04-15T19:24:06,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:10,671][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5m/301968ms] ago, timed out [1.4m/85525ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [753]
[2022-04-15T19:24:17,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20295299618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:30,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:39,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887512805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:16,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:25,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46066807928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:36,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:49,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19464267448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:59,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23561ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:17,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23560624101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:34,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:40,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [57978ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:26:51,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418354432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:27:29,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.1s/53193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:27:40,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [53192ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:27:49,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.1s/53192378789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:28:17,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:28:47,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47537136577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:29:14,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:29:33,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58999100636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:00,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46325342713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:54,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:31:21,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53436342063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:31:45,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:32:15,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51173605259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:32:39,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:33:09,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53592511253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:33:45,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:34:06,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64114500916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:34:43,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:35:36,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59561714321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:37:10,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:40:22,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146588930168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:40:08,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4db40ea9] took [581329ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:42:59,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:45:42,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338730747731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:48:43,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:51:33,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350506481508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:54:11,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:57:03,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323685946312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:59:45,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:02:35,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339128869347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:05:17,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:07:33,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326109892487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:10:19,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:12:43,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309565056075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:15:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:17:34,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293438112670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:20:17,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:15:49,967][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [603003ms] which is above the warn threshold of [5s]
[2022-04-15T20:22:46,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305041546419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:25:19,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:27:45,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290027092567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:30:19,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:32:48,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309163373271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:35:18,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:38:00,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301289647152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:41:07,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:43:45,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326128119452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:44:11,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [326128ms] which is above the warn threshold of [5000ms]
[2022-04-15T20:48:28,309][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T20:48:28,380][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T20:48:28,381][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T20:48:36,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T20:48:36,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T20:48:36,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T20:48:36,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T20:48:36,788][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T20:48:36,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T20:48:36,790][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T20:48:36,791][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T20:48:36,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T20:48:36,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T20:48:36,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T20:48:36,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T20:48:36,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T20:48:36,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T20:48:36,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T20:48:36,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T20:48:36,803][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T20:48:36,804][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T20:48:36,806][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T20:48:36,807][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T20:48:36,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T20:48:36,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T20:48:36,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T20:48:36,813][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T20:48:36,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T20:48:36,815][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T20:48:36,817][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T20:48:36,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T20:48:36,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T20:48:36,820][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T20:48:36,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T20:48:36,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T20:48:36,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T20:48:36,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T20:48:36,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T20:48:36,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T20:48:36,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T20:48:36,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T20:48:36,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T20:48:36,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T20:48:36,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T20:48:36,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T20:48:36,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T20:48:36,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T20:48:36,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T20:48:36,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T20:48:36,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T20:48:36,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T20:48:36,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T20:48:36,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T20:48:36,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T20:48:36,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T20:48:36,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T20:48:36,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T20:48:36,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T20:48:36,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T20:48:36,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T20:48:36,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T20:48:36,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T20:48:36,966][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.1gb], net total_space [125.8gb], types [ext4]
[2022-04-15T20:48:36,967][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T20:48:37,892][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T20:48:56,442][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T20:48:56,453][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-15T20:48:56,458][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T20:48:56,462][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-15T20:48:56,465][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T20:48:56,468][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-15T20:48:56,472][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-15T20:48:56,474][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T20:48:56,477][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-15T20:48:56,479][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-15T20:48:56,481][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-15T20:48:56,483][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T20:48:56,484][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T20:48:56,484][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T20:48:56,485][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T20:48:58,217][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T20:48:58,477][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T20:48:59,734][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T20:49:01,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T20:49:01,125][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T20:49:01,220][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T20:49:01,235][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T20:49:01,645][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T20:49:05,543][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T20:49:05,733][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 251, version: 9896, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T20:49:05,993][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 251, version: 9896, reason: Publication{term=251, version=9896}
[2022-04-15T20:49:06,180][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T20:49:06,181][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T20:49:08,624][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T20:49:08,651][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [52] indices into cluster_state
[2022-04-15T20:49:09,985][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T20:49:09,986][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T20:49:11,211][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-15T20:49:11,462][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-15T20:49:11,926][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-15T20:49:12,012][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T20:49:12,017][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T20:49:12,021][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T20:49:12,601][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T20:49:12,826][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T20:49:16,058][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T20:49:22,363][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-15T20:49:49,529][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:49,798][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,016][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,030][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,436][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,613][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,783][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:52,810][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:53,058][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:53,574][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:59:26,645][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:02:54,860][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:03:12,431][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:09:32,794][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1219] overhead, spent [316ms] collecting in the last [1.2s]
[2022-04-15T21:11:17,966][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1298][32] duration [881ms], collections [1]/[2.3s], total [881ms]/[2.8s], memory [1.3gb]->[245.6mb]/[2gb], all_pools {[young] [1.1gb]->[12mb]/[0b]}{[old] [232.1mb]->[232.1mb]/[2gb]}{[survivor] [6.7mb]->[5.5mb]/[0b]}
[2022-04-15T21:11:19,971][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1298] overhead, spent [881ms] collecting in the last [2.3s]
[2022-04-15T21:11:23,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [9494ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:11:38,061][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1306][33] duration [1.7s], collections [1]/[2.9s], total [1.7s]/[4.5s], memory [317.6mb]->[239.2mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [232.1mb]->[232.2mb]/[2gb]}{[survivor] [5.5mb]->[7mb]/[0b]}
[2022-04-15T21:11:39,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1306] overhead, spent [1.7s] collecting in the last [2.9s]
[2022-04-15T21:11:40,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [6105ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:12:14,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [6038ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:12:24,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [5181ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:12:39,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [10387ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:15:20,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:19:47,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6186874314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:20:26,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:21:53,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431561757443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:22:37,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:22:51,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131404530517ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:23:16,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39414ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:23:32,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39413784552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:32,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/435934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:37,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/435934363322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:44,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:52,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12722688909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:59,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:31:07,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14378838592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:31:12,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:06,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13728511222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:31:01,715][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57676}] took [131404ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:32:07,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.6s/55642ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:07,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55bb42c2, interval=5s}] took [13728ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:32:08,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.6s/55642551435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:26,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18636ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:29,389][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1320][36] duration [7.1m], collections [3]/[19.6m], total [7.1m]/[7.2m], memory [287.2mb]->[258.7mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [232.2mb]->[236.6mb]/[2gb]}{[survivor] [7mb]->[5.7mb]/[0b]}
[2022-04-15T21:32:29,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635591519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:31,463][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1320] overhead, spent [7.1m] collecting in the last [19.6m]
[2022-04-15T21:32:31,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:36,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5329065460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:36,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [23964ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:32:53,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:56,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15240705673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:01,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:10,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14661077707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:15,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [12971ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:33:14,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:16,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12971709502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:28,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:29,012][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1324][37] duration [4.5s], collections [1]/[1.7s], total [4.5s]/[7.3m], memory [314.3mb]->[330.3mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [236.6mb]->[236.6mb]/[2gb]}{[survivor] [5.7mb]->[3.4mb]/[0b]}
[2022-04-15T21:33:29,397][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1324] overhead, spent [4.5s] collecting in the last [1.7s]
[2022-04-15T21:33:29,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9268228607ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:29,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [9868ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:33:29,524][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [20.7m/1243619ms] ago, timed out [41.9s/41918ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [9553]
[2022-04-15T21:33:29,589][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [9553] timed out after [1201701ms]
[2022-04-15T21:34:08,165][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1345][38] duration [919ms], collections [1]/[2s], total [919ms]/[7.3m], memory [296mb]->[242.3mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [236.6mb]->[236.6mb]/[2gb]}{[survivor] [3.4mb]->[5.7mb]/[0b]}
[2022-04-15T21:34:08,770][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1345] overhead, spent [919ms] collecting in the last [2s]
[2022-04-15T21:34:09,431][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:34:16,353][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1347][39] duration [2.6s], collections [1]/[1.6s], total [2.6s]/[7.3m], memory [322.3mb]->[326.3mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [236.6mb]->[236.6mb]/[2gb]}{[survivor] [5.7mb]->[13mb]/[0b]}
[2022-04-15T21:34:21,683][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1347] overhead, spent [2.6s] collecting in the last [1.6s]
[2022-04-15T21:34:22,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [11117ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:34:33,568][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1348][40] duration [3.2s], collections [1]/[16.3s], total [3.2s]/[7.4m], memory [326.3mb]->[264.3mb]/[2gb], all_pools {[young] [0b]->[16mb]/[0b]}{[old] [236.6mb]->[250mb]/[2gb]}{[survivor] [13mb]->[2.2mb]/[0b]}
[2022-04-15T21:34:37,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [9453ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:34:52,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [5237ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:36:57,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:36:59,622][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1350][41] duration [51.7s], collections [1]/[12s], total [51.7s]/[8.2m], memory [296.3mb]->[332.3mb]/[2gb], all_pools {[young] [48mb]->[88mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [2.2mb]->[2.2mb]/[0b]}
[2022-04-15T21:37:06,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1350] overhead, spent [51.7s] collecting in the last [12s]
[2022-04-15T21:37:11,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68266575835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:16,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [110464ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:37:21,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:32,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31447459654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:42,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:51,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21336898855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:00,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:20,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16703396331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:28,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29535ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:46,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29535470429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:04,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:23,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35390566118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:35,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:42,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30673261334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:27,328][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35391ms] which is above the warn threshold of [5s]
[2022-04-15T21:39:58,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:17,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20478801816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:26,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:27,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31069644802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:31,276][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [366015ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T21:40:43,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8191ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:46,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10975984359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:45,011][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6.4m] publication of cluster state version [9965] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T21:40:47,921][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:40272}] took [21549ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:40:50,906][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1351][42] duration [4.9s], collections [1]/[5.7m], total [4.9s]/[8.3m], memory [332.3mb]->[304.3mb]/[2gb], all_pools {[young] [88mb]->[56mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [2.2mb]->[2.2mb]/[0b]}
[2022-04-15T21:40:53,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [16854ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:41:13,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:41:15,004][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1356][43] duration [7s], collections [1]/[10.7s], total [7s]/[8.4m], memory [332.3mb]->[254.4mb]/[2gb], all_pools {[young] [84mb]->[44mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [2.2mb]->[4.4mb]/[0b]}
[2022-04-15T21:41:14,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9851453360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:41:17,100][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1356] overhead, spent [7s] collecting in the last [10.7s]
[2022-04-15T21:41:19,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [15115ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:41:58,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [12261ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:42:17,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55bb42c2, interval=5s}] took [5609ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:43:18,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [43260ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:44:19,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:44:50,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9946644617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:45:41,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:46:12,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63097210403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:46:50,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:47:20,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73795584975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:47:32,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:15,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.1s/56145352539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:46,022][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58296}] took [272709ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:48:33,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:58,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59691959408ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:52:24,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:49,777][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [59692ms] which is above the warn threshold of [5s]
[2022-04-15T21:52:37,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231880050151ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:52:51,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:52:52,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [28200ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:53:07,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28200323628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:53:30,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:53:56,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39017585934ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:07,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:10,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@660d3297] took [36541ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:54:22,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36541954497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:49,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:09,615][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=251, version=9965}] took [6.5m] which is above the warn threshold of [30s]: [running task [Publication{term=251, version=9965}]] took [216ms], [connecting to new nodes] took [117ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [4983ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [11672ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc440@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [160ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [155ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@7c05388] took [43ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [47ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [32036ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [72743ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [660ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [718ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [45991ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [15288ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [79645ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [575ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [338ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [101354ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [24219ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [193ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [24054ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [262ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [164ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [3772ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [775ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [699ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [192ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [9503ms], [org.elasticsearch.license.LicenseService@16e477b6] took [106ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [194ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [2906ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [375ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-04-15T21:55:06,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42478837211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:55:28,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:55:42,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38587977424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:56:07,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:56:31,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38374428913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:56:47,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:09,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40115627586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:27,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:35,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39506548134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:47,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:58:28,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21862463742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:58:59,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:59:12,499][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1363][44] duration [2.7m], collections [1]/[15.1m], total [2.7m]/[11.2m], memory [314.4mb]->[260.3mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [4.4mb]->[6.3mb]/[0b]}
[2022-04-15T21:59:18,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66410430098ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:59:37,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [88272ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:59:37,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:00:36,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37711756856ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:06,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:18,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93654627293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:27,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:59,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21884414698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:02:20,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:02:28,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51716636706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:02:39,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:16,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18483245548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:31,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:21,440][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18483ms] which is above the warn threshold of [5s]
[2022-04-15T22:03:46,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53737386273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:54,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:30,804][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10015] timed out after [492304ms]
[2022-04-15T22:04:06,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23299182565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:04:45,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:04:54,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49995655920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:05:16,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:05:35,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30173851887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:05:51,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:06:01,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35673477288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:06:15,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [35673ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:06:17,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:00,681][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T22:06:56,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26054516641ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:10,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:18,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53412525994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:38,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:47,573][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.7m/706257ms] to compute cluster state update for [put-mapping [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-15T22:07:51,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28329251314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:08:03,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:08:26,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485172377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:08:49,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:09:05,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45181839311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:09:37,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:09:52,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48099032147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:03,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:09,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@27d5829a] took [27440ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:10:20,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27440486011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:37,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:11:03,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32636910267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:09,439][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.4m/929229ms] ago, timed out [7.2m/436925ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10015]
[2022-04-15T22:11:27,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:12:09,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46035408185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:13:15,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112134ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:13:26,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [112133ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:13:39,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112133661396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:14:06,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:14:40,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47182262997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:16:06,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:16:56,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107450150082ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:17:16,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:18:23,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85516213262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:19:27,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:20:07,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122221516555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:20:55,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:23:35,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97024966611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:24:14,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:24:46,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189251730554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:25:10,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67255ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:25:24,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67255198583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:25:46,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31224ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:26:08,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31223751911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:26:27,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:26:47,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46291631175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:27:26,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.8s/58887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:27:51,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.7s/58796333129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:28:43,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:28:57,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76694812868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:29:26,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:29:45,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31067771806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:29:56,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:30:16,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42931233585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:31:05,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:31:18,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68627599436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:31:39,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:37:41,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33114336803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:18,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:37:45,852][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1322896ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T22:38:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400051242643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:17,751][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [21.5m/1295455ms] ago, timed out [1.6m/101742ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10154]
[2022-04-15T22:38:31,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:34,140][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1367][45] duration [5.1m], collections [1]/[26.4m], total [5.1m]/[16.3m], memory [284.3mb]->[255.3mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [6.3mb]->[5.3mb]/[0b]}
[2022-04-15T22:38:37,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12417826770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:43,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [412469ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:38:45,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:34,563][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [30.1m] publication of cluster state version [9966] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T22:38:57,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14598970153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:39:19,533][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25735ms] which is above the warn threshold of [5s]
[2022-04-15T22:39:19,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:39:30,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25734447960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:39:01,067][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10154] timed out after [1193713ms]
[2022-04-15T22:39:48,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:40:07,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37227949040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:40:42,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.9s/52955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:40:46,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.9s/52954527343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:09,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:40,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27499619055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:49,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:54,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40163262264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:01,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:06,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12078731691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:15,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:20,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14277387180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:26,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:25,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [14277ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:42:30,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11047107207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:33,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:44,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@525916e6] took [7091ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:42:44,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7091280267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:54,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:03,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19859403797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:13,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:24,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [19500ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:43:23,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19500033952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:32,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:48,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19667893059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:13,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:20,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35976554775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:31,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:29,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@2f2fae0a, interval=1m}] took [20919ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:44:39,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20919200622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:49,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:02,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18465027319ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:41,533][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10255] timed out after [59027ms]
[2022-04-15T22:44:44,213][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [10254] timed out after [59027ms]
[2022-04-15T22:45:25,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [28808ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:45:22,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:29,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28808871032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:35,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:41,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17189852152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:38,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [17189ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:45:46,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:51,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12130983486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:51,416][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=251, version=9966}] took [6.5m] which is above the warn threshold of [30s]: [running task [Publication{term=251, version=9966}]] took [275ms], [connecting to new nodes] took [765ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [146999ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [2285ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc440@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [1ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [1421ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [70ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@7c05388] took [470ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [237ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [100283ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [86ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [60705ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [598ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [26270ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [2516ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [24475ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [9388ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [7040ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [150ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [2520ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [202ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [304ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [77ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [0ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-04-15T22:45:56,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:03,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10145351926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:20,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:37,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23429573407ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:49,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:47:03,910][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [50.4m/3028864ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [1.1h/3990510ms]
[2022-04-15T22:47:05,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28366221494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:47:37,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:47:56,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [48795ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:47:55,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48795806819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:11,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:44,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34609388050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:57,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@6fd30db2] took [34609ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:49:03,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.3s/50300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:49:28,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50299612044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:50:00,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:44,236][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.6m/337864ms] ago, timed out [4.6m/278837ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10254]
[2022-04-15T22:50:27,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56672574673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:50:49,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:51:23,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50126275008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:51:37,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:51:58,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48205848865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:52:27,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50s/50085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:52:45,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50s/50085058969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:53:23,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55s/55095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:53:51,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55s/55094071646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:54:10,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:55:05,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47170828458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:55:25,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:56:13,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [73873ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:56:19,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73873527614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:56:58,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:57:37,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93377092626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:56:20,628][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [73874ms] which is above the warn threshold of [5s]
[2022-04-15T22:58:05,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66546ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:57:05,602][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [10276] timed out after [431528ms]
[2022-04-15T22:58:57,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66401828469ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:00:04,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:58:44,736][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.4m/929171ms] ago, timed out [14.5m/870144ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10255]
[2022-04-15T23:01:02,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121527762447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:02:24,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:02:53,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139613471668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:03:55,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89356ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:05:08,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89355531562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:05:53,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119974ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:06:16,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119974517563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:06:58,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:11,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65209228019ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:28,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:46,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29313676920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:09,127][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10277] timed out after [524905ms]
[2022-04-15T23:08:04,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:18,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35609459629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:33,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:49,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29540565097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:09:08,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:09:30,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34182481591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:52,556][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [20.3m/1221451ms] ago, timed out [13.1m/789923ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10276]
[2022-04-15T23:09:47,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:10:12,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39852641233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:10:41,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:10:55,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.8s/53839631046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:11:14,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:11:52,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29349531998ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:12:09,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:13:35,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59179003687ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:12:39,125][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.2s/10294ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d280719b], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b7b3c778]], which exceeds the warn threshold of [10s]
[2022-04-15T23:14:21,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:14:43,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131268457551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:15:24,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [54769ms] which is above the warn threshold of [5000ms]
[2022-04-15T23:15:20,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:15:51,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54769569054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:16:29,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:17:17,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73292810911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:18:00,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:18:52,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87208747397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:20:14,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:22:04,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137799030052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:20:24,236][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [137799ms] which is above the warn threshold of [5s]
[2022-04-15T23:24:24,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/247696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:25:13,804][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [36.1m/2169889ms] ago, timed out [27.4m/1644984ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10277]
[2022-04-15T23:25:36,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/247696256964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:26:57,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:27:47,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149045904700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:28:17,233][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.7s/10788ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b7b3c778], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@714fccdc]], which exceeds the warn threshold of [10s]
[2022-04-15T23:28:47,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:29:53,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113532937665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:31:17,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:33:13,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149396502987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:35:41,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260978ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:35:43,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [260563ms] which is above the warn threshold of [5000ms]
[2022-04-15T23:38:12,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260563465994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:40:36,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/300200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:44:09,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/300164167238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:46:35,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:48:26,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@4382391e] took [659068ms] which is above the warn threshold of [5000ms]
[2022-04-15T23:49:03,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358903905444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:51:40,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:54:12,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294943897958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:57:05,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:59:38,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333417306561ns] on relative clock which is above the warn threshold of [5000ms]
