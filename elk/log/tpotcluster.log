[2022-03-29T00:00:04,448][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.29] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-29T00:00:05,289][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.29][0]]]).
[2022-03-29T00:00:06,951][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:13,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:21,225][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:31,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:35,801][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:39,859][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:41,252][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:42,864][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:43,895][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:44,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:45,414][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:46,334][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:46,922][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:49,079][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:53,957][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:00:58,787][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:03,534][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:10,514][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:15,193][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:23,095][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:34,583][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:01:50,986][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11.4s] publication of cluster state version [4215] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:02:35,529][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:02:42,148][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.8s/16817ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:02:59,005][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12002ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:03:03,982][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.6s] publication of cluster state version [4216] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:03:19,281][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:31,797][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:45,375][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:03:46,554][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:07,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:09,457][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:09,458][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][820][93] duration [1.1s], collections [1]/[2.7s], total [1.1s]/[3.1m], memory [1.3gb]->[240.6mb]/[2gb], all_pools {[young] [1.1gb]->[24mb]/[0b]}{[old] [203.6mb]->[204.6mb]/[2gb]}{[survivor] [10.9mb]->[12mb]/[0b]}
[2022-03-29T00:04:09,649][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][820] overhead, spent [1.1s] collecting in the last [2.7s]
[2022-03-29T00:04:18,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:22,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][828][94] duration [1.4s], collections [1]/[1.1s], total [1.4s]/[3.1m], memory [276.6mb]->[280.6mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [204.6mb]->[204.6mb]/[2gb]}{[survivor] [12mb]->[12mb]/[0b]}
[2022-03-29T00:04:22,985][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][828] overhead, spent [1.4s] collecting in the last [1.1s]
[2022-03-29T00:04:25,718][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][829][95] duration [1.1s], collections [1]/[5.2s], total [1.1s]/[3.2m], memory [280.6mb]->[216.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [204.6mb]->[211.9mb]/[2gb]}{[survivor] [12mb]->[4.9mb]/[0b]}
[2022-03-29T00:04:30,878][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:04:40,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][836][96] duration [5s], collections [1]/[6.1s], total [5s]/[3.2m], memory [280.9mb]->[217.5mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [4.9mb]->[5.5mb]/[0b]}
[2022-03-29T00:04:40,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:40,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5825300232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:40,434][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][836] overhead, spent [5s] collecting in the last [6.1s]
[2022-03-29T00:04:52,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,399][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][837][97] duration [7s], collections [1]/[1.9s], total [7s]/[3.4m], memory [217.5mb]->[297.5mb]/[2gb], all_pools {[young] [0b]->[20mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [5.5mb]->[6.5mb]/[0b]}
[2022-03-29T00:04:53,478][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][837] overhead, spent [7s] collecting in the last [1.9s]
[2022-03-29T00:04:53,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [10206ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9542982608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:04:53,933][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [21199ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:04:54,131][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [22.4s] publication of cluster state version [4222] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:04:59,748][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:02,960][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][842][98] duration [1.6s], collections [1]/[3.8s], total [1.6s]/[3.4m], memory [274.5mb]->[222.9mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.5mb]->[6.9mb]/[0b]}
[2022-03-29T00:05:03,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][842] overhead, spent [1.6s] collecting in the last [3.8s]
[2022-03-29T00:05:10,710][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][848][99] duration [738ms], collections [1]/[1.7s], total [738ms]/[3.4m], memory [278.9mb]->[276.5mb]/[2gb], all_pools {[young] [64mb]->[56mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.9mb]->[8.5mb]/[0b]}
[2022-03-29T00:05:10,869][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][848] overhead, spent [738ms] collecting in the last [1.7s]
[2022-03-29T00:05:15,066][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][849][100] duration [2.1s], collections [1]/[4.3s], total [2.1s]/[3.4m], memory [276.5mb]->[286.1mb]/[2gb], all_pools {[young] [56mb]->[72mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [8.5mb]->[6.1mb]/[0b]}
[2022-03-29T00:05:15,560][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][849] overhead, spent [2.1s] collecting in the last [4.3s]
[2022-03-29T00:05:15,406][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:23,360][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][853][101] duration [1.5s], collections [1]/[1.3s], total [1.5s]/[3.5m], memory [302.1mb]->[306.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.1mb]->[5.7mb]/[0b]}
[2022-03-29T00:05:23,553][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][853] overhead, spent [1.5s] collecting in the last [1.3s]
[2022-03-29T00:05:24,498][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:38,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][862][102] duration [1.9s], collections [1]/[3.6s], total [1.9s]/[3.5m], memory [297.7mb]->[226.6mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [5.7mb]->[6.6mb]/[0b]}
[2022-03-29T00:05:39,351][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][862] overhead, spent [1.9s] collecting in the last [3.6s]
[2022-03-29T00:05:49,638][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:05:50,083][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][867][103] duration [3.2s], collections [1]/[4.9s], total [3.2s]/[3.5m], memory [274.6mb]->[219.9mb]/[2gb], all_pools {[young] [60mb]->[48mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6.6mb]->[7.9mb]/[0b]}
[2022-03-29T00:05:50,196][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][867] overhead, spent [3.2s] collecting in the last [4.9s]
[2022-03-29T00:05:50,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5207ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:05:50,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:01,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][873][104] duration [1.9s], collections [1]/[3.1s], total [1.9s]/[3.6m], memory [303.9mb]->[218mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [7.9mb]->[6mb]/[0b]}
[2022-03-29T00:06:01,826][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][873] overhead, spent [1.9s] collecting in the last [3.1s]
[2022-03-29T00:06:03,738][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:07,177][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][875][105] duration [2s], collections [1]/[1.2s], total [2s]/[3.6m], memory [270mb]->[306mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [211.9mb]->[211.9mb]/[2gb]}{[survivor] [6mb]->[9.8mb]/[0b]}
[2022-03-29T00:06:07,378][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][875] overhead, spent [2s] collecting in the last [1.2s]
[2022-03-29T00:06:09,191][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][876][106] duration [861ms], collections [1]/[4.9s], total [861ms]/[3.6m], memory [306mb]->[222mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [211.9mb]->[213.2mb]/[2gb]}{[survivor] [9.8mb]->[8.8mb]/[0b]}
[2022-03-29T00:06:16,352][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][882] overhead, spent [263ms] collecting in the last [1s]
[2022-03-29T00:06:24,023][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:06:39,759][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][901] overhead, spent [370ms] collecting in the last [1.2s]
[2022-03-29T00:06:56,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][910][112] duration [3.6s], collections [1]/[5.6s], total [3.6s]/[3.7m], memory [295.6mb]->[221.1mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [213.2mb]->[213.2mb]/[2gb]}{[survivor] [6.3mb]->[7.8mb]/[0b]}
[2022-03-29T00:07:00,366][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][910] overhead, spent [3.6s] collecting in the last [5.6s]
[2022-03-29T00:07:01,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5455ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:07:09,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:09,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][911][113] duration [2.2s], collections [1]/[7.8s], total [2.2s]/[3.7m], memory [221.1mb]->[293.2mb]/[2gb], all_pools {[young] [4mb]->[80mb]/[0b]}{[old] [213.2mb]->[213.2mb]/[2gb]}{[survivor] [7.8mb]->[7.9mb]/[0b]}
[2022-03-29T00:07:10,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5274535049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:10,293][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][911] overhead, spent [2.2s] collecting in the last [7.8s]
[2022-03-29T00:07:10,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5674ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:07:12,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][912][114] duration [3.8s], collections [1]/[7.6s], total [3.8s]/[3.8m], memory [293.2mb]->[256.9mb]/[2gb], all_pools {[young] [80mb]->[36mb]/[0b]}{[old] [213.2mb]->[214mb]/[2gb]}{[survivor] [7.9mb]->[6.9mb]/[0b]}
[2022-03-29T00:07:12,844][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][912] overhead, spent [3.8s] collecting in the last [7.6s]
[2022-03-29T00:07:45,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:45,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11568672739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:07:46,839][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][920][115] duration [8.1s], collections [1]/[2.9s], total [8.1s]/[3.9m], memory [304.9mb]->[304.9mb]/[2gb], all_pools {[young] [84mb]->[84mb]/[0b]}{[old] [214mb]->[214mb]/[2gb]}{[survivor] [6.9mb]->[6.9mb]/[0b]}
[2022-03-29T00:07:51,154][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][920] overhead, spent [8.1s] collecting in the last [2.9s]
[2022-03-29T00:07:53,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [24156ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:25,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:26,172][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][922][116] duration [10.7s], collections [1]/[17.3s], total [10.7s]/[4.1m], memory [263.4mb]->[219.2mb]/[2gb], all_pools {[young] [68mb]->[28mb]/[0b]}{[old] [214mb]->[214.2mb]/[2gb]}{[survivor] [5.3mb]->[4.9mb]/[0b]}
[2022-03-29T00:08:26,563][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][922] overhead, spent [10.7s] collecting in the last [17.3s]
[2022-03-29T00:08:26,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [14086ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:26,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14086186061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:27,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:08:31,917][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24s/24014ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:08:51,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,543][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][930][117] duration [6.4s], collections [1]/[1.6s], total [6.4s]/[4.2m], memory [287.2mb]->[287.2mb]/[2gb], all_pools {[young] [68mb]->[72mb]/[0b]}{[old] [214.2mb]->[214.2mb]/[2gb]}{[survivor] [4.9mb]->[4.9mb]/[0b]}
[2022-03-29T00:08:51,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8110480180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][930] overhead, spent [6.4s] collecting in the last [1.6s]
[2022-03-29T00:08:51,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [10915ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:51,585][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38574}] took [8511ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:59,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][932][118] duration [3.1s], collections [1]/[1.4s], total [3.1s]/[4.3m], memory [248.1mb]->[252.1mb]/[2gb], all_pools {[young] [28mb]->[88mb]/[0b]}{[old] [214.2mb]->[214.2mb]/[2gb]}{[survivor] [5.8mb]->[5.8mb]/[0b]}
[2022-03-29T00:08:59,515][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][932] overhead, spent [3.1s] collecting in the last [1.4s]
[2022-03-29T00:08:59,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5359ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:08:58,854][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [15160ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:09:07,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:09:07,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][933][119] duration [5.2s], collections [1]/[6.3s], total [5.2s]/[4.4m], memory [252.1mb]->[219.6mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [214.2mb]->[214.4mb]/[2gb]}{[survivor] [5.8mb]->[5.1mb]/[0b]}
[2022-03-29T00:09:08,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6468673732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:09:08,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][933] overhead, spent [5.2s] collecting in the last [6.3s]
[2022-03-29T00:09:08,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6468ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:09:00,546][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.1s] publication of cluster state version [4229] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:09:17,459][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][939][120] duration [1s], collections [1]/[1.8s], total [1s]/[4.4m], memory [299.6mb]->[241mb]/[2gb], all_pools {[young] [80mb]->[44mb]/[0b]}{[old] [214.4mb]->[214.4mb]/[2gb]}{[survivor] [5.1mb]->[6.6mb]/[0b]}
[2022-03-29T00:09:18,067][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][939] overhead, spent [1s] collecting in the last [1.8s]
[2022-03-29T00:09:25,905][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][944] overhead, spent [550ms] collecting in the last [1.4s]
[2022-03-29T00:09:33,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][949][122] duration [1s], collections [1]/[2s], total [1s]/[4.4m], memory [297.3mb]->[220.8mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [214.4mb]->[214.4mb]/[2gb]}{[survivor] [6.9mb]->[6.3mb]/[0b]}
[2022-03-29T00:09:33,523][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][949] overhead, spent [1s] collecting in the last [2s]
[2022-03-29T00:09:41,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][955] overhead, spent [421ms] collecting in the last [1.1s]
[2022-03-29T00:09:50,430][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][962][124] duration [727ms], collections [1]/[1.7s], total [727ms]/[4.4m], memory [280.7mb]->[221.8mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [214.5mb]->[214.5mb]/[2gb]}{[survivor] [6.1mb]->[7.2mb]/[0b]}
[2022-03-29T00:09:50,735][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][962] overhead, spent [727ms] collecting in the last [1.7s]
[2022-03-29T00:09:58,941][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][968] overhead, spent [394ms] collecting in the last [1.2s]
[2022-03-29T00:10:06,183][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][974] overhead, spent [314ms] collecting in the last [1.1s]
[2022-03-29T00:10:34,686][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][998] overhead, spent [301ms] collecting in the last [1s]
[2022-03-29T00:19:22,359][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [7559] timed out after [15963ms]
[2022-03-29T00:19:29,227][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [25.7s/25720ms] ago, timed out [9.7s/9757ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [7559]
[2022-03-29T00:20:43,320][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1532][144] duration [3s], collections [1]/[5.4s], total [3s]/[4.5m], memory [1.3gb]->[224.8mb]/[2gb], all_pools {[young] [1gb]->[0b]/[0b]}{[old] [216.7mb]->[216.8mb]/[2gb]}{[survivor] [6.4mb]->[8mb]/[0b]}
[2022-03-29T00:20:46,638][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1532] overhead, spent [3s] collecting in the last [5.4s]
[2022-03-29T00:20:48,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1533][145] duration [1.8s], collections [1]/[5.6s], total [1.8s]/[4.6m], memory [224.8mb]->[260.3mb]/[2gb], all_pools {[young] [0b]->[36mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [8mb]->[7.4mb]/[0b]}
[2022-03-29T00:20:49,249][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1533] overhead, spent [1.8s] collecting in the last [5.6s]
[2022-03-29T00:20:55,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1536][146] duration [1.4s], collections [1]/[2.6s], total [1.4s]/[4.6m], memory [264.3mb]->[232.2mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [7.4mb]->[7.3mb]/[0b]}
[2022-03-29T00:20:55,988][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1536] overhead, spent [1.4s] collecting in the last [2.6s]
[2022-03-29T00:21:07,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1540][147] duration [2.6s], collections [1]/[5.4s], total [2.6s]/[4.6m], memory [248.2mb]->[226.1mb]/[2gb], all_pools {[young] [24mb]->[8mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [7.3mb]->[5.2mb]/[0b]}
[2022-03-29T00:21:08,578][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1540] overhead, spent [2.6s] collecting in the last [5.4s]
[2022-03-29T00:21:26,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:21:27,423][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1547][148] duration [3.9s], collections [1]/[6.3s], total [3.9s]/[4.7m], memory [290.1mb]->[223.5mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [216.8mb]->[216.8mb]/[2gb]}{[survivor] [5.2mb]->[6.7mb]/[0b]}
[2022-03-29T00:21:27,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1547] overhead, spent [3.9s] collecting in the last [6.3s]
[2022-03-29T00:21:27,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5198938076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:21:27,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5198ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:22:10,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:22:11,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740151619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:22:12,039][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1559][149] duration [9.9s], collections [1]/[14.7s], total [9.9s]/[4.8m], memory [279.5mb]->[224.3mb]/[2gb], all_pools {[young] [56mb]->[36mb]/[0b]}{[old] [216.8mb]->[216.9mb]/[2gb]}{[survivor] [6.7mb]->[7.4mb]/[0b]}
[2022-03-29T00:22:13,598][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1559] overhead, spent [9.9s] collecting in the last [14.7s]
[2022-03-29T00:22:19,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [21678ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:22:32,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [7550ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:22,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:22,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@b7390be] took [22529ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:23,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20728762092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:24,139][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1561][150] duration [15.1s], collections [1]/[31.2s], total [15.1s]/[5.1m], memory [268.3mb]->[233.5mb]/[2gb], all_pools {[young] [44mb]->[52mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[8.6mb]/[0b]}
[2022-03-29T00:23:24,901][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1561] overhead, spent [15.1s] collecting in the last [31.2s]
[2022-03-29T00:23:37,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,393][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38574}] took [10536ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735577427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:38,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1562][151] duration [6.3s], collections [1]/[15.2s], total [6.3s]/[5.2m], memory [233.5mb]->[268.6mb]/[2gb], all_pools {[young] [52mb]->[48mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [8.6mb]->[7.6mb]/[0b]}
[2022-03-29T00:23:38,801][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1562] overhead, spent [6.3s] collecting in the last [15.2s]
[2022-03-29T00:23:56,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77bf7db2, interval=5s}] took [7379ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:23:56,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:57,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379302816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:23:57,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1568][152] duration [5.9s], collections [1]/[8.6s], total [5.9s]/[5.3m], memory [284.6mb]->[244.3mb]/[2gb], all_pools {[young] [88mb]->[28mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.6mb]->[7.4mb]/[0b]}
[2022-03-29T00:23:57,275][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1568] overhead, spent [5.9s] collecting in the last [8.6s]
[2022-03-29T00:23:57,544][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:23:57,721][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.7s/13783ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:24:05,783][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1573][153] duration [1.2s], collections [1]/[2.7s], total [1.2s]/[5.3m], memory [272.3mb]->[232.4mb]/[2gb], all_pools {[young] [48mb]->[32mb]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-29T00:24:06,128][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1573] overhead, spent [1.2s] collecting in the last [2.7s]
[2022-03-29T00:24:34,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7917ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:24:34,574][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1585][154] duration [5.7s], collections [1]/[2.4s], total [5.7s]/[5.4m], memory [308.4mb]->[312.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [216.9mb]->[216.9mb]/[2gb]}{[survivor] [7.4mb]->[6.4mb]/[0b]}
[2022-03-29T00:24:34,793][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1585] overhead, spent [5.7s] collecting in the last [2.4s]
[2022-03-29T00:24:34,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7916834588ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:24:34,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [9009ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:24:41,062][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=146, version=4230}] took [33.6s] which is above the warn threshold of [30s]: [running task [Publication{term=146, version=4230}]] took [103ms], [connecting to new nodes] took [26ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [146ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [1409ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [137ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [24ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016a7960@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [29ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [144ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801316e58@7c05388] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013da000@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [34ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [12953ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [13048ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [83ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [1166ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [855ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [2138ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [44ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [492ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [42ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [232ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [24ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [127ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-03-29T00:24:52,201][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1592][155] duration [2.8s], collections [1]/[1.6s], total [2.8s]/[5.5m], memory [283.4mb]->[311.4mb]/[2gb], all_pools {[young] [60mb]->[20mb]/[0b]}{[old] [216.9mb]->[217.2mb]/[2gb]}{[survivor] [6.4mb]->[6.3mb]/[0b]}
[2022-03-29T00:24:54,240][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1592] overhead, spent [2.8s] collecting in the last [1.6s]
[2022-03-29T00:24:54,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7540ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:43,612][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1598][156] duration [21.2s], collections [1]/[2.9s], total [21.2s]/[5.8m], memory [279.5mb]->[287.5mb]/[2gb], all_pools {[young] [60mb]->[12mb]/[0b]}{[old] [217.2mb]->[217.3mb]/[2gb]}{[survivor] [6.3mb]->[6.9mb]/[0b]}
[2022-03-29T00:25:42,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:43,298][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38900}] took [26419ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:44,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25522368770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:44,144][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1598] overhead, spent [21.2s] collecting in the last [2.9s]
[2022-03-29T00:25:47,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [25722ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:25:47,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:25:54,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5748842916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:00,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:06,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13093898888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:10,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [9183ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:26:09,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:04,483][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13094ms] which is above the warn threshold of [5s]
[2022-03-29T00:26:13,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9183731479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:15,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:17,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738345569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:18,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5738ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:26:38,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:39,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18586912199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:26:40,206][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1601][157] duration [14.6s], collections [1]/[24.5s], total [14.6s]/[6.1m], memory [248.3mb]->[230.6mb]/[2gb], all_pools {[young] [32mb]->[8mb]/[0b]}{[old] [217.3mb]->[217.4mb]/[2gb]}{[survivor] [6.9mb]->[9.2mb]/[0b]}
[2022-03-29T00:26:43,668][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1601] overhead, spent [14.6s] collecting in the last [24.5s]
[2022-03-29T00:26:45,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7764ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:00,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:27:01,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5198711914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:27:01,190][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1605][158] duration [3.6s], collections [1]/[1.4s], total [3.6s]/[6.1m], memory [302.6mb]->[310.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [217.4mb]->[218.5mb]/[2gb]}{[survivor] [9.2mb]->[8.8mb]/[0b]}
[2022-03-29T00:27:01,507][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1605] overhead, spent [3.6s] collecting in the last [1.4s]
[2022-03-29T00:27:01,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:03,156][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:27:08,612][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1610][160] duration [722ms], collections [1]/[2s], total [722ms]/[6.2m], memory [300.1mb]->[229.8mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [218.5mb]->[219.3mb]/[2gb]}{[survivor] [9.5mb]->[6.4mb]/[0b]}
[2022-03-29T00:27:08,995][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1610] overhead, spent [722ms] collecting in the last [2s]
[2022-03-29T00:27:09,602][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:27:17,569][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1615][161] duration [1.8s], collections [1]/[3.5s], total [1.8s]/[6.2m], memory [281.8mb]->[224.6mb]/[2gb], all_pools {[young] [56mb]->[60mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [6.4mb]->[5.2mb]/[0b]}
[2022-03-29T00:27:17,782][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1615] overhead, spent [1.8s] collecting in the last [3.5s]
[2022-03-29T00:27:28,364][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1620][162] duration [2.1s], collections [1]/[3.9s], total [2.1s]/[6.2m], memory [304.6mb]->[225.8mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [5.2mb]->[6.4mb]/[0b]}
[2022-03-29T00:27:30,705][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1620] overhead, spent [2.1s] collecting in the last [3.9s]
[2022-03-29T00:27:32,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7268ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:27:34,658][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1621][163] duration [1.1s], collections [1]/[6.4s], total [1.1s]/[6.2m], memory [225.8mb]->[242.1mb]/[2gb], all_pools {[young] [28mb]->[16mb]/[0b]}{[old] [219.3mb]->[219.3mb]/[2gb]}{[survivor] [6.4mb]->[6.7mb]/[0b]}
[2022-03-29T00:27:43,997][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1624][164] duration [2.5s], collections [1]/[4.9s], total [2.5s]/[6.3m], memory [246.1mb]->[268.8mb]/[2gb], all_pools {[young] [24mb]->[44mb]/[0b]}{[old] [219.3mb]->[219.5mb]/[2gb]}{[survivor] [6.7mb]->[5.3mb]/[0b]}
[2022-03-29T00:27:44,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1624] overhead, spent [2.5s] collecting in the last [4.9s]
[2022-03-29T00:27:46,126][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:28:25,542][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=146, version=4233}] took [33.9s] which is above the warn threshold of [30s]: [running task [Publication{term=146, version=4233}]] took [19ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [4283ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [154ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016a7960@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [104ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801316e58@7c05388] took [1ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013da000@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [22ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [12294ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [2013ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [70ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [2565ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [128ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [4484ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [59ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [4367ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [2060ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [635ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [64ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [22ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [55ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [0ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.bulk.TransportShardBulkAction$1@5b3a0f6a]]]] took [209ms]
[2022-03-29T00:28:36,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:28:36,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@48bc072c] took [8851ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:28:37,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8451787139ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:28:38,232][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1645][165] duration [5.2s], collections [1]/[10.6s], total [5.2s]/[6.4m], memory [304.8mb]->[249mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.3mb]->[5.5mb]/[0b]}
[2022-03-29T00:28:42,212][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1645] overhead, spent [5.2s] collecting in the last [10.6s]
[2022-03-29T00:28:44,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7916ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:13,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:15,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17099018287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:16,693][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1649][166] duration [12.3s], collections [1]/[2.7s], total [12.3s]/[6.6m], memory [265mb]->[313mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.5mb]->[8.4mb]/[0b]}
[2022-03-29T00:29:18,361][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1649] overhead, spent [12.3s] collecting in the last [2.7s]
[2022-03-29T00:29:26,870][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38922}] took [9030ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:26,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [31292ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:29:26,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:27,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7628433739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:28,582][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1650][167] duration [4.9s], collections [1]/[32.6s], total [4.9s]/[6.7m], memory [313mb]->[283.5mb]/[2gb], all_pools {[young] [12mb]->[56mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [8.4mb]->[8mb]/[0b]}
[2022-03-29T00:29:36,134][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1653][168] duration [2s], collections [1]/[4s], total [2s]/[6.7m], memory [311.5mb]->[227.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [8mb]->[7.5mb]/[0b]}
[2022-03-29T00:29:36,607][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1653] overhead, spent [2s] collecting in the last [4s]
[2022-03-29T00:29:58,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:59,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12388181396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:29:59,987][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1659][169] duration [9.4s], collections [1]/[1.5s], total [9.4s]/[6.8m], memory [291.1mb]->[315.1mb]/[2gb], all_pools {[young] [64mb]->[36mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [7.5mb]->[5.5mb]/[0b]}
[2022-03-29T00:30:00,470][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1659] overhead, spent [9.4s] collecting in the last [1.5s]
[2022-03-29T00:30:00,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [12825ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:30:02,668][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:04,681][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.5s/22508ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:30:12,225][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1664][170] duration [2.5s], collections [1]/[1.3s], total [2.5s]/[6.9m], memory [281mb]->[313mb]/[2gb], all_pools {[young] [56mb]->[32mb]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [5.5mb]->[6.3mb]/[0b]}
[2022-03-29T00:30:13,033][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1664] overhead, spent [2.5s] collecting in the last [1.3s]
[2022-03-29T00:30:13,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5250ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:30:22,342][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1669] overhead, spent [1.5s] collecting in the last [1.3s]
[2022-03-29T00:30:19,539][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:34,416][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10007ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [31] unchanged indices
[2022-03-29T00:30:35,603][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.3s] publication of cluster state version [4235] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-29T00:30:54,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:30:58,155][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.9s/12912ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-29T00:31:05,528][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1694][172] duration [2.2s], collections [1]/[4.3s], total [2.2s]/[7m], memory [313.8mb]->[224.6mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [219.5mb]->[219.5mb]/[2gb]}{[survivor] [6.2mb]->[5.1mb]/[0b]}
[2022-03-29T00:31:05,999][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1694] overhead, spent [2.2s] collecting in the last [4.3s]
[2022-03-29T00:31:11,293][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T00:31:32,738][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1710][173] duration [2.7s], collections [1]/[5.2s], total [2.7s]/[7m], memory [304.6mb]->[225.7mb]/[2gb], all_pools {[young] [80mb]->[68mb]/[0b]}{[old] [219.5mb]->[219.7mb]/[2gb]}{[survivor] [5.1mb]->[5.9mb]/[0b]}
[2022-03-29T00:31:34,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1710] overhead, spent [2.7s] collecting in the last [5.2s]
[2022-03-29T00:31:49,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:31:49,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1714][174] duration [4.5s], collections [1]/[7.5s], total [4.5s]/[7.1m], memory [305.7mb]->[230.6mb]/[2gb], all_pools {[young] [80mb]->[56mb]/[0b]}{[old] [219.7mb]->[219.8mb]/[2gb]}{[survivor] [5.9mb]->[6.7mb]/[0b]}
[2022-03-29T00:31:49,947][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1714] overhead, spent [4.5s] collecting in the last [7.5s]
[2022-03-29T00:31:49,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6081ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:31:49,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6081482670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:31:58,829][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1717][175] duration [2.7s], collections [1]/[5.2s], total [2.7s]/[7.1m], memory [294.6mb]->[229.1mb]/[2gb], all_pools {[young] [68mb]->[20mb]/[0b]}{[old] [219.8mb]->[219.8mb]/[2gb]}{[survivor] [6.7mb]->[9.2mb]/[0b]}
[2022-03-29T00:31:59,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1717] overhead, spent [2.7s] collecting in the last [5.2s]
[2022-03-29T00:32:09,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:10,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7104933778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:09,944][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1718][176] duration [5.5s], collections [1]/[4.3s], total [5.5s]/[7.2m], memory [229.1mb]->[313.1mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [219.8mb]->[220.5mb]/[2gb]}{[survivor] [9.2mb]->[7.8mb]/[0b]}
[2022-03-29T00:32:10,292][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1718] overhead, spent [5.5s] collecting in the last [4.3s]
[2022-03-29T00:32:10,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [7305ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:19,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:20,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8130486128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:24,608][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1719][177] duration [6.2s], collections [1]/[19.7s], total [6.2s]/[7.3m], memory [313.1mb]->[235.5mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [220.5mb]->[220.6mb]/[2gb]}{[survivor] [7.8mb]->[6.9mb]/[0b]}
[2022-03-29T00:32:25,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1719] overhead, spent [6.2s] collecting in the last [19.7s]
[2022-03-29T00:32:28,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:51,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@2a5a3cfa] took [13915ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:32:51,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:52,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13715138373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:32:53,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1722][178] duration [10.5s], collections [1]/[18.9s], total [10.5s]/[7.5m], memory [239.5mb]->[260.5mb]/[2gb], all_pools {[young] [16mb]->[32mb]/[0b]}{[old] [220.6mb]->[220.6mb]/[2gb]}{[survivor] [6.9mb]->[7.8mb]/[0b]}
[2022-03-29T00:32:54,063][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1722] overhead, spent [10.5s] collecting in the last [18.9s]
[2022-03-29T00:33:05,135][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1726][179] duration [2.5s], collections [1]/[1.4s], total [2.5s]/[7.5m], memory [280.5mb]->[284.5mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [220.6mb]->[220.6mb]/[2gb]}{[survivor] [7.8mb]->[7.4mb]/[0b]}
[2022-03-29T00:33:05,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1726] overhead, spent [2.5s] collecting in the last [1.4s]
[2022-03-29T00:33:05,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5580ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:33:15,928][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1732][180] duration [2.2s], collections [1]/[3.5s], total [2.2s]/[7.6m], memory [280.1mb]->[230.6mb]/[2gb], all_pools {[young] [52mb]->[28mb]/[0b]}{[old] [220.6mb]->[220.7mb]/[2gb]}{[survivor] [7.4mb]->[5.9mb]/[0b]}
[2022-03-29T00:33:16,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1732] overhead, spent [2.2s] collecting in the last [3.5s]
[2022-03-29T00:33:25,614][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1737][181] duration [1.5s], collections [1]/[2.5s], total [1.5s]/[7.6m], memory [262.6mb]->[225.9mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [5.9mb]->[5.2mb]/[0b]}
[2022-03-29T00:33:26,286][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1737] overhead, spent [1.5s] collecting in the last [2.5s]
[2022-03-29T00:33:32,869][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1740][182] duration [1.4s], collections [1]/[1.1s], total [1.4s]/[7.6m], memory [249.9mb]->[313.9mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [5.2mb]->[7.7mb]/[0b]}
[2022-03-29T00:33:33,862][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1740] overhead, spent [1.4s] collecting in the last [1.1s]
[2022-03-29T00:33:39,606][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1741][183] duration [3.3s], collections [1]/[8.8s], total [3.3s]/[7.7m], memory [313.9mb]->[244mb]/[2gb], all_pools {[young] [0b]->[36mb]/[0b]}{[old] [220.7mb]->[220.7mb]/[2gb]}{[survivor] [7.7mb]->[7.2mb]/[0b]}
[2022-03-29T00:33:40,175][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1741] overhead, spent [3.3s] collecting in the last [8.8s]
[2022-03-29T00:33:40,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [5913ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:33:54,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:33:55,548][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1745][184] duration [5.4s], collections [1]/[1.9s], total [5.4s]/[7.8m], memory [300mb]->[316mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [220.7mb]->[221.4mb]/[2gb]}{[survivor] [7.2mb]->[6.5mb]/[0b]}
[2022-03-29T00:33:55,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251125898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:33:57,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1745] overhead, spent [5.4s] collecting in the last [1.9s]
[2022-03-29T00:33:58,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [12188ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:36:54,576][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:38940}] took [157718ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:36:58,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:36:58,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1751] overhead, spent [2.4m] collecting in the last [4.2s]
[2022-03-29T00:37:13,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157518145053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:16,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [158118ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:37:23,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:30,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32403ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:37:32,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32403076861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:41,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:37:53,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719061400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:03,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:11,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21871101760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:28,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:42,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22754831399ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:52,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:38:59,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@59dfe26f] took [25444ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:39:07,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25444858617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:29,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:47,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38253402042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:05,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:19,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [72483ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:40:30,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34229894778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:39:48,169][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38254ms] which is above the warn threshold of [5s]
[2022-03-29T00:40:55,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50282ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:40:59,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [50282ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:41:10,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50282026189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:24,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:37,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29904802093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:41:54,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:10,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30366249200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:38,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44361ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:42:58,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44360774509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:13,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:25,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35052698535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:43:42,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28188267875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:29,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:44:48,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45659958352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:09,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:22,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41202298523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:36,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:45,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222358581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:56,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20596ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:45:54,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [27222ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:45:31,110][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8761] timed out after [262450ms]
[2022-03-29T00:46:04,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20596147625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:19,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:28,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22772399224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:39,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:48,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18872320898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:46:57,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:10,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19129165867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:36,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:50,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37193522889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:48:10,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:48:18,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [73354ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:48:26,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36160644844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:47:58,594][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [37194ms] which is above the warn threshold of [5s]
[2022-03-29T00:48:50,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32263ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:48:53,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:08,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32263282354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:23,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:42,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40920058378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:49:58,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:14,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34033118200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:34,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:50:59,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35330950039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:19,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:34,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [40554ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:51:33,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40554729451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:51:53,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [32970ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:51:50,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:09,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32970699836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:24,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:47,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37233563416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:07,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:52:46,571][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [13.5m/812754ms] ago, timed out [9.1m/550304ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8761]
[2022-03-29T00:53:12,636][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [43430ms] which is above the warn threshold of [5s]
[2022-03-29T00:53:17,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43430766005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:30,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:40,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21443317887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:53,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [21443ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:53:53,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:53:31,131][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8831] timed out after [183810ms]
[2022-03-29T00:54:04,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24962833494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:16,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:24,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21769557309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:34,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:34,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6229f0b7, interval=1m}] took [19207ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:54:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19207660074ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:54:51,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:05,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17114582095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:15,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:23,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22786695025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:34,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19992ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:44,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19991202445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:55:47,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [19991ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:55:54,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:03,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18543956361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:12,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:20,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [18888ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:56:22,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18888706251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:34,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:43,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21960425086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:56:53,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:04,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19812415737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:15,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:31,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21524731309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:45,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:57:55,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508088434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:10,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:22,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25139815057ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:31,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20703ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:37,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20702881384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [9734ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:40,393][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.7m/587831ms] ago, timed out [6.7m/404021ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8831]
[2022-03-29T00:58:38,416][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20703ms] which is above the warn threshold of [5s]
[2022-03-29T00:58:42,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9734592903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T00:58:42,965][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [8893] timed out after [106610ms]
[2022-03-29T00:59:11,399][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:53020}] took [9368ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:59:11,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [6766ms] which is above the warn threshold of [5000ms]
[2022-03-29T00:59:29,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [8541ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:00:59,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@57a193ee, interval=5s}] took [50720ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:02:54,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@77bf7db2, interval=5s}] took [6563ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:02:59,669][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15514ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@53fb3fe1]], which exceeds the warn threshold of [10s]
[2022-03-29T01:06:06,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4cb7d97d, interval=1s}] took [109173ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:06:19,123][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [19.7s/19726ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf767083], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8cb6533a]], which exceeds the warn threshold of [10s]
[2022-03-29T01:12:21,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:14:53,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/147912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:17:02,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/148304069485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:19:42,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:22:24,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292470109777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:25:31,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:28:32,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343476050295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:31:44,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:33:17,109][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [34.8m/2089711ms] ago, timed out [33m/1983101ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{dy80hAwmTV2j1llaU5jUkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [8893]
[2022-03-29T01:34:55,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379004857617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:38:21,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:38:57,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017f9258@40d0cf29] took [396462ms] which is above the warn threshold of [5000ms]
[2022-03-29T01:41:47,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396462211638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T01:45:41,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T01:48:34,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398362757695ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:04:55,636][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T03:04:55,649][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T03:04:55,650][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T03:05:01,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T03:05:01,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T03:05:01,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T03:05:01,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T03:05:01,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T03:05:01,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T03:05:01,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T03:05:01,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T03:05:01,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T03:05:01,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T03:05:01,945][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T03:05:01,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T03:05:01,947][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T03:05:01,949][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T03:05:01,950][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T03:05:01,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T03:05:01,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T03:05:01,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T03:05:01,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T03:05:01,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T03:05:01,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T03:05:01,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T03:05:01,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T03:05:01,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T03:05:01,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T03:05:01,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T03:05:01,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T03:05:01,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T03:05:01,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T03:05:01,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T03:05:01,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T03:05:01,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T03:05:01,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T03:05:01,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T03:05:01,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T03:05:01,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T03:05:01,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T03:05:01,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T03:05:01,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T03:05:01,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T03:05:01,988][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T03:05:01,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T03:05:01,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T03:05:01,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T03:05:01,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T03:05:01,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T03:05:01,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T03:05:01,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T03:05:01,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T03:05:01,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T03:05:01,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T03:05:01,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T03:05:01,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T03:05:01,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T03:05:02,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T03:05:02,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T03:05:02,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T03:05:02,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T03:05:02,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T03:05:02,098][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T03:05:02,099][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T03:05:02,589][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T03:05:16,597][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T03:05:16,604][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,606][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T03:05:16,608][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,609][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,610][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,611][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,613][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T03:05:16,614][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T03:05:16,615][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T03:05:16,616][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T03:05:16,617][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T03:05:16,619][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T03:05:16,620][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T03:05:17,853][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T03:05:18,122][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T03:05:19,094][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T03:05:20,124][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T03:05:20,132][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T03:05:20,273][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T03:05:20,276][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T03:05:20,608][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T03:05:23,623][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T03:05:23,826][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 147, version: 4238, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T03:05:24,110][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{M5Rm2yggQ6anzsBv2oDMcQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 147, version: 4238, reason: Publication{term=147, version=4238}
[2022-03-29T03:05:24,350][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T03:05:24,354][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T03:07:09,953][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T03:07:10,008][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T03:07:10,009][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T03:07:16,336][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T03:07:16,339][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T03:07:16,340][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T03:07:16,341][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T03:07:16,343][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T03:07:16,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T03:07:16,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T03:07:16,346][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T03:07:16,347][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T03:07:16,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T03:07:16,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T03:07:16,349][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T03:07:16,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T03:07:16,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T03:07:16,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T03:07:16,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T03:07:16,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T03:07:16,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T03:07:16,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T03:07:16,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T03:07:16,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T03:07:16,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T03:07:16,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T03:07:16,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T03:07:16,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T03:07:16,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T03:07:16,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T03:07:16,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T03:07:16,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T03:07:16,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T03:07:16,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T03:07:16,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T03:07:16,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T03:07:16,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T03:07:16,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T03:07:16,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T03:07:16,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T03:07:16,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T03:07:16,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T03:07:16,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T03:07:16,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T03:07:16,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T03:07:16,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T03:07:16,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T03:07:16,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T03:07:16,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T03:07:16,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T03:07:16,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T03:07:16,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T03:07:16,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T03:07:16,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T03:07:16,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T03:07:16,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T03:07:16,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T03:07:16,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T03:07:16,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T03:07:16,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T03:07:16,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T03:07:16,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T03:07:16,489][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T03:07:16,490][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T03:07:16,875][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T03:07:31,606][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T03:07:31,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T03:07:34,015][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T03:07:36,314][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T03:07:38,729][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T03:07:43,741][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T03:07:43,748][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T03:07:43,885][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T03:07:43,887][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T03:07:44,537][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T03:08:02,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5859ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:08:57,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@398115be] took [11029ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:09:21,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [7242ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:09:42,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [10806ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:05,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:28,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [6149ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:10:25,638][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9597ms] which is above the warn threshold of [5s]
[2022-03-29T03:10:45,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [9606ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:01,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5484ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:12,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:11:26,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [5808ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:13:01,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [38032ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:13:38,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [5961ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:16:14,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [40546ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:19:59,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [83485ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:19:50,145][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [94481ms] which is above the warn threshold of [5s]
[2022-03-29T03:20:37,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [6396ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:21:37,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [42602ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:22:18,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [12325ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:23:06,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [30931ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:24:32,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [23687ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:24:09,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:24:05,332][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26489ms] which is above the warn threshold of [5s]
[2022-03-29T03:25:03,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11161483891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:17,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79343124036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:42,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:25:38,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [79343ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:25:50,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24763936888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:02,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:02,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [19946ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:26:12,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19946505707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:25,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:35,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22695058496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:44,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:26:50,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [43491ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:26:53,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20796434931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:01,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [16083ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:00,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:10,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16083290578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:20,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:23,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [19569ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:31,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19569527119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:47,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18675149083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:57,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:27:59,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [36440ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:07,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17765484221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:18,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:18,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [20201ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:27,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20201923492ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:31,665][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20202ms] which is above the warn threshold of [5s]
[2022-03-29T03:28:37,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:48,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20314416752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:28:50,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [20314ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:28:56,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:04,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19405727056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:13,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:20,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [16199ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:29:18,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199006411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:27,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:34,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14607565815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:40,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:44,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [13883ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:29:49,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13883279594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:29:56,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:02,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713177217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:07,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:07,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [11672ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:13,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11672257278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:22,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [13445ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:21,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:25,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13445878624ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:31,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:35,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [10105ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:30:35,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105106284ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:50,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:30:58,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031601477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:10,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:20,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20637983234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:31:22,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [20637ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:31:38,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:00,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26049946844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:21,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:39,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472240626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:32:54,990][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [41472ms] which is above the warn threshold of [5s]
[2022-03-29T03:33:01,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:16,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [82028ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:33:27,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40556092909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:38,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:33:52,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39247869018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:05,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:16,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25870121889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:29,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:39,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [49280ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:34:39,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23409927622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:53,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:34:55,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [24766ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:35:04,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24766154007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:18,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:29,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25961384899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:38,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:35:47,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [45986ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:35:46,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20024870484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:01,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:03,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@398115be] took [22584ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:36:16,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22584537964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:33,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:36:46,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30082484704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:01,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:19,364][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28500ms] which is above the warn threshold of [5s]
[2022-03-29T03:37:19,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28500744792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:37,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:37:47,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [65236ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:37:59,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36735996836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:17,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:19,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [38450ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:38:39,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38450590497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:38:53,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:10,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39023656366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:19,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [39023ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:39:33,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:39:51,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37382014529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:16,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:30,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43710863222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:47,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:40:59,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [73192ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:41:04,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29482028140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:28,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:40,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40986691765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:53,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:41:53,294][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40987ms] which is above the warn threshold of [5s]
[2022-03-29T03:42:07,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27148630597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:23,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:42,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [56030ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:42:37,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28882264745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:42:57,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:43:12,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35067403308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:43:29,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:00,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30558250915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:14,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [30558ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:44:22,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:44:40,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53907127544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:00,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:16,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38762859207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:29,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:35,151][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27834ms] which is above the warn threshold of [5s]
[2022-03-29T03:45:41,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [66597ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:45:42,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27834696628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:45:58,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@430c934f, interval=1m}] took [29621ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:45:57,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:11,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29621062502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:25,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:27,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [27311ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:46:38,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27311254743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:46:51,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:05,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26629393927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:18,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [26629ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:47:21,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:37,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29213070582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:47:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [31681ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:47:53,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:09,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31681459836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:27,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:40,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32890472264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:48:54,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:33,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [61472ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:50:38,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28582525458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:52,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:50:57,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [119075ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:51:04,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119075138476ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:51:16,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:51:16,565][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23023ms] which is above the warn threshold of [5s]
[2022-03-29T03:51:50,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23022818511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T03:54:30,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T03:56:03,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [182378ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:57:21,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182378665365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:00:40,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [352800ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:00:48,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:05:07,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352800395322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:08:50,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/484518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:11:04,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [484849ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:12:54,422][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [484849ms] which is above the warn threshold of [5s]
[2022-03-29T04:13:03,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/484849088500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:17:13,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:21:08,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525459272172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:25:37,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:25:39,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [503200ms] which is above the warn threshold of [5000ms]
[2022-03-29T03:27:21,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T04:28:43,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503200862111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:32:14,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:35:19,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399200492532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:38:29,619][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [399200ms] which is above the warn threshold of [5s]
[2022-03-29T04:38:36,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:40:41,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [382448ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:42:09,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382448855253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:45:20,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:45:53,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [404738ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:48:05,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404738509552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:50:47,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:53:34,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326522723966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T04:56:50,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T04:57:45,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [361595ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:59:53,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361595940180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:02:55,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367708ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:03:00,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@731a4124, interval=1m}] took [367838ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:05:58,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367838217160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:09:01,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:10:42,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [357487ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:12:11,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357487506339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:04:45,476][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [367839ms] which is above the warn threshold of [5s]
[2022-03-29T05:16:57,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:20:08,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481305887369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:23:11,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:24:54,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [372270ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:26:18,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372270076985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:29:40,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/385176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:30:10,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@646aedfa, interval=5s}] took [384905ms] which is above the warn threshold of [5000ms]
[2022-03-29T04:59:28,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T05:32:36,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384905563778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:35:27,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:35:36,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@72915275, interval=30s}] took [354537ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:38:14,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354537806701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:41:14,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:44:38,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342536657470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:44:54,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [342536ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:46:07,973][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [342537ms] which is above the warn threshold of [5s]
[2022-03-29T05:48:18,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:51:23,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426527140739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T05:55:22,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T05:58:11,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [415449ms] which is above the warn threshold of [5000ms]
[2022-03-29T05:59:17,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415449313320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:02:01,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:05:41,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407743001912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:10:41,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:16:37,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [481674ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:19:00,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/481674227636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:19:24,092][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [481674ms] which is above the warn threshold of [5s]
[2022-03-29T06:23:08,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7m/764779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:23:52,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7f99b2d2, interval=5s}] took [764703ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:26:48,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7m/764703805274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:32:18,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/551987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:35:40,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/551848255984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:39:10,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:05:56,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@1f022b3e] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T06:43:06,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416961644651ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T06:47:47,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T06:47:51,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@767f605d, interval=1s}] took [514544ms] which is above the warn threshold of [5000ms]
[2022-03-29T06:49:52,733][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [514545ms] which is above the warn threshold of [5s]
[2022-03-29T06:52:49,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514544217224ns] on relative clock which is above the warn threshold of [5000ms]
