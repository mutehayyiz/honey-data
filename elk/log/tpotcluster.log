[2022-04-15T16:28:58,859][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T16:28:58,879][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T16:28:58,880][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T16:29:06,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T16:29:06,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T16:29:06,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T16:29:06,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T16:29:06,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T16:29:06,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T16:29:06,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T16:29:06,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T16:29:06,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T16:29:06,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T16:29:06,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T16:29:06,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T16:29:06,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T16:29:06,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T16:29:06,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T16:29:06,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T16:29:06,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T16:29:06,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T16:29:06,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T16:29:06,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T16:29:06,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T16:29:06,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T16:29:06,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T16:29:06,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T16:29:06,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T16:29:06,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T16:29:06,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T16:29:06,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T16:29:06,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T16:29:06,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T16:29:06,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T16:29:06,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T16:29:06,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T16:29:06,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T16:29:06,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T16:29:06,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T16:29:06,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T16:29:06,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T16:29:06,571][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.4gb], net total_space [125.8gb], types [ext4]
[2022-04-15T16:29:06,572][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T16:29:06,938][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T16:29:19,789][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T16:29:19,792][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T16:29:20,870][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T16:29:20,996][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T16:29:21,710][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T16:29:22,464][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T16:29:22,465][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T16:29:22,496][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T16:29:22,497][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T16:29:22,683][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T16:29:25,138][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T16:29:25,256][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 249, version: 9743, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T16:29:25,475][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 249, version: 9743, reason: Publication{term=249, version=9743}
[2022-04-15T16:29:25,590][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T16:29:25,590][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T16:29:26,281][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T16:29:26,287][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [51] indices into cluster_state
[2022-04-15T16:29:26,993][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T16:29:26,995][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T16:29:27,876][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-15T16:29:28,196][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T16:29:28,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T16:29:28,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T16:29:29,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:29,149][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:31,540][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T16:29:31,592][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-15T16:29:31,614][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-15T16:29:32,208][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:32,212][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:32,409][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:39,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T16:29:39,712][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-15T16:29:39,715][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,523][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T16:29:40,533][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,734][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,740][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:42,103][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:42,103][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:43,761][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.14][0]]]).
[2022-04-15T16:29:46,738][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-15T16:29:46,966][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-15T16:30:31,251][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 1033 finished with response BulkByScrollResponse[took=180.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-15T16:30:33,246][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 1062 finished with response BulkByScrollResponse[took=1.8s,timed_out=false,sliceId=null,updated=923,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-15T16:30:41,334][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-15T16:31:02,632][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.15] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-15T16:31:02,832][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.15][0]]]).
[2022-04-15T16:31:02,961][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,059][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,075][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,164][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,265][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,561][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,713][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:14,461][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:18,315][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:25,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:25,624][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:58,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [21021ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:31:58,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:56200}] took [17702ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:32:45,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [13619ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:39,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [50698ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:44,857][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:39,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5052873314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,007][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/110289ms] to compute cluster state update for [put-mapping [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-15T16:34:45,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59935170929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7c2b5886, interval=5s}] took [59935ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,370][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [60136ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T16:34:45,577][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][15] duration [2.5s], collections [1]/[1.8m], total [2.5s]/[3.4s], memory [621.2mb]->[289.2mb]/[2gb], all_pools {[young] [372mb]->[32mb]/[0b]}{[old] [179.2mb]->[223.2mb]/[2gb]}{[survivor] [74mb]->[34mb]/[0b]}
[2022-04-15T16:34:45,715][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,050][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,320][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,471][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,661][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,748][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,845][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,947][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:51,028][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:51,977][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:54,992][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:55,066][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:55,710][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:02,996][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:21,593][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:29,601][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:45,108][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:37:02,824][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/BByN9-ZNSay2Jx7Y4INJ4w] deleting index
[2022-04-15T16:37:57,040][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:40:57,767][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-1970.01.01] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-15T16:40:57,861][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-1970.01.01][0]]]).
[2022-04-15T16:40:57,918][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-1970.01.01/768VpBTdRaq7sY8TL1VTBA] update_mapping [_doc]
[2022-04-15T16:41:16,407][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:41:16,889][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:41:17,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:45:09,590][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:46:27,647][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:47:18,011][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:48:39,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:49:53,858][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:49:54,805][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:50:55,859][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:50:55,920][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:51:04,952][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:53:35,049][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:54:20,231][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:57:28,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [39778ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:02:44,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T16:57:27,241][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:47982}] took [20072ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:09:27,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738986263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:11:57,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7m/827710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:14:44,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7m/827351886048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:17:02,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:19:35,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292872190294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:23:19,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:26:33,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320186177290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:27:16,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7c2b5886, interval=5s}] took [320186ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:29:25,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:32:46,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433852585741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:35:27,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:38:45,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361533922365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:39:35,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017e7548@229537ec] took [361533ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:41:57,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:45:58,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376234576774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:49:08,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:52:33,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445510161957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:55:28,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:57:59,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368063366226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:00:20,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:01:13,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@150a185c, interval=1m}] took [305035ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:04:30,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305035025168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:03:41,331][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48024}] took [305035ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:07:19,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:05:25,987][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/90898ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6e931853]], which exceeds the warn threshold of [10s]
[2022-04-15T18:11:59,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417581340987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:11:59,139][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48014}] took [417581ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:16:16,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/534783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:14:49,712][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.2s/14203ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cfc3bd12], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@205e664a], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b4f712ef]], which exceeds the warn threshold of [10s]
[2022-04-15T18:20:41,175][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T18:20:41,199][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T18:20:41,200][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T18:20:47,916][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T18:20:47,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T18:20:47,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T18:20:47,926][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T18:20:47,926][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T18:20:47,928][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T18:20:47,928][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T18:20:47,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T18:20:47,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T18:20:47,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T18:20:47,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T18:20:47,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T18:20:48,043][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.3gb], net total_space [125.8gb], types [ext4]
[2022-04-15T18:20:48,044][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T18:20:48,638][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T18:22:26,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:33,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9465130028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [46991ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38261975048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:58,278][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T18:22:58,287][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,288][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T18:22:58,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,291][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,291][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,292][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-15T18:22:58,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T18:22:58,296][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T18:22:58,298][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,299][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T18:22:59,379][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T18:22:59,547][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T18:23:00,595][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T18:23:01,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T18:23:01,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T18:23:01,598][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T18:23:01,599][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T18:23:01,847][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T18:23:04,347][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T18:23:04,521][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 250, version: 9869, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T18:23:04,739][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 250, version: 9869, reason: Publication{term=250, version=9869}
[2022-04-15T18:23:04,887][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T18:23:04,888][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T18:24:31,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:47,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54877333755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:49,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [70411ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:24:52,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:56,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34627385527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:00,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:03,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852133259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:24,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:28,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24030552232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:32,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:36,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7580037029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:42,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:46,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10234621280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:49,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:56,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517664436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:02,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:06,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11698226405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:10,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:13,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8868062263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:16,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:18,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5874287011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:21,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:24,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5295815051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:28,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:32,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7061536935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:36,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7240051537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249757195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:41,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57126}] took [5946ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:26:43,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@653741e5] took [111244ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:26:45,862][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T18:26:49,469][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9870}] took [3.7m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9870}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [1ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [87ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [458ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [3ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [9ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [5ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [217896ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [49ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [35ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [22ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [145ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [58ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [13ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@6b5d5b43] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [1ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@70204568] took [129ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [1ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [77ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [3355ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@7de096e0] took [1574ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [10ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [15ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:26:50,074][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [52] indices into cluster_state
[2022-04-15T18:27:13,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13620ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-15T18:27:14,749][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.4s] publication of cluster state version [9871] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57126}] took [23626ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57130}] took [17718ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57132}] took [17518ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57128}] took [17718ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:41,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:39092}] took [22653ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:54,414][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-15T18:27:55,246][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:55,246][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:55,881][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:57,188][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9871}] took [42.3s] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9871}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [41356ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [20ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [0ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [36ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [66ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [367ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [106ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [24ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [44ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [42ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [19ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [34ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [17ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:28:16,943][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T18:28:19,464][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T18:28:21,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4614a747] took [5424ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:28:30,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [6795ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:28:47,787][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [74] timed out after [24152ms]
[2022-04-15T18:28:49,647][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [28.3s/28353ms] ago, timed out [4.2s/4201ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [74]
[2022-04-15T18:29:05,683][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-15T18:29:16,880][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T18:29:16,922][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T18:29:16,922][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T18:29:23,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@7a91bbf] took [5644ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:29:30,794][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T18:29:32,406][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T18:29:32,717][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [468ms] collecting in the last [1.7s]
[2022-04-15T18:29:36,258][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][91] overhead, spent [378ms] collecting in the last [1.2s]
[2022-04-15T18:30:29,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:30:49,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11549193224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:12,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:17,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30167218663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:26,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:31,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27689965628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:49,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:06,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24274841745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:14,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:24,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25384450876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:29,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:25,767][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9885}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9885}]] took [0ms], [connecting to new nodes] took [69ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [1004ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [4235ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [30ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [33ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [65ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [26ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [50ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [501ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [1894ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [229ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [3127ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1491ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [12504ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [53ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [37883ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [23721ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [227ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [1527ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [78ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [15ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [71ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [201ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [67ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@5e0c4e03]]]] took [15732ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@ba0f0a3]]]] took [30743ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@25ef4db1]]]] took [9197ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4f53ad21]]]] took [12567ms]
[2022-04-15T18:32:32,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15200112406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:44,630][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57180}] took [15200ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:45,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:48,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15395924278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:51,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:51,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4f53c0da] took [176426ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:53,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6411901117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:56,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:58,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5356ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:59,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5356698394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:02,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [5097ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:33:31,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:32,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12031283867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:39,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:41,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7726ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:33:42,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6129855847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:46,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:44,602][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7726ms] which is above the warn threshold of [5s]
[2022-04-15T18:33:49,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7094112922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:53,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:58,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6574704227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:05,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:06,119][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57180}] took [17970ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:12,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396172032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:18,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:23,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [38309ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:23,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13244869723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:26,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:31,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984618601ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:30,835][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57182}] took [8985ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:34,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:35,287][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57186}] took [6925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:34,867][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57184}] took [6925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:43,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925091854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:50,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:53,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16603400115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:55,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:00,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5144155346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:07,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:23,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11639440320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:27,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:32,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20085199770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:27,442][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [124325ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T18:35:38,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:41,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11019135935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:44,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:43,687][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.7m] publication of cluster state version [9886] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:35:55,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6688525846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:00,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:03,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015661204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:07,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:12,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517402059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:18,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:21,794][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [10658ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:22,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10657671230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:26,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:30,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8146308736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:35,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:40,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8726906670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:44,790][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [8727ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:44,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:49,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@63b2653e] took [109275ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:48,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9779743896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [6562ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,938][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [6562ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:55,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6562058405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:01,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:02,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9538ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:05,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9538709861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:10,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:15,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9634308706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:20,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9806416089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:30,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:35,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9963917148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:36,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9963ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:38,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:37,083][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9964ms] which is above the warn threshold of [5s]
[2022-04-15T18:37:44,395][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [8413ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8412625501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:47,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:49,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9245ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:50,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9245455814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:53,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:55,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5225695569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:00,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:01,135][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9886}] took [2.2m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9886}]] took [176ms], [connecting to new nodes] took [944ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [14923ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [9153ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [215ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [205ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [58ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [19698ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [589ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [1164ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [334ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [51ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [110ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [46156ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [13477ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [438ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [110ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [3657ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [267ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [2972ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [42ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [4365ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [1210ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [73ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:38:01,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6886616489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:01,761][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [5.2m/314904ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [8.2m/494217ms]
[2022-04-15T18:38:19,951][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.4s] publication of cluster state version [9887] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:38:32,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12604ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:38:49,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:55,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15457270051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:09,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12376792147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:10,636][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57200}] took [12377ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:39:19,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:28,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17235685524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:45,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:47,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [25031ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:39:55,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25031605108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:06,908][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57200}] took [25031ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:40:07,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:20,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21805840195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:32,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:46,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25775216831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:56,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:07,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23451873749ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:17,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22350245512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:39,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:49,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20873953033ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:59,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:08,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20152186343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:17,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:24,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18514968381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:30,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:36,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13330442511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:42,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:46,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11212412231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:50,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:54,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8319361007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:57,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:59,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@7c3c46bd] took [192585ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:00,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6798756459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:02,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:03,928][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9887}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9887}]] took [98ms], [connecting to new nodes] took [1023ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [7798ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [14005ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [484ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [282ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [170ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [6036ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [2157ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [499ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [182ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [155547ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [79ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [22266ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [711ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [402ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [13093ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [479ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [7929ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [54ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [4823ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [2525ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [959ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [114ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [227ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [62ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4b0a729d]]]] took [3028ms]
[2022-04-15T18:43:06,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6057664874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:09,092][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10.2m/617952ms] which is longer than the warn threshold of [300000ms]; there are currently [1] pending tasks, the oldest of which has age [10.3m/621014ms]
[2022-04-15T18:43:09,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:09,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256501427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:23,590][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11760ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T18:43:24,239][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.1s] publication of cluster state version [9888] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:43:30,648][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:49028}] took [17801ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:45,853][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][137][11] duration [2.1s], collections [1]/[1.1s], total [2.1s]/[3.2s], memory [261.5mb]->[305.5mb]/[2gb], all_pools {[young] [156mb]->[0b]/[0b]}{[old] [98.7mb]->[98.7mb]/[2gb]}{[survivor] [6.7mb]->[13.4mb]/[0b]}
[2022-04-15T18:43:47,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][137] overhead, spent [2.1s] collecting in the last [1.1s]
[2022-04-15T18:43:50,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10589ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:53,552][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.8s/13857ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1377531d], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6411fc55], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f8aaa8fa], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b246ae5e]], which exceeds the warn threshold of [10s]
[2022-04-15T18:44:20,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@5db5e0c6] took [5066ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:44:34,388][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][12] duration [3s], collections [1]/[5.3s], total [3s]/[6.3s], memory [180.2mb]->[116.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [98.7mb]->[110.9mb]/[2gb]}{[survivor] [13.4mb]->[5.6mb]/[0b]}
[2022-04-15T18:44:34,962][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [3s] collecting in the last [5.3s]
[2022-04-15T18:44:34,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:44:55,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6439ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:45:05,803][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57202}] took [10839ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:46:35,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:43,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5075893122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:51,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:56,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16896414874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:03,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:07,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@6fe235c0] took [121063ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:09,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12575315599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:14,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:15,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [10349ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:22,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349790648ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:30,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:35,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [13470ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:36,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13470504449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:42,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:49,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13704101318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:55,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:01,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13685309744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:09,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:15,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14702389375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:22,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:26,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572455168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:20,176][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14703ms] which is above the warn threshold of [5s]
[2022-04-15T18:48:32,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:43,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10348976756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:52,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:57,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [30296ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:49:00,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19947052169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:07,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14155048846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:26,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:30,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [19807ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:49:34,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19807432467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:41,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:49,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15297418544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:54,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:59,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12712ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:50:02,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12712676850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:10,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:17,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16113125027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:22,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:29,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12019842305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:34,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11888ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:41,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11887725293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:49,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:54,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14096523545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:01,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:52,465][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [343332ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T18:51:08,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12596536300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:19,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:28,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18748848676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:38,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:18,488][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6.4m] publication of cluster state version [9891] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:51:45,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033063319ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:53,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:00,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14649192228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:07,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:15,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14145840497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:24,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:31,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17189876446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:34,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4b275c7e] took [149480ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:52:37,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13822ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:45,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13822520526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:50,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:58,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12624921036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:02,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12624ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:53:07,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:15,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16106886622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:21,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:24,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260558073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:29,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:34,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945581248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:39,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9540ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:33,785][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [408] timed out after [57815ms]
[2022-04-15T18:53:44,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9539554154ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:41,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9539ms] which is above the warn threshold of [5s]
[2022-04-15T18:53:50,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [10266ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:53:49,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:55,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10266620571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:02,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:09,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11925978531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:58,359][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85567ms] ago, timed out [27.7s/27752ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [408]
[2022-04-15T18:54:15,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:54:17,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:21,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15507620960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:29,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:36,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11744891203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:41,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:47,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [24407ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:54:49,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12662264407ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:56,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:02,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15164775618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:07,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:15,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10610435308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:22,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:30,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14974763719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:35,592][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9891}] took [3.4m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9891}]] took [276ms], [connecting to new nodes] took [1356ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [79ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [41378ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [10686ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [86ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [74ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [3116ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [1104ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [428ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [1ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [253ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [68747ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [219ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [19224ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [899ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [242ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [18427ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1774ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [18873ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [69ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [15673ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [12170ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [2030ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [79ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [232ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [756ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [65ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [147ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:55:40,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:49,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17131500762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:58,768][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.9m/719303ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [12.5m/750655ms]
[2022-04-15T18:55:58,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:04,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19581077344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:12,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:19,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13331061506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:28,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:36,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726649479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:47,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:53,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21029125089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:58,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:00,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@6537fcd8] took [136493ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:00,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9944582355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:03,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:06,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6003225212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:06,725][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [59s/59034ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-15T18:57:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:14,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7390ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:16,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7390154306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:25,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:34,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13336370355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:41,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:41,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [16304ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:43,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16304809725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:46,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:49,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139961081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:52,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6139ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:54,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:48,490][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6140ms] which is above the warn threshold of [5s]
[2022-04-15T18:57:59,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6895165533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:03,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:09,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9864708841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:16,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [22536ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:16,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:20,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12671448706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:26,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:28,051][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [10404ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:34,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404046759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:40,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:42,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [13419ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:55,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13419878775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:59,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:04,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19296255383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:09,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [19297ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:59:09,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:16,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10289723068ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:15,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10289ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:59:22,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:25,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12044892518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:28,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:41,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959486565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:43,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:45,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144118498ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:55,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:03,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11607929826ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:07,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:11,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779268274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:15,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:18,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8149491377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:22,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:24,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6750758018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:39,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:41,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9613468829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:50,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11253467808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,522][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [11254ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,970][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [206081ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T19:01:01,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@5115e147] took [94017ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:00:55,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,384][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57206}] took [5268ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5268313169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:05,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,896][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.7m] publication of cluster state version [9892] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:01:05,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9329138051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:05,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9954ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:01:08,178][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [17.4m/1048501ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [14m/845109ms]
[2022-04-15T19:01:12,678][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][176][13] duration [1.9s], collections [1]/[1s], total [1.9s]/[8.3s], memory [184.6mb]->[204.6mb]/[2gb], all_pools {[young] [68mb]->[88mb]/[0b]}{[old] [110.9mb]->[110.9mb]/[2gb]}{[survivor] [5.6mb]->[5.6mb]/[0b]}
[2022-04-15T19:01:13,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][176] overhead, spent [1.9s] collecting in the last [1s]
[2022-04-15T19:01:18,707][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][14] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[9.9s], memory [137.4mb]->[124.8mb]/[2gb], all_pools {[young] [16mb]->[12mb]/[0b]}{[old] [110.9mb]->[115.7mb]/[2gb]}{[survivor] [10.5mb]->[9mb]/[0b]}
[2022-04-15T19:01:19,216][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-04-15T19:01:24,513][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][15] duration [2s], collections [1]/[2.5s], total [2s]/[12s], memory [124.8mb]->[208.8mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [115.7mb]->[115.7mb]/[2gb]}{[survivor] [9mb]->[13.7mb]/[0b]}
[2022-04-15T19:01:24,854][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [2s] collecting in the last [2.5s]
[2022-04-15T19:01:30,141][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11143ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T19:01:39,291][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.7s] publication of cluster state version [9893] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:01:45,419][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57206}] took [5595ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:00,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@24d5ea58] took [21424ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:01,973][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57210}] took [6651ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:23,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9066ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:28,308][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57214}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:33,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:34,151][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [535] timed out after [26140ms]
[2022-04-15T19:03:03,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9352ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:03:20,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7200ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:03:40,431][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9893}] took [1.8m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9893}]] took [0ms], [connecting to new nodes] took [262ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [8105ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [6883ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [96ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [65ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [64ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [217ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [373ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [1973ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [530ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [79ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [41144ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [10684ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [590ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [521ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [14320ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [811ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [10619ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [6480ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [3959ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [938ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [142ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [140ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [476ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T19:04:56,890][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.2s/32202ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.15][0]], allocationId [GekoCVcoSgiAIPPGQR_JzQ], primary term [2], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.15][0]], allocationId [GekoCVcoSgiAIPPGQR_JzQ], primary term [2], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-15T19:04:51,259][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168599ms] ago, timed out [2.3m/142459ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [535]
[2022-04-15T19:06:41,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@1f7a1064] took [177710ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:06:27,864][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T19:07:02,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [10928ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:07:31,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12240ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:05,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:08:05,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11067ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:09,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9066619793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:08:09,270][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10267ms] which is above the warn threshold of [5s]
[2022-04-15T19:08:05,594][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [586] timed out after [74520ms]
[2022-04-15T19:08:06,737][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85988ms] ago, timed out [11.4s/11468ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [586]
[2022-04-15T19:08:10,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:27,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9205ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:48,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9915ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:20,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57216}] took [11607ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:41,788][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [187703ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T19:09:47,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@49c9a0fe] took [54470ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:47,558][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.4m] publication of cluster state version [9894] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:10:07,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [8430ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:10:34,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11292ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:10:54,810][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8521ms] which is above the warn threshold of [5s]
[2022-04-15T19:12:23,911][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9894}] took [2.5m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9894}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [125ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [199ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [36ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [0ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [73ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [331ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [69634ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [22783ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [715ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [758ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [12041ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1544ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [14591ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [14301ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [9414ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [3863ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [136ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [59ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [183ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [60ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T19:12:41,543][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.6m/520271ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [11.1m/670692ms]
[2022-04-15T19:12:57,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4d812620] took [135688ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:05,050][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.7s/15753ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-15T19:13:08,075][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57216}] took [164408ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:25,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6670ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:38,272][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57218}] took [5720ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:38,143][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [28234ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-15T19:13:43,850][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [35.1s] publication of cluster state version [9895] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:14:27,749][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57224}] took [9406ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:14:45,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@3988865f] took [59579ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:18,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:15:19,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20524726052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:15:21,432][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][16] duration [13.7s], collections [1]/[30.1s], total [13.7s]/[25.7s], memory [205.4mb]->[136.2mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [115.7mb]->[120.9mb]/[2gb]}{[survivor] [13.7mb]->[11.2mb]/[0b]}
[2022-04-15T19:15:25,831][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [13.7s] collecting in the last [30.1s]
[2022-04-15T19:15:28,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10409ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:40,251][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57226}] took [12262ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:41,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7259ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:42,743][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6649ms] which is above the warn threshold of [5s]
[2022-04-15T19:18:50,880][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57226}] took [48923ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:19:06,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@35354b1f] took [188017ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:20:37,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:10,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32063153765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:24,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:27,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [91539ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:21:41,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47542955654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:55,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:03,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31013618907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:19,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:30,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23604145509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:41,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:55,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21819102996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:08,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:39,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25564011493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:45,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:52,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39665831381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:53,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [39665ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:23:38,179][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [753] timed out after [216443ms]
[2022-04-15T19:24:06,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:10,671][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5m/301968ms] ago, timed out [1.4m/85525ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [753]
[2022-04-15T19:24:17,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20295299618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:30,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:39,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887512805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:16,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:25,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46066807928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:36,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:49,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19464267448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:59,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23561ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:17,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23560624101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:34,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:40,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [57978ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:26:51,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418354432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:27:29,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.1s/53193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:27:40,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [53192ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:27:49,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.1s/53192378789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:28:17,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:28:47,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47537136577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:29:14,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:29:33,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58999100636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:00,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46325342713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:54,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:31:21,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53436342063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:31:45,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:32:15,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51173605259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:32:39,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:33:09,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53592511253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:33:45,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:34:06,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64114500916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:34:43,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:35:36,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59561714321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:37:10,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:40:22,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146588930168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:40:08,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4db40ea9] took [581329ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:42:59,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:45:42,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338730747731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:48:43,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:51:33,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350506481508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:54:11,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:57:03,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323685946312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:59:45,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:02:35,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339128869347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:05:17,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:07:33,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326109892487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:10:19,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:12:43,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309565056075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:15:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:17:34,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293438112670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:20:17,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:15:49,967][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [603003ms] which is above the warn threshold of [5s]
[2022-04-15T20:22:46,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305041546419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:25:19,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:27:45,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290027092567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:30:19,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:32:48,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309163373271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:35:18,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:38:00,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301289647152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:41:07,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:43:45,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326128119452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:44:11,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [326128ms] which is above the warn threshold of [5000ms]
[2022-04-15T20:48:28,309][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T20:48:28,380][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T20:48:28,381][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T20:48:36,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T20:48:36,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T20:48:36,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T20:48:36,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T20:48:36,788][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T20:48:36,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T20:48:36,790][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T20:48:36,791][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T20:48:36,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T20:48:36,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T20:48:36,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T20:48:36,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T20:48:36,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T20:48:36,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T20:48:36,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T20:48:36,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T20:48:36,803][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T20:48:36,804][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T20:48:36,806][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T20:48:36,807][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T20:48:36,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T20:48:36,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T20:48:36,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T20:48:36,813][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T20:48:36,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T20:48:36,815][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T20:48:36,817][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T20:48:36,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T20:48:36,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T20:48:36,820][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T20:48:36,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T20:48:36,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T20:48:36,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T20:48:36,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T20:48:36,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T20:48:36,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T20:48:36,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T20:48:36,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T20:48:36,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T20:48:36,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T20:48:36,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T20:48:36,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T20:48:36,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T20:48:36,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T20:48:36,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T20:48:36,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T20:48:36,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T20:48:36,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T20:48:36,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T20:48:36,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T20:48:36,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T20:48:36,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T20:48:36,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T20:48:36,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T20:48:36,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T20:48:36,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T20:48:36,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T20:48:36,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T20:48:36,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T20:48:36,966][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.1gb], net total_space [125.8gb], types [ext4]
[2022-04-15T20:48:36,967][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T20:48:37,892][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T20:48:56,442][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T20:48:56,453][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-15T20:48:56,458][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T20:48:56,462][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-15T20:48:56,465][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T20:48:56,468][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-15T20:48:56,472][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-15T20:48:56,474][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T20:48:56,477][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-15T20:48:56,479][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-15T20:48:56,481][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-15T20:48:56,483][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T20:48:56,484][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T20:48:56,484][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T20:48:56,485][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T20:48:58,217][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T20:48:58,477][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T20:48:59,734][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T20:49:01,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T20:49:01,125][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T20:49:01,220][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T20:49:01,235][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T20:49:01,645][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T20:49:05,543][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T20:49:05,733][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 251, version: 9896, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T20:49:05,993][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 251, version: 9896, reason: Publication{term=251, version=9896}
[2022-04-15T20:49:06,180][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T20:49:06,181][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T20:49:08,624][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T20:49:08,651][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [52] indices into cluster_state
[2022-04-15T20:49:09,985][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T20:49:09,986][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T20:49:11,211][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-15T20:49:11,462][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-15T20:49:11,926][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-15T20:49:12,012][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T20:49:12,017][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T20:49:12,021][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T20:49:12,601][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T20:49:12,826][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T20:49:16,058][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T20:49:22,363][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-15T20:49:49,529][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:49,798][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,016][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,030][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,436][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,613][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:50,783][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:52,810][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:53,058][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:49:53,574][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T20:59:26,645][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:02:54,860][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:03:12,431][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:09:32,794][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1219] overhead, spent [316ms] collecting in the last [1.2s]
[2022-04-15T21:11:17,966][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1298][32] duration [881ms], collections [1]/[2.3s], total [881ms]/[2.8s], memory [1.3gb]->[245.6mb]/[2gb], all_pools {[young] [1.1gb]->[12mb]/[0b]}{[old] [232.1mb]->[232.1mb]/[2gb]}{[survivor] [6.7mb]->[5.5mb]/[0b]}
[2022-04-15T21:11:19,971][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1298] overhead, spent [881ms] collecting in the last [2.3s]
[2022-04-15T21:11:23,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [9494ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:11:38,061][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1306][33] duration [1.7s], collections [1]/[2.9s], total [1.7s]/[4.5s], memory [317.6mb]->[239.2mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [232.1mb]->[232.2mb]/[2gb]}{[survivor] [5.5mb]->[7mb]/[0b]}
[2022-04-15T21:11:39,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1306] overhead, spent [1.7s] collecting in the last [2.9s]
[2022-04-15T21:11:40,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [6105ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:12:14,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [6038ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:12:24,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [5181ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:12:39,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [10387ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:15:20,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:19:47,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6186874314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:20:26,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:21:53,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431561757443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:22:37,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:22:51,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131404530517ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:23:16,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39414ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:23:32,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39413784552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:32,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/435934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:37,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/435934363322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:44,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:52,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12722688909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:30:59,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:31:07,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14378838592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:31:12,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:06,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13728511222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:31:01,715][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57676}] took [131404ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:32:07,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.6s/55642ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:07,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55bb42c2, interval=5s}] took [13728ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:32:08,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.6s/55642551435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:26,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18636ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:29,389][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1320][36] duration [7.1m], collections [3]/[19.6m], total [7.1m]/[7.2m], memory [287.2mb]->[258.7mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [232.2mb]->[236.6mb]/[2gb]}{[survivor] [7mb]->[5.7mb]/[0b]}
[2022-04-15T21:32:29,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635591519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:31,463][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1320] overhead, spent [7.1m] collecting in the last [19.6m]
[2022-04-15T21:32:31,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:36,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5329065460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:36,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [23964ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:32:53,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:32:56,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15240705673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:01,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:10,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14661077707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:15,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [12971ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:33:14,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:16,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12971709502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:28,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:29,012][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1324][37] duration [4.5s], collections [1]/[1.7s], total [4.5s]/[7.3m], memory [314.3mb]->[330.3mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [236.6mb]->[236.6mb]/[2gb]}{[survivor] [5.7mb]->[3.4mb]/[0b]}
[2022-04-15T21:33:29,397][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1324] overhead, spent [4.5s] collecting in the last [1.7s]
[2022-04-15T21:33:29,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9268228607ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:33:29,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [9868ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:33:29,524][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [20.7m/1243619ms] ago, timed out [41.9s/41918ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [9553]
[2022-04-15T21:33:29,589][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [9553] timed out after [1201701ms]
[2022-04-15T21:34:08,165][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1345][38] duration [919ms], collections [1]/[2s], total [919ms]/[7.3m], memory [296mb]->[242.3mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [236.6mb]->[236.6mb]/[2gb]}{[survivor] [3.4mb]->[5.7mb]/[0b]}
[2022-04-15T21:34:08,770][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1345] overhead, spent [919ms] collecting in the last [2s]
[2022-04-15T21:34:09,431][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T21:34:16,353][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1347][39] duration [2.6s], collections [1]/[1.6s], total [2.6s]/[7.3m], memory [322.3mb]->[326.3mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [236.6mb]->[236.6mb]/[2gb]}{[survivor] [5.7mb]->[13mb]/[0b]}
[2022-04-15T21:34:21,683][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1347] overhead, spent [2.6s] collecting in the last [1.6s]
[2022-04-15T21:34:22,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [11117ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:34:33,568][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1348][40] duration [3.2s], collections [1]/[16.3s], total [3.2s]/[7.4m], memory [326.3mb]->[264.3mb]/[2gb], all_pools {[young] [0b]->[16mb]/[0b]}{[old] [236.6mb]->[250mb]/[2gb]}{[survivor] [13mb]->[2.2mb]/[0b]}
[2022-04-15T21:34:37,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [9453ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:34:52,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [5237ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:36:57,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:36:59,622][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1350][41] duration [51.7s], collections [1]/[12s], total [51.7s]/[8.2m], memory [296.3mb]->[332.3mb]/[2gb], all_pools {[young] [48mb]->[88mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [2.2mb]->[2.2mb]/[0b]}
[2022-04-15T21:37:06,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1350] overhead, spent [51.7s] collecting in the last [12s]
[2022-04-15T21:37:11,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68266575835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:16,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [110464ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:37:21,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:32,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31447459654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:42,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:37:51,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21336898855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:00,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:20,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16703396331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:28,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29535ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:38:46,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29535470429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:04,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:23,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35390566118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:35,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:42,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30673261334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:39:27,328][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35391ms] which is above the warn threshold of [5s]
[2022-04-15T21:39:58,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:17,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20478801816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:26,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:27,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31069644802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:31,276][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [366015ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T21:40:43,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8191ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:46,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10975984359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:40:45,011][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6.4m] publication of cluster state version [9965] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T21:40:47,921][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:40272}] took [21549ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:40:50,906][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1351][42] duration [4.9s], collections [1]/[5.7m], total [4.9s]/[8.3m], memory [332.3mb]->[304.3mb]/[2gb], all_pools {[young] [88mb]->[56mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [2.2mb]->[2.2mb]/[0b]}
[2022-04-15T21:40:53,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [16854ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:41:13,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:41:15,004][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1356][43] duration [7s], collections [1]/[10.7s], total [7s]/[8.4m], memory [332.3mb]->[254.4mb]/[2gb], all_pools {[young] [84mb]->[44mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [2.2mb]->[4.4mb]/[0b]}
[2022-04-15T21:41:14,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9851453360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:41:17,100][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1356] overhead, spent [7s] collecting in the last [10.7s]
[2022-04-15T21:41:19,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [15115ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:41:58,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [12261ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:42:17,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55bb42c2, interval=5s}] took [5609ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:43:18,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [43260ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:44:19,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:44:50,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9946644617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:45:41,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:46:12,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63097210403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:46:50,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:47:20,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73795584975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:47:32,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:15,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.1s/56145352539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:46,022][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58296}] took [272709ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:48:33,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:58,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59691959408ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:52:24,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:48:49,777][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [59692ms] which is above the warn threshold of [5s]
[2022-04-15T21:52:37,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231880050151ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:52:51,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:52:52,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [28200ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:53:07,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28200323628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:53:30,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:53:56,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39017585934ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:07,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:10,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@660d3297] took [36541ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:54:22,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36541954497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:49,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:54:09,615][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=251, version=9965}] took [6.5m] which is above the warn threshold of [30s]: [running task [Publication{term=251, version=9965}]] took [216ms], [connecting to new nodes] took [117ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [4983ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [11672ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc440@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [160ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [155ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@7c05388] took [43ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [47ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [32036ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [72743ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [660ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [718ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [45991ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [15288ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [79645ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [575ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [338ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [101354ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [24219ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [193ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [24054ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [262ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [164ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [3772ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [775ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [699ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [192ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [9503ms], [org.elasticsearch.license.LicenseService@16e477b6] took [106ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [194ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [2906ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [375ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-04-15T21:55:06,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42478837211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:55:28,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:55:42,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38587977424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:56:07,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:56:31,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38374428913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:56:47,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:09,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40115627586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:27,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:35,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39506548134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:57:47,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:58:28,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21862463742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:58:59,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T21:59:12,499][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1363][44] duration [2.7m], collections [1]/[15.1m], total [2.7m]/[11.2m], memory [314.4mb]->[260.3mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [4.4mb]->[6.3mb]/[0b]}
[2022-04-15T21:59:18,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66410430098ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T21:59:37,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [88272ms] which is above the warn threshold of [5000ms]
[2022-04-15T21:59:37,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:00:36,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37711756856ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:06,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:18,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93654627293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:27,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:01:59,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21884414698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:02:20,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:02:28,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51716636706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:02:39,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:16,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18483245548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:31,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:21,440][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18483ms] which is above the warn threshold of [5s]
[2022-04-15T22:03:46,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53737386273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:54,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:03:30,804][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10015] timed out after [492304ms]
[2022-04-15T22:04:06,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23299182565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:04:45,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:04:54,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49995655920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:05:16,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:05:35,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30173851887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:05:51,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:06:01,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35673477288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:06:15,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [35673ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:06:17,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:00,681][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T22:06:56,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26054516641ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:10,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:18,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53412525994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:38,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:07:47,573][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.7m/706257ms] to compute cluster state update for [put-mapping [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-15T22:07:51,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28329251314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:08:03,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:08:26,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485172377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:08:49,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:09:05,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45181839311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:09:37,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:09:52,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48099032147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:03,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:09,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@27d5829a] took [27440ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:10:20,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27440486011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:37,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:11:03,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32636910267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:10:09,439][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.4m/929229ms] ago, timed out [7.2m/436925ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10015]
[2022-04-15T22:11:27,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:12:09,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46035408185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:13:15,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112134ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:13:26,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [112133ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:13:39,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112133661396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:14:06,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:14:40,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47182262997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:16:06,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:16:56,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107450150082ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:17:16,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:18:23,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85516213262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:19:27,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:20:07,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122221516555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:20:55,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:23:35,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97024966611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:24:14,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:24:46,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/189251730554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:25:10,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67255ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:25:24,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67255198583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:25:46,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31224ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:26:08,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31223751911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:26:27,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:26:47,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46291631175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:27:26,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.8s/58887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:27:51,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.7s/58796333129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:28:43,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:28:57,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76694812868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:29:26,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:29:45,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31067771806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:29:56,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:30:16,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42931233585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:31:05,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:31:18,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68627599436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:31:39,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:37:41,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33114336803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:18,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:37:45,852][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1322896ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T22:38:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400051242643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:17,751][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [21.5m/1295455ms] ago, timed out [1.6m/101742ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10154]
[2022-04-15T22:38:31,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:34,140][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1367][45] duration [5.1m], collections [1]/[26.4m], total [5.1m]/[16.3m], memory [284.3mb]->[255.3mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [250mb]->[250mb]/[2gb]}{[survivor] [6.3mb]->[5.3mb]/[0b]}
[2022-04-15T22:38:37,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12417826770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:43,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [412469ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:38:45,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:38:34,563][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [30.1m] publication of cluster state version [9966] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T22:38:57,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14598970153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:39:19,533][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25735ms] which is above the warn threshold of [5s]
[2022-04-15T22:39:19,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:39:30,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25734447960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:39:01,067][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10154] timed out after [1193713ms]
[2022-04-15T22:39:48,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:40:07,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37227949040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:40:42,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.9s/52955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:40:46,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.9s/52954527343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:09,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:40,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27499619055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:49,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:41:54,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40163262264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:01,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:06,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12078731691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:15,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:20,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14277387180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:26,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:25,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [14277ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:42:30,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11047107207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:33,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:44,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@525916e6] took [7091ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:42:44,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7091280267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:42:54,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:03,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19859403797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:13,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:24,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [19500ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:43:23,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19500033952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:32,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:43:48,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19667893059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:13,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:20,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35976554775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:31,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:29,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@2f2fae0a, interval=1m}] took [20919ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:44:39,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20919200622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:49,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:02,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18465027319ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:44:41,533][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10255] timed out after [59027ms]
[2022-04-15T22:44:44,213][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [10254] timed out after [59027ms]
[2022-04-15T22:45:25,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [28808ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:45:22,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:29,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28808871032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:35,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:41,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17189852152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:38,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [17189ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:45:46,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:51,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12130983486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:45:51,416][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=251, version=9966}] took [6.5m] which is above the warn threshold of [30s]: [running task [Publication{term=251, version=9966}]] took [275ms], [connecting to new nodes] took [765ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@21bd0d8e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@6d7e8b0c] took [146999ms], [org.elasticsearch.script.ScriptService@560a5988] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [0ms], [org.elasticsearch.snapshots.RestoreService@75ea5228] took [0ms], [org.elasticsearch.ingest.IngestService@6cd58172] took [2285ms], [org.elasticsearch.action.ingest.IngestActionForwarder@68174379] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016cc440@4304276f] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@64d85635] took [0ms], [org.elasticsearch.tasks.TaskManager@244695cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@761816b7] took [1ms], [org.elasticsearch.cluster.InternalClusterInfoService@64bda51] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6c9d3359] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7074e5e7] took [1421ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15e3ba29] took [70ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@7c05388] took [470ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7e8e58b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6edd7ccc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@616bfea9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@5673daf] took [237ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@63e3c819] took [100283ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a0d3a1d] took [86ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@44027c3c] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@170c7b73] took [60705ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@42896614] took [598ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@1d4a4a73] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@4afe367a] took [26270ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c72e955] took [2516ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@3edd0bb5] took [24475ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@37ed63bc] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@159b3114] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@55d4b2b7] took [9388ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@39079607] took [7040ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@23552e63] took [150ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5a276363] took [2520ms], [org.elasticsearch.node.ResponseCollectorService@5f95618e] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@208892ec] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2aa486a8] took [202ms], [org.elasticsearch.shutdown.PluginShutdownService@4b79c4e4] took [304ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@33baf2ae] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2aacf3e0] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22acb0ec] took [0ms], [org.elasticsearch.license.LicenseService@16e477b6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fd11600] took [77ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@136bb706] took [0ms], [org.elasticsearch.gateway.GatewayService@367fa86b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@51a71461] took [0ms]
[2022-04-15T22:45:56,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:03,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10145351926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:20,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:37,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23429573407ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:46:49,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:47:03,910][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [50.4m/3028864ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [1.1h/3990510ms]
[2022-04-15T22:47:05,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28366221494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:47:37,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:47:56,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [48795ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:47:55,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48795806819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:11,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:44,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34609388050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:57,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@6fd30db2] took [34609ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:49:03,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.3s/50300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:49:28,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50299612044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:50:00,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:48:44,236][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.6m/337864ms] ago, timed out [4.6m/278837ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10254]
[2022-04-15T22:50:27,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56672574673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:50:49,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:51:23,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50126275008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:51:37,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:51:58,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48205848865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:52:27,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50s/50085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:52:45,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50s/50085058969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:53:23,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55s/55095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:53:51,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55s/55094071646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:54:10,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:55:05,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47170828458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:55:25,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:56:13,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [73873ms] which is above the warn threshold of [5000ms]
[2022-04-15T22:56:19,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73873527614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:56:58,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:57:37,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93377092626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T22:56:20,628][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [73874ms] which is above the warn threshold of [5s]
[2022-04-15T22:58:05,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66546ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:57:05,602][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [10276] timed out after [431528ms]
[2022-04-15T22:58:57,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66401828469ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:00:04,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T22:58:44,736][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.4m/929171ms] ago, timed out [14.5m/870144ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10255]
[2022-04-15T23:01:02,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121527762447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:02:24,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:02:53,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139613471668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:03:55,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89356ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:05:08,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89355531562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:05:53,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119974ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:06:16,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119974517563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:06:58,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:11,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65209228019ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:28,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:46,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29313676920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:07:09,127][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [10277] timed out after [524905ms]
[2022-04-15T23:08:04,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:18,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35609459629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:33,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:49,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29540565097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:09:08,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:09:30,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34182481591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:08:52,556][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [20.3m/1221451ms] ago, timed out [13.1m/789923ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10276]
[2022-04-15T23:09:47,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:10:12,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39852641233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:10:41,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:10:55,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.8s/53839631046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:11:14,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:11:52,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29349531998ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:12:09,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:13:35,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59179003687ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:12:39,125][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.2s/10294ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d280719b], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b7b3c778]], which exceeds the warn threshold of [10s]
[2022-04-15T23:14:21,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:14:43,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/131268457551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:15:24,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36c6369c, interval=1s}] took [54769ms] which is above the warn threshold of [5000ms]
[2022-04-15T23:15:20,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:15:51,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54769569054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:16:29,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:17:17,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73292810911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:18:00,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:18:52,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87208747397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:20:14,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:22:04,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137799030052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:20:24,236][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [137799ms] which is above the warn threshold of [5s]
[2022-04-15T23:24:24,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/247696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:25:13,804][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [36.1m/2169889ms] ago, timed out [27.4m/1644984ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hk-5Jv0WRLmwaHwVNJYWXA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [10277]
[2022-04-15T23:25:36,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/247696256964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:26:57,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:27:47,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149045904700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:28:17,233][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.7s/10788ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b7b3c778], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@714fccdc]], which exceeds the warn threshold of [10s]
[2022-04-15T23:28:47,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:29:53,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113532937665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:31:17,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:33:13,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/149396502987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:35:41,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260978ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:35:43,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@56acbb1f, interval=5s}] took [260563ms] which is above the warn threshold of [5000ms]
[2022-04-15T23:38:12,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260563465994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:40:36,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/300200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:44:09,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/300164167238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:46:35,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:48:26,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017df9b8@4382391e] took [659068ms] which is above the warn threshold of [5000ms]
[2022-04-15T23:49:03,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358903905444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:51:40,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:54:12,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294943897958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T23:57:05,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T23:59:38,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333417306561ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T00:53:37,526][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-16T00:53:37,556][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-16T00:53:37,557][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-16T00:53:42,241][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-16T00:53:42,242][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-16T00:53:42,243][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-16T00:53:42,243][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-16T00:53:42,244][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-16T00:53:42,244][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-16T00:53:42,244][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-16T00:53:42,244][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-16T00:53:42,245][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-16T00:53:42,245][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-16T00:53:42,246][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-16T00:53:42,246][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-16T00:53:42,247][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-16T00:53:42,247][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-16T00:53:42,247][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-16T00:53:42,248][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-16T00:53:42,248][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-16T00:53:42,249][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-16T00:53:42,249][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-16T00:53:42,249][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-16T00:53:42,250][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-16T00:53:42,250][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-16T00:53:42,250][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-16T00:53:42,251][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-16T00:53:42,251][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-16T00:53:42,252][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-16T00:53:42,252][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-16T00:53:42,252][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-16T00:53:42,253][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-16T00:53:42,253][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-16T00:53:42,253][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-16T00:53:42,254][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-16T00:53:42,254][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-16T00:53:42,255][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-16T00:53:42,255][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-16T00:53:42,255][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-16T00:53:42,256][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-16T00:53:42,256][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-16T00:53:42,257][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-16T00:53:42,257][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-16T00:53:42,257][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-16T00:53:42,258][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-16T00:53:42,258][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-16T00:53:42,259][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-16T00:53:42,259][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-16T00:53:42,259][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-16T00:53:42,260][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-16T00:53:42,260][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-16T00:53:42,260][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-16T00:53:42,261][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-16T00:53:42,261][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-16T00:53:42,261][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-16T00:53:42,262][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-16T00:53:42,262][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-16T00:53:42,262][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-16T00:53:42,263][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-16T00:53:42,263][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-16T00:53:42,263][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-16T00:53:42,264][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-16T00:53:42,325][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.6gb], net total_space [125.8gb], types [ext4]
[2022-04-16T00:53:42,326][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-16T00:53:42,710][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-16T00:53:53,551][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-16T00:53:53,557][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-16T00:53:53,558][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-16T00:53:53,560][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-16T00:53:53,561][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-16T00:53:53,561][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-16T00:53:53,562][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-16T00:53:53,563][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-16T00:53:53,563][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-16T00:53:53,564][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-16T00:53:53,565][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-16T00:53:53,565][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-16T00:53:53,566][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-16T00:53:53,566][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-16T00:53:53,567][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-16T00:53:54,648][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-16T00:53:54,807][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-16T00:53:55,630][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-16T00:53:56,495][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-16T00:53:56,515][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-16T00:53:56,831][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-16T00:53:56,887][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-16T00:55:58,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T00:56:17,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88525956398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T00:56:19,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T00:56:20,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50118900266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T00:56:20,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [141135ms] which is above the warn threshold of [5000ms]
[2022-04-16T00:56:33,694][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-16T00:56:36,234][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-16T00:56:36,450][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 252, version: 9967, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-16T00:56:36,683][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 252, version: 9967, reason: Publication{term=252, version=9967}
[2022-04-16T00:56:36,836][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-16T00:56:36,836][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-16T00:56:44,965][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-16T00:56:44,986][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [52] indices into cluster_state
[2022-04-16T00:56:45,898][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-16T00:56:45,900][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-16T00:58:20,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T00:58:53,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33739638751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T00:59:12,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T00:59:29,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47226128270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T00:59:46,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:00:11,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38067504397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:00:33,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:00:49,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46894008897ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:01:04,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:01:20,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31966168785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:01:37,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:01:52,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32284543889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:02:04,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:02:24,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27601501373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:02:39,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:02:53,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35096686348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:03:11,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:03:27,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31895794414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:03:40,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:03:52,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29250374901ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:04:06,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:04:21,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25403331303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:04:30,368][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [7m/423860ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [188], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [188], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-16T01:04:35,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:04:50,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29777513364ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:05:08,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31844ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:05:24,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31844653555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:05:30,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:05:55,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23438534593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:06:26,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.8s/55829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:06:42,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.8s/55828869157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:07:21,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:07:59,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28288377015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:08:16,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:08:30,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80799123725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:08:38,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:08:51,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23671586705ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:09:18,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:09:28,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40027336201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:09:37,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:09:49,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18650105857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:10:03,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25324ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:10:41,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25323540843ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:10:53,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.6s/49641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:11:03,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.6s/49641606320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:11:39,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:11:48,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45592448737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:11:56,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:12:05,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17495532906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:12:29,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:12:50,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17961294508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:13:00,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:13:02,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47256442753ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:13:53,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:14:01,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5374603410ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:14:07,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:14:09,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@49ef0151] took [1020073ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:14:10,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13806141517ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:14:14,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:14:16,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6794248867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:14:17,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6794ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:14:28,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [7801ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:14:45,380][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9417ms] which is above the warn threshold of [5s]
[2022-04-16T01:14:56,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [5023ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:15:02,919][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [296507ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-16T01:15:06,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [8035ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:15:11,033][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [8.6m] publication of cluster state version [9971] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:15:30,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7053ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:15:33,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7052965623ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:15:21,654][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [75] timed out after [44189ms]
[2022-04-16T01:15:36,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:15:36,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6557941849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:15:44,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:15:44,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [8027ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:15:47,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6805396954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:15:50,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [5384ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:16:09,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5803ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:16:10,821][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2m/121009ms] ago, timed out [1.2m/76820ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [75]
[2022-04-16T01:16:28,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:16:29,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9729054062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:16:29,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [13015ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:16:52,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6296ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:16:51,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:16:54,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5896496996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:16:56,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:16:56,840][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9971}] took [1.5m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9971}]] took [134ms], [connecting to new nodes] took [735ms], [applying settings] took [1ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [44660ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [423ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [51ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [113ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [179ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [453ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [5236ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [665ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [365ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [242ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [23424ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [481ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [296ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [1508ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [227ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [4475ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [8370ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [2243ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [383ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [0ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [0ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:16:56,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5971635401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:16:57,082][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58728}] took [1173113ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:16:57,083][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58730}] took [1173113ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:16:57,084][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58718}] took [1176690ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:17:11,711][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
	at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1715) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
	Suppressed: java.net.SocketException: Broken pipe
		at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
		at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
		at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
		at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
		at sun.security.ssl.SSLSocketOutputRecord.encodeAlert(SSLSocketOutputRecord.java:82) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:400) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:312) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:463) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
		at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
		at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
		at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
		at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
		at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: java.io.EOFException: SSL peer shut down incorrectly
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:483) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:472) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	... 25 more
[2022-04-16T01:17:23,694][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-16T01:17:24,037][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-16T01:17:24,038][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-16T01:17:37,708][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][73][9] duration [2.5s], collections [1]/[4.3s], total [2.5s]/[2.7s], memory [768.3mb]->[105.9mb]/[2gb], all_pools {[young] [692mb]->[0b]/[0b]}{[old] [54.4mb]->[54.4mb]/[2gb]}{[survivor] [21.9mb]->[51.4mb]/[0b]}
[2022-04-16T01:17:39,973][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][73] overhead, spent [2.5s] collecting in the last [4.3s]
[2022-04-16T01:17:40,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [8178ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:17:48,932][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11934ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [50] unchanged indices
[2022-04-16T01:17:49,151][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [16.1s] publication of cluster state version [9979] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:17:56,186][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58802}] took [6665ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:17:55,847][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][76] overhead, spent [552ms] collecting in the last [1.8s]
[2022-04-16T01:17:56,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6248ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:17:58,021][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58806}] took [8274ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:18:04,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@4eb16486] took [6671ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:18:17,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6325ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:18:32,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7870ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:19:22,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:19:42,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35089460012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:19:57,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [36370ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:20:00,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:18,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40201431678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:31,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:04,363][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [160] timed out after [37678ms]
[2022-04-16T01:20:33,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31925591630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:36,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5836ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:39,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5835990628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:43,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:45,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6280673947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:47,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:48,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:20:49,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5459071080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:20:51,464][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8510ms] which is above the warn threshold of [5s]
[2022-04-16T01:20:57,980][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/172525ms] ago, timed out [2.2m/134847ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [160]
[2022-04-16T01:21:21,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5685ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:21:14,559][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9979}] took [3.4m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9979}]] took [0ms], [connecting to new nodes] took [54ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [7494ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [3222ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [0ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [3171ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [78ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [1963ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [62ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [124ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [120ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [150326ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [14172ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [330ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [5199ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [3200ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [6144ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [112ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [3164ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [3506ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [160ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [54ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [1539ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [70ms], [org.elasticsearch.license.LicenseService@343663f0] took [1ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [174ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:21:21,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5684924427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:21:22,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@55b4b271] took [11173ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:21:41,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:21:42,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [13328ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:21:43,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7549392400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:21:47,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5877ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:21:48,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5877ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:21:48,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5877411738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:21:58,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:02,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5658790683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:02,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7860ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:22:01,006][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58824}] took [11250ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:22:04,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:06,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5239840980ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:06,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [5239ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:22:10,187][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58824}] took [5858ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:22:21,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:22,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10029227901ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:55,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:22:58,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25815265289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:02,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:11,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6966589135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:16,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:20,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13827800679ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:24,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:28,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8517026230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:31,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:33,807][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58824}] took [15452ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:23:35,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6934310743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:41,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:44,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10637357287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:52,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:54,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10913896031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:23:59,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@7bd5d320] took [108771ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:24:14,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6466ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:24:26,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10976ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:24:28,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10976148718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:24:39,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:24:29,884][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [181344ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [50] unchanged indices
[2022-04-16T01:24:43,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7141186294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:24:43,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7741ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:24:45,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:24:46,416][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.3m] publication of cluster state version [9980] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:24:47,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6299219561ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:24:47,964][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6299ms] which is above the warn threshold of [5s]
[2022-04-16T01:25:02,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7848ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:25:03,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@4a17c4c5] took [12049ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:25:03,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7847586193ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:25:10,523][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-16T01:25:22,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5545ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:25:24,407][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-16T01:25:54,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@368e97b4] took [16009ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:25:58,125][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9980}] took [1.1m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9980}]] took [238ms], [connecting to new nodes] took [367ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [993ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [13781ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [53ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [95ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [0ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [696ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [37ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [89ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [18654ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [12404ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [640ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [282ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [10440ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [280ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [4098ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [2152ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [1838ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [484ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [35ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [0ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [0ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [65ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:26:08,523][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.6m/518526ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [8.3m/503616ms]
[2022-04-16T01:26:13,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:26:28,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [11801ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:26:28,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:26:31,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11400954147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:26:33,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:26:35,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5778384822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:26:34,790][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58856}] took [5779ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:26:34,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@7991ad8c] took [5778ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:26:35,126][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.9s/21953ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-16T01:26:50,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11361ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:26:51,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11360905030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:26:51,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [11360ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:26:56,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5599ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:27:07,603][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5203ms] which is above the warn threshold of [5s]
[2022-04-16T01:27:23,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:27:23,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [7778ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:27:26,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7578766694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:27:30,266][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:40786}] took [348838ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:27:42,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6178ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:27:46,056][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [52709ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-16T01:27:47,206][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [56.2s] publication of cluster state version [9981] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:28:00,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [9171ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:28:00,960][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58864}] took [9172ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:28:00,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8971ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:28:02,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8970982485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:28:05,345][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][120][11] duration [5.4s], collections [1]/[13.2s], total [5.4s]/[8.7s], memory [195.2mb]->[134.7mb]/[2gb], all_pools {[young] [84mb]->[32mb]/[0b]}{[old] [103.6mb]->[103.6mb]/[2gb]}{[survivor] [7.6mb]->[11.1mb]/[0b]}
[2022-04-16T01:28:08,752][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][120] overhead, spent [5.4s] collecting in the last [13.2s]
[2022-04-16T01:28:09,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [9779ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:28:38,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5200ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:28:45,079][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][133][12] duration [1.6s], collections [1]/[3.6s], total [1.6s]/[10.4s], memory [194.7mb]->[117.2mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [103.6mb]->[107mb]/[2gb]}{[survivor] [11.1mb]->[10.1mb]/[0b]}
[2022-04-16T01:28:45,614][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][133] overhead, spent [1.6s] collecting in the last [3.6s]
[2022-04-16T01:28:51,552][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9981}] took [51.7s] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9981}]] took [196ms], [connecting to new nodes] took [313ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [44661ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [3067ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [0ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [51ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [0ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [372ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [37ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [900ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [154ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [201ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [104ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [93ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [51ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [116ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [56ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [1ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:29:08,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5434ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:29:18,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [8401ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:29:19,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7801ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:29:20,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7801023956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:29:33,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:29:33,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [14411ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:29:47,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14411534128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:29:56,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:29:58,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23392259985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:06,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:07,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@3e0a50c2] took [10068ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:30:07,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10068263483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:14,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:15,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6975ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:30:15,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6375557256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:21,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:21,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [6197ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:30:22,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6197712676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:30:22,083][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][13] duration [4s], collections [1]/[14.1s], total [4s]/[14.5s], memory [173.2mb]->[181.7mb]/[2gb], all_pools {[young] [56mb]->[60mb]/[0b]}{[old] [107mb]->[113.3mb]/[2gb]}{[survivor] [10.1mb]->[8.3mb]/[0b]}
[2022-04-16T01:30:22,084][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [4s] collecting in the last [14.1s]
[2022-04-16T01:30:27,434][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][145][14] duration [3s], collections [1]/[4.4s], total [3s]/[17.5s], memory [181.7mb]->[123.4mb]/[2gb], all_pools {[young] [60mb]->[8mb]/[0b]}{[old] [113.3mb]->[113.3mb]/[2gb]}{[survivor] [8.3mb]->[10mb]/[0b]}
[2022-04-16T01:30:27,932][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][145] overhead, spent [3s] collecting in the last [4.4s]
[2022-04-16T01:30:27,928][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] starting SLM retention snapshot cleanup task
[2022-04-16T01:30:34,120][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][15] duration [2.5s], collections [1]/[4.4s], total [2.5s]/[20.1s], memory [195.4mb]->[128.7mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [113.3mb]->[115.9mb]/[2gb]}{[survivor] [10mb]->[12.8mb]/[0b]}
[2022-04-16T01:30:34,064][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] there are no repositories to fetch, SLM retention snapshot cleanup task complete
[2022-04-16T01:30:34,469][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [2.5s] collecting in the last [4.4s]
[2022-04-16T01:30:49,533][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14015ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-16T01:30:52,232][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.2s] publication of cluster state version [9982] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:31:03,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:31:05,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5648010338ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:31:07,761][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][158][16] duration [3.7s], collections [1]/[6.1s], total [3.7s]/[23.8s], memory [196.7mb]->[132.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [115.9mb]->[119.6mb]/[2gb]}{[survivor] [12.8mb]->[13mb]/[0b]}
[2022-04-16T01:31:08,375][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-16T01:31:09,282][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][158] overhead, spent [3.7s] collecting in the last [6.1s]
[2022-04-16T01:31:10,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7149ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:31:50,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:31:51,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18607768484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:31:52,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@2df54849] took [20608ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:31:53,333][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][161][17] duration [11.3s], collections [1]/[24.5s], total [11.3s]/[35.2s], memory [208.6mb]->[142.7mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [119.6mb]->[125mb]/[2gb]}{[survivor] [13mb]->[9.7mb]/[0b]}
[2022-04-16T01:31:53,945][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [11.3s] collecting in the last [24.5s]
[2022-04-16T01:32:08,857][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9982}] took [1.2m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9982}]] took [57ms], [connecting to new nodes] took [707ms], [applying settings] took [45ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [10483ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [2576ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [129ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [137ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [62ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [1334ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [70ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [328ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [59ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [41100ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [1041ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [112ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [668ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [256ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [3350ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [59ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [3824ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [5055ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [67ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [426ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [153ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [206ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [0ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [69ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:32:13,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6403ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:32:21,477][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5857ms] which is above the warn threshold of [5s]
[2022-04-16T01:32:36,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:32:39,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9261685288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:32:41,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [10062ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:32:41,046][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.8s/15895ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003][0]], allocationId [uZksvV6EQb-Esfa04Rhe4g], primary term [21], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003][0]], allocationId [uZksvV6EQb-Esfa04Rhe4g], primary term [21], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.11][0]], allocationId [5b4pPgx6Qi-ppLh-YsxZHw], primary term [15], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.11][0]], allocationId [5b4pPgx6Qi-ppLh-YsxZHw], primary term [15], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003][0]], allocationId [uZksvV6EQb-Esfa04Rhe4g], primary term [21], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003][0]], allocationId [uZksvV6EQb-Esfa04Rhe4g], primary term [21], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.09][0]], allocationId [ECZ6BVHgRHSSTDP_nNiqZw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.09][0]], allocationId [ECZ6BVHgRHSSTDP_nNiqZw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.11][0]], allocationId [5b4pPgx6Qi-ppLh-YsxZHw], primary term [15], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.11][0]], allocationId [5b4pPgx6Qi-ppLh-YsxZHw], primary term [15], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.09][0]], allocationId [ECZ6BVHgRHSSTDP_nNiqZw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.09][0]], allocationId [ECZ6BVHgRHSSTDP_nNiqZw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-16T01:32:43,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:32:48,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7319841359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:32:51,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:32:55,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7739653736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:01,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:04,354][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58898}] took [18144ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:33:05,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10403714683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:06,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [10403ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:33:04,354][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [486] timed out after [25322ms]
[2022-04-16T01:33:08,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:12,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7119923435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:15,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:20,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6950902035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:23,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:25,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [15882ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:33:26,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8931564704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:33:41,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [8360ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:34:01,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [10992ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:34:44,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:34:45,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26447144700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:34:46,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@4ee24f5b] took [39272ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:34:50,241][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][171][18] duration [11.9s], collections [1]/[58.4s], total [11.9s]/[47.1s], memory [194.7mb]->[189.7mb]/[2gb], all_pools {[young] [64mb]->[52mb]/[0b]}{[old] [125mb]->[131mb]/[2gb]}{[survivor] [9.7mb]->[6.7mb]/[0b]}
[2022-04-16T01:34:48,028][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.3m/141812ms] ago, timed out [1.9m/116490ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [486]
[2022-04-16T01:34:53,555][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [112527ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-16T01:34:58,655][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.1m] publication of cluster state version [9983] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:35:32,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:35:33,569][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][181][19] duration [6.8s], collections [1]/[1.4s], total [6.8s]/[54s], memory [213.7mb]->[213.7mb]/[2gb], all_pools {[young] [76mb]->[88mb]/[0b]}{[old] [131mb]->[131mb]/[2gb]}{[survivor] [6.7mb]->[6.7mb]/[0b]}
[2022-04-16T01:35:33,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9980022750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:35:34,160][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][181] overhead, spent [6.8s] collecting in the last [1.4s]
[2022-04-16T01:35:34,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [14164ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:35:45,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:35:47,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5310290598ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:35:48,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5310ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:36:03,257][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9983}] took [59.5s] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9983}]] took [36ms], [connecting to new nodes] took [113ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [582ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [214ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [1ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [34ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [34ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [0ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [609ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [55ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [51ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [29978ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [3633ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [158ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [9291ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [876ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [4862ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [125ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [4702ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [1543ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [966ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [1046ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [12ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [81ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [69ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [109ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [0ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:36:08,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7800ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:36:32,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:36:38,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13530988306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:36:38,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][20] duration [8.7s], collections [1]/[1.8s], total [8.7s]/[1m], memory [211.6mb]->[219.6mb]/[2gb], all_pools {[young] [76mb]->[84mb]/[0b]}{[old] [131mb]->[131mb]/[2gb]}{[survivor] [8.6mb]->[8.6mb]/[0b]}
[2022-04-16T01:36:40,067][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [8.7s] collecting in the last [1.8s]
[2022-04-16T01:36:40,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:36:42,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9251442329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:36:42,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [23582ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:36:55,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:36:57,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5906ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:36:57,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5106002396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:37:13,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5471ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:37:25,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6075ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:37:42,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6803ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:38:20,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@7288029a] took [35080ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:38:19,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:38:25,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16270256101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:38:20,537][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [108504ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-16T01:38:26,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [6610ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:38:26,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:38:26,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6610847019ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:38:26,493][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58938}] took [6611ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:38:26,610][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.1m] publication of cluster state version [9984] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:38:27,615][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][22] duration [12.8s], collections [2]/[51s], total [12.8s]/[1.2m], memory [189.2mb]->[157.9mb]/[2gb], all_pools {[young] [52mb]->[20mb]/[0b]}{[old] [132.1mb]->[136mb]/[2gb]}{[survivor] [9.1mb]->[5.9mb]/[0b]}
[2022-04-16T01:38:28,003][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [12.8s] collecting in the last [51s]
[2022-04-16T01:38:36,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [5489ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:38:36,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:38:37,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5489481842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:38:42,056][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][199][23] duration [3.9s], collections [1]/[6.8s], total [3.9s]/[1.3m], memory [221.9mb]->[152mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [136mb]->[136mb]/[2gb]}{[survivor] [5.9mb]->[12mb]/[0b]}
[2022-04-16T01:38:43,211][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][199] overhead, spent [3.9s] collecting in the last [6.8s]
[2022-04-16T01:38:44,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [8026ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:39:03,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:39:04,305][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][24] duration [6.7s], collections [1]/[3.9s], total [6.7s]/[1.4m], memory [184mb]->[192mb]/[2gb], all_pools {[young] [36mb]->[44mb]/[0b]}{[old] [136mb]->[136mb]/[2gb]}{[survivor] [12mb]->[12mb]/[0b]}
[2022-04-16T01:39:13,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10674513820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:39:15,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:39:14,691][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [6.7s] collecting in the last [3.9s]
[2022-04-16T01:39:15,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11812987363ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:39:15,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [23488ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:39:26,798][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][203][25] duration [6.1s], collections [1]/[29.3s], total [6.1s]/[1.5m], memory [192mb]->[222.5mb]/[2gb], all_pools {[young] [44mb]->[72mb]/[0b]}{[old] [136mb]->[144.8mb]/[2gb]}{[survivor] [12mb]->[5.6mb]/[0b]}
[2022-04-16T01:39:30,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [8464ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:39:56,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:39:56,986][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][26] duration [10.8s], collections [1]/[13s], total [10.8s]/[1.7m], memory [222.5mb]->[234.5mb]/[2gb], all_pools {[young] [72mb]->[88mb]/[0b]}{[old] [144.8mb]->[144.8mb]/[2gb]}{[survivor] [5.6mb]->[5.6mb]/[0b]}
[2022-04-16T01:39:57,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16432039488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:39:58,164][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [10.8s] collecting in the last [13s]
[2022-04-16T01:39:58,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [24445ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:40:13,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:40:20,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7250252764ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:40:21,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:40:21,894][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][206][28] duration [8.3s], collections [2]/[5.3s], total [8.3s]/[1.8m], memory [163.4mb]->[239.4mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [144.8mb]->[146.9mb]/[2gb]}{[survivor] [6.5mb]->[6.7mb]/[0b]}
[2022-04-16T01:40:22,714][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][206] overhead, spent [8.3s] collecting in the last [5.3s]
[2022-04-16T01:40:22,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8752096387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:40:31,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [16202ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:40:31,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:40:31,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9647146361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:40:38,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][207][29] duration [3.9s], collections [1]/[31.5s], total [3.9s]/[1.9m], memory [239.4mb]->[169.8mb]/[2gb], all_pools {[young] [0b]->[20mb]/[0b]}{[old] [146.9mb]->[146.9mb]/[2gb]}{[survivor] [6.7mb]->[6.8mb]/[0b]}
[2022-04-16T01:40:53,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [6393ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:41:03,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:41:31,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:41:32,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16750062829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:41:50,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:41:51,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19695038207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:41:57,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@12cf051a] took [19695ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:41:57,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:41:58,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6937155428ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:42:27,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:42:43,841][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/config%3A7.17.0][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50570}] took [123571ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:42:43,852][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/space%3Adefault][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50642}] took [123571ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:42:45,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19383196728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:42:50,005][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][213][31] duration [20.6s], collections [2]/[51.3s], total [20.6s]/[2.2m], memory [217.8mb]->[160.8mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [146.9mb]->[149mb]/[2gb]}{[survivor] [6.8mb]->[7.7mb]/[0b]}
[2022-04-16T01:42:50,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:42:52,772][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][213] overhead, spent [20.6s] collecting in the last [51.3s]
[2022-04-16T01:42:52,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33428965238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:43:15,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [52812ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:43:16,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:43:17,965][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/config%3A7.17.0][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50644}] took [59009ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:43:22,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25579958094ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:43:25,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:43:27,464][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50646}] took [115093ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:43:31,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10068877685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:43:37,589][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9984}] took [4.9m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9984}]] took [118ms], [connecting to new nodes] took [104ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [130385ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [59ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [0ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [0ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [458ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [30915ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [54ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [22650ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [109ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [19433ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [106ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [54517ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [169ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [36039ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [5280ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [684ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [96ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [94ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [288ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [831ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [222ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [1ms], [org.elasticsearch.license.LicenseService@343663f0] took [187ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [96ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:44:21,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:44:23,156][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50648}] took [54745ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:44:33,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54744669777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:44:46,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:44:55,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25207481468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:45:02,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:45:12,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257045634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:45:49,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:01,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20566098145ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:01,786][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1m/65844ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.04.06][0]], allocationId [_6pA-PPVQbWLBC6QYqwsNw], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.06][0]], allocationId [_6pA-PPVQbWLBC6QYqwsNw], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-16T01:46:01,465][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [754] timed out after [190607ms]
[2022-04-16T01:46:03,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:04,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [62055ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:46:10,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41489418933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:20,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:29,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16263266480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:40,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:46:51,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20334919005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:47:00,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:47:12,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18950659757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:47:13,869][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:50640}] took [132862ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:47:25,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26255ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:47:42,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26255394311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:47:56,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:48:06,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30201917315ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:48:15,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19878ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:48:17,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [56457ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:48:25,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19877214035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:48:35,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:48:47,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19970766735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:48:58,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:50:55,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22354386802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:51:02,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125416ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:51:03,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125415876485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:51:56,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:52:00,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53930621722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:52:12,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:52:15,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16035914617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:52:01,150][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [10.1m/606218ms] ago, timed out [6.9m/415611ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [754]
[2022-04-16T01:52:24,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:52:50,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11776173782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:52:54,997][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58982}] took [11776ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:53:01,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:53:13,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36396620420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:53:21,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:53:31,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20487091946ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:53:39,421][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][216][32] duration [31.5s], collections [1]/[4.9m], total [31.5s]/[2.7m], memory [212.8mb]->[169.4mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [149mb]->[149mb]/[2gb]}{[survivor] [7.7mb]->[12.3mb]/[0b]}
[2022-04-16T01:53:39,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17416ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:53:46,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [86075ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:53:45,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17415566045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:53:55,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:54:04,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16433028719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:54:09,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14168ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:54:11,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14168652538ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:54:20,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [10518ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:54:19,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:54:19,242][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [430719ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-16T01:54:21,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518020378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:54:23,409][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [8.3m] publication of cluster state version [9985] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T01:54:25,852][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5141ms] which is above the warn threshold of [5s]
[2022-04-16T01:55:00,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:55:02,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22678108918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:55:31,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@191f774a] took [71476ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:55:31,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:55:33,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12381439921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:55:35,282][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][217][34] duration [25.4s], collections [2]/[3.2m], total [25.4s]/[3.2m], memory [169.4mb]->[235.2mb]/[2gb], all_pools {[young] [16mb]->[68mb]/[0b]}{[old] [149mb]->[159.7mb]/[2gb]}{[survivor] [12.3mb]->[11.4mb]/[0b]}
[2022-04-16T01:55:36,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [5677ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:55:48,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:55:49,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10107738365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:56:00,029][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][218][35] duration [5.2s], collections [1]/[16.3s], total [5.2s]/[3.3m], memory [235.2mb]->[174mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [159.7mb]->[166.7mb]/[2gb]}{[survivor] [11.4mb]->[7.3mb]/[0b]}
[2022-04-16T01:56:02,456][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][218] overhead, spent [5.2s] collecting in the last [16.3s]
[2022-04-16T01:56:04,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [12840ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:56:22,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7739ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:56:55,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:56:56,786][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:58996}] took [29066ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:56:57,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@507f1680] took [29266ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:56:58,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6832981748ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:57:24,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [18705ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:57:53,229][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9985}] took [2.8m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9985}]] took [316ms], [connecting to new nodes] took [426ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [10186ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [1080ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [1ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [72ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [76ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [2349ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [70ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [501ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [59ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [147ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [61041ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [125ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [8719ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [59ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [33276ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [134ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [17739ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [65ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [16115ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [12778ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [86ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [1830ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [87ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [165ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [670ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [0ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T01:57:49,944][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10967ms] which is above the warn threshold of [5s]
[2022-04-16T01:57:52,226][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [860] timed out after [54565ms]
[2022-04-16T01:57:58,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [7804ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:58:20,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:04,770][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [39s/39063ms] to notify listeners on successful publication of cluster state (version: 9985, uuid: P8Gc1b38SmOQ8XHJ_gbXgA) for [shard-started StartedShardEntry{shardId [[logstash-2022.04.06][0]], allocationId [_6pA-PPVQbWLBC6QYqwsNw], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.06][0]], allocationId [_6pA-PPVQbWLBC6QYqwsNw], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-16T01:58:29,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13952619983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:12,411][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [13.5m/813663ms] which is longer than the warn threshold of [300000ms]; there are currently [10] pending tasks, the oldest of which has age [22.2m/1336150ms]
[2022-04-16T01:59:12,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:21,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53219396844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:19,382][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.2m/136810ms] ago, timed out [1.3m/82245ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [860]
[2022-04-16T01:59:31,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:31,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [17567ms] which is above the warn threshold of [5000ms]
[2022-04-16T01:59:35,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17567892895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:45,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14803ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T01:59:54,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14802729740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:00:07,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:00:17,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19869792576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:00:35,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [34672ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:00:51,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.9s/43951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:01:06,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.9s/43951682704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:01:24,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:01:42,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32316057208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:01:53,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31522ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:01:53,945][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/109707ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [26], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [26], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.05][0]], allocationId [yACvWLoGQxe9i6xH-lEIBg], primary term [33], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.05][0]], allocationId [yACvWLoGQxe9i6xH-lEIBg], primary term [33], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [26], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [26], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-16T02:02:07,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31522072625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:02:21,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27652ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:02:42,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27651656658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:03:29,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67491ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:03:46,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67491094711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:04:01,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:04:18,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32399725579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:04:30,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:04:40,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29397625359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:04:50,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:04:56,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19832203545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:10,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:23,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18667200764ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:30,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:36,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21930388691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:43,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:50,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12278536969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:05:54,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:06:01,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12469637683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:06:07,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12681ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:07:23,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12681738653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:07:37,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:07:43,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89572780409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:07:46,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@4f29e390] took [375894ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:07:54,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:08:05,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16589318506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:08:10,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:08:13,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [16678ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:08:38,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678128554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:08:47,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:08:57,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36493414761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:09:05,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:09:21,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033802640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:09:43,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:10:04,914][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [36553ms] which is above the warn threshold of [5s]
[2022-04-16T02:10:03,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36552384140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:10:15,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:10:26,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33681522834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:10:35,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:10:49,738][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][224][36] duration [50.8s], collections [1]/[9.6m], total [50.8s]/[4.1m], memory [218mb]->[212.1mb]/[2gb], all_pools {[young] [44mb]->[48mb]/[0b]}{[old] [166.7mb]->[166.7mb]/[2gb]}{[survivor] [7.3mb]->[13.4mb]/[0b]}
[2022-04-16T02:10:50,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19401818134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:11:17,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [89635ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:11:20,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:11:35,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45216766003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:11:53,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:12:03,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33436591580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:12:21,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27789ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:12:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27788720791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:13:58,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:01,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96595592231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:04,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:15,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6745925196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:19,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:23,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15422998871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:32,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:23,741][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [909] timed out after [402637ms]
[2022-04-16T02:14:35,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12317378744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:14:34,193][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.7m/402637ms] ago, timed out [0s/0ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [909]
[2022-04-16T02:14:49,571][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][225][37] duration [52.3s], collections [1]/[5.2m], total [52.3s]/[5m], memory [212.1mb]->[203.6mb]/[2gb], all_pools {[young] [48mb]->[32mb]/[0b]}{[old] [166.7mb]->[171.3mb]/[2gb]}{[survivor] [13.4mb]->[8.2mb]/[0b]}
[2022-04-16T02:14:53,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [20966ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:14:47,873][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [617751ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-16T02:15:05,468][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7805ms] which is above the warn threshold of [5s]
[2022-04-16T02:14:58,788][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11.5m] publication of cluster state version [9986] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T02:15:19,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [13807ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:15:52,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8888ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:15:55,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12484406425ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:15:56,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@5c6cfb8a] took [31327ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:16:09,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [9205ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:16:15,644][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59038}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:16:52,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29088ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:16:56,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29088035317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:19,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9104ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:24,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9103917643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:25,195][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][228][38] duration [21.1s], collections [1]/[1m], total [21.1s]/[5.3m], memory [255.6mb]->[183.3mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [171.3mb]->[171.3mb]/[2gb]}{[survivor] [8.2mb]->[8mb]/[0b]}
[2022-04-16T02:17:30,568][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][228] overhead, spent [21.1s] collecting in the last [1m]
[2022-04-16T02:17:30,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:37,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [38775ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:17:37,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29671707343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:39,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [7166ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:17:38,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:41,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7166003469ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:43,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:43,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5300194370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:51,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:54,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6750843136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:17:58,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:01,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8513464838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:06,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:01,719][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8514ms] which is above the warn threshold of [5s]
[2022-04-16T02:18:12,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7758407883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:19,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:23,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13149798821ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:27,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:30,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8016156006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:33,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:35,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5744093683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:18:37,188][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=252, version=9986}] took [3.2m] which is above the warn threshold of [30s]: [running task [Publication{term=252, version=9986}]] took [70ms], [connecting to new nodes] took [345ms], [applying settings] took [67ms], [org.elasticsearch.repositories.RepositoriesService@2030d365] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@bfdf06c] took [7508ms], [org.elasticsearch.script.ScriptService@46bc6a29] took [96ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [0ms], [org.elasticsearch.snapshots.RestoreService@399606ae] took [0ms], [org.elasticsearch.ingest.IngestService@5d0cff60] took [3246ms], [org.elasticsearch.action.ingest.IngestActionForwarder@78c43bca] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016caca0@3db4dcdc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@341e0eb2] took [99ms], [org.elasticsearch.tasks.TaskManager@ed300ae] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@579070d5] took [188ms], [org.elasticsearch.cluster.InternalClusterInfoService@79657ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d0b31a] took [0ms], [org.elasticsearch.indices.SystemIndexManager@523c461d] took [3639ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6b2d757c] took [83ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@686b0295] took [804ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4989cc46] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@499ccce8] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@f40f12] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1094737c] took [83ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@52a18ace] took [120316ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@50154f70] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6926302a] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@13328cb1] took [7131ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@aa48656] took [61ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@443a943e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@1c455bf3] took [16197ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@1f1b45d3] took [312ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1b151bce] took [17731ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@35ada8cc] took [78ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2072ae73] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5d746805] took [11696ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@546a3ab0] took [4407ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@9b7887a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@26e64847] took [514ms], [org.elasticsearch.node.ResponseCollectorService@23dc4991] took [66ms], [org.elasticsearch.snapshots.SnapshotShardsService@bf74a06] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3e36e75a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a8634ab] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@16a19fa5] took [121ms], [org.elasticsearch.indices.store.IndicesStore@7b07a834] took [133ms], [org.elasticsearch.persistent.PersistentTasksNodeService@5d862124] took [0ms], [org.elasticsearch.license.LicenseService@343663f0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6870d586] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@60589c01] took [0ms], [org.elasticsearch.gateway.GatewayService@3b189dc8] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@490f390b] took [0ms]
[2022-04-16T02:19:02,014][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [34.2m/2054890ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [42m/2525192ms]
[2022-04-16T02:19:17,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@1784f7d8] took [78628ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:20:05,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [38377ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:21:39,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:21:42,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [28260ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:21:45,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16009738625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:21:52,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:21:57,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13259767117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:21:48,847][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1012] timed out after [113433ms]
[2022-04-16T02:22:06,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:22:15,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13613957843ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:22:04,198][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13260ms] which is above the warn threshold of [5s]
[2022-04-16T02:22:24,043][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.8m/172305ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-16T02:22:24,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:22:35,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17941875054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:22:52,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:23:15,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [44101ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:23:09,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26160112350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:23:28,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:23:30,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [37725ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:23:40,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37725363809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:23:55,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:24:06,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26166434440ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:24:20,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:24:25,446][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59038}] took [296016ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:24:33,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25501720219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:24:45,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:24:53,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25517898606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:25:01,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:25:12,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16402755244ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:25:24,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:25:40,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22890739587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:27:34,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119373ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:27:49,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119372943138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:28:14,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.2s/49295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:28:30,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.2s/49295554292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:28:47,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:29:56,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33663407167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:30:15,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:30:39,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87230388351ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:31:02,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48436ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:31:26,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48435554892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:33:51,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/169817ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:17,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/169817375723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:22,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@1c66c46d] took [577167ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:34:22,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:25,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30058354981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:30,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:31,890][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59068}] took [8400ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:34:35,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8399461294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:40,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:44,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9565135944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:48,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8387ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:50,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8386580312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:34:51,584][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][232][40] duration [1.4m], collections [2]/[12.2m], total [1.4m]/[6.7m], memory [247.3mb]->[206.7mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [171.3mb]->[174.8mb]/[2gb]}{[survivor] [8mb]->[7.8mb]/[0b]}
[2022-04-16T02:35:52,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:35:57,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [80538ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:35:57,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57632792694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:36:03,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:36:07,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12465472782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:36:22,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [14946ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:36:19,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:36:31,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14946486157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:36:34,418][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59068}] took [27412ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:36:43,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23781ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:36:55,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23781438727ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:37:05,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:37:20,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22809461455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:37:30,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24466ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:37:47,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24466100714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:38:07,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:37:35,437][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [17.7m/1067889ms] ago, timed out [15.9m/954456ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1012]
[2022-04-16T02:38:17,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36794848071ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:38:32,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:38:07,238][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24466ms] which is above the warn threshold of [5s]
[2022-04-16T02:38:44,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24019240772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:38:56,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:39:13,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25325810932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:39:32,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:39:53,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35459962086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:40:17,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:40:38,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42113998053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:40:37,175][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [953806ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-16T02:40:54,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:41:10,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40638229206ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:41:27,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:41:02,700][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.6m] publication of cluster state version [9987] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-16T02:41:43,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31458476073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:42:02,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35699ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:42:17,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35698946243ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:42:35,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:42:52,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32902531205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:43:11,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:43:25,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33326052365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:43:41,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:43:53,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32179483297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:44:08,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:44:22,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27673108850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:44:35,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:44:46,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28121940085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:44:55,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@2def6b41] took [339572ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:44:57,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:45:06,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20913497573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:45:21,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:45:49,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24632532523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:46:00,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38687ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:46:04,020][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][41] duration [49.4s], collections [1]/[10.4m], total [49.4s]/[7.6m], memory [206.7mb]->[201.7mb]/[2gb], all_pools {[young] [36mb]->[24mb]/[0b]}{[old] [174.8mb]->[174.8mb]/[2gb]}{[survivor] [7.8mb]->[6.8mb]/[0b]}
[2022-04-16T02:46:08,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38686927296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:46:12,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [63319ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:46:21,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21244ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:46:29,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21243224989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:46:41,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:47:04,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20193435205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:47:04,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [20193ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:47:22,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:47:35,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39900310715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:47:53,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:48:03,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31515088004ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:48:17,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:48:33,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23428761090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:48:51,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:49:05,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35193662455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:49:53,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:50:14,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61448793858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:50:32,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:50:43,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [97916ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:50:53,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36468074113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:50:28,012][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [61448ms] which is above the warn threshold of [5s]
[2022-04-16T02:51:05,213][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1121] timed out after [255708ms]
[2022-04-16T02:51:11,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:51:24,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40259698706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:51:47,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:52:01,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36820593048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:52:03,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [36820ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:52:21,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34840ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:52:32,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34840341509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:52:50,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:53:05,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28078699408ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:53:25,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35583ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:53:39,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35583280914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:53:56,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:54:11,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29390269325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:54:26,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:54:40,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32242948358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:54:53,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:55:02,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [58471ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:55:14,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26228312931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:55:23,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31567ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:55:36,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31567338546ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:55:33,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [31567ms] which is above the warn threshold of [5000ms]
[2022-04-16T02:56:07,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:56:16,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43587857891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:56:23,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15436ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:56:32,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15435585107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:56:40,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:56:48,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17613237650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:56:58,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:57:09,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16825988494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:57:20,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:57:31,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23456848514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:57:50,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:58:07,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28420853582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:58:24,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:58:37,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33846747310ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:58:52,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:59:05,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29507870219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:59:26,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T02:59:45,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33094978753ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T02:59:56,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:00:09,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31170860611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:00:17,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:00:27,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20310272964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:00:46,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:00:54,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26860853815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:00:53,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@6034c372] took [320131ms] which is above the warn threshold of [5000ms]
[2022-04-16T03:00:59,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:01:10,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15863993893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:01:22,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:01:25,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1f904e87, interval=5s}] took [22089ms] which is above the warn threshold of [5000ms]
[2022-04-16T03:01:36,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22089312284ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:01:48,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:01:58,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25732813277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:02:09,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:02:27,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21228668076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:02:57,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.4s/46449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:03:23,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.4s/46448939771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:02:35,434][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21229ms] which is above the warn threshold of [5s]
[2022-04-16T03:10:24,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:10:32,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448880476134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:10:41,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16473ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:10:47,024][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [26m/1565484ms] ago, timed out [21.8m/1309776ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{ErZNH_1xRLGL_FWHnv7i9w}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1121]
[2022-04-16T03:10:47,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16472194482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:10:58,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:08,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16843117042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:11,474][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][236][42] duration [5.9m], collections [1]/[16.3m], total [5.9m]/[13.5m], memory [233.7mb]->[184.4mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [174.8mb]->[174.8mb]/[2gb]}{[survivor] [6.8mb]->[9.6mb]/[0b]}
[2022-04-16T03:11:16,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18231ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:20,129][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][236] overhead, spent [5.9m] collecting in the last [16.3m]
[2022-04-16T03:11:24,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [500426ms] which is above the warn threshold of [5000ms]
[2022-04-16T03:11:25,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18231176831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:32,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:41,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15994309406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:50,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17524ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:11:58,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17523629302ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:12:08,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:12:18,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17870966521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:12:24,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:12:31,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16995522960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:12:38,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:12:52,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12417466621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:13:07,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:13:20,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29864348221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:13:31,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:13:44,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23890141058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:13:50,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [53754ms] which is above the warn threshold of [5000ms]
[2022-04-16T03:13:54,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:14:05,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23731503544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:14:18,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:14:29,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23662578124ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:14:45,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:15:02,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27061588839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:15:19,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:15:35,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33819522929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:15:54,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:16:07,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33507207299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:16:36,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:17:17,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40380170120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:17:49,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:18:19,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75316993977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:18:32,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:18:47,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44310204252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:19:19,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45798ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:19:45,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45798342743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:47:16,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5111/0x00000008017e04b0@402a6a98] took [323856ms] which is above the warn threshold of [5000ms]
[2022-04-16T03:47:46,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3m/1699074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:49:10,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3m/1699073359052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:50:00,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:50:57,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139662692976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:52:19,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:53:14,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@46ce61e7, interval=5s}] took [137012ms] which is above the warn threshold of [5000ms]
[2022-04-16T03:54:50,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137012299967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T03:56:13,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/237131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T03:58:10,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/237130915448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:00:42,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266339ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:03:39,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266339329523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:04:36,684][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [266340ms] which is above the warn threshold of [5s]
[2022-04-16T04:06:59,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:09:58,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367033410340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:13:01,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/371341ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:16:35,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/371369442058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:19:51,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404373ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:23:33,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403971167233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:27:05,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/439396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:30:21,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/439417163471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:34:37,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417212ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:38:02,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417307303409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:41:48,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/466167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:45:08,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/466220476521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:46:35,990][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][238][43] duration [25.4m], collections [1]/[1.3h], total [25.4m]/[38.9m], memory [228.4mb]->[197.3mb]/[2gb], all_pools {[young] [52mb]->[16mb]/[0b]}{[old] [174.8mb]->[177.3mb]/[2gb]}{[survivor] [9.6mb]->[8mb]/[0b]}
[2022-04-16T04:48:28,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:50:05,320][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][238] overhead, spent [25.4m] collecting in the last [1.3h]
[2022-04-16T04:52:18,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400934457431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T04:54:25,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5f126fc5, interval=1s}] took [867154ms] which is above the warn threshold of [5000ms]
[2022-04-16T04:55:32,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424128ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T04:59:11,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424558013499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T05:02:32,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420237ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T05:05:45,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419639550901ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-16T05:09:33,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-16T05:13:28,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/409157523188ns] on relative clock which is above the warn threshold of [5000ms]
