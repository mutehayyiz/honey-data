[2022-03-25T17:46:56,505][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T17:46:56,553][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T17:46:56,554][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T17:47:13,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T17:47:13,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T17:47:13,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T17:47:13,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T17:47:13,920][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T17:47:13,921][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T17:47:13,922][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T17:47:13,923][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T17:47:13,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T17:47:13,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T17:47:13,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T17:47:13,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T17:47:13,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T17:47:13,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T17:47:13,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T17:47:13,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T17:47:13,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T17:47:13,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T17:47:13,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T17:47:13,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T17:47:13,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T17:47:13,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T17:47:13,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T17:47:13,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T17:47:13,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T17:47:13,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T17:47:13,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T17:47:13,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T17:47:13,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T17:47:13,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T17:47:13,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T17:47:13,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T17:47:13,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T17:47:13,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T17:47:14,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T17:47:14,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T17:47:14,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T17:47:14,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T17:47:14,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T17:47:14,024][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T17:47:14,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T17:47:14,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T17:47:14,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T17:47:14,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T17:47:14,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T17:47:14,047][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T17:47:14,047][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T17:47:14,048][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T17:47:14,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T17:47:14,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T17:47:14,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T17:47:14,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T17:47:14,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T17:47:14,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T17:47:14,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T17:47:14,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T17:47:14,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T17:47:14,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T17:47:14,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T17:47:14,299][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104gb], net total_space [125.8gb], types [ext4]
[2022-03-25T17:47:14,315][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T17:47:15,097][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T17:47:41,902][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T17:47:41,913][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T17:47:44,130][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T17:47:44,423][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T17:47:46,261][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T17:47:48,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T17:47:48,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T17:47:48,181][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T17:47:48,183][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T17:47:48,623][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T17:47:52,262][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T17:47:52,483][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 117, version: 3307, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T17:47:52,813][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 117, version: 3307, reason: Publication{term=117, version=3307}
[2022-03-25T17:47:53,210][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T17:47:53,211][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T17:47:55,214][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T17:47:55,251][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T17:47:56,841][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T17:47:56,842][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T17:47:58,094][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:58,491][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:59,215][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:59,316][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-25T17:47:59,327][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-25T17:47:59,330][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-25T17:48:01,401][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T17:48:01,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T17:48:06,310][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][18] overhead, spent [300ms] collecting in the last [1s]
[2022-03-25T17:48:08,914][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T17:48:23,704][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.25][0]]]).
[2022-03-25T17:48:24,144][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-25T17:48:24,665][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-25T17:49:29,651][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 504 finished with response BulkByScrollResponse[took=478.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T17:49:32,624][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 525 finished with response BulkByScrollResponse[took=2.9s,timed_out=false,sliceId=null,updated=1039,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T17:49:43,738][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-25T18:40:56,756][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T18:53:43,446][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T18:59:47,606][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T19:00:00,368][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4328] overhead, spent [569ms] collecting in the last [1.1s]
[2022-03-25T19:04:18,185][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4569][98] duration [2.4s], collections [1]/[3.7s], total [2.4s]/[7.6s], memory [1.2gb]->[177.9mb]/[2gb], all_pools {[young] [1gb]->[4mb]/[0b]}{[old] [168.4mb]->[168.4mb]/[2gb]}{[survivor] [8.7mb]->[9.5mb]/[0b]}
[2022-03-25T19:04:19,761][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4569] overhead, spent [2.4s] collecting in the last [3.7s]
[2022-03-25T19:04:25,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [10724ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:04:29,781][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4571][99] duration [1.4s], collections [1]/[2.8s], total [1.4s]/[9s], memory [197.9mb]->[182mb]/[2gb], all_pools {[young] [52mb]->[24mb]/[0b]}{[old] [168.4mb]->[169.2mb]/[2gb]}{[survivor] [9.5mb]->[8.8mb]/[0b]}
[2022-03-25T19:04:30,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4571] overhead, spent [1.4s] collecting in the last [2.8s]
[2022-03-25T19:04:52,801][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4577][100] duration [2s], collections [1]/[3.9s], total [2s]/[11s], memory [214mb]->[242mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [169.2mb]->[169.2mb]/[2gb]}{[survivor] [8.8mb]->[9.5mb]/[0b]}
[2022-03-25T19:04:53,967][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4577] overhead, spent [2s] collecting in the last [3.9s]
[2022-03-25T19:04:55,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [8929ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:05:36,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [21616ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:06:27,390][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:37172}] took [13206ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:06:58,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:11,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17431827175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:15,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [26298ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:07:15,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26298ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:21,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26298054036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:25,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:28,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [10527ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:07:32,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10527761630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:23,991][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26298ms] which is above the warn threshold of [5s]
[2022-03-25T19:07:44,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:50,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18250754263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:08:05,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:08:41,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [20257ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:08:41,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20257936508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:11,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66164ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:17,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66164336526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:31,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12863447260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:48,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24904ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:00,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24904796383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:16,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:34,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28080859831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:44,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:47,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [27758ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:10:49,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758010201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:03,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [17827ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:11:02,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:12,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17827198896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:16,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:21,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14369898133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:24,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:19,357][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14370ms] which is above the warn threshold of [5s]
[2022-03-25T19:11:27,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [7976ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:11:28,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7976224930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:38,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [6092ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:12:25,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:12:26,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43301401616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:12:29,182][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4589][101] duration [30.4s], collections [1]/[55.7s], total [30.4s]/[41.5s], memory [258.8mb]->[193.4mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [169.2mb]->[170.1mb]/[2gb]}{[survivor] [9.5mb]->[7.3mb]/[0b]}
[2022-03-25T19:12:29,065][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [32901] timed out after [71464ms]
[2022-03-25T19:12:30,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4589] overhead, spent [30.4s] collecting in the last [55.7s]
[2022-03-25T19:12:31,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [6174ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:12:34,443][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.5m/451011ms] ago, timed out [6.3m/379547ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [32901]
[2022-03-25T19:12:56,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [5427ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:13:15,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [9659ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:14:27,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [50964ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:18:37,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:19:16,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24575424343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:19:43,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:05,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65148679130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:39,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:48,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [121854ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:21:07,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56706215790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:23:11,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:38,921][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [65149ms] which is above the warn threshold of [5s]
[2022-03-25T19:24:37,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150858076373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:24:49,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:26:40,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98564707333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:02,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/133798ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:19,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/133798205583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:30,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:38,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27380024265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:43,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:30,562][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33066] timed out after [371278ms]
[2022-03-25T19:27:47,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14045012735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:49,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:48,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4599][102] duration [1.4m], collections [1]/[8m], total [1.4m]/[2.1m], memory [225.4mb]->[186.1mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-25T19:27:51,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5339125149ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:51,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [19384ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:27:55,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:59,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6396666769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:03,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:05,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [7923ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:27:58,568][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.3m/558237ms] ago, timed out [3.1m/186959ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33066]
[2022-03-25T19:28:06,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7923648031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:09,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:11,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6506112771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:10,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [6506ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:16,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [6905ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:16,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:27,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6905766542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:37,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:44,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [20409ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:46,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20409206380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:54,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:56,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [16562ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:01,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16562907176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:07,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:14,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13495022644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:22,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:26,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14613956808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:25,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [14613ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:31,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [9301ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:31,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:38,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9301760803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,693][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [37.4s/37410ms] ago, timed out [0s/0ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33126]
[2022-03-25T19:29:46,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [13122ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:50,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13122256963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:02,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,257][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33126] timed out after [37410ms]
[2022-03-25T19:30:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476619774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:26,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:40,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24079280312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:48,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [24079ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:30:54,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:01,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914627885ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:10,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:16,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16962305169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:23,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:32,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13815134368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:32,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [30777ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:31:13,916][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16963ms] which is above the warn threshold of [5s]
[2022-03-25T19:31:44,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:56,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20604565563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:12,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:27,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28235774198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:45,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:04,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32853177729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:17,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:27,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31978845533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:31,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [31978ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:33:41,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:55,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24117731252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:09,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:27,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27865933874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:53,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:15,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38798970596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:34,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:52,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46106475305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:54,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [46106ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:36:10,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:30,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34062310888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:48,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:47,369][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33182] timed out after [117186ms]
[2022-03-25T19:37:01,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39370681162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:37:24,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32318ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:37:44,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32318096264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:50,724][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [39371ms] which is above the warn threshold of [5s]
[2022-03-25T19:38:18,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:38:40,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50525538257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:21,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:17,079][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.5s/20526ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93]], which exceeds the warn threshold of [10s]
[2022-03-25T19:40:10,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67529145259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:52,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [67529ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:40:52,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:26,718][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:37180}] took [150373ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:41:33,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93589702063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:41:59,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:16,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67349234040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:31,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:52,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32204612075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:07,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:28,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35139925862ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:51,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:44:03,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@4c5e33d1] took [44827ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:44:17,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44827664644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:44:41,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:02,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48050334855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:23,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:33,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958836384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:47,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:57,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [28463ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:46:01,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28463489349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:19,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:26,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [32147ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:46:34,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32147720356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:54,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:10,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35438493693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:15,634][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.1m/909933ms] ago, timed out [13.2m/792747ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33182]
[2022-03-25T19:47:24,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:43,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28152052937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:58,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:04,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [35278ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:48:08,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35278367777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:25,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:29,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [27422ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:48:41,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27422457062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:55,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:49:11,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30219065515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:28,342][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33271] timed out after [183059ms]
[2022-03-25T19:49:20,704][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10986ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0]], which exceeds the warn threshold of [10s]
[2022-03-25T19:49:33,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:09,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31659155326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:39,713][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.4s/17407ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@95bd4ef1]], which exceeds the warn threshold of [10s]
[2022-03-25T19:50:41,197][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40706}] took [89301ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:50:45,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:55,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77603991973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:11,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:26,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26030020562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:33,339][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40684}] took [103634ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:51:35,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [26030ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:51:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:54,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30957882736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:56:12,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:56:43,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270571251994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:01,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:13,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48609265786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:26,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:27,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@59cd0a64] took [25428ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:57:39,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25428147920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:58,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:01,978][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.1m/847822ms] ago, timed out [11m/664763ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33271]
[2022-03-25T19:58:05,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831762065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:16,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4613][103] duration [3.6m], collections [1]/[6.6m], total [3.6m]/[5.8m], memory [254.1mb]->[187.6mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [8mb]->[5.4mb]/[0b]}
[2022-03-25T19:58:19,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:22,963][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4613] overhead, spent [3.6m] collecting in the last [6.6m]
[2022-03-25T19:58:25,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19941912137ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:25,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [52773ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:58:39,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:47,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20594361043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:08,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:33,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29109027589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:47,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [37771ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:59:47,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:54,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37771644334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:08,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:34,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21711780943ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:53,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:11,820][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40688}] took [45241ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:01:11,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45240699162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:15,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [45240ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:01:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:47,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37797219032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:02:07,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:02:29,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28051543340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:03:10,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:52,809][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33333] timed out after [140248ms]
[2022-03-25T20:02:13,914][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28052ms] which is above the warn threshold of [5s]
[2022-03-25T20:03:36,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66575536362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:04:06,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:05:42,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58244757285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:05:59,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:06:16,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114668192682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:06:30,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:07:10,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31828887822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:07:11,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [31828ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:07:37,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:02,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66489628213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:44,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45264467266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:08,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:16,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@61661069] took [45422ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:09:28,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45422658334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:53,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:20,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.6s/44639698135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:35,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:11:00,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472507228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:11:32,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58s/58021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:30,626][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44639ms] which is above the warn threshold of [5s]
[2022-03-25T20:11:56,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58203900545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:12:16,907][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14m/845860ms] ago, timed out [11.7m/705612ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33333]
[2022-03-25T20:12:24,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [58203ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:12:25,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:13:33,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53292884968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:14:33,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:14:55,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127417383810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:15:14,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:15:35,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41070333827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:02,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:28,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47649460881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:47,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45844ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:17:30,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45844050512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:01,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:57,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73293848606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:19:11,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:19:29,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69922473716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:49,925][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33409] timed out after [366096ms]
[2022-03-25T20:19:47,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:09,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36110978754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:42,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:42,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [50945ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:21:18,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50945347426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:21:47,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:11,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69820139104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:31,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:23:01,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43113470025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:14,310][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [69820ms] which is above the warn threshold of [5s]
[2022-03-25T20:23:41,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:23:59,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70847132487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:21,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:35,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39639395822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:47,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:55,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [25862ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:24:59,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25862115846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:25:13,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:25:16,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [26521ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:25:35,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26521059095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:01,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:27,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36533791135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:39,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:47,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48377322444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:53:14,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4m/1584266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:55:40,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4m/1584266404672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:56:39,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4619][104] duration [22.1m], collections [1]/[26.7m], total [22.1m]/[27.9m], memory [243.6mb]->[175.6mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [5.4mb]->[5.4mb]/[0b]}
[2022-03-25T20:58:00,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:59:45,529][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4619] overhead, spent [22.1m] collecting in the last [26.7m]
[2022-03-25T21:00:33,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296750798431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:02:37,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [1881017ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:02:53,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:05:25,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292145004438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:08:07,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:10:50,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313933965622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:12:24,446][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [58.9m/3537673ms] ago, timed out [52.8m/3171577ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33409]
[2022-03-25T21:14:02,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:16:46,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307272735975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:19:02,329][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.8s/21840ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93]], which exceeds the warn threshold of [10s]
[2022-03-25T21:19:23,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:21:45,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357627262708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:16:21,329][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [307273ms] which is above the warn threshold of [5s]
[2022-03-25T21:22:24,440][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.8s/32852ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T21:24:13,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:26:58,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301544254369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:28:09,749][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.5s/10582ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T21:30:51,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:33:27,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392548510415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:26:40,608][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33485] timed out after [2572007ms]
[2022-03-25T21:36:13,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:38:40,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313791736947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:41:42,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:44:43,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342577554963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:46:50,278][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.5m/694093ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [12.3m/741466ms]
[2022-03-25T21:48:13,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:50:27,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [391411ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:50:32,899][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [29m/1741873ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [29.1m/1750738ms]
[2022-03-25T21:50:57,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391411321665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:54:11,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [344322ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:54:09,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:57:38,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344322819398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:00:41,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:57:29,252][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/111228ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@95bd4ef1], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T22:04:38,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401884462127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:10:55,140][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T22:10:55,208][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T22:10:55,211][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T22:11:04,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T22:11:04,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T22:11:04,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T22:11:04,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T22:11:04,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T22:11:04,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T22:11:04,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T22:11:04,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T22:11:04,422][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T22:11:04,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T22:11:04,424][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T22:11:04,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T22:11:04,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T22:11:04,430][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T22:11:04,432][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T22:11:04,434][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T22:11:04,436][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T22:11:04,439][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T22:11:04,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T22:11:04,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T22:11:04,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T22:11:04,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T22:11:04,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T22:11:04,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T22:11:04,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T22:11:04,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T22:11:04,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T22:11:04,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T22:11:04,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T22:11:04,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T22:11:04,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T22:11:04,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T22:11:04,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T22:11:04,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T22:11:04,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T22:11:04,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T22:11:04,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T22:11:04,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T22:11:04,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T22:11:04,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T22:11:04,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T22:11:04,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T22:11:04,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T22:11:04,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T22:11:04,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T22:11:04,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T22:11:04,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T22:11:04,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T22:11:04,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T22:11:04,501][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T22:11:04,607][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.8gb], net total_space [125.8gb], types [ext4]
[2022-03-25T22:11:04,609][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T22:11:05,064][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T22:11:19,671][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T22:11:19,679][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T22:11:19,682][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,683][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-25T22:11:19,687][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,689][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T22:11:19,690][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T22:11:19,692][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,693][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T22:11:21,050][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T22:11:21,255][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T22:11:23,329][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T22:11:25,875][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T22:11:25,889][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T22:11:26,092][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T22:11:26,101][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T22:11:26,715][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T22:11:33,330][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T22:11:33,614][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 118, version: 3348, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T22:11:34,113][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 118, version: 3348, reason: Publication{term=118, version=3348}
[2022-03-25T22:11:36,888][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T22:11:36,932][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T22:11:40,913][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T22:12:43,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:12:44,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7509ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:14:21,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21456633825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:14:39,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:14:52,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140076244527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:15:14,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:00,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34176985191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:15,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [174253ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:16:24,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:35,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75259746665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:41,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:46,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16787969799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:52,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:54,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12022878658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:58,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:02,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6424749114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:05,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:09,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7433347976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:11,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:12,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204232007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:12,442][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3349}] took [5.4m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3349}]] took [0ms], [connecting to new nodes] took [20ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [50ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [188ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [1655ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [9ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [0ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [20ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [83ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [18ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [3ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [9ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [34ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [4ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [4ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@21a3c20] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [3ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@2e661a53] took [9ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [0ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [11ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [0ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [2ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [1720ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@38d52050] took [329836ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [42ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [42ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:17:13,046][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T22:17:12,931][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39714}] took [33053ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:17:15,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@51c28ffa] took [52450ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:17:19,748][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-25T22:17:19,746][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-25T22:17:20,500][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-25T22:17:21,154][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,317][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-25T22:17:21,509][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/config%3A7.17.0, params: {index=.kibana_7.17.0, id=config:7.17.0}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][config:7.17.0]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,666][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/space%3Adefault, params: {index=.kibana_7.17.0, id=space:default}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][space:default]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,757][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T22:17:21,857][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T22:19:27,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@634595fc] took [96554ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:19:57,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8017ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:20:23,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [11599ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:20:37,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:20:23,390][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9999ms] which is above the warn threshold of [5s]
[2022-03-25T22:20:40,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8867931415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:20:47,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:00,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862549429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:05,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [18521ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:21:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18521072164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:06,212][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [195505ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [25] unchanged indices
[2022-03-25T22:21:21,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:25,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257943654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:29,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:34,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8121776198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:38,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:40,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [9148ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:21:32,919][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.1m] publication of cluster state version [3352] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-25T22:21:43,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9148234875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:50,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:53,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70a8e64b, interval=5s}] took [11351ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:22:00,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11351952328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:15,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:28,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23617793568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:38,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:44,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23613384484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:29,720][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [56] timed out after [44924ms]
[2022-03-25T22:22:53,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:10,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15476690326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:18,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:18,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [63516ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:25,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24426559439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:30,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:34,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13090294246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:36,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [13090ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:40,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:45,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8938395580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:51,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67d1dedd, interval=1m}] took [12441ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:51,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:54,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12441410473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:59,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:00,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [7896ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:24:04,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7896552751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:10,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:14,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [10791ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:24:20,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10791911221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:28,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:32,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18246965640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:37,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:42,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9256499383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:46,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:50,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9410464457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:54,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7014ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:57,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7013776830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:01,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:05,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517369129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:13,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:19,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8271920785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:22,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:28,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13024824539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:34,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@51a65b55] took [83125ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:33,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:38,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10383722736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:42,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:42,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [9138ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:46,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9138746833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:52,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:55,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9447642534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:56,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [9447ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:58,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:01,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6514831719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:06,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:09,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8568826690ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:05,997][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [60] timed out after [89980ms]
[2022-03-25T22:26:10,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [12599ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:09,382][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [61] timed out after [35485ms]
[2022-03-25T22:26:21,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6054ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:21,783][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6255ms] which is above the warn threshold of [5s]
[2022-03-25T22:26:41,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8818ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:50,015][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3352}] took [4.9m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3352}]] took [340ms], [connecting to new nodes] took [1879ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [332ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [131205ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [0ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [806ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [290ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [1ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [271ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [68ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [339ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [153ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [1711ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [12954ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [1210ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [272ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [333ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [82195ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [133ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [10720ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [576ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [123ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [7784ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [389ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [12567ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [127ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [13073ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [5001ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [753ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [130ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [66ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [0ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:26:55,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7939ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:27:20,048][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.5s/17552ms] to notify listeners on successful publication of cluster state (version: 3352, uuid: Z61V7ePbSGKdeoXbXCUUWw) for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [89], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [89], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [31], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [31], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-25T22:27:26,484][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10m/601335ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [10m/601773ms]
[2022-03-25T22:27:43,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:27:36,434][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3m/184436ms] ago, timed out [1.5m/94456ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [60]
[2022-03-25T22:27:44,298][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.1m/490220ms] ago, timed out [7.4m/445296ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [56]
[2022-03-25T22:27:44,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6392053009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:27:15,189][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketTimeoutException: null
	at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:110) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-25T22:27:44,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@562ad2d6] took [43340ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:27:48,415][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.4m/144721ms] ago, timed out [1.8m/109236ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [61]
[2022-03-25T22:27:58,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6001ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:07,047][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65] timed out after [21902ms]
[2022-03-25T22:28:14,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6831ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:40,761][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5606ms] which is above the warn threshold of [5s]
[2022-03-25T22:28:42,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:50,433][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39714}] took [611367ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:38,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@20891f77] took [47907ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:52,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7827ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:58,768][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [71] timed out after [64493ms]
[2022-03-25T22:30:00,571][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [113565ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [25] unchanged indices
[2022-03-25T22:30:00,849][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [72] timed out after [21642ms]
[2022-03-25T22:30:07,107][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.1m] publication of cluster state version [3353] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-25T22:30:32,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6743ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:30:55,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@33fc8944] took [15168ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:30:56,658][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/191775ms] ago, timed out [2.8m/169873ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65]
[2022-03-25T22:31:04,235][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [80] timed out after [22346ms]
[2022-03-25T22:31:06,257][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/86581ms] ago, timed out [1m/64939ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [72]
[2022-03-25T22:31:06,257][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.1m/131686ms] ago, timed out [1.1m/67193ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [71]
[2022-03-25T22:31:13,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5955ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:21,903][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [39.6s/39676ms] ago, timed out [17.3s/17330ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [80]
[2022-03-25T22:31:21,640][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [81] timed out after [24172ms]
[2022-03-25T22:31:25,660][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [29.4s/29461ms] ago, timed out [5.2s/5289ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [81]
[2022-03-25T22:31:27,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6446ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:39,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6985ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:51,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5651ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:32:01,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6302ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:59,077][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3353}] took [1.7m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3353}]] took [53ms], [connecting to new nodes] took [525ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [7816ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [0ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [316ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [155ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [101ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [51ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [138ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [2085ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [494ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [4ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [342ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [232ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [36644ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [9765ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [791ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [398ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [10487ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [486ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [12232ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [118ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [249ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [8752ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [3795ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [311ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [2054ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [56ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [97ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [48ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [341ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [48ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [0ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:33:47,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@4e214b72] took [99676ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:01,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5032ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:20,143][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2m/120578ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-25T22:34:29,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8639ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:57,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [17408ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:36:54,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:38:02,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5165797971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:37:49,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [85051ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:39:24,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:39:52,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159957984247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:40:33,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:41:36,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61589604827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:43:27,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:45:55,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173885158101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:47:00,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:47:43,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218584186744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:49:11,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:50:46,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127079825867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:52:11,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/186960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:52:58,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/186959830872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:53:40,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:55:22,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89362844379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:56:48,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:57:35,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187493783819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:58:29,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:58:53,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103723539477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:59:27,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@3dca0ae] took [1148678ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:59:38,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:00:25,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69620749208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:01:29,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:02:06,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109267493825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:02:39,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:03:10,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70001213370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:03:48,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:04:24,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70670960726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:03,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:40,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [74295ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:05:30,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74295620605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:06:24,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:07:02,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76597084010ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:35,870][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [74296ms] which is above the warn threshold of [5s]
[2022-03-25T23:07:32,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:08:04,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72619469411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:08:34,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:09:11,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61628805599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:10:18,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:10:31,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [102596ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:10:59,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102596780343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:11:30,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:11:32,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@625fb4bb, interval=30s}] took [73040ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:12:00,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73040553218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:12:35,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:13:15,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65278156830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:13:58,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:14:48,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81538025314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:16:52,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:18:42,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173571902567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:21:19,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:23:50,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263627140079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:22:41,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43280}] took [263627ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:26:11,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:28:25,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292323478499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:33:19,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:38:41,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403969572235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:42:03,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:44:36,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540056550106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:47:22,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:50:39,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324771832285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:53:28,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:53:57,176][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [4696785ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [23] unchanged indices
[2022-03-25T23:56:15,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361734784179ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:59:31,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361430ms] on absolute clock which is above the warn threshold of [5000ms]
