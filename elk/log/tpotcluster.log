[2022-03-25T17:46:56,505][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T17:46:56,553][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T17:46:56,554][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T17:47:13,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T17:47:13,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T17:47:13,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T17:47:13,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T17:47:13,920][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T17:47:13,921][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T17:47:13,922][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T17:47:13,923][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T17:47:13,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T17:47:13,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T17:47:13,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T17:47:13,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T17:47:13,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T17:47:13,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T17:47:13,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T17:47:13,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T17:47:13,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T17:47:13,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T17:47:13,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T17:47:13,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T17:47:13,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T17:47:13,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T17:47:13,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T17:47:13,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T17:47:13,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T17:47:13,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T17:47:13,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T17:47:13,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T17:47:13,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T17:47:13,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T17:47:13,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T17:47:13,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T17:47:13,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T17:47:13,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T17:47:14,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T17:47:14,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T17:47:14,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T17:47:14,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T17:47:14,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T17:47:14,024][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T17:47:14,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T17:47:14,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T17:47:14,045][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T17:47:14,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T17:47:14,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T17:47:14,047][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T17:47:14,047][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T17:47:14,048][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T17:47:14,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T17:47:14,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T17:47:14,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T17:47:14,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T17:47:14,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T17:47:14,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T17:47:14,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T17:47:14,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T17:47:14,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T17:47:14,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T17:47:14,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T17:47:14,299][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104gb], net total_space [125.8gb], types [ext4]
[2022-03-25T17:47:14,315][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T17:47:15,097][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T17:47:41,902][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T17:47:41,913][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T17:47:44,130][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T17:47:44,423][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T17:47:46,261][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T17:47:48,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T17:47:48,122][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T17:47:48,181][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T17:47:48,183][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T17:47:48,623][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T17:47:52,262][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T17:47:52,483][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 117, version: 3307, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T17:47:52,813][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 117, version: 3307, reason: Publication{term=117, version=3307}
[2022-03-25T17:47:53,210][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T17:47:53,211][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T17:47:55,214][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T17:47:55,251][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T17:47:56,841][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T17:47:56,842][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T17:47:58,094][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:58,491][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:59,215][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-25T17:47:59,316][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-25T17:47:59,327][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-25T17:47:59,330][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-25T17:48:01,401][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T17:48:01,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T17:48:06,310][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][18] overhead, spent [300ms] collecting in the last [1s]
[2022-03-25T17:48:08,914][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T17:48:23,704][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.25][0]]]).
[2022-03-25T17:48:24,144][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-25T17:48:24,665][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-25T17:49:29,651][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 504 finished with response BulkByScrollResponse[took=478.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T17:49:32,624][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 525 finished with response BulkByScrollResponse[took=2.9s,timed_out=false,sliceId=null,updated=1039,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T17:49:43,738][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-25T18:40:56,756][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T18:53:43,446][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T18:59:47,606][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T19:00:00,368][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4328] overhead, spent [569ms] collecting in the last [1.1s]
[2022-03-25T19:04:18,185][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4569][98] duration [2.4s], collections [1]/[3.7s], total [2.4s]/[7.6s], memory [1.2gb]->[177.9mb]/[2gb], all_pools {[young] [1gb]->[4mb]/[0b]}{[old] [168.4mb]->[168.4mb]/[2gb]}{[survivor] [8.7mb]->[9.5mb]/[0b]}
[2022-03-25T19:04:19,761][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4569] overhead, spent [2.4s] collecting in the last [3.7s]
[2022-03-25T19:04:25,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [10724ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:04:29,781][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4571][99] duration [1.4s], collections [1]/[2.8s], total [1.4s]/[9s], memory [197.9mb]->[182mb]/[2gb], all_pools {[young] [52mb]->[24mb]/[0b]}{[old] [168.4mb]->[169.2mb]/[2gb]}{[survivor] [9.5mb]->[8.8mb]/[0b]}
[2022-03-25T19:04:30,192][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4571] overhead, spent [1.4s] collecting in the last [2.8s]
[2022-03-25T19:04:52,801][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4577][100] duration [2s], collections [1]/[3.9s], total [2s]/[11s], memory [214mb]->[242mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [169.2mb]->[169.2mb]/[2gb]}{[survivor] [8.8mb]->[9.5mb]/[0b]}
[2022-03-25T19:04:53,967][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4577] overhead, spent [2s] collecting in the last [3.9s]
[2022-03-25T19:04:55,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [8929ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:05:36,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [21616ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:06:27,390][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:37172}] took [13206ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:06:58,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:11,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17431827175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:15,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [26298ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:07:15,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26298ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:21,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26298054036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:25,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:28,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [10527ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:07:32,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10527761630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:23,991][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26298ms] which is above the warn threshold of [5s]
[2022-03-25T19:07:44,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:07:50,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18250754263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:08:05,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:08:41,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [20257ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:08:41,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20257936508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:11,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66164ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:17,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66164336526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:31,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12863447260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:09:48,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24904ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:00,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24904796383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:16,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:34,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28080859831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:44,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:10:47,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [27758ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:10:49,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758010201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:03,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [17827ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:11:02,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:12,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17827198896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:16,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:21,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14369898133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:24,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:19,357][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14370ms] which is above the warn threshold of [5s]
[2022-03-25T19:11:27,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [7976ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:11:28,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7976224930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:11:38,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [6092ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:12:25,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:12:26,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43301401616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:12:29,182][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4589][101] duration [30.4s], collections [1]/[55.7s], total [30.4s]/[41.5s], memory [258.8mb]->[193.4mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [169.2mb]->[170.1mb]/[2gb]}{[survivor] [9.5mb]->[7.3mb]/[0b]}
[2022-03-25T19:12:29,065][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [32901] timed out after [71464ms]
[2022-03-25T19:12:30,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4589] overhead, spent [30.4s] collecting in the last [55.7s]
[2022-03-25T19:12:31,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [6174ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:12:34,443][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.5m/451011ms] ago, timed out [6.3m/379547ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [32901]
[2022-03-25T19:12:56,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [5427ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:13:15,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [9659ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:14:27,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [50964ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:18:37,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:19:16,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24575424343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:19:43,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:05,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65148679130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:39,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:48,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [121854ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:21:07,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56706215790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:23:11,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:20:38,921][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [65149ms] which is above the warn threshold of [5s]
[2022-03-25T19:24:37,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/150858076373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:24:49,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:26:40,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/98564707333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:02,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/133798ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:19,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/133798205583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:30,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:38,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27380024265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:43,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:30,562][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33066] timed out after [371278ms]
[2022-03-25T19:27:47,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14045012735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:49,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:48,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4599][102] duration [1.4m], collections [1]/[8m], total [1.4m]/[2.1m], memory [225.4mb]->[186.1mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-25T19:27:51,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5339125149ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:51,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [19384ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:27:55,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:27:59,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6396666769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:03,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:05,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [7923ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:27:58,568][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.3m/558237ms] ago, timed out [3.1m/186959ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33066]
[2022-03-25T19:28:06,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7923648031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:09,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:11,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6506112771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:10,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [6506ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:16,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@34ffc1b4, interval=5s}] took [6905ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:16,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:27,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6905766542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:37,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:44,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [20409ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:28:46,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20409206380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:54,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:28:56,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [16562ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:01,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16562907176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:07,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:14,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13495022644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:22,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:26,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14613956808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:25,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [14613ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:31,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [9301ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:31,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:38,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9301760803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,693][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [37.4s/37410ms] ago, timed out [0s/0ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33126]
[2022-03-25T19:29:46,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [13122ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:29:50,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13122256963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:02,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:29:44,257][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33126] timed out after [37410ms]
[2022-03-25T19:30:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476619774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:26,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:40,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24079280312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:30:48,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [24079ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:30:54,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:01,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914627885ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:10,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:16,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16962305169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:23,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:32,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13815134368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:32,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [30777ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:31:13,916][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16963ms] which is above the warn threshold of [5s]
[2022-03-25T19:31:44,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:31:56,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20604565563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:12,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:27,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28235774198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:32:45,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:04,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32853177729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:17,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:27,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31978845533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:31,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [31978ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:33:41,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:33:55,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24117731252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:09,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:27,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27865933874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:34:53,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:15,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38798970596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:34,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:52,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46106475305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:54,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [46106ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:36:10,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:30,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34062310888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:48,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:35:47,369][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33182] timed out after [117186ms]
[2022-03-25T19:37:01,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39370681162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:37:24,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32318ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:37:44,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32318096264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:36:50,724][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [39371ms] which is above the warn threshold of [5s]
[2022-03-25T19:38:18,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:38:40,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50525538257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:21,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:17,079][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.5s/20526ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93]], which exceeds the warn threshold of [10s]
[2022-03-25T19:40:10,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67529145259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:52,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [67529ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:40:52,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:39:26,718][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:37180}] took [150373ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:41:33,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93589702063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:41:59,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:16,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67349234040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:31,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:42:52,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32204612075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:07,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:28,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35139925862ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:43:51,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:44:03,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@4c5e33d1] took [44827ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:44:17,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44827664644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:44:41,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:02,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48050334855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:23,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:33,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958836384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:47,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:45:57,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [28463ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:46:01,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28463489349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:19,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:26,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [32147ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:46:34,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32147720356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:46:54,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:10,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35438493693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:15,634][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [15.1m/909933ms] ago, timed out [13.2m/792747ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33182]
[2022-03-25T19:47:24,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:43,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28152052937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:47:58,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:04,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [35278ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:48:08,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35278367777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:25,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:29,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [27422ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:48:41,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27422457062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:55,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:49:11,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30219065515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:48:28,342][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33271] timed out after [183059ms]
[2022-03-25T19:49:20,704][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10986ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0]], which exceeds the warn threshold of [10s]
[2022-03-25T19:49:33,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:09,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31659155326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:39,713][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.4s/17407ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@95bd4ef1]], which exceeds the warn threshold of [10s]
[2022-03-25T19:50:41,197][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40706}] took [89301ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:50:45,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:50:55,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77603991973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:11,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:26,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26030020562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:33,339][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40684}] took [103634ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:51:35,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [26030ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:51:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:51:54,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30957882736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:56:12,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:56:43,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270571251994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:01,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:13,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48609265786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:26,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:27,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@59cd0a64] took [25428ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:57:39,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25428147920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:57:58,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:01,978][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.1m/847822ms] ago, timed out [11m/664763ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33271]
[2022-03-25T19:58:05,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831762065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:16,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4613][103] duration [3.6m], collections [1]/[6.6m], total [3.6m]/[5.8m], memory [254.1mb]->[187.6mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [8mb]->[5.4mb]/[0b]}
[2022-03-25T19:58:19,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:22,963][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4613] overhead, spent [3.6m] collecting in the last [6.6m]
[2022-03-25T19:58:25,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19941912137ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:25,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [52773ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:58:39,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:58:47,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20594361043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:08,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:33,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29109027589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:47,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [37771ms] which is above the warn threshold of [5000ms]
[2022-03-25T19:59:47,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T19:59:54,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37771644334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:08,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:34,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21711780943ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:00:53,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:11,820][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:40688}] took [45241ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:01:11,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45240699162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:15,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [45240ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:01:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:47,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37797219032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:02:07,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:02:29,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28051543340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:03:10,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:01:52,809][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33333] timed out after [140248ms]
[2022-03-25T20:02:13,914][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28052ms] which is above the warn threshold of [5s]
[2022-03-25T20:03:36,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66575536362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:04:06,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.6s/58630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:05:42,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58244757285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:05:59,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:06:16,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114668192682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:06:30,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:07:10,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31828887822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:07:11,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [31828ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:07:37,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:02,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66489628213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:08:44,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45264467266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:08,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:16,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017f0258@61661069] took [45422ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:09:28,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45422658334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:09:53,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:20,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.6s/44639698135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:35,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:11:00,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41472507228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:11:32,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58s/58021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:10:30,626][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44639ms] which is above the warn threshold of [5s]
[2022-03-25T20:11:56,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.2s/58203900545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:12:16,907][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14m/845860ms] ago, timed out [11.7m/705612ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33333]
[2022-03-25T20:12:24,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [58203ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:12:25,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:13:33,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53292884968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:14:33,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:14:55,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127417383810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:15:14,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:15:35,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41070333827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:02,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:28,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47649460881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:16:47,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45844ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:17:30,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45844050512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:01,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:57,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73293848606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:19:11,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:19:29,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69922473716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:18:49,925][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33409] timed out after [366096ms]
[2022-03-25T20:19:47,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:09,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36110978754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:42,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:20:42,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [50945ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:21:18,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50945347426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:21:47,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:11,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69820139104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:31,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:23:01,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43113470025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:22:14,310][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [69820ms] which is above the warn threshold of [5s]
[2022-03-25T20:23:41,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:23:59,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70847132487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:21,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:35,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39639395822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:47,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:24:55,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [25862ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:24:59,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25862115846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:25:13,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:25:16,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [26521ms] which is above the warn threshold of [5000ms]
[2022-03-25T20:25:35,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26521059095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:01,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:27,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36533791135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:39,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:26:47,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48377322444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:53:14,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4m/1584266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:55:40,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4m/1584266404672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T20:56:39,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4619][104] duration [22.1m], collections [1]/[26.7m], total [22.1m]/[27.9m], memory [243.6mb]->[175.6mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [170.1mb]->[170.1mb]/[2gb]}{[survivor] [5.4mb]->[5.4mb]/[0b]}
[2022-03-25T20:58:00,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T20:59:45,529][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4619] overhead, spent [22.1m] collecting in the last [26.7m]
[2022-03-25T21:00:33,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296750798431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:02:37,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [1881017ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:02:53,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:05:25,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292145004438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:08:07,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:10:50,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313933965622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:12:24,446][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [58.9m/3537673ms] ago, timed out [52.8m/3171577ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{NoJb1TyVRMe45HgPVdtbzw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [33409]
[2022-03-25T21:14:02,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:16:46,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307272735975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:19:02,329][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.8s/21840ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93]], which exceeds the warn threshold of [10s]
[2022-03-25T21:19:23,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:21:45,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/357627262708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:16:21,329][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [307273ms] which is above the warn threshold of [5s]
[2022-03-25T21:22:24,440][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.8s/32852ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T21:24:13,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:26:58,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301544254369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:28:09,749][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.5s/10582ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T21:30:51,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:33:27,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392548510415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:26:40,608][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [33485] timed out after [2572007ms]
[2022-03-25T21:36:13,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:38:40,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313791736947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:41:42,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:44:43,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342577554963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:46:50,278][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.5m/694093ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [12.3m/741466ms]
[2022-03-25T21:48:13,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:50:27,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@31fe4723, interval=1s}] took [391411ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:50:32,899][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [29m/1741873ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [29.1m/1750738ms]
[2022-03-25T21:50:57,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/391411321665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T21:54:11,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2db05f0e, interval=5s}] took [344322ms] which is above the warn threshold of [5000ms]
[2022-03-25T21:54:09,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:57:38,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344322819398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:00:41,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T21:57:29,252][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/111228ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@11387f93], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@95bd4ef1], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1954a2b0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce78624a]], which exceeds the warn threshold of [10s]
[2022-03-25T22:04:38,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/401884462127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:10:55,140][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T22:10:55,208][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T22:10:55,211][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T22:11:04,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T22:11:04,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T22:11:04,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T22:11:04,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T22:11:04,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T22:11:04,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T22:11:04,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T22:11:04,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T22:11:04,422][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T22:11:04,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T22:11:04,424][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T22:11:04,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T22:11:04,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T22:11:04,430][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T22:11:04,432][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T22:11:04,434][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T22:11:04,436][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T22:11:04,439][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T22:11:04,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T22:11:04,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T22:11:04,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T22:11:04,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T22:11:04,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T22:11:04,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T22:11:04,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T22:11:04,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T22:11:04,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T22:11:04,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T22:11:04,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T22:11:04,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T22:11:04,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T22:11:04,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T22:11:04,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T22:11:04,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T22:11:04,475][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T22:11:04,477][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T22:11:04,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T22:11:04,483][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T22:11:04,485][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T22:11:04,486][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T22:11:04,487][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T22:11:04,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T22:11:04,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T22:11:04,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T22:11:04,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T22:11:04,494][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T22:11:04,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T22:11:04,495][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T22:11:04,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T22:11:04,497][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T22:11:04,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T22:11:04,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T22:11:04,501][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T22:11:04,607][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.8gb], net total_space [125.8gb], types [ext4]
[2022-03-25T22:11:04,609][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T22:11:05,064][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T22:11:19,671][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T22:11:19,679][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T22:11:19,682][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,683][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-25T22:11:19,687][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-25T22:11:19,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-25T22:11:19,689][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T22:11:19,690][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T22:11:19,692][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T22:11:19,693][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T22:11:21,050][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T22:11:21,255][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T22:11:23,329][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T22:11:25,875][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T22:11:25,889][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T22:11:26,092][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T22:11:26,101][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T22:11:26,715][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T22:11:33,330][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T22:11:33,614][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 118, version: 3348, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T22:11:34,113][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 118, version: 3348, reason: Publication{term=118, version=3348}
[2022-03-25T22:11:36,888][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T22:11:36,932][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T22:11:40,913][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T22:12:43,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:12:44,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7509ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:14:21,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21456633825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:14:39,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:14:52,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140076244527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:15:14,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:00,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34176985191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:15,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [174253ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:16:24,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:35,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75259746665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:41,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:46,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16787969799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:52,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:54,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12022878658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:16:58,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:02,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6424749114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:05,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:09,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7433347976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:11,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:12,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204232007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:17:12,442][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3349}] took [5.4m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3349}]] took [0ms], [connecting to new nodes] took [20ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [50ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [188ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [1655ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [9ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [0ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [20ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [83ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [18ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [3ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [9ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [34ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [4ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [4ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@21a3c20] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [3ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@2e661a53] took [9ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [0ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [11ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [0ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [2ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [1720ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@38d52050] took [329836ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [42ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [42ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:17:13,046][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T22:17:12,931][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39714}] took [33053ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:17:15,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@51c28ffa] took [52450ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:17:19,748][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-25T22:17:19,746][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-25T22:17:20,500][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-25T22:17:21,154][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,317][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-25T22:17:21,509][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/config%3A7.17.0, params: {index=.kibana_7.17.0, id=config:7.17.0}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][config:7.17.0]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,666][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/space%3Adefault, params: {index=.kibana_7.17.0, id=space:default}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][space:default]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-03-25T22:17:21,757][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T22:17:21,857][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T22:19:27,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@634595fc] took [96554ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:19:57,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8017ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:20:23,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [11599ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:20:37,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:20:23,390][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9999ms] which is above the warn threshold of [5s]
[2022-03-25T22:20:40,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8867931415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:20:47,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:00,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862549429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:05,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [18521ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:21:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18521072164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:06,212][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [195505ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [25] unchanged indices
[2022-03-25T22:21:21,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:25,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15257943654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:29,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:34,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8121776198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:38,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:40,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [9148ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:21:32,919][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.1m] publication of cluster state version [3352] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-25T22:21:43,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9148234875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:50,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:21:53,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70a8e64b, interval=5s}] took [11351ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:22:00,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11351952328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:15,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:28,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23617793568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:38,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:44,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23613384484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:22:29,720][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [56] timed out after [44924ms]
[2022-03-25T22:22:53,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:10,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15476690326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:18,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:18,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [63516ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:25,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24426559439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:30,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:34,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13090294246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:36,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [13090ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:40,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:45,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8938395580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:51,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67d1dedd, interval=1m}] took [12441ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:23:51,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:54,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12441410473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:23:59,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:00,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [7896ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:24:04,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7896552751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:10,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:14,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [10791ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:24:20,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10791911221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:28,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:32,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18246965640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:37,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:42,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9256499383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:46,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:50,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9410464457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:54,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7014ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:24:57,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7013776830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:01,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:05,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517369129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:13,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:19,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8271920785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:22,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:28,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13024824539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:34,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@51a65b55] took [83125ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:33,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:38,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10383722736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:42,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:42,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [9138ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:46,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9138746833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:52,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:55,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9447642534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:25:56,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [9447ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:25:58,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:01,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6514831719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:06,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:09,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8568826690ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:26:05,997][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [60] timed out after [89980ms]
[2022-03-25T22:26:10,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [12599ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:09,382][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [61] timed out after [35485ms]
[2022-03-25T22:26:21,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6054ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:21,783][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6255ms] which is above the warn threshold of [5s]
[2022-03-25T22:26:41,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8818ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:26:50,015][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3352}] took [4.9m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3352}]] took [340ms], [connecting to new nodes] took [1879ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [332ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [131205ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [0ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [806ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [290ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [1ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [271ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [68ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [339ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [153ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [1711ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [12954ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [1210ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [272ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [333ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [82195ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [133ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [10720ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [576ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [123ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [7784ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [389ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [12567ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [127ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [13073ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [5001ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [753ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [130ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [66ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [0ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:26:55,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7939ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:27:20,048][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [17.5s/17552ms] to notify listeners on successful publication of cluster state (version: 3352, uuid: Z61V7ePbSGKdeoXbXCUUWw) for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [89], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [89], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [31], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [31], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-25T22:27:26,484][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10m/601335ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [10m/601773ms]
[2022-03-25T22:27:43,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:27:36,434][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3m/184436ms] ago, timed out [1.5m/94456ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [60]
[2022-03-25T22:27:44,298][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.1m/490220ms] ago, timed out [7.4m/445296ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [56]
[2022-03-25T22:27:44,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6392053009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:27:15,189][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketTimeoutException: null
	at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:110) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-25T22:27:44,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@562ad2d6] took [43340ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:27:48,415][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.4m/144721ms] ago, timed out [1.8m/109236ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [61]
[2022-03-25T22:27:58,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6001ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:07,047][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65] timed out after [21902ms]
[2022-03-25T22:28:14,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6831ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:40,761][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5606ms] which is above the warn threshold of [5s]
[2022-03-25T22:28:42,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:28:50,433][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:39714}] took [611367ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:38,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@20891f77] took [47907ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:52,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [7827ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:29:58,768][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [71] timed out after [64493ms]
[2022-03-25T22:30:00,571][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [113565ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [25] unchanged indices
[2022-03-25T22:30:00,849][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [72] timed out after [21642ms]
[2022-03-25T22:30:07,107][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.1m] publication of cluster state version [3353] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-25T22:30:32,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6743ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:30:55,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@33fc8944] took [15168ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:30:56,658][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/191775ms] ago, timed out [2.8m/169873ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65]
[2022-03-25T22:31:04,235][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [80] timed out after [22346ms]
[2022-03-25T22:31:06,257][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/86581ms] ago, timed out [1m/64939ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [72]
[2022-03-25T22:31:06,257][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.1m/131686ms] ago, timed out [1.1m/67193ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [71]
[2022-03-25T22:31:13,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5955ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:21,903][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [39.6s/39676ms] ago, timed out [17.3s/17330ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [80]
[2022-03-25T22:31:21,640][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [81] timed out after [24172ms]
[2022-03-25T22:31:25,660][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [29.4s/29461ms] ago, timed out [5.2s/5289ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hn_tk-8YR76At9qVG4QLDQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [81]
[2022-03-25T22:31:27,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6446ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:39,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6985ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:51,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5651ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:32:01,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [6302ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:31:59,077][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=118, version=3353}] took [1.7m] which is above the warn threshold of [30s]: [running task [Publication{term=118, version=3353}]] took [53ms], [connecting to new nodes] took [525ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@2eb370d3] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68ed7d63] took [7816ms], [org.elasticsearch.script.ScriptService@63780af2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [0ms], [org.elasticsearch.snapshots.RestoreService@36d181a8] took [0ms], [org.elasticsearch.ingest.IngestService@3554f474] took [316ms], [org.elasticsearch.action.ingest.IngestActionForwarder@16d847b6] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c7740@1f67e780] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2581e905] took [155ms], [org.elasticsearch.tasks.TaskManager@37e45e95] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63554a28] took [101ms], [org.elasticsearch.cluster.InternalClusterInfoService@5253de77] took [51ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@2237153f] took [138ms], [org.elasticsearch.indices.SystemIndexManager@556dee6b] took [2085ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6fe2adde] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@590dab89] took [494ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4ddf4264] took [4ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3dbbad43] took [342ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bac08@133960ee] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@55e93bd6] took [232ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@2f1054aa] took [36644ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3c797e5a] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6b4217f9] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1c3e2f65] took [9765ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@4ed1e08c] took [791ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@5dec0904] took [398ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@26ea309f] took [10487ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3d14ce9] took [486ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4b427620] took [12232ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@ad63a44] took [118ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@18fd8bd1] took [249ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48490162] took [8752ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2e244ce4] took [3795ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@7d4ed883] took [311ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@6bc16a7f] took [2054ms], [org.elasticsearch.node.ResponseCollectorService@678e588f] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@792472af] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@722527c8] took [56ms], [org.elasticsearch.shutdown.PluginShutdownService@12572961] took [97ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@386bc7e3] took [48ms], [org.elasticsearch.indices.store.IndicesStore@7ef845be] took [341ms], [org.elasticsearch.persistent.PersistentTasksNodeService@44756f53] took [0ms], [org.elasticsearch.license.LicenseService@5e67b0a0] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@43c4741] took [48ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15570bdd] took [0ms], [org.elasticsearch.gateway.GatewayService@46ebf691] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@32edfc67] took [0ms]
[2022-03-25T22:33:47,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@4e214b72] took [99676ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:01,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [5032ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:20,143][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2m/120578ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-25T22:34:29,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [8639ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:34:57,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [17408ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:36:54,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:38:02,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5165797971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:37:49,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [85051ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:39:24,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:39:52,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/159957984247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:40:33,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:41:36,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61589604827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:43:27,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:45:55,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173885158101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:47:00,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:47:43,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/218584186744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:49:11,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:50:46,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127079825867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:52:11,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/186960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:52:58,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/186959830872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:53:40,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:55:22,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89362844379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:56:48,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:57:35,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187493783819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:58:29,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T22:58:53,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103723539477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T22:59:27,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017df2f0@3dca0ae] took [1148678ms] which is above the warn threshold of [5000ms]
[2022-03-25T22:59:38,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:00:25,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69620749208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:01:29,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:02:06,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109267493825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:02:39,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:03:10,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70001213370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:03:48,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:04:24,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70670960726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:03,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:40,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@943c51f, interval=1s}] took [74295ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:05:30,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74295620605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:06:24,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:07:02,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76597084010ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:05:35,870][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [74296ms] which is above the warn threshold of [5s]
[2022-03-25T23:07:32,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:08:04,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72619469411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:08:34,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:09:11,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61628805599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:10:18,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:10:31,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@48ef8c28, interval=5s}] took [102596ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:10:59,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102596780343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:11:30,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:11:32,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@625fb4bb, interval=30s}] took [73040ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:12:00,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73040553218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:12:35,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:13:15,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65278156830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:13:58,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:14:48,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81538025314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:16:52,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:18:42,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173571902567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:21:19,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:23:50,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263627140079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:22:41,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43280}] took [263627ms] which is above the warn threshold of [5000ms]
[2022-03-25T23:26:11,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:28:25,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292323478499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:33:19,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:38:41,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403969572235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:42:03,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:44:36,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540056550106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:47:22,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:50:39,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324771832285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:53:28,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T23:53:57,176][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [4696785ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [23] unchanged indices
[2022-03-25T23:56:15,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361734784179ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T23:59:31,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:42:10,467][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T00:42:10,494][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T00:42:10,497][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T00:42:17,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T00:42:17,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T00:42:17,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T00:42:17,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T00:42:17,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T00:42:17,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T00:42:17,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T00:42:17,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T00:42:17,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T00:42:17,095][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T00:42:17,096][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T00:42:17,096][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T00:42:17,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T00:42:17,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T00:42:17,101][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T00:42:17,102][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T00:42:17,103][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T00:42:17,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T00:42:17,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T00:42:17,107][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T00:42:17,108][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T00:42:17,109][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T00:42:17,110][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T00:42:17,111][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T00:42:17,111][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T00:42:17,112][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T00:42:17,113][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T00:42:17,114][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T00:42:17,116][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T00:42:17,117][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T00:42:17,117][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T00:42:17,118][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T00:42:17,120][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T00:42:17,121][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T00:42:17,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T00:42:17,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T00:42:17,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T00:42:17,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T00:42:17,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T00:42:17,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T00:42:17,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T00:42:17,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T00:42:17,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T00:42:17,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T00:42:17,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T00:42:17,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T00:42:17,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T00:42:17,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T00:42:17,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T00:42:17,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T00:42:17,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T00:42:17,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T00:42:17,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T00:42:17,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T00:42:17,142][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T00:42:17,142][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T00:42:17,143][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T00:42:17,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T00:42:17,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T00:42:17,252][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.9gb], net total_space [125.8gb], types [ext4]
[2022-03-26T00:42:17,253][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T00:42:17,685][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T00:42:37,840][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T00:42:37,848][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T00:42:42,120][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T00:42:42,381][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T00:43:44,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [6330ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:44:31,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:45:33,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15354950944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:45:49,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:46:04,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92927613443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:46:24,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:46:38,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@59f8a4ec, interval=1m}] took [34760ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:46:52,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34760410006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:22,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:29,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [56960ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:47:41,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56960130833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:53,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:47:56,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6cbb4d8b, interval=30s}] took [33673ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:48:04,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33673548894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:48:18,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:48:22,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [24793ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:48:39,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24793906046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:48:58,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:49:17,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36426514201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:49:45,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:07,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38291216981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:19,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:19,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [45301ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:50:28,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45301823103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:32,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:50:33,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15007691939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:53:10,315][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T00:53:11,677][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T00:53:11,685][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T00:53:11,778][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T00:53:11,781][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T00:53:12,161][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T00:53:15,507][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T00:53:15,631][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 119, version: 3355, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T00:53:15,850][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 119, version: 3355, reason: Publication{term=119, version=3355}
[2022-03-26T00:53:15,981][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T00:53:15,981][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T00:53:17,286][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T00:53:17,300][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-26T00:53:19,335][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T00:53:19,337][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T00:53:20,885][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-03-26T00:53:22,599][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T00:53:22,615][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T00:53:22,618][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T00:53:26,495][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T00:53:27,967][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T00:53:53,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@73889450] took [7515ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:53:53,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][26][15] duration [2.3s], collections [1]/[8.2s], total [2.3s]/[3.4s], memory [306.4mb]->[134.4mb]/[2gb], all_pools {[young] [184mb]->[4mb]/[0b]}{[old] [111.8mb]->[111.8mb]/[2gb]}{[survivor] [10.5mb]->[18.5mb]/[0b]}
[2022-03-26T00:53:54,250][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][26] overhead, spent [2.3s] collecting in the last [8.2s]
[2022-03-26T00:53:54,701][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48588}] took [21860ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:12,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][34][16] duration [2.2s], collections [1]/[1.5s], total [2.2s]/[5.6s], memory [186.4mb]->[206.4mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [111.8mb]->[126.1mb]/[2gb]}{[survivor] [18.5mb]->[5.2mb]/[0b]}
[2022-03-26T00:54:12,969][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][34] overhead, spent [2.2s] collecting in the last [1.5s]
[2022-03-26T00:54:25,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5324ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:54:25,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5323783973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:54:25,773][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][40][17] duration [2.1s], collections [1]/[6.3s], total [2.1s]/[7.7s], memory [207.3mb]->[134.7mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [126.1mb]->[126.1mb]/[2gb]}{[survivor] [5.2mb]->[8.5mb]/[0b]}
[2022-03-26T00:54:19,140][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43802}] took [6924ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:25,836][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][40] overhead, spent [2.1s] collecting in the last [6.3s]
[2022-03-26T00:54:35,090][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43794}] took [9689ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:35,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@5f784320] took [5416ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:36,142][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][44][18] duration [2.9s], collections [1]/[5.9s], total [2.9s]/[10.7s], memory [194.7mb]->[169mb]/[2gb], all_pools {[young] [60mb]->[48mb]/[0b]}{[old] [126.1mb]->[126.1mb]/[2gb]}{[survivor] [8.5mb]->[14.8mb]/[0b]}
[2022-03-26T00:54:36,548][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][44] overhead, spent [2.9s] collecting in the last [5.9s]
[2022-03-26T00:54:38,307][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T00:54:44,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5465ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:54:46,018][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10765ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [26] unchanged indices
[2022-03-26T00:54:49,473][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][48][19] duration [841ms], collections [1]/[3.3s], total [841ms]/[11.5s], memory [221mb]->[148.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [126.1mb]->[132.5mb]/[2gb]}{[survivor] [14.8mb]->[16mb]/[0b]}
[2022-03-26T00:54:49,416][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.3s] publication of cluster state version [3376] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T00:54:55,197][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][50][20] duration [1.3s], collections [1]/[1.1s], total [1.3s]/[12.9s], memory [212.5mb]->[228.5mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [132.5mb]->[147.9mb]/[2gb]}{[survivor] [16mb]->[2.8mb]/[0b]}
[2022-03-26T00:54:55,556][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][50] overhead, spent [1.3s] collecting in the last [1.1s]
[2022-03-26T00:54:58,687][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [564ms] collecting in the last [1.4s]
[2022-03-26T00:55:05,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][55][22] duration [1.7s], collections [1]/[3.2s], total [1.7s]/[15.2s], memory [227.5mb]->[153.1mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [147.9mb]->[147.9mb]/[2gb]}{[survivor] [3.5mb]->[5.1mb]/[0b]}
[2022-03-26T00:55:06,072][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][55] overhead, spent [1.7s] collecting in the last [3.2s]
[2022-03-26T00:55:18,569][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10181ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [27] unchanged indices
[2022-03-26T00:55:23,204][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.5s] publication of cluster state version [3379] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T00:55:34,122][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T00:55:34,143][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][64][23] duration [3.6s], collections [1]/[5.7s], total [3.6s]/[18.9s], memory [233.1mb]->[156.4mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [147.9mb]->[147.9mb]/[2gb]}{[survivor] [5.1mb]->[8.4mb]/[0b]}
[2022-03-26T00:55:34,670][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][64] overhead, spent [3.6s] collecting in the last [5.7s]
[2022-03-26T00:55:38,380][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-03-26T00:55:43,677][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-03-26T00:55:50,607][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40226}] took [27130ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:56:12,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:56:13,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7575783842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:56:13,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [7575ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:56:15,088][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][78][24] duration [4s], collections [1]/[12.8s], total [4s]/[23s], memory [232.4mb]->[162.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [147.9mb]->[149.2mb]/[2gb]}{[survivor] [8.4mb]->[13.4mb]/[0b]}
[2022-03-26T00:56:15,556][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][78] overhead, spent [4s] collecting in the last [12.8s]
[2022-03-26T00:56:17,692][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [35839ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [24] unchanged indices
[2022-03-26T00:56:19,879][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [40.1s] publication of cluster state version [3380] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T00:56:32,732][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T00:56:33,342][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T00:56:34,793][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.26] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-26T00:56:36,924][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [364ms] collecting in the last [1.1s]
[2022-03-26T00:56:46,017][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][26] duration [3.8s], collections [1]/[5.1s], total [3.8s]/[27.2s], memory [231.7mb]->[170.3mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [155.7mb]->[160.4mb]/[2gb]}{[survivor] [12mb]->[9.8mb]/[0b]}
[2022-03-26T00:56:46,367][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [3.8s] collecting in the last [5.1s]
[2022-03-26T00:56:46,426][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-26T00:56:46,659][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.26][0]]]).
[2022-03-26T00:56:50,883][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [400ms] collecting in the last [1.1s]
[2022-03-26T00:56:53,563][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:56:55,112][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:56:59,515][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][102][31] duration [844ms], collections [1]/[1.3s], total [844ms]/[29.1s], memory [225.8mb]->[257.8mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [8mb]->[6.8mb]/[0b]}
[2022-03-26T00:56:59,593][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][102] overhead, spent [844ms] collecting in the last [1.3s]
[2022-03-26T00:56:59,587][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:00,034][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:12,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:12,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7085ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:57:12,654][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:12,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6084493165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:12,861][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.3s/11349ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc, _doc]], which exceeds the warn threshold of [10s]
[2022-03-26T00:57:13,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][107][32] duration [4.3s], collections [1]/[8.4s], total [4.3s]/[33.5s], memory [252.6mb]->[215.7mb]/[2gb], all_pools {[young] [84mb]->[68mb]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [6.8mb]->[5.9mb]/[0b]}
[2022-03-26T00:57:13,760][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [4.3s] collecting in the last [8.4s]
[2022-03-26T00:57:22,440][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][108][33] duration [2.3s], collections [1]/[1.3s], total [2.3s]/[35.9s], memory [215.7mb]->[251.7mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [5.9mb]->[5.6mb]/[0b]}
[2022-03-26T00:57:22,733][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][108] overhead, spent [2.3s] collecting in the last [1.3s]
[2022-03-26T00:57:22,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7964ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:57:28,095][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][109][34] duration [2.1s], collections [1]/[11.6s], total [2.1s]/[38s], memory [251.7mb]->[167.9mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [5.6mb]->[6.1mb]/[0b]}
[2022-03-26T00:57:29,581][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T00:57:32,747][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-03-26T00:57:33,361][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-03-26T00:57:34,238][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:57:50,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:51,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8597852961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:57:54,302][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][117][35] duration [6.8s], collections [1]/[10s], total [6.8s]/[44.8s], memory [235.9mb]->[168.5mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [6.1mb]->[6.7mb]/[0b]}
[2022-03-26T00:57:55,972][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [6.8s] collecting in the last [10s]
[2022-03-26T00:57:56,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [14400ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:04,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6020ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:33,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7259ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:32,106][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3391}] took [34.6s] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3391}]] took [29ms], [connecting to new nodes] took [58ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [659ms], [org.elasticsearch.script.ScriptService@287d7733] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [1079ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [83ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [15640ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [434ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [151ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [1800ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [115ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [8390ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [4ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [3669ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [2047ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [138ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [64ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [0ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [62ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [0ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T00:58:54,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [19517ms] which is above the warn threshold of [5000ms]
[2022-03-26T00:58:54,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T00:58:54,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10336637479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T00:58:57,140][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][121][36] duration [6.8s], collections [1]/[29.1s], total [6.8s]/[51.7s], memory [224.5mb]->[193.6mb]/[2gb], all_pools {[young] [56mb]->[20mb]/[0b]}{[old] [161.8mb]->[161.8mb]/[2gb]}{[survivor] [6.7mb]->[11.7mb]/[0b]}
[2022-03-26T00:59:05,299][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][124][37] duration [2.1s], collections [1]/[3.5s], total [2.1s]/[53.8s], memory [249.6mb]->[192.1mb]/[2gb], all_pools {[young] [76mb]->[40mb]/[0b]}{[old] [161.8mb]->[167.1mb]/[2gb]}{[survivor] [11.7mb]->[5mb]/[0b]}
[2022-03-26T00:59:08,291][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][124] overhead, spent [2.1s] collecting in the last [3.5s]
[2022-03-26T00:59:15,693][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][38] duration [2.2s], collections [1]/[4.5s], total [2.2s]/[56s], memory [256.1mb]->[169.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [5mb]->[2.3mb]/[0b]}
[2022-03-26T00:59:15,901][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [2.2s] collecting in the last [4.5s]
[2022-03-26T00:59:15,911][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:20,402][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][129][39] duration [1.3s], collections [1]/[1.3s], total [1.3s]/[57.4s], memory [201.4mb]->[261.4mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [2.3mb]->[3mb]/[0b]}
[2022-03-26T00:59:20,663][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][129] overhead, spent [1.3s] collecting in the last [1.3s]
[2022-03-26T00:59:21,397][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:21,547][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:28,784][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:28,628][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][134][40] duration [993ms], collections [1]/[2.1s], total [993ms]/[58.4s], memory [254.1mb]->[171.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [3mb]->[4mb]/[0b]}
[2022-03-26T00:59:29,036][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][134] overhead, spent [993ms] collecting in the last [2.1s]
[2022-03-26T00:59:34,136][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T00:59:34,523][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T00:59:41,352][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][141][41] duration [1s], collections [1]/[1.3s], total [1s]/[59.4s], memory [251.1mb]->[259.1mb]/[2gb], all_pools {[young] [80mb]->[92mb]/[0b]}{[old] [167.1mb]->[167.1mb]/[2gb]}{[survivor] [4mb]->[4mb]/[0b]}
[2022-03-26T00:59:41,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][141] overhead, spent [1s] collecting in the last [1.3s]
[2022-03-26T00:59:43,975][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T00:59:48,891][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-03-26T00:59:53,370][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T00:59:59,248][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T00:59:59,739][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T01:00:03,888][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][42] duration [2.2s], collections [1]/[1s], total [2.2s]/[1m], memory [247.4mb]->[174.1mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [167.1mb]->[167.8mb]/[2gb]}{[survivor] [8.3mb]->[6.2mb]/[0b]}
[2022-03-26T01:00:04,216][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [2.2s] collecting in the last [1s]
[2022-03-26T01:00:07,108][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:18,330][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:18,821][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][163][43] duration [2.3s], collections [1]/[4.1s], total [2.3s]/[1m], memory [242.1mb]->[190.6mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [167.8mb]->[167.8mb]/[2gb]}{[survivor] [6.2mb]->[6.7mb]/[0b]}
[2022-03-26T01:00:18,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][163] overhead, spent [2.3s] collecting in the last [4.1s]
[2022-03-26T01:00:20,330][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [531ms] collecting in the last [1.5s]
[2022-03-26T01:00:30,437][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:40,520][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:00:48,770][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][184][49] duration [1.5s], collections [1]/[3s], total [1.5s]/[1.1m], memory [198.7mb]->[177.2mb]/[2gb], all_pools {[young] [28mb]->[32mb]/[0b]}{[old] [167.8mb]->[167.8mb]/[2gb]}{[survivor] [6.8mb]->[9.3mb]/[0b]}
[2022-03-26T01:00:49,242][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][184] overhead, spent [1.5s] collecting in the last [3s]
[2022-03-26T01:00:54,364][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [588ms] collecting in the last [1.2s]
[2022-03-26T01:00:58,488][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][190] overhead, spent [380ms] collecting in the last [1.2s]
[2022-03-26T01:01:01,764][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][192] overhead, spent [611ms] collecting in the last [1.4s]
[2022-03-26T01:01:02,506][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:01:05,289][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][193][53] duration [1.1s], collections [1]/[2.9s], total [1.1s]/[1.1m], memory [183.5mb]->[180.1mb]/[2gb], all_pools {[young] [28mb]->[28mb]/[0b]}{[old] [168.6mb]->[171.1mb]/[2gb]}{[survivor] [10.8mb]->[8.9mb]/[0b]}
[2022-03-26T01:01:05,639][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][193] overhead, spent [1.1s] collecting in the last [2.9s]
[2022-03-26T01:01:10,655][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][54] duration [1.7s], collections [1]/[3.3s], total [1.7s]/[1.1m], memory [252.1mb]->[178.5mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [8.9mb]->[7.3mb]/[0b]}
[2022-03-26T01:01:11,199][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [1.7s] collecting in the last [3.3s]
[2022-03-26T01:01:18,070][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][55] duration [3.3s], collections [1]/[2.2s], total [3.3s]/[1.2m], memory [178.5mb]->[266.5mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-26T01:01:18,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [3.3s] collecting in the last [2.2s]
[2022-03-26T01:01:18,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6474ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:01:27,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:01:43,981][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.3s/20397ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc]], which exceeds the warn threshold of [10s]
[2022-03-26T01:01:43,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:01:44,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13008858728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:01:44,023][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][56] duration [10.3s], collections [1]/[1.2s], total [10.3s]/[1.4m], memory [263.1mb]->[263.1mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [8mb]->[5.4mb]/[0b]}
[2022-03-26T01:01:44,133][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [10.3s] collecting in the last [1.2s]
[2022-03-26T01:01:44,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [15361ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:02:03,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:02:03,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3e3bb723, interval=1m}] took [6030ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:02:04,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6030500813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:02:05,342][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][57] duration [4.7s], collections [1]/[7.7s], total [4.7s]/[1.4m], memory [244.6mb]->[198.5mb]/[2gb], all_pools {[young] [72mb]->[24mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [5.4mb]->[7.3mb]/[0b]}
[2022-03-26T01:02:06,776][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [4.7s] collecting in the last [7.7s]
[2022-03-26T01:02:32,216][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][219][58] duration [2.7s], collections [1]/[3.9s], total [2.7s]/[1.5m], memory [226.5mb]->[230.5mb]/[2gb], all_pools {[young] [48mb]->[64mb]/[0b]}{[old] [171.1mb]->[171.1mb]/[2gb]}{[survivor] [7.3mb]->[7.3mb]/[0b]}
[2022-03-26T01:02:32,455][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][219] overhead, spent [2.7s] collecting in the last [3.9s]
[2022-03-26T01:02:38,704][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][222][59] duration [1.3s], collections [1]/[3.3s], total [1.3s]/[1.5m], memory [196.4mb]->[180.4mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [171.1mb]->[171.6mb]/[2gb]}{[survivor] [9.2mb]->[8.7mb]/[0b]}
[2022-03-26T01:02:38,872][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][222] overhead, spent [1.3s] collecting in the last [3.3s]
[2022-03-26T01:02:39,048][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:02:40,141][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][223] overhead, spent [410ms] collecting in the last [1.5s]
[2022-03-26T01:02:40,362][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:02:41,045][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:02:53,157][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][231][63] duration [1s], collections [1]/[1.1s], total [1s]/[1.5m], memory [235.3mb]->[267.3mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [171.6mb]->[171.6mb]/[2gb]}{[survivor] [7.6mb]->[6mb]/[0b]}
[2022-03-26T01:02:53,565][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][231] overhead, spent [1s] collecting in the last [1.1s]
[2022-03-26T01:03:01,188][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:03:23,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][242][64] duration [2.7s], collections [1]/[10.1s], total [2.7s]/[1.6m], memory [261.7mb]->[181mb]/[2gb], all_pools {[young] [84mb]->[12mb]/[0b]}{[old] [171.6mb]->[171.7mb]/[2gb]}{[survivor] [6mb]->[9.2mb]/[0b]}
[2022-03-26T01:03:23,602][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][242] overhead, spent [2.7s] collecting in the last [10.1s]
[2022-03-26T01:03:27,027][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [19137ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T01:03:28,209][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.2s] publication of cluster state version [3405] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T01:03:34,148][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][246][65] duration [3.3s], collections [1]/[6s], total [3.3s]/[1.6m], memory [257mb]->[183.5mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [171.7mb]->[172.2mb]/[2gb]}{[survivor] [9.2mb]->[7.3mb]/[0b]}
[2022-03-26T01:03:35,033][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][246] overhead, spent [3.3s] collecting in the last [6s]
[2022-03-26T01:03:44,219][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][249][66] duration [2.9s], collections [1]/[1.4s], total [2.9s]/[1.7m], memory [255.5mb]->[180.2mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-26T01:03:44,602][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][249] overhead, spent [2.9s] collecting in the last [1.4s]
[2022-03-26T01:04:22,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7341ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:04:53,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [11324ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:04:53,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:04:53,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9724037547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:04:54,156][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][262][67] duration [4.2s], collections [1]/[18s], total [4.2s]/[1.8m], memory [264.2mb]->[183mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [8mb]->[10.8mb]/[0b]}
[2022-03-26T01:05:04,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:05:05,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8502447726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:05:05,192][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][263][68] duration [6.5s], collections [1]/[1.2s], total [6.5s]/[1.9m], memory [183mb]->[259mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [172.2mb]->[174.7mb]/[2gb]}{[survivor] [10.8mb]->[7.7mb]/[0b]}
[2022-03-26T01:05:06,494][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][263] overhead, spent [6.5s] collecting in the last [1.2s]
[2022-03-26T01:05:06,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [11677ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:05:22,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6368ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:05:24,167][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1124] timed out after [16631ms]
[2022-03-26T01:05:39,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:05:57,176][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1126] timed out after [15563ms]
[2022-03-26T01:06:12,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [15268ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:06:12,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:06:13,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11464424683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:06:15,171][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][276][69] duration [7.5s], collections [1]/[17.8s], total [7.5s]/[2m], memory [266.5mb]->[191.5mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [174.7mb]->[174.7mb]/[2gb]}{[survivor] [7.7mb]->[8.7mb]/[0b]}
[2022-03-26T01:06:16,341][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][276] overhead, spent [7.5s] collecting in the last [17.8s]
[2022-03-26T01:07:01,971][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1142] timed out after [15700ms]
[2022-03-26T01:07:01,900][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1141] timed out after [16100ms]
[2022-03-26T01:07:18,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:18,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5886997341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:19,187][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40366}] took [9185ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:07:18,899][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][297][70] duration [4.8s], collections [1]/[1.6s], total [4.8s]/[2.1m], memory [263.5mb]->[267.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [174.7mb]->[176.1mb]/[2gb]}{[survivor] [8.7mb]->[5.6mb]/[0b]}
[2022-03-26T01:07:23,469][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][297] overhead, spent [4.8s] collecting in the last [1.6s]
[2022-03-26T01:07:23,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:24,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5698577093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:24,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [11785ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:07:35,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:07:56,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6774ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:08:06,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:07:58,420][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1154] timed out after [17327ms]
[2022-03-26T01:07:58,382][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1153] timed out after [20329ms]
[2022-03-26T01:08:08,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5046726208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:08:29,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6574ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:08:42,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:08:43,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9470501883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:08:46,053][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][309][71] duration [5s], collections [1]/[22.1s], total [5s]/[2.2m], memory [257.8mb]->[221.9mb]/[2gb], all_pools {[young] [80mb]->[40mb]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [5.6mb]->[5.8mb]/[0b]}
[2022-03-26T01:09:10,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:09:11,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11511195899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:09:15,008][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][314][72] duration [5.9s], collections [1]/[13.3s], total [5.9s]/[2.3m], memory [229.9mb]->[180.7mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [5.8mb]->[4.5mb]/[0b]}
[2022-03-26T01:09:17,311][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][314] overhead, spent [5.9s] collecting in the last [13.3s]
[2022-03-26T01:09:19,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [21424ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:09:21,883][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1159] timed out after [32293ms]
[2022-03-26T01:09:22,850][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.4m/268713ms] ago, timed out [4.2m/252082ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1124]
[2022-03-26T01:09:22,502][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1160] timed out after [32093ms]
[2022-03-26T01:09:38,479][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [41.2s/41251ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@19e46b8a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9e693ae8]], which exceeds the warn threshold of [10s]
[2022-03-26T01:09:42,834][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.9m/176669ms] ago, timed out [2.6m/160569ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1141]
[2022-03-26T01:09:51,058][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3m/183673ms] ago, timed out [2.7m/167973ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1142]
[2022-03-26T01:10:00,056][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.3m/141329ms] ago, timed out [2m/121000ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1153]
[2022-03-26T01:10:06,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@291f3494] took [5403ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:10:20,630][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1191] timed out after [16410ms]
[2022-03-26T01:10:25,123][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1192] timed out after [17010ms]
[2022-03-26T01:10:30,201][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168746ms] ago, timed out [2.5m/151419ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1154]
[2022-03-26T01:10:40,746][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.7m/345856ms] ago, timed out [5.5m/330293ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1126]
[2022-03-26T01:10:42,955][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.8m/113306ms] ago, timed out [1.3m/81013ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1159]
[2022-03-26T01:10:44,328][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/114707ms] ago, timed out [1.3m/82614ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1160]
[2022-03-26T01:10:50,329][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [45.8s/45830ms] ago, timed out [29.4s/29420ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1191]
[2022-03-26T01:10:57,286][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_monitoring/bulk?system_id=kibana&system_api_version=7&interval=10000ms][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43880}] took [131603ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:10:56,455][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [49s/49026ms] ago, timed out [32s/32016ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1192]
[2022-03-26T01:11:18,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:04,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7787ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:46,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:12:47,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5764ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:47,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5364411326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:12:48,545][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][365][73] duration [3.8s], collections [1]/[7s], total [3.8s]/[2.3m], memory [256.7mb]->[195mb]/[2gb], all_pools {[young] [88mb]->[12mb]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [4.5mb]->[6.8mb]/[0b]}
[2022-03-26T01:12:48,222][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40426}] took [6807ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:48,222][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40432}] took [5364ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:12:48,671][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][365] overhead, spent [3.8s] collecting in the last [7s]
[2022-03-26T01:13:12,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:12,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5938376342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:13,443][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][372][74] duration [4.7s], collections [1]/[1.5s], total [4.7s]/[2.4m], memory [243mb]->[251mb]/[2gb], all_pools {[young] [64mb]->[68mb]/[0b]}{[old] [176.1mb]->[176.1mb]/[2gb]}{[survivor] [6.8mb]->[6.8mb]/[0b]}
[2022-03-26T01:13:14,324][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][372] overhead, spent [4.7s] collecting in the last [1.5s]
[2022-03-26T01:13:14,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9215ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:13:32,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:32,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [5657ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:13:33,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5457810437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:13:33,399][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][377][75] duration [3.8s], collections [1]/[7.6s], total [3.8s]/[2.5m], memory [245.3mb]->[193.1mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [176.1mb]->[177.1mb]/[2gb]}{[survivor] [9.2mb]->[7.9mb]/[0b]}
[2022-03-26T01:13:33,474][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][377] overhead, spent [3.8s] collecting in the last [7.6s]
[2022-03-26T01:13:53,095][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][384][76] duration [2.6s], collections [1]/[1.6s], total [2.6s]/[2.5m], memory [217.1mb]->[273.1mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [177.1mb]->[177.1mb]/[2gb]}{[survivor] [7.9mb]->[9.7mb]/[0b]}
[2022-03-26T01:13:53,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][384] overhead, spent [2.6s] collecting in the last [1.6s]
[2022-03-26T01:13:55,635][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][385][77] duration [1.1s], collections [1]/[6s], total [1.1s]/[2.5m], memory [273.1mb]->[187mb]/[2gb], all_pools {[young] [0b]->[20mb]/[0b]}{[old] [177.1mb]->[178.3mb]/[2gb]}{[survivor] [9.7mb]->[8.6mb]/[0b]}
[2022-03-26T01:14:01,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][387][79] duration [1.6s], collections [1]/[1.1s], total [1.6s]/[2.6m], memory [232.2mb]->[272.2mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [178.8mb]->[180.7mb]/[2gb]}{[survivor] [9.4mb]->[7.9mb]/[0b]}
[2022-03-26T01:14:04,003][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][387] overhead, spent [1.6s] collecting in the last [1.1s]
[2022-03-26T01:14:04,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6526ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:14:11,237][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][390][80] duration [1.8s], collections [1]/[1.1s], total [1.8s]/[2.6m], memory [240.7mb]->[189.4mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [180.7mb]->[180.7mb]/[2gb]}{[survivor] [7.9mb]->[8.6mb]/[0b]}
[2022-03-26T01:14:11,402][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][390] overhead, spent [1.8s] collecting in the last [1.1s]
[2022-03-26T01:14:15,616][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][392][82] duration [834ms], collections [1]/[2.4s], total [834ms]/[2.6m], memory [201.4mb]->[187.7mb]/[2gb], all_pools {[young] [16mb]->[28mb]/[0b]}{[old] [180.7mb]->[180.7mb]/[2gb]}{[survivor] [8.6mb]->[6.9mb]/[0b]}
[2022-03-26T01:14:15,837][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][392] overhead, spent [834ms] collecting in the last [2.4s]
[2022-03-26T01:14:31,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][402][84] duration [1.5s], collections [1]/[3s], total [1.5s]/[2.7m], memory [263.5mb]->[189.8mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [180.7mb]->[180.7mb]/[2gb]}{[survivor] [6.7mb]->[9mb]/[0b]}
[2022-03-26T01:14:31,497][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][402] overhead, spent [1.5s] collecting in the last [3s]
[2022-03-26T01:14:34,438][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][403][85] duration [824ms], collections [1]/[2.9s], total [824ms]/[2.7m], memory [189.8mb]->[189.2mb]/[2gb], all_pools {[young] [16mb]->[24mb]/[0b]}{[old] [180.7mb]->[181.8mb]/[2gb]}{[survivor] [9mb]->[7.3mb]/[0b]}
[2022-03-26T01:14:34,878][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][403] overhead, spent [824ms] collecting in the last [2.9s]
[2022-03-26T01:14:45,977][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][407][86] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[2.7m], memory [221.2mb]->[211.2mb]/[2gb], all_pools {[young] [32mb]->[32mb]/[0b]}{[old] [181.8mb]->[181.8mb]/[2gb]}{[survivor] [7.3mb]->[9.3mb]/[0b]}
[2022-03-26T01:14:46,579][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][407] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-03-26T01:14:51,791][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][408][87] duration [2.7s], collections [1]/[5.8s], total [2.7s]/[2.7m], memory [211.2mb]->[191mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [181.8mb]->[182.3mb]/[2gb]}{[survivor] [9.3mb]->[8.6mb]/[0b]}
[2022-03-26T01:14:52,234][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][408] overhead, spent [2.7s] collecting in the last [5.8s]
[2022-03-26T01:14:59,164][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][410][88] duration [3.4s], collections [1]/[5.3s], total [3.4s]/[2.8m], memory [203mb]->[191.7mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [182.3mb]->[182.3mb]/[2gb]}{[survivor] [8.6mb]->[9.3mb]/[0b]}
[2022-03-26T01:14:59,441][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][410] overhead, spent [3.4s] collecting in the last [5.3s]
[2022-03-26T01:14:59,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:15:33,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [26718ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:16:10,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [9750ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:16:12,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:16:33,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6108390361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:16:48,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:00,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36543086853ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:13,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:26,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25389161832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:40,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:17:50,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@e119509] took [26389ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:17:55,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26389417383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:07,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:21,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27566198070ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:32,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:47,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24289412872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:52,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [20911ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:18:52,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:18:59,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20911442409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:04,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:04,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [12642ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:19:09,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12642710552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:16,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:25,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11576660323ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:34,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:24,058][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1550] timed out after [85410ms]
[2022-03-26T01:19:41,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17216895952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:44,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [17216ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:19:49,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:19:54,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15340862375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:00,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [8723ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:19:57,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:09,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8723930551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:13,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:16,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16426600022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:16,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [16426ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:20:17,072][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [309852ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T01:20:17,164][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.5m/154695ms] ago, timed out [1.1m/69285ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1550]
[2022-03-26T01:20:17,164][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [5.2m] publication of cluster state version [3406] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T01:20:32,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:33,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12470630477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:20:33,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@33ab4b62] took [13271ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:20:36,227][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][418][89] duration [9.1s], collections [1]/[16.8s], total [9.1s]/[3m], memory [243.7mb]->[232.8mb]/[2gb], all_pools {[young] [56mb]->[40mb]/[0b]}{[old] [182.3mb]->[184.1mb]/[2gb]}{[survivor] [9.3mb]->[8.7mb]/[0b]}
[2022-03-26T01:20:36,958][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][418] overhead, spent [9.1s] collecting in the last [16.8s]
[2022-03-26T01:20:36,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5114ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:20:43,043][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][420][90] duration [2.5s], collections [1]/[4.4s], total [2.5s]/[3m], memory [252.8mb]->[207.9mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [8.7mb]->[7.8mb]/[0b]}
[2022-03-26T01:20:43,388][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][420] overhead, spent [2.5s] collecting in the last [4.4s]
[2022-03-26T01:22:04,626][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40476}] took [5080ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:22:29,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:22:30,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [14000ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:22:30,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13400319638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:22:31,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][462][91] duration [10.2s], collections [1]/[17.3s], total [10.2s]/[3.2m], memory [275.9mb]->[195.9mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [7.8mb]->[7.8mb]/[0b]}
[2022-03-26T01:22:32,481][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][462] overhead, spent [10.2s] collecting in the last [17.3s]
[2022-03-26T01:22:43,829][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43892}] took [6813ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:22:44,770][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:22:47,215][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.6s/12677ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc]], which exceeds the warn threshold of [10s]
[2022-03-26T01:23:05,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:05,708][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][471][92] duration [9s], collections [1]/[2s], total [9s]/[3.3m], memory [251.9mb]->[255.9mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [7.8mb]->[7.3mb]/[0b]}
[2022-03-26T01:23:05,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12410377818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:05,913][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][471] overhead, spent [9s] collecting in the last [2s]
[2022-03-26T01:23:05,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [13611ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:10,746][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [20.5s] publication of cluster state version [3407] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T01:23:24,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [7796ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:24,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:25,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7596601507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:23:28,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][475][93] duration [5.5s], collections [1]/[12.7s], total [5.5s]/[3.4m], memory [267.4mb]->[197.3mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [184.1mb]->[184.1mb]/[2gb]}{[survivor] [7.3mb]->[9.2mb]/[0b]}
[2022-03-26T01:23:29,869][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][475] overhead, spent [5.5s] collecting in the last [12.7s]
[2022-03-26T01:23:27,318][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7796ms] which is above the warn threshold of [5s]
[2022-03-26T01:23:31,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7145ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:48,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7698ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:23:50,400][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40476}] took [10785ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:24:08,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@20c218a3] took [9831ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:24:35,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [11574ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:25:03,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [12494ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:24:56,126][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1793] timed out after [32919ms]
[2022-03-26T01:25:35,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [17045ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:26:15,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:22,387][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40518}] took [14717ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:26:23,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9842ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:26:30,962][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6760ms] which is above the warn threshold of [5s]
[2022-03-26T01:26:23,842][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3407}] took [2.9m] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3407}]] took [40ms], [connecting to new nodes] took [30ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [14129ms], [org.elasticsearch.script.ScriptService@287d7733] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [212ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [55ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [45ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [2755ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [209ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [276ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [64ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [63ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [71708ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [19413ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [1259ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [236ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [16295ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [2970ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [20573ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [16425ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [14010ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [497ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [2021ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [88ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [157ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [335ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [0ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [89ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [0ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T01:26:29,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5701894187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:38,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:47,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23090063911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:26:55,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:27:07,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17090242795ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:27:07,046][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40518}] took [17091ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:27:07,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@4a2ad09d] took [17090ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:27:15,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:27:24,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19227867578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:28:58,540][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40518}] took [103653ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:28:58,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103653ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:28:59,172][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][481][94] duration [1.2m], collections [1]/[2.8m], total [1.2m]/[4.7m], memory [261.3mb]->[224mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [184.1mb]->[185mb]/[2gb]}{[survivor] [9.2mb]->[6.9mb]/[0b]}
[2022-03-26T01:28:59,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103653512600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:28:59,239][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][481] overhead, spent [1.2m] collecting in the last [2.8m]
[2022-03-26T01:28:59,529][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.8m/289646ms] ago, timed out [4.2m/256727ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1793]
[2022-03-26T01:29:06,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:29:07,597][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][483][95] duration [4.4s], collections [1]/[6.3s], total [4.4s]/[4.7m], memory [276mb]->[190.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.9mb]->[5.1mb]/[0b]}
[2022-03-26T01:29:07,598][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][483] overhead, spent [4.4s] collecting in the last [6.3s]
[2022-03-26T01:29:07,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5716222756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:29:47,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][501][96] duration [1.4s], collections [1]/[1.3s], total [1.4s]/[4.8m], memory [234.1mb]->[242.1mb]/[2gb], all_pools {[young] [48mb]->[16mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [5.1mb]->[6.7mb]/[0b]}
[2022-03-26T01:29:47,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][501] overhead, spent [1.4s] collecting in the last [1.3s]
[2022-03-26T01:29:48,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:30:07,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [5215ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:24,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@27121f33] took [6156ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:34,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [6116ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:51,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [7967ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:30:43,370][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1936] timed out after [17192ms]
[2022-03-26T01:31:09,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [14511ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:31:09,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:31:09,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14311280333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:31:09,523][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3408}] took [1.3m] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3408}]] took [0ms], [connecting to new nodes] took [94ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [5601ms], [org.elasticsearch.script.ScriptService@287d7733] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [327ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [46ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [317ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [65ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [34979ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [7831ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [363ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [54ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [4904ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [10162ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [55ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [15517ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [18ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [1ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [0ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [0ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [8ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [0ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T01:31:09,590][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][509][97] duration [2.6s], collections [1]/[26.3s], total [2.6s]/[4.8m], memory [275.8mb]->[216mb]/[2gb], all_pools {[young] [84mb]->[24mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.7mb]->[6.9mb]/[0b]}
[2022-03-26T01:31:09,601][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [44.1s/44162ms] ago, timed out [26.9s/26970ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1936]
[2022-03-26T01:31:09,682][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T01:31:12,862][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][510][98] duration [1.6s], collections [1]/[3s], total [1.6s]/[4.8m], memory [216mb]->[191.3mb]/[2gb], all_pools {[young] [24mb]->[12mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.9mb]->[6.2mb]/[0b]}
[2022-03-26T01:31:13,048][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][510] overhead, spent [1.6s] collecting in the last [3s]
[2022-03-26T01:31:13,295][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] starting SLM retention snapshot cleanup task
[2022-03-26T01:31:22,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [8313ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:31:40,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9399ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:02,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:07,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11041463308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:26,144][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&_source=alert.executionStatus%2Cnamespace%2Cnamespaces%2Ctype%2Creferences%2CmigrationVersion%2CcoreMigrationVersion%2Cupdated_at%2CoriginId%2CexecutionStatus&rest_total_hits_as_int=true&size=1][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:43926}] took [32148ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:25,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@4343a70e] took [12442ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:26,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:35,199][INFO ][o.e.x.s.SnapshotRetentionTask] [tpotcluster-node-01] there are no repositories to fetch, SLM retention snapshot cleanup task complete
[2022-03-26T01:32:37,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23834216504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:41,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:46,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15448762977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:50,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:32:56,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [8645ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:32:54,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8645133203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:00,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:06,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [10003ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:33:08,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10003086411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:14,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:15,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [13681ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:33:21,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13681731824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:28,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:35,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13016546600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:26,477][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1971] timed out after [71613ms]
[2022-03-26T01:33:44,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:47,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [30223ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:33:52,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17207009626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:58,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:33:59,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [14019ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:34:04,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14019604704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:11,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:24,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12751816419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:45,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:34:57,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33448426326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:35:11,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:35:34,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26220233185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:35:43,283][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=119, version=3409}] took [3.8m] which is above the warn threshold of [30s]: [running task [Publication{term=119, version=3409}]] took [217ms], [connecting to new nodes] took [1364ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@50988f1f] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1490810] took [43318ms], [org.elasticsearch.script.ScriptService@287d7733] took [77ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [0ms], [org.elasticsearch.snapshots.RestoreService@2e9c6e3d] took [0ms], [org.elasticsearch.ingest.IngestService@675cfa76] took [2380ms], [org.elasticsearch.action.ingest.IngestActionForwarder@245bdd41] took [208ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c6ca0@59befffc] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@141e9f70] took [0ms], [org.elasticsearch.tasks.TaskManager@141158cf] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@6eda2ed9] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@59f2c62f] took [165ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6cafc442] took [131ms], [org.elasticsearch.indices.SystemIndexManager@7d0ab4b4] took [17631ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@12a0f71d] took [114ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@78a09f05] took [388ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@54c27894] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2e89f7bc] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@320d560d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@57e8bb4e] took [62ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@27a41f4e] took [63442ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@23cf10ff] took [62ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1eb47a14] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@45a7c9d5] took [20764ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@238b1b05] took [820ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@50553a64] took [459ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@6d0c8ea0] took [11538ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3a68ee18] took [1224ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@161e0bb5] took [20606ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@6db67940] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7b9cfe9e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@62fd3d12] took [35698ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@4773bdee] took [23177ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4f1929e9] took [212ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@56c5b33a] took [7349ms], [org.elasticsearch.node.ResponseCollectorService@c03e8c1] took [290ms], [org.elasticsearch.snapshots.SnapshotShardsService@28270c1b] took [199ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2e773e97] took [764ms], [org.elasticsearch.shutdown.PluginShutdownService@55500400] took [677ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@46f9e4d0] took [1421ms], [org.elasticsearch.indices.store.IndicesStore@6edca1d8] took [493ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4f310118] took [259ms], [org.elasticsearch.license.LicenseService@3da4f10a] took [319ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3fabe82d] took [175ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@d7e7e22] took [351ms], [org.elasticsearch.gateway.GatewayService@45159b3b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@389bf113] took [0ms]
[2022-03-26T01:35:51,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:03,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [65726ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:36:11,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39506669544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:20,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:21,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [30081ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:36:29,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30081704563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:37,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:45,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@148cc201] took [17024ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:36:45,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17024468085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:36:55,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17839ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:04,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17838258425ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:11,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:18,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16812125969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:24,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:28,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779391497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:33,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:37:35,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [9158ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:39:59,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9158504249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:21,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168286ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:25,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39305dea, interval=5s}] took [168286ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:40:33,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168286103738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:49,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:40:27,358][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [168287ms] which is above the warn threshold of [5s]
[2022-03-26T01:40:26,831][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [2005] timed out after [72612ms]
[2022-03-26T01:41:06,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25905306037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:41:20,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:41:40,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32818844919ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:01,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39973ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:24,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39972805885ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:29,208][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][517][99] duration [2m], collections [1]/[3.4m], total [2m]/[6.8m], memory [235.3mb]->[197mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [185mb]->[185mb]/[2gb]}{[survivor] [6.2mb]->[8mb]/[0b]}
[2022-03-26T01:41:48,381][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2006] timed out after [55588ms]
[2022-03-26T01:42:43,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:45,151][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][517] overhead, spent [2m] collecting in the last [3.4m]
[2022-03-26T01:42:53,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42543146936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:42:54,977][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/382139ms] ago, timed out [5.1m/309527ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2005]
[2022-03-26T01:43:01,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [115334ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:43:11,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:43:37,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28514868693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:44:02,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:44:37,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47209533913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:44:52,789][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11.9m/715728ms] ago, timed out [10.7m/644115ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1971]
[2022-03-26T01:45:02,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63913ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:45:28,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63912913746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:45:42,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:45:56,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39134622443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:08,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:21,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [66147ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:46:23,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27012950746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:41,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:46:56,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32032511391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:10,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:14,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@7941dda2] took [30004ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:47:17,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30004647446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:26,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:33,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14153676397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:41,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17364ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:47:49,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17364031461ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:01,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@59f8a4ec, interval=1m}] took [17536ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:48:00,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:16,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17536736441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:33,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:43,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33617464401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:43,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [33617ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:48:50,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17006ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:57,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17006601294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:04,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:48:53,985][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [2040] timed out after [129683ms]
[2022-03-26T01:49:09,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@575f9587, interval=5s}] took [13890ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:49:11,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13890277119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:21,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18080643731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:44,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21855ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:49:21,099][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2041] timed out after [113568ms]
[2022-03-26T01:49:59,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21854551879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:50:15,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:50:27,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31874467209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:50:26,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [31874ms] which is above the warn threshold of [5000ms]
[2022-03-26T01:50:59,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:52:05,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41160635837ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:53:10,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:55:25,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/128339864802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:57:10,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/243383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T01:57:14,403][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.4m/384884ms] ago, timed out [4.2m/255201ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2040]
[2022-03-26T01:59:25,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/243382897449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T01:55:45,756][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [128340ms] which is above the warn threshold of [5s]
[2022-03-26T02:01:41,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:04:23,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269379797247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:07:36,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:10:51,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337773369215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:14:10,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:17:24,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408673857446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:20:28,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:23:38,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378317180888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:26:57,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:30:02,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389492542175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:34:12,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:37:03,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@cf5da9c, interval=1s}] took [397915ms] which is above the warn threshold of [5000ms]
[2022-03-26T02:37:29,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397915621451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:41:46,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/491344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:44:53,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/490885532314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:48:19,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:48:14,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5093/0x00000008017de930@506af249] took [490885ms] which is above the warn threshold of [5000ms]
[2022-03-26T02:51:37,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/386980112740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T02:55:02,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3e3bb723, interval=1m}] took [406378ms] which is above the warn threshold of [5000ms]
[2022-03-26T02:54:59,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T02:57:10,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406378673554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:00:26,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:03:09,146][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3h/5023343ms] ago, timed out [1.3h/4967755ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{qLCnUvAcSuGndCc0BC8VCQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2006]
[2022-03-26T03:04:15,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326347398264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:09:49,809][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T03:09:49,823][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T03:09:49,824][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T03:09:56,585][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T03:09:56,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T03:09:56,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T03:09:56,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T03:09:56,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T03:09:56,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T03:09:56,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T03:09:56,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T03:09:56,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T03:09:56,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T03:09:56,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T03:09:56,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T03:09:56,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T03:09:56,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T03:09:56,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T03:09:56,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T03:09:56,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T03:09:56,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T03:09:56,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T03:09:56,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T03:09:56,613][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T03:09:56,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T03:09:56,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T03:09:56,617][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T03:09:56,619][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T03:09:56,619][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T03:09:56,622][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T03:09:56,623][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T03:09:56,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T03:09:56,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T03:09:56,625][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T03:09:56,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T03:09:56,629][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T03:09:56,630][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T03:09:56,631][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T03:09:56,632][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T03:09:56,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T03:09:56,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T03:09:56,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T03:09:56,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T03:09:56,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T03:09:56,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T03:09:56,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T03:09:56,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T03:09:56,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T03:09:56,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T03:09:56,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T03:09:56,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T03:09:56,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T03:09:56,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T03:09:56,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T03:09:56,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T03:09:56,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T03:09:56,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T03:09:56,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T03:09:56,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T03:09:56,660][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T03:09:56,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T03:09:56,662][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T03:09:56,744][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.8gb], net total_space [125.8gb], types [ext4]
[2022-03-26T03:09:56,745][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T03:09:57,178][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T03:10:16,644][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T03:10:16,650][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-26T03:10:16,652][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T03:10:16,656][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-26T03:10:16,658][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T03:10:16,661][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-26T03:10:16,665][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-26T03:10:16,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T03:10:16,669][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-26T03:10:16,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-26T03:10:16,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-26T03:10:16,673][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T03:10:16,676][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T03:10:16,677][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T03:10:16,678][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T03:10:18,115][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T03:10:18,352][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T03:10:19,697][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T03:10:20,817][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T03:10:20,820][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T03:10:20,910][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T03:10:20,913][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T03:10:21,221][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T03:10:24,909][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T03:10:25,261][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 120, version: 3410, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T03:10:25,614][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 120, version: 3410, reason: Publication{term=120, version=3410}
[2022-03-26T03:10:25,781][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T03:10:25,782][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T03:10:27,260][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T03:10:27,288][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [28] indices into cluster_state
[2022-03-26T03:11:36,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:38,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [10254ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:11:45,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6477158569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:45,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [54459ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:11:45,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.4s/54459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:46,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.4s/54459365626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:11:46,698][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40844}] took [69346ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:11:50,641][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3412}] took [1.3m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3412}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [81492ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [18ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [0ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [39ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [60ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [59ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [452ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [308ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [10ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [17ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [12ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [14ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [43ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [0ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [26ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [0ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:11:54,470][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T03:11:54,659][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T03:11:56,398][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][18] overhead, spent [391ms] collecting in the last [1.5s]
[2022-03-26T03:12:14,619][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T03:12:14,710][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T03:12:14,721][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T03:12:15,943][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-26T03:12:24,617][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][39][14] duration [1.2s], collections [1]/[3s], total [1.2s]/[2.8s], memory [203.9mb]->[101.6mb]/[2gb], all_pools {[young] [116mb]->[8mb]/[0b]}{[old] [79.9mb]->[79.9mb]/[2gb]}{[survivor] [8mb]->[17.6mb]/[0b]}
[2022-03-26T03:12:25,200][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][39] overhead, spent [1.2s] collecting in the last [3s]
[2022-03-26T03:13:09,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@6546140a] took [5701ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:13:20,843][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11263ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [26] unchanged indices
[2022-03-26T03:13:34,334][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [27s] publication of cluster state version [3423] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:13:42,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6205ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:15:21,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@54494e75] took [88960ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:16:02,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [11339ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:16:57,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38367ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:00,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38367255005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:03,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:09,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7947272144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:17,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:22,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [21279ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:17:21,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13332145785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:26,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:30,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8852553451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:17:59,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:06,035][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][71][15] duration [24.9s], collections [1]/[1.6m], total [24.9s]/[27.7s], memory [169.6mb]->[103.9mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [79.9mb]->[95.9mb]/[2gb]}{[survivor] [17.6mb]->[8mb]/[0b]}
[2022-03-26T03:18:05,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10187464876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:10,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:10,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [10187ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:18:13,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34028675276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:05,014][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10187ms] which is above the warn threshold of [5s]
[2022-03-26T03:18:17,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:22,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7766562110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:27,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [7984ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:18:25,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:30,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7984044116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:33,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:34,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7759ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:18:37,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7759625915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:39,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:41,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6173340813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:54,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:56,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8407116114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:18:55,074][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3423}] took [4.8m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3423}]] took [256ms], [connecting to new nodes] took [676ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [39344ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [30837ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [277ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [497ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [82ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [235ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [34214ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [150ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [2945ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [79ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [2133ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [487ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [324ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [132055ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [13446ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [668ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [563ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [11268ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [692ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [7332ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [128ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [6906ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [2858ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [437ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [1448ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [135ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [123ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [425ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [144ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [1ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:19:19,098][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.8m/412004ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [6.8m/411569ms]
[2022-03-26T03:19:48,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@409e8065] took [68200ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:19:59,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:20:19,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:22,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [6187ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:20:30,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5987194797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:40,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:42,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20595663847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:20:55,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [5251ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:03,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@2df03855] took [43985ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:35,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:22:36,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [7929ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:35,819][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [152846ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [28] unchanged indices
[2022-03-26T03:22:36,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7929173585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:22:36,467][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40848}] took [585176ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:22:36,606][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.9m] publication of cluster state version [3424] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:22:41,510][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-26T03:22:46,097][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][87][16] duration [889ms], collections [1]/[2.8s], total [889ms]/[28.6s], memory [171.9mb]->[103.9mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [95.9mb]->[95.9mb]/[2gb]}{[survivor] [8mb]->[8mb]/[0b]}
[2022-03-26T03:22:46,289][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][87] overhead, spent [889ms] collecting in the last [2.8s]
[2022-03-26T03:23:08,651][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][97][17] duration [1.6s], collections [1]/[3.3s], total [1.6s]/[30.3s], memory [155.9mb]->[106.2mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [95.9mb]->[95.9mb]/[2gb]}{[survivor] [8mb]->[10.2mb]/[0b]}
[2022-03-26T03:23:08,789][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [1.6s] collecting in the last [3.3s]
[2022-03-26T03:23:14,740][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][100][18] duration [1.5s], collections [1]/[3s], total [1.5s]/[31.8s], memory [142.2mb]->[112.5mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [95.9mb]->[101.8mb]/[2gb]}{[survivor] [10.2mb]->[10.7mb]/[0b]}
[2022-03-26T03:23:15,363][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][100] overhead, spent [1.5s] collecting in the last [3s]
[2022-03-26T03:23:59,164][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [20393ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:24:01,006][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.9s] publication of cluster state version [3427] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:24:37,209][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][129][19] duration [2.7s], collections [1]/[4.8s], total [2.7s]/[34.6s], memory [192.5mb]->[116.8mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [101.8mb]->[104.8mb]/[2gb]}{[survivor] [10.7mb]->[12mb]/[0b]}
[2022-03-26T03:24:38,068][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][129] overhead, spent [2.7s] collecting in the last [4.8s]
[2022-03-26T03:24:39,320][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [15796ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [28] unchanged indices
[2022-03-26T03:24:40,724][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.1s] publication of cluster state version [3428] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:25:02,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:25:03,573][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][136][20] duration [6.6s], collections [1]/[2.1s], total [6.6s]/[41.2s], memory [192.8mb]->[200.8mb]/[2gb], all_pools {[young] [76mb]->[24mb]/[0b]}{[old] [104.8mb]->[111.9mb]/[2gb]}{[survivor] [12mb]->[5mb]/[0b]}
[2022-03-26T03:25:03,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][136] overhead, spent [6.6s] collecting in the last [2.1s]
[2022-03-26T03:25:03,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8788ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:25:03,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8388547234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:25:03,944][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-26T03:25:19,283][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][143][21] duration [2s], collections [1]/[2.3s], total [2s]/[43.3s], memory [193mb]->[197mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [5mb]->[7.1mb]/[0b]}
[2022-03-26T03:25:19,509][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][143] overhead, spent [2s] collecting in the last [2.3s]
[2022-03-26T03:25:23,277][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][22] duration [1.7s], collections [1]/[6.9s], total [1.7s]/[45s], memory [197mb]->[118.4mb]/[2gb], all_pools {[young] [8mb]->[0b]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [7.1mb]->[6.5mb]/[0b]}
[2022-03-26T03:25:29,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][146][23] duration [1.3s], collections [1]/[1.7s], total [1.3s]/[46.4s], memory [182.4mb]->[206.4mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [6.5mb]->[8.6mb]/[0b]}
[2022-03-26T03:25:30,162][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][146] overhead, spent [1.3s] collecting in the last [1.7s]
[2022-03-26T03:25:36,823][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][24] duration [3.6s], collections [1]/[4.2s], total [3.6s]/[50s], memory [206.4mb]->[204.5mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [111.9mb]->[111.9mb]/[2gb]}{[survivor] [8.6mb]->[10.1mb]/[0b]}
[2022-03-26T03:25:37,723][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [3.6s] collecting in the last [4.2s]
[2022-03-26T03:25:39,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7395ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:25:54,819][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][153][25] duration [1.8s], collections [1]/[3.6s], total [1.8s]/[51.9s], memory [178mb]->[123.7mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [111.9mb]->[113.9mb]/[2gb]}{[survivor] [10.1mb]->[9.8mb]/[0b]}
[2022-03-26T03:25:55,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][153] overhead, spent [1.8s] collecting in the last [3.6s]
[2022-03-26T03:25:59,813][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][26] duration [2s], collections [1]/[4.4s], total [2s]/[53.9s], memory [123.7mb]->[124.5mb]/[2gb], all_pools {[young] [0b]->[12mb]/[0b]}{[old] [113.9mb]->[114.8mb]/[2gb]}{[survivor] [9.8mb]->[9.7mb]/[0b]}
[2022-03-26T03:26:00,341][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [2s] collecting in the last [4.4s]
[2022-03-26T03:26:07,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:07,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [5694ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:26:08,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5494262552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:09,056][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][155][27] duration [4s], collections [1]/[9.5s], total [4s]/[57.9s], memory [124.5mb]->[131.4mb]/[2gb], all_pools {[young] [12mb]->[8mb]/[0b]}{[old] [114.8mb]->[116.8mb]/[2gb]}{[survivor] [9.7mb]->[14.6mb]/[0b]}
[2022-03-26T03:26:10,234][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][155] overhead, spent [4s] collecting in the last [9.5s]
[2022-03-26T03:26:25,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:25,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [7586ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:26:25,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6785646693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:26,824][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][158][28] duration [2.8s], collections [1]/[10.5s], total [2.8s]/[1m], memory [199.4mb]->[143.7mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [116.8mb]->[122.4mb]/[2gb]}{[survivor] [14.6mb]->[13.2mb]/[0b]}
[2022-03-26T03:26:27,537][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][158] overhead, spent [2.8s] collecting in the last [10.5s]
[2022-03-26T03:26:30,044][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16150ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T03:26:36,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:37,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [5279ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:26:37,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5279692189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:26:36,914][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.8s] publication of cluster state version [3431] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:26:50,123][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40916}] took [5265ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:06,441][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40916}] took [6948ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:20,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:21,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10971574347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:21,577][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][171][29] duration [8.5s], collections [1]/[2.3s], total [8.5s]/[1.1m], memory [207.7mb]->[211.7mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [122.4mb]->[129.3mb]/[2gb]}{[survivor] [13.2mb]->[9.1mb]/[0b]}
[2022-03-26T03:27:22,115][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][171] overhead, spent [8.5s] collecting in the last [2.3s]
[2022-03-26T03:27:21,577][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.9s/12903ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [27], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.18][0]], allocationId [qXD0VyAkRfKwNTH77YG2Yg], primary term [38], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.17][0]], allocationId [FwvWdzKBSpOHHe0J2dJwwg], primary term [42], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-26T03:27:23,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [14425ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:37,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:38,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9126328511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:39,585][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][174][30] duration [5.5s], collections [1]/[1.4s], total [5.5s]/[1.2m], memory [214.5mb]->[222.5mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [129.3mb]->[134.3mb]/[2gb]}{[survivor] [9.1mb]->[5.7mb]/[0b]}
[2022-03-26T03:27:40,129][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][174] overhead, spent [5.5s] collecting in the last [1.4s]
[2022-03-26T03:27:40,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [13055ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:27:42,945][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [17058ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:27:45,736][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [21.7s] publication of cluster state version [3432] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:27:59,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6287ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:27:59,767][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][175][31] duration [4.3s], collections [1]/[31.4s], total [4.3s]/[1.3m], memory [222.5mb]->[140.8mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [5.7mb]->[6.5mb]/[0b]}
[2022-03-26T03:27:59,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6287014349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:32,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:32,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8831ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:28:33,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6230187668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:35,269][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3432}] took [47.3s] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3432}]] took [92ms], [connecting to new nodes] took [32ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [526ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [11197ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [39ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [41ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [69ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [51ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [90ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [13840ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [3848ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [226ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [2358ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [357ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [3888ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [9546ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [882ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [128ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [0ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:28:46,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:47,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@1f238b3a] took [9078ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:28:47,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8678798772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:28:47,428][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][188][32] duration [5.4s], collections [1]/[10.4s], total [5.4s]/[1.4m], memory [220.8mb]->[176.1mb]/[2gb], all_pools {[young] [84mb]->[36mb]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [6.5mb]->[5.7mb]/[0b]}
[2022-03-26T03:28:47,429][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][188] overhead, spent [5.4s] collecting in the last [10.4s]
[2022-03-26T03:29:04,411][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16794ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [24] unchanged indices
[2022-03-26T03:29:13,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:29:14,510][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][33] duration [5.9s], collections [1]/[3.4s], total [5.9s]/[1.5m], memory [192.1mb]->[196.1mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [5.7mb]->[7.2mb]/[0b]}
[2022-03-26T03:29:14,741][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [5.9s] collecting in the last [3.4s]
[2022-03-26T03:29:14,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8478422113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:29:14,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9078ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:29:14,433][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.9s] publication of cluster state version [3433] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:30:00,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:30:01,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11290038635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:30:01,365][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][34] duration [6.3s], collections [1]/[3.4s], total [6.3s]/[1.6m], memory [193.5mb]->[193.5mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [134.3mb]->[134.3mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-03-26T03:30:02,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [6.3s] collecting in the last [3.4s]
[2022-03-26T03:30:03,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [15513ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:30:10,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@233130dd] took [5340ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:30:45,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6785ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:30:55,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5995ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:31:05,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:09,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6768552268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:09,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@6a8e418c] took [7399ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:31:10,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:10,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5375442321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:31:11,473][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][212][35] duration [4.4s], collections [1]/[21.5s], total [4.4s]/[1.6m], memory [183.5mb]->[148.5mb]/[2gb], all_pools {[young] [40mb]->[4mb]/[0b]}{[old] [134.3mb]->[136.5mb]/[2gb]}{[survivor] [9.2mb]->[8mb]/[0b]}
[2022-03-26T03:31:42,210][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:40950}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:31:52,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@20167a91] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:32:23,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:26,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9492223224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:27,782][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][225][36] duration [5.7s], collections [1]/[11.1s], total [5.7s]/[1.7m], memory [228.5mb]->[145.9mb]/[2gb], all_pools {[young] [83.9mb]->[4mb]/[0b]}{[old] [136.5mb]->[136.5mb]/[2gb]}{[survivor] [8mb]->[9.4mb]/[0b]}
[2022-03-26T03:32:28,029][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][225] overhead, spent [5.7s] collecting in the last [11.1s]
[2022-03-26T03:32:43,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:43,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6500894168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:32:43,966][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][228][37] duration [4s], collections [1]/[1.3s], total [4s]/[1.8m], memory [209.9mb]->[229.9mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [136.5mb]->[137.8mb]/[2gb]}{[survivor] [9.4mb]->[8.8mb]/[0b]}
[2022-03-26T03:32:43,967][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][228] overhead, spent [4s] collecting in the last [1.3s]
[2022-03-26T03:32:43,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6901ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:32:45,449][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3433}] took [3.5m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3433}]] took [0ms], [connecting to new nodes] took [198ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [164974ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [7637ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [128ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [125ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [720ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [119ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [118ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [29729ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [804ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [130ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [207ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [75ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [35ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [374ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [202ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [155ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [0ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [38ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:32:50,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][230][38] duration [2s], collections [1]/[4.5s], total [2s]/[1.8m], memory [214.7mb]->[150.6mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [137.8mb]->[137.8mb]/[2gb]}{[survivor] [8.8mb]->[12.7mb]/[0b]}
[2022-03-26T03:32:51,119][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][230] overhead, spent [2s] collecting in the last [4.5s]
[2022-03-26T03:32:56,772][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][231][39] duration [2.2s], collections [1]/[7s], total [2.2s]/[1.9m], memory [150.6mb]->[153.9mb]/[2gb], all_pools {[young] [4mb]->[8mb]/[0b]}{[old] [137.8mb]->[142.5mb]/[2gb]}{[survivor] [12.7mb]->[11.3mb]/[0b]}
[2022-03-26T03:32:57,071][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][231] overhead, spent [2.2s] collecting in the last [7s]
[2022-03-26T03:33:06,614][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][40] duration [3.2s], collections [1]/[1.7s], total [3.2s]/[1.9m], memory [193.9mb]->[237.9mb]/[2gb], all_pools {[young] [68mb]->[32mb]/[0b]}{[old] [142.5mb]->[148.3mb]/[2gb]}{[survivor] [11.3mb]->[10.1mb]/[0b]}
[2022-03-26T03:33:07,211][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][233] overhead, spent [3.2s] collecting in the last [1.7s]
[2022-03-26T03:33:07,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7487ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:33:18,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:18,821][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][234][41] duration [6.7s], collections [1]/[17.8s], total [6.7s]/[2m], memory [237.9mb]->[163.2mb]/[2gb], all_pools {[young] [32mb]->[0b]/[0b]}{[old] [148.3mb]->[152.4mb]/[2gb]}{[survivor] [10.1mb]->[10.8mb]/[0b]}
[2022-03-26T03:33:19,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478264215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:19,410][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][234] overhead, spent [6.7s] collecting in the last [17.8s]
[2022-03-26T03:33:20,767][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14020ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T03:33:24,028][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.3s] publication of cluster state version [3434] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:33:40,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:40,826][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9434ms] which is above the warn threshold of [5s]
[2022-03-26T03:33:41,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8634198936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:33:41,898][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][240][42] duration [5.4s], collections [1]/[1.1s], total [5.4s]/[2.1m], memory [215.2mb]->[227.2mb]/[2gb], all_pools {[young] [52mb]->[4mb]/[0b]}{[old] [152.4mb]->[156.3mb]/[2gb]}{[survivor] [10.8mb]->[9.8mb]/[0b]}
[2022-03-26T03:33:41,899][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][240] overhead, spent [5.4s] collecting in the last [1.1s]
[2022-03-26T03:33:41,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9234ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:13,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:34:13,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@2aa8fcab] took [14949ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:13,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7643258429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:34:20,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5937ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:29,638][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3434}] took [1m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3434}]] took [142ms], [connecting to new nodes] took [132ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [2785ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [1551ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [34ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [32ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [484ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [42ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [39ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [23011ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [5425ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [123ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [14719ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [5057ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [5688ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [3954ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [571ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [56ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [58ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:34:42,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:34:43,118][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][43] duration [5.8s], collections [1]/[14.7s], total [5.8s]/[2.2m], memory [218.2mb]->[222.2mb]/[2gb], all_pools {[young] [52mb]->[84mb]/[0b]}{[old] [156.3mb]->[156.3mb]/[2gb]}{[survivor] [9.8mb]->[9.8mb]/[0b]}
[2022-03-26T03:34:43,716][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][248] overhead, spent [5.8s] collecting in the last [14.7s]
[2022-03-26T03:34:43,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [12559ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:34:43,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10447901434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:35:20,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5594ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:35:34,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@6e314064] took [8625ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:36:09,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [24706ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:36:39,855][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [108110ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:36:44,871][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [3435] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:36:57,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5327ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:37:00,987][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/86102ms] ago, timed out [21.3s/21366ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [748]
[2022-03-26T03:37:12,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6635ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:37:33,527][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [748] timed out after [64736ms]
[2022-03-26T03:37:49,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5547ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:38:07,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5005ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:38:21,282][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3435}] took [1.4m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3435}]] took [229ms], [connecting to new nodes] took [383ms], [applying settings] took [76ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [690ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [7041ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [215ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [161ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [105ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [0ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [826ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [163ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [51ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [28505ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [9447ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [413ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [549ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [12639ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [499ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [10380ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [130ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [7925ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [4509ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [1180ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [72ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [87ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [73ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [165ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [277ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [0ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [88ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:38:31,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@c72165d] took [11396ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:38:46,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:39:05,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:05,500][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [33.5s/33529ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-26T03:39:11,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10722189675ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:17,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [12648ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:39:17,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:21,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12648037566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:26,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:22,806][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12648ms] which is above the warn threshold of [5s]
[2022-03-26T03:39:26,830][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [829] timed out after [45698ms]
[2022-03-26T03:39:51,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9099ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:39:51,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9099154716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:52,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:53,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26359441725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:39:57,141][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85064ms] ago, timed out [39.3s/39366ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [829]
[2022-03-26T03:40:07,949][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][275][44] duration [14.2s], collections [1]/[41.6s], total [14.2s]/[2.5m], memory [252.8mb]->[177.8mb]/[2gb], all_pools {[young] [88mb]->[8mb]/[0b]}{[old] [161mb]->[161mb]/[2gb]}{[survivor] [7.7mb]->[8.7mb]/[0b]}
[2022-03-26T03:40:09,423][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][275] overhead, spent [14.2s] collecting in the last [41.6s]
[2022-03-26T03:40:10,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8206ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:40:38,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@53397fc0] took [9104ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:40:56,389][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [877] timed out after [18272ms]
[2022-03-26T03:41:00,282][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [22s/22075ms] ago, timed out [3.8s/3803ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [877]
[2022-03-26T03:40:59,858][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [67099ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T03:41:05,489][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.7m] publication of cluster state version [3436] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T03:41:43,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:41:44,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6483ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:41:46,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5682867752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:41:48,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5129ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:42:32,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:42:32,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [13965ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:42:33,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12564020125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:42:35,758][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][300][45] duration [8s], collections [1]/[22.4s], total [8s]/[2.6m], memory [249.8mb]->[171.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [161mb]->[164.4mb]/[2gb]}{[survivor] [8.7mb]->[7.2mb]/[0b]}
[2022-03-26T03:42:37,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][300] overhead, spent [8s] collecting in the last [22.4s]
[2022-03-26T03:42:38,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6256ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:43:10,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7582ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:43:46,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [14371ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:13,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [7916ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:39,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:40,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9240ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:41,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5259043507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:40,235][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [993] timed out after [33528ms]
[2022-03-26T03:44:40,818][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10041ms] which is above the warn threshold of [5s]
[2022-03-26T03:44:46,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:51,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6231517658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:56,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:44:58,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [11542ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:44:58,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11542605333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T03:45:10,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [8460ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:45:27,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@371bb03b] took [6615ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:45:39,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [9607ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:45:40,094][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=120, version=3436}] took [4.4m] which is above the warn threshold of [30s]: [running task [Publication{term=120, version=3436}]] took [54ms], [connecting to new nodes] took [57ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@3ee05e50] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@615dd2b9] took [124802ms], [org.elasticsearch.script.ScriptService@bbdaaa7] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [0ms], [org.elasticsearch.snapshots.RestoreService@37d60f3] took [0ms], [org.elasticsearch.ingest.IngestService@cbab23f] took [16527ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6a34c99d] took [499ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d9320@481948df] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@208bc3d7] took [392ms], [org.elasticsearch.tasks.TaskManager@7979b6b3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@35cde020] took [152ms], [org.elasticsearch.cluster.InternalClusterInfoService@1722712f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@24edcfba] took [4ms], [org.elasticsearch.indices.SystemIndexManager@ca88d96] took [3345ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@380bde8] took [83ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@54b68d59] took [589ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@d492db1] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@1cae6dea] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@5ced407c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@1db5699c] took [225ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@511a9914] took [80174ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@69107078] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6bffa3b6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1868d01] took [10544ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@60f28ee0] took [411ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4c5ca595] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@5a284dbb] took [6042ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3fd3bb87] took [517ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6f732d7a] took [5132ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1b023bfa] took [47ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5c6873a0] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@45b4d27a] took [7492ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@33393500] took [4710ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@52a36311] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3b243763] took [2138ms], [org.elasticsearch.node.ResponseCollectorService@414f8432] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@793aafea] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3048e037] took [71ms], [org.elasticsearch.shutdown.PluginShutdownService@2a4d5672] took [5ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5c3e6215] took [149ms], [org.elasticsearch.indices.store.IndicesStore@b2a321f] took [3442ms], [org.elasticsearch.persistent.PersistentTasksNodeService@22dc9861] took [1ms], [org.elasticsearch.license.LicenseService@3a89d9c7] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4fe8583d] took [141ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3997ce6f] took [0ms], [org.elasticsearch.gateway.GatewayService@23b15332] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@19cc6f9c] took [0ms]
[2022-03-26T03:46:09,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [13733ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:46:24,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [6589ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:46:16,907][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1033] timed out after [47693ms]
[2022-03-26T03:46:39,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [5692ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:46:47,663][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/79518ms] ago, timed out [31.8s/31825ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1033]
[2022-03-26T03:47:17,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [19099ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:47:17,362][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.2m/197714ms] ago, timed out [2.7m/164186ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{cgwQ75v-SamgzrC_w8g-Dg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [993]
[2022-03-26T03:47:42,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@58f1f34b] took [9859ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:48:09,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@a5cdc1b, interval=5s}] took [10489ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:53:16,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [61817ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:58:58,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [37383ms] which is above the warn threshold of [5000ms]
[2022-03-26T03:59:33,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T03:59:31,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@46743e7f] took [12218ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:00:03,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11769551584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:00:35,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:00:54,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64501964388ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:01:17,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:01:35,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42400732615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:01:59,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:02:24,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41400038652ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:02:45,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:03:05,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45467682491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:02:56,187][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1068] timed out after [812715ms]
[2022-03-26T04:03:22,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:03:12,267][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [45468ms] which is above the warn threshold of [5s]
[2022-03-26T04:03:52,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36271261420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:04:17,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:04:40,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56986318331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:05:11,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.9s/54979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:05:44,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [54978ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:05:35,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.9s/54978963340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:02,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:17,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51112353978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:31,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:42,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28746960234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:06:58,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:13,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27352443020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:29,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:41,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30672725685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:07:51,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [30672ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:07:56,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:12,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27226799527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:28,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:39,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32770569348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:08:55,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:09,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26253571806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:24,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:38,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27519849462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:09:44,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e6768@176bc469] took [27519ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:10:01,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37952ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:10:20,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37951996375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:10:46,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:11:01,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43632951797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:11:21,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36069ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:11:40,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36069059060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:00,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3401ed61, interval=1s}] took [75503ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:12:00,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:20,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39434711264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:39,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:54,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35571588419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:13:10,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:12:52,047][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35572ms] which is above the warn threshold of [5s]
[2022-03-26T04:13:25,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34121067275ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:14:22,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:15:31,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68946305444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:16:36,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:16:46,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6c8a0771, interval=5s}] took [134226ms] which is above the warn threshold of [5000ms]
[2022-03-26T04:17:51,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134226472625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:19:22,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:21:04,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166432283899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:23:06,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:25:16,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216618221108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:28:23,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T04:31:14,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313862476629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T04:34:00,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T06:59:29,389][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T06:59:29,413][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T06:59:29,415][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T06:59:38,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T06:59:38,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T06:59:38,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T06:59:38,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T06:59:38,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T06:59:38,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T06:59:38,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T06:59:38,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T06:59:38,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T06:59:38,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T06:59:38,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T06:59:38,610][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T06:59:38,610][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T06:59:38,611][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T06:59:38,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T06:59:38,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T06:59:38,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T06:59:38,616][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T06:59:38,617][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T06:59:38,617][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T06:59:38,618][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T06:59:38,619][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T06:59:38,620][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T06:59:38,623][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T06:59:38,623][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T06:59:38,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T06:59:38,625][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T06:59:38,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T06:59:38,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T06:59:38,636][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T06:59:38,636][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T06:59:38,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T06:59:38,638][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T06:59:38,638][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T06:59:38,639][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T06:59:38,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T06:59:38,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T06:59:38,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T06:59:38,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T06:59:38,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T06:59:38,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T06:59:38,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T06:59:38,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T06:59:38,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T06:59:38,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T06:59:38,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T06:59:38,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T06:59:38,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T06:59:38,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T06:59:38,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T06:59:38,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T06:59:38,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T06:59:38,660][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T06:59:38,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T06:59:38,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T06:59:38,662][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T06:59:38,662][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T06:59:38,663][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T06:59:38,664][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T06:59:38,781][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.8gb], net total_space [125.8gb], types [ext4]
[2022-03-26T06:59:38,783][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T06:59:39,304][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T07:00:59,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11854ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:01:07,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11521965231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:01:08,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:01:08,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.8s/53881880735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:01:22,719][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T07:01:22,725][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T07:01:22,726][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp]
[2022-03-26T07:01:22,726][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp]
[2022-03-26T07:01:22,727][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp]
[2022-03-26T07:01:22,728][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T07:01:22,728][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T07:01:22,729][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T07:01:24,250][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T07:01:24,818][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T07:01:26,055][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T07:01:27,028][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T07:01:27,029][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T07:01:27,136][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T07:01:27,142][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T07:01:27,447][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T07:01:29,970][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T07:01:30,128][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 121, version: 3437, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T07:01:30,301][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 121, version: 3437, reason: Publication{term=121, version=3437}
[2022-03-26T07:01:30,428][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T07:01:30,428][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T07:01:31,485][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T07:01:31,497][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [28] indices into cluster_state
[2022-03-26T07:01:32,430][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T07:01:32,432][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T07:01:33,683][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-26T07:01:34,694][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T07:01:34,764][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T07:01:34,775][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T07:02:11,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:15,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11285095299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:20,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:23,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9422702046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:30,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:36,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9906502148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:30,188][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [23179ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [28] unchanged indices
[2022-03-26T07:02:38,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:39,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8523451635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:02:42,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@6e6153fa] took [22389ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:02:43,455][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-26T07:02:50,275][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][24][12] duration [1.5s], collections [1]/[2.1s], total [1.5s]/[2.2s], memory [719.8mb]->[101.8mb]/[2gb], all_pools {[young] [656mb]->[0b]/[0b]}{[old] [63.8mb]->[63.8mb]/[2gb]}{[survivor] [4mb]->[38mb]/[0b]}
[2022-03-26T07:02:50,398][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][24] overhead, spent [1.5s] collecting in the last [2.1s]
[2022-03-26T07:02:56,880][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][26][13] duration [3s], collections [1]/[4.8s], total [3s]/[5.2s], memory [141.8mb]->[106.3mb]/[2gb], all_pools {[young] [40mb]->[4mb]/[0b]}{[old] [63.8mb]->[99.3mb]/[2gb]}{[survivor] [38mb]->[7mb]/[0b]}
[2022-03-26T07:02:57,090][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][26] overhead, spent [3s] collecting in the last [4.8s]
[2022-03-26T07:03:13,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6926ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:03:13,092][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T07:03:13,060][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T07:03:16,746][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [19736ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [28] unchanged indices
[2022-03-26T07:03:18,589][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.7s] publication of cluster state version [3450] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:03:23,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-26T07:03:49,026][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][49][14] duration [1.8s], collections [1]/[1.2s], total [1.8s]/[7s], memory [190.3mb]->[194.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [99.3mb]->[99.3mb]/[2gb]}{[survivor] [7mb]->[8mb]/[0b]}
[2022-03-26T07:03:49,492][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][49] overhead, spent [1.8s] collecting in the last [1.2s]
[2022-03-26T07:04:00,181][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11s] publication of cluster state version [3453] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_APPLY_COMMIT]
[2022-03-26T07:04:29,232][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][66][15] duration [2.1s], collections [1]/[2.3s], total [2.1s]/[9.2s], memory [171.3mb]->[175.3mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [99.3mb]->[99.3mb]/[2gb]}{[survivor] [8mb]->[11.9mb]/[0b]}
[2022-03-26T07:04:29,817][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][66] overhead, spent [2.1s] collecting in the last [2.3s]
[2022-03-26T07:04:30,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [8099ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:04:31,421][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:41424}] took [10301ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:04:41,228][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T07:04:49,922][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][76][16] duration [1.7s], collections [1]/[3.2s], total [1.7s]/[10.9s], memory [191.3mb]->[116.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [99.3mb]->[103.6mb]/[2gb]}{[survivor] [11.9mb]->[13mb]/[0b]}
[2022-03-26T07:04:50,287][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][76] overhead, spent [1.7s] collecting in the last [3.2s]
[2022-03-26T07:05:04,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [7047ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:05:28,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@1b7bc06] took [21812ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:05:39,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:05:41,819][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [49674ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T07:05:46,930][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [56.2s] publication of cluster state version [3455] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:06:30,166][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][99][17] duration [2.9s], collections [1]/[6s], total [2.9s]/[13.9s], memory [192.7mb]->[117.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [103.6mb]->[110mb]/[2gb]}{[survivor] [13mb]->[7.4mb]/[0b]}
[2022-03-26T07:06:30,895][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [2.9s] collecting in the last [6s]
[2022-03-26T07:06:44,432][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [698ms] collecting in the last [2s]
[2022-03-26T07:06:46,854][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][108][19] duration [748ms], collections [1]/[2.2s], total [748ms]/[15.3s], memory [123.6mb]->[124.9mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [110mb]->[115.8mb]/[2gb]}{[survivor] [13.6mb]->[9.1mb]/[0b]}
[2022-03-26T07:06:47,044][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][108] overhead, spent [748ms] collecting in the last [2.2s]
[2022-03-26T07:06:57,472][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][110][20] duration [2.1s], collections [1]/[1.7s], total [2.1s]/[17.5s], memory [136.9mb]->[208.9mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [115.8mb]->[117.3mb]/[2gb]}{[survivor] [9.1mb]->[9.8mb]/[0b]}
[2022-03-26T07:06:58,332][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][110] overhead, spent [2.1s] collecting in the last [1.7s]
[2022-03-26T07:06:59,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9146ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:07:10,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6278ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:07:35,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@335f0984] took [22136ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:07:50,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9719ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:08:19,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5406ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:08:19,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5406286928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:08:21,797][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [44442ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T07:08:22,671][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [54.9s] publication of cluster state version [3459] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:08:45,187][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][21] duration [1.5s], collections [1]/[3.2s], total [1.5s]/[19.1s], memory [199.1mb]->[128.6mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [117.3mb]->[121.2mb]/[2gb]}{[survivor] [9.8mb]->[7.3mb]/[0b]}
[2022-03-26T07:08:45,428][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [1.5s] collecting in the last [3.2s]
[2022-03-26T07:08:46,243][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:08:57,632][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][135][22] duration [1.3s], collections [1]/[1.5s], total [1.3s]/[20.4s], memory [208.6mb]->[212.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [121.2mb]->[121.2mb]/[2gb]}{[survivor] [7.3mb]->[12mb]/[0b]}
[2022-03-26T07:08:57,802][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][135] overhead, spent [1.3s] collecting in the last [1.5s]
[2022-03-26T07:09:21,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:09:21,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9023ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:09:21,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6421637966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:09:22,638][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][23] duration [4.7s], collections [1]/[9.9s], total [4.7s]/[25.1s], memory [185.2mb]->[149.6mb]/[2gb], all_pools {[young] [56mb]->[20mb]/[0b]}{[old] [121.2mb]->[124.6mb]/[2gb]}{[survivor] [12mb]->[9mb]/[0b]}
[2022-03-26T07:09:22,936][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [4.7s] collecting in the last [9.9s]
[2022-03-26T07:09:29,577][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][146][24] duration [2.1s], collections [1]/[1.5s], total [2.1s]/[27.2s], memory [189.6mb]->[209.6mb]/[2gb], all_pools {[young] [56mb]->[80mb]/[0b]}{[old] [124.6mb]->[124.6mb]/[2gb]}{[survivor] [9mb]->[9mb]/[0b]}
[2022-03-26T07:09:29,898][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][146] overhead, spent [2.1s] collecting in the last [1.5s]
[2022-03-26T07:09:33,599][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][148][25] duration [1.1s], collections [1]/[2.2s], total [1.1s]/[28.4s], memory [148.1mb]->[142.4mb]/[2gb], all_pools {[young] [16mb]->[20mb]/[0b]}{[old] [127.3mb]->[128.7mb]/[2gb]}{[survivor] [8.8mb]->[9.6mb]/[0b]}
[2022-03-26T07:09:33,859][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][148] overhead, spent [1.1s] collecting in the last [2.2s]
[2022-03-26T07:09:40,941][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][150][26] duration [2.9s], collections [1]/[5.1s], total [2.9s]/[31.4s], memory [202.4mb]->[143.8mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [128.7mb]->[131mb]/[2gb]}{[survivor] [9.6mb]->[12.8mb]/[0b]}
[2022-03-26T07:09:42,141][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][150] overhead, spent [2.9s] collecting in the last [5.1s]
[2022-03-26T07:09:50,553][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [15912ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T07:09:52,071][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.2s] publication of cluster state version [3463] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:10:11,580][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][161][27] duration [2.9s], collections [1]/[1.6s], total [2.9s]/[34.3s], memory [179.8mb]->[215.8mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [131mb]->[136.9mb]/[2gb]}{[survivor] [12.8mb]->[10.2mb]/[0b]}
[2022-03-26T07:10:12,269][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [2.9s] collecting in the last [1.6s]
[2022-03-26T07:10:12,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [8437ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:10:18,542][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [18024ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [25] unchanged indices
[2022-03-26T07:10:21,914][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [23.6s] publication of cluster state version [3464] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:10:49,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:11:19,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:11:19,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [22850ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:11:21,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18448274146ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:11:24,540][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.6s/35657ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-26T07:11:39,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@76ef4e32] took [10407ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:11:43,591][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][172][28] duration [12.6s], collections [1]/[45.3s], total [12.6s]/[46.9s], memory [227.2mb]->[150.9mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [136.9mb]->[143.9mb]/[2gb]}{[survivor] [10.2mb]->[2.9mb]/[0b]}
[2022-03-26T07:11:45,233][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][172] overhead, spent [12.6s] collecting in the last [45.3s]
[2022-03-26T07:11:46,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5152ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:12:01,175][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [27142ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [24] unchanged indices
[2022-03-26T07:12:03,603][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [34.3s] publication of cluster state version [3465] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:12:49,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:12:49,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7278068570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:12:50,285][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][188][29] duration [5.8s], collections [1]/[3.1s], total [5.8s]/[52.8s], memory [234.9mb]->[148.8mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [143.9mb]->[143.9mb]/[2gb]}{[survivor] [2.9mb]->[4.8mb]/[0b]}
[2022-03-26T07:12:51,053][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][188] overhead, spent [5.8s] collecting in the last [3.1s]
[2022-03-26T07:12:51,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [10026ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:13:19,711][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=121, version=3465}] took [1.1m] which is above the warn threshold of [30s]: [running task [Publication{term=121, version=3465}]] took [0ms], [connecting to new nodes] took [83ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@ebed2d1] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68154492] took [60437ms], [org.elasticsearch.script.ScriptService@5c5c0db8] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [0ms], [org.elasticsearch.snapshots.RestoreService@f36d607] took [0ms], [org.elasticsearch.ingest.IngestService@4dc532a7] took [158ms], [org.elasticsearch.action.ingest.IngestActionForwarder@71c8d76a] took [33ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d7740@33bf5699] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@a802807] took [0ms], [org.elasticsearch.tasks.TaskManager@6e3473fa] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@59f09ad1] took [72ms], [org.elasticsearch.cluster.InternalClusterInfoService@588efbbb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@22fabf86] took [0ms], [org.elasticsearch.indices.SystemIndexManager@783a0c4f] took [380ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@521458b7] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x000000080130ee58@37d27d8d] took [129ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4df7bd65] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7c4cc338] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801408c08@796240e1] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@7b9a36c6] took [83ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@108f95af] took [1258ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@61685f7c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1351775d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@306159f4] took [1599ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@29dd9588] took [128ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2b7ff798] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@330c8635] took [1394ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [760ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@918c759] took [2084ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4cac6227] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@bcabad] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@70ce1e88] took [864ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@415e345f] took [84ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@714dcd21] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@360a610f] took [110ms], [org.elasticsearch.node.ResponseCollectorService@646575d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@65842e7f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@7abfc32b] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@1fd8d76a] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@618bed45] took [41ms], [org.elasticsearch.indices.store.IndicesStore@69114a6] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@75a47474] took [0ms], [org.elasticsearch.license.LicenseService@2ac70849] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1aa23b90] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1a9f4da] took [0ms], [org.elasticsearch.gateway.GatewayService@51b4899b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1f4695b7] took [0ms]
[2022-03-26T07:13:28,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:13:28,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5382872192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:13:29,054][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][30] duration [4.2s], collections [1]/[2.1s], total [4.2s]/[57.1s], memory [224.8mb]->[236.8mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [143.9mb]->[143.9mb]/[2gb]}{[survivor] [4.8mb]->[10.6mb]/[0b]}
[2022-03-26T07:13:29,079][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [4.2s] collecting in the last [2.1s]
[2022-03-26T07:13:29,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6618ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:13:35,557][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][207][31] duration [1.7s], collections [1]/[3.8s], total [1.7s]/[58.9s], memory [226.6mb]->[165mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [143.9mb]->[148.1mb]/[2gb]}{[survivor] [10.6mb]->[12.8mb]/[0b]}
[2022-03-26T07:13:35,927][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][207] overhead, spent [1.7s] collecting in the last [3.8s]
[2022-03-26T07:13:59,543][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [18915ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [26] unchanged indices
[2022-03-26T07:14:01,247][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [20.9s] publication of cluster state version [3467] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:14:15,272][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][225][32] duration [2.6s], collections [1]/[1.3s], total [2.6s]/[1m], memory [237mb]->[241mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [148.1mb]->[154.1mb]/[2gb]}{[survivor] [12.8mb]->[9.1mb]/[0b]}
[2022-03-26T07:14:15,516][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][225] overhead, spent [2.6s] collecting in the last [1.3s]
[2022-03-26T07:14:44,609][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][239][33] duration [3.7s], collections [1]/[2.1s], total [3.7s]/[1m], memory [207.2mb]->[167.2mb]/[2gb], all_pools {[young] [48mb]->[4mb]/[0b]}{[old] [154.1mb]->[160.5mb]/[2gb]}{[survivor] [9.1mb]->[6.7mb]/[0b]}
[2022-03-26T07:14:45,127][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][239] overhead, spent [3.7s] collecting in the last [2.1s]
[2022-03-26T07:14:45,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6182ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:15:12,205][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=121, version=3469}] took [36.2s] which is above the warn threshold of [30s]: [running task [Publication{term=121, version=3469}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@ebed2d1] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68154492] took [26362ms], [org.elasticsearch.script.ScriptService@5c5c0db8] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [0ms], [org.elasticsearch.snapshots.RestoreService@f36d607] took [0ms], [org.elasticsearch.ingest.IngestService@4dc532a7] took [208ms], [org.elasticsearch.action.ingest.IngestActionForwarder@71c8d76a] took [87ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d7740@33bf5699] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@a802807] took [0ms], [org.elasticsearch.tasks.TaskManager@6e3473fa] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@59f09ad1] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@588efbbb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@22fabf86] took [0ms], [org.elasticsearch.indices.SystemIndexManager@783a0c4f] took [1096ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@521458b7] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x000000080130ee58@37d27d8d] took [201ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4df7bd65] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7c4cc338] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801408c08@796240e1] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@7b9a36c6] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@108f95af] took [7562ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@61685f7c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1351775d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@306159f4] took [176ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@29dd9588] took [52ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2b7ff798] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@330c8635] took [41ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [48ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@918c759] took [21ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4cac6227] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@bcabad] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@70ce1e88] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@415e345f] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@714dcd21] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@360a610f] took [53ms], [org.elasticsearch.node.ResponseCollectorService@646575d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@65842e7f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@7abfc32b] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@1fd8d76a] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@618bed45] took [0ms], [org.elasticsearch.indices.store.IndicesStore@69114a6] took [146ms], [org.elasticsearch.persistent.PersistentTasksNodeService@75a47474] took [0ms], [org.elasticsearch.license.LicenseService@2ac70849] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1aa23b90] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1a9f4da] took [0ms], [org.elasticsearch.gateway.GatewayService@51b4899b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1f4695b7] took [0ms]
[2022-03-26T07:15:21,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:15:22,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7054189580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:15:24,344][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][250][34] duration [5.4s], collections [1]/[9s], total [5.4s]/[1.1m], memory [247.2mb]->[175mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [160.5mb]->[160.5mb]/[2gb]}{[survivor] [6.7mb]->[6.5mb]/[0b]}
[2022-03-26T07:15:24,448][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:41490}] took [11376ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:15:24,963][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][250] overhead, spent [5.4s] collecting in the last [9s]
[2022-03-26T07:15:47,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:15:47,595][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:41498}] took [7503ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:15:47,675][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][258][35] duration [3.5s], collections [1]/[7.9s], total [3.5s]/[1.2m], memory [207mb]->[168.3mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [160.5mb]->[160.5mb]/[2gb]}{[survivor] [6.5mb]->[7.8mb]/[0b]}
[2022-03-26T07:15:47,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6101878975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:15:47,746][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][258] overhead, spent [3.5s] collecting in the last [7.9s]
[2022-03-26T07:15:47,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6301ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:16:03,999][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] to [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:16:07,944][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [46.9s/46917ms] to compute cluster state update for [ilm-move-to-step {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}], nextStep [{"phase":"hot","action":"rollover","name":"attempt-rollover"}]}[org.elasticsearch.xpack.ilm.MoveToNextStepUpdateTask@8f2c2314], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@aad4533e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2f59229c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@681435f5]], which exceeds the warn threshold of [10s]
[2022-03-26T07:16:24,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9652ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:16:41,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9805ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:17:32,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16324ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:17:15,509][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [64206ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [27] unchanged indices
[2022-03-26T07:17:34,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16323902873ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:17:35,724][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.4m] publication of cluster state version [3470] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:17:37,087][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][273][36] duration [10.7s], collections [1]/[5.9s], total [10.7s]/[1.4m], memory [240.3mb]->[256.3mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [160.5mb]->[160.5mb]/[2gb]}{[survivor] [7.8mb]->[8mb]/[0b]}
[2022-03-26T07:17:38,299][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][273] overhead, spent [10.7s] collecting in the last [5.9s]
[2022-03-26T07:17:39,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [23475ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:18:25,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:18:25,711][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][287][37] duration [7.4s], collections [1]/[11.7s], total [7.4s]/[1.5m], memory [228.5mb]->[169.2mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [160.5mb]->[160.5mb]/[2gb]}{[survivor] [8mb]->[8.7mb]/[0b]}
[2022-03-26T07:18:25,756][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][287] overhead, spent [7.4s] collecting in the last [11.7s]
[2022-03-26T07:18:25,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11036824777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:18:25,789][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:41490}] took [11437ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:18:35,956][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [37.4s/37465ms] to notify listeners on successful publication of cluster state (version: 3470, uuid: jX2MXzyUTL6E1lDb8DP4Eg) for [ilm-move-to-step {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}], nextStep [{"phase":"hot","action":"rollover","name":"attempt-rollover"}]}[org.elasticsearch.xpack.ilm.MoveToNextStepUpdateTask@8f2c2314], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@aad4533e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2f59229c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@681435f5]], which exceeds the warn threshold of [10s]
[2022-03-26T07:18:39,645][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-26T07:18:55,198][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][298][38] duration [2.1s], collections [1]/[4.4s], total [2.1s]/[1.5m], memory [245.2mb]->[170.2mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [160.5mb]->[162.9mb]/[2gb]}{[survivor] [8.7mb]->[7.2mb]/[0b]}
[2022-03-26T07:18:55,838][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][298] overhead, spent [2.1s] collecting in the last [4.4s]
[2022-03-26T07:18:59,310][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [5.3m/318473ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [4.9m/294173ms]
[2022-03-26T07:19:16,323][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][306][39] duration [2.2s], collections [1]/[1.8s], total [2.2s]/[1.6m], memory [238.2mb]->[246.2mb]/[2gb], all_pools {[young] [72mb]->[76mb]/[0b]}{[old] [162.9mb]->[162.9mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-03-26T07:19:16,981][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][306] overhead, spent [2.2s] collecting in the last [1.8s]
[2022-03-26T07:19:17,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [7733ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:19:40,358][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][317][40] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[1.6m], memory [242.9mb]->[171.6mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [162.9mb]->[162.9mb]/[2gb]}{[survivor] [8mb]->[8.7mb]/[0b]}
[2022-03-26T07:19:40,762][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][317] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-03-26T07:20:08,620][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][332][41] duration [1.8s], collections [1]/[2.5s], total [1.8s]/[1.6m], memory [251.6mb]->[251.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [162.9mb]->[162.9mb]/[2gb]}{[survivor] [8.7mb]->[8mb]/[0b]}
[2022-03-26T07:20:09,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][332] overhead, spent [1.8s] collecting in the last [2.5s]
[2022-03-26T07:20:09,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5438ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:20:28,080][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][341][42] duration [2s], collections [1]/[1.3s], total [2s]/[1.7m], memory [226.9mb]->[258.9mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [162.9mb]->[162.9mb]/[2gb]}{[survivor] [8mb]->[7.8mb]/[0b]}
[2022-03-26T07:20:28,706][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][341] overhead, spent [2s] collecting in the last [1.3s]
[2022-03-26T07:20:29,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5585ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:21:10,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:21:11,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5733605678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:21:11,004][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6934ms] which is above the warn threshold of [5s]
[2022-03-26T07:21:11,003][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][357][43] duration [3.1s], collections [1]/[3.5s], total [3.1s]/[1.7m], memory [254.7mb]->[254.7mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [162.9mb]->[162.9mb]/[2gb]}{[survivor] [7.8mb]->[10.2mb]/[0b]}
[2022-03-26T07:21:11,205][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][357] overhead, spent [3.1s] collecting in the last [3.5s]
[2022-03-26T07:21:11,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6533ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:21:23,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2324de62, interval=5s}] took [5110ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:21:25,313][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][361][44] duration [3.2s], collections [1]/[7.3s], total [3.2s]/[1.8m], memory [253.2mb]->[177.9mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [162.9mb]->[166.2mb]/[2gb]}{[survivor] [10.2mb]->[7.7mb]/[0b]}
[2022-03-26T07:21:26,451][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][361] overhead, spent [3.2s] collecting in the last [7.3s]
[2022-03-26T07:22:00,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@654a1f65] took [5169ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:22:33,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [23992ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:23:08,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6854ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:23:50,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [26680ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:24:09,885][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.1m/129173ms] ago, timed out [1.1m/66399ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1001]
[2022-03-26T07:24:34,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:24:54,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6807ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:24:59,027][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1001] timed out after [62774ms]
[2022-03-26T07:25:49,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27865ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:25:50,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [36048ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:25:52,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27865033570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:25:56,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:25:57,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6904782732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:25:59,440][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] creating index, cause [rollover_data_stream], templates [.deprecation-indexing-template], shards [1]/[1]
[2022-03-26T07:26:04,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@36d744c5] took [8380ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:26:23,653][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][377][45] duration [14.6s], collections [1]/[1m], total [14.6s]/[2m], memory [257.9mb]->[177.7mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [166.2mb]->[166.2mb]/[2gb]}{[survivor] [7.7mb]->[7.4mb]/[0b]}
[2022-03-26T07:26:26,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [13809ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:26:32,086][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1106] timed out after [26242ms]
[2022-03-26T07:26:44,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [10547ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:26:51,656][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [46.9s/46998ms] ago, timed out [20.7s/20756ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1106]
[2022-03-26T07:27:03,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:27:17,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [8004ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:27:46,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9007ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:27:58,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:27:57,831][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1142] timed out after [24816ms]
[2022-03-26T07:28:08,167][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [37.6s/37625ms] ago, timed out [12.8s/12809ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1142]
[2022-03-26T07:28:09,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [7205ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:28:22,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6803ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:28:36,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6671ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:28:56,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@52ca9ede] took [14470ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:28:56,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:28:58,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11268434075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:29:02,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5430ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:29:43,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:29:44,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3a8ccb31, interval=5s}] took [18233ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:29:45,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17432950141ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:29:49,295][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][390][46] duration [12.3s], collections [1]/[24.6s], total [12.3s]/[2.2m], memory [257.7mb]->[173.7mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [166.2mb]->[166.2mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-26T07:29:49,294][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] updating number_of_replicas to [0] for indices [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002]
[2022-03-26T07:29:51,203][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][390] overhead, spent [12.3s] collecting in the last [24.6s]
[2022-03-26T07:29:51,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [8728ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:30:06,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@1d0fb64c] took [9006ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:30:36,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5632ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:31:12,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5246ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:31:18,142][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.3m/738182ms] to compute cluster state update for [rollover_index source [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] to target [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002]], which exceeds the warn threshold of [10s]
[2022-03-26T07:31:25,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5566ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:31:35,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [8091ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:31:49,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5785ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:32:01,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5602ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:32:11,716][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [22.5s] publication of cluster state version [3472] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:32:13,077][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1315] timed out after [24287ms]
[2022-03-26T07:32:22,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9004ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:32:47,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [13741ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:33:46,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:33:48,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41032340869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:33:51,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:33:53,455][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][409][47] duration [32.1s], collections [1]/[27.5s], total [32.1s]/[2.7m], memory [257.7mb]->[257.7mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [166.2mb]->[166.2mb]/[2gb]}{[survivor] [7.4mb]->[7.4mb]/[0b]}
[2022-03-26T07:33:53,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6204377457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:33:58,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:33:58,496][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][409] overhead, spent [32.1s] collecting in the last [27.5s]
[2022-03-26T07:34:01,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [57730ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:34:02,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6813512603ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:04,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:10,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6222985209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:11,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@58464c5] took [6222ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:34:11,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:14,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7088520997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:17,040][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5043ms] which is above the warn threshold of [5s]
[2022-03-26T07:34:17,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:20,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5043710209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:24,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:27,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7170371574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:31,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:33,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [7232ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:34:35,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7232489619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:38,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:40,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6309118423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:34:40,540][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1370] timed out after [32844ms]
[2022-03-26T07:34:49,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [11924ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:34:47,868][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [39.7s/39731ms] ago, timed out [6.8s/6887ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1370]
[2022-03-26T07:35:10,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [12022ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:35:44,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:35:52,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:35:58,341][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1404] timed out after [26820ms]
[2022-03-26T07:36:09,448][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [38.5s/38525ms] ago, timed out [11.7s/11705ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1404]
[2022-03-26T07:36:09,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [10704ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:36:22,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:36:35,548][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.8m/291415ms] ago, timed out [4.4m/267128ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1315]
[2022-03-26T07:36:36,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:36:52,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@7f251ab1] took [5403ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:36:52,271][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [115551ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [2] indices and skipped [27] unchanged indices
[2022-03-26T07:37:05,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [11144ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:37:18,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:37:28,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5327ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:37:24,275][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1434] timed out after [29877ms]
[2022-03-26T07:37:28,782][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [35.8s/35804ms] ago, timed out [5.9s/5927ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1434]
[2022-03-26T07:37:36,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:37:51,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [8804ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:38:01,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5175ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:38:12,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5558ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:38:21,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@8bf7917] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:38:33,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5478ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:38:50,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [9552ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:39:30,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:39:31,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37241136312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:39:45,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][429][48] duration [29.7s], collections [1]/[58.2s], total [29.7s]/[3.2m], memory [257.7mb]->[190.2mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [166.2mb]->[166.2mb]/[2gb]}{[survivor] [7.4mb]->[8mb]/[0b]}
[2022-03-26T07:39:48,336][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][429] overhead, spent [29.7s] collecting in the last [58.2s]
[2022-03-26T07:39:50,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [11205ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:40:05,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:40:40,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5002ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:41:03,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:41:22,406][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=121, version=3472}] took [4.2m] which is above the warn threshold of [30s]: [running task [Publication{term=121, version=3472}]] took [135ms], [connecting to new nodes] took [652ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@ebed2d1] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68154492] took [184817ms], [org.elasticsearch.script.ScriptService@5c5c0db8] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [1ms], [org.elasticsearch.snapshots.RestoreService@f36d607] took [0ms], [org.elasticsearch.ingest.IngestService@4dc532a7] took [278ms], [org.elasticsearch.action.ingest.IngestActionForwarder@71c8d76a] took [86ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d7740@33bf5699] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@a802807] took [1ms], [org.elasticsearch.tasks.TaskManager@6e3473fa] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@59f09ad1] took [99ms], [org.elasticsearch.cluster.InternalClusterInfoService@588efbbb] took [54ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@22fabf86] took [13ms], [org.elasticsearch.indices.SystemIndexManager@783a0c4f] took [2095ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@521458b7] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x000000080130ee58@37d27d8d] took [529ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4df7bd65] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7c4cc338] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801408c08@796240e1] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@7b9a36c6] took [66ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@108f95af] took [33966ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@61685f7c] took [54ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1351775d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@306159f4] took [7706ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@29dd9588] took [499ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2b7ff798] took [125ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@330c8635] took [9302ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [12405ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@918c759] took [363ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4cac6227] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@bcabad] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@70ce1e88] took [100ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@415e345f] took [14ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@714dcd21] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@360a610f] took [151ms], [org.elasticsearch.node.ResponseCollectorService@646575d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@65842e7f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@7abfc32b] took [14ms], [org.elasticsearch.shutdown.PluginShutdownService@1fd8d76a] took [14ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@618bed45] took [13ms], [org.elasticsearch.indices.store.IndicesStore@69114a6] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@75a47474] took [0ms], [org.elasticsearch.license.LicenseService@2ac70849] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1aa23b90] took [12ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1a9f4da] took [0ms], [org.elasticsearch.gateway.GatewayService@51b4899b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1f4695b7] took [0ms]
[2022-03-26T07:41:23,109][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][440][49] duration [1.8s], collections [1]/[4.7s], total [1.8s]/[3.3m], memory [238.2mb]->[175mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [166.2mb]->[166.2mb]/[2gb]}{[survivor] [8mb]->[8.7mb]/[0b]}
[2022-03-26T07:41:23,307][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][440] overhead, spent [1.8s] collecting in the last [4.7s]
[2022-03-26T07:41:27,196][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [27.7m/1667853ms] which is longer than the warn threshold of [300000ms]; there are currently [10] pending tasks, the oldest of which has age [22.4m/1349468ms]
[2022-03-26T07:41:29,904][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:41:30,824][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:41:34,360][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][448] overhead, spent [273ms] collecting in the last [1s]
[2022-03-26T07:41:35,145][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002][0]]]).
[2022-03-26T07:41:39,728][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:41:41,092][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:41:43,858][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [null] to [{"phase":"new","action":"complete","name":"complete"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:41:43,893][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] to [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:41:44,384][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:41:44,866][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:41:45,291][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"new","action":"complete","name":"complete"}] to [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:41:45,313][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] to [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:41:45,358][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] to [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:41:46,203][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] to [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:41:46,291][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] to [{"phase":"hot","action":"complete","name":"complete"}] in policy [.deprecation-indexing-ilm-policy]
[2022-03-26T07:42:16,367][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-26T07:42:29,444][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][483][55] duration [768ms], collections [1]/[1.5s], total [768ms]/[3.3m], memory [245.5mb]->[257.5mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [166.6mb]->[166.7mb]/[2gb]}{[survivor] [6.8mb]->[8.5mb]/[0b]}
[2022-03-26T07:42:30,416][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][483] overhead, spent [768ms] collecting in the last [1.5s]
[2022-03-26T07:42:39,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [5564ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:42:47,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@c872bd7] took [6014ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:42:57,522][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=121, version=3483}] took [32.5s] which is above the warn threshold of [30s]: [running task [Publication{term=121, version=3483}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@ebed2d1] took [34ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68154492] took [847ms], [org.elasticsearch.script.ScriptService@5c5c0db8] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [0ms], [org.elasticsearch.snapshots.RestoreService@f36d607] took [0ms], [org.elasticsearch.ingest.IngestService@4dc532a7] took [63ms], [org.elasticsearch.action.ingest.IngestActionForwarder@71c8d76a] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d7740@33bf5699] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@a802807] took [0ms], [org.elasticsearch.tasks.TaskManager@6e3473fa] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@59f09ad1] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@588efbbb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@22fabf86] took [0ms], [org.elasticsearch.indices.SystemIndexManager@783a0c4f] took [92ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@521458b7] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x000000080130ee58@37d27d8d] took [33ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4df7bd65] took [12ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7c4cc338] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801408c08@796240e1] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@7b9a36c6] took [6ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@108f95af] took [13260ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@61685f7c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1351775d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@306159f4] took [6338ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@29dd9588] took [172ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2b7ff798] took [96ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@330c8635] took [4070ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [570ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@918c759] took [2296ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4cac6227] took [14ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@bcabad] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@70ce1e88] took [2636ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@415e345f] took [1173ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@714dcd21] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@360a610f] took [361ms], [org.elasticsearch.node.ResponseCollectorService@646575d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@65842e7f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@7abfc32b] took [10ms], [org.elasticsearch.shutdown.PluginShutdownService@1fd8d76a] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@618bed45] took [14ms], [org.elasticsearch.indices.store.IndicesStore@69114a6] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@75a47474] took [0ms], [org.elasticsearch.license.LicenseService@2ac70849] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1aa23b90] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1a9f4da] took [0ms], [org.elasticsearch.gateway.GatewayService@51b4899b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1f4695b7] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.bulk.TransportShardBulkAction$1@42b9b5a9]]]] took [226ms]
[2022-03-26T07:43:06,302][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1727] timed out after [15775ms]
[2022-03-26T07:43:21,723][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T07:43:22,554][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [35.3s/35306ms] ago, timed out [19.5s/19531ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1727]
[2022-03-26T07:43:22,838][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.3s/20390ms] to compute cluster state update for [put-mapping [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg][_doc]], which exceeds the warn threshold of [10s]
[2022-03-26T07:43:49,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:43:49,985][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][503][56] duration [4.3s], collections [1]/[5.6s], total [4.3s]/[3.4m], memory [223.3mb]->[235.3mb]/[2gb], all_pools {[young] [52mb]->[84mb]/[0b]}{[old] [166.7mb]->[166.7mb]/[2gb]}{[survivor] [8.5mb]->[8.5mb]/[0b]}
[2022-03-26T07:43:50,273][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][503] overhead, spent [4.3s] collecting in the last [5.6s]
[2022-03-26T07:43:51,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5664707263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:43:51,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [7834ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:43:55,325][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [29035ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [28] unchanged indices
[2022-03-26T07:43:57,370][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [33.7s] publication of cluster state version [3484] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T07:43:58,569][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1787] timed out after [16008ms]
[2022-03-26T07:52:43,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/508189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:53:02,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/508188692827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:53:19,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [546684ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:53:18,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:53:37,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36821213276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:53:51,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:53:59,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2324de62, interval=5s}] took [32234ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:54:07,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32234878728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:54:22,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:54:40,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30793100785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:55:02,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39561ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:55:19,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39561054583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:55:41,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35022ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:56:14,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35021599055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:56:42,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:57:05,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65082492962ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:57:28,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:57:52,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@34142d28] took [46869ms] which is above the warn threshold of [5000ms]
[2022-03-26T07:57:53,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46869353280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:58:12,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:58:26,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43169985866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:58:39,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:58:54,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28310149508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:59:11,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:58:45,737][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28310ms] which is above the warn threshold of [5s]
[2022-03-26T07:59:27,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31207936143ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T07:59:42,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T07:59:59,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31312133682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:00:11,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [60749ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:00:10,071][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=121, version=3484}] took [15.6m] which is above the warn threshold of [30s]: [running task [Publication{term=121, version=3484}]] took [69ms], [connecting to new nodes] took [151ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@ebed2d1] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68154492] took [5315ms], [org.elasticsearch.script.ScriptService@5c5c0db8] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [0ms], [org.elasticsearch.snapshots.RestoreService@f36d607] took [0ms], [org.elasticsearch.ingest.IngestService@4dc532a7] took [608ms], [org.elasticsearch.action.ingest.IngestActionForwarder@71c8d76a] took [68ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d7740@33bf5699] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@a802807] took [54ms], [org.elasticsearch.tasks.TaskManager@6e3473fa] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@59f09ad1] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@588efbbb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@22fabf86] took [16ms], [org.elasticsearch.indices.SystemIndexManager@783a0c4f] took [816ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@521458b7] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x000000080130ee58@37d27d8d] took [151ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4df7bd65] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7c4cc338] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801408c08@796240e1] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@7b9a36c6] took [23ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@108f95af] took [656889ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@61685f7c] took [4ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1351775d] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@306159f4] took [99261ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@29dd9588] took [2089ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2b7ff798] took [1148ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@330c8635] took [79022ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [9127ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@918c759] took [42226ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4cac6227] took [380ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@bcabad] took [323ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@70ce1e88] took [41448ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@415e345f] took [16626ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@714dcd21] took [75ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@360a610f] took [4804ms], [org.elasticsearch.node.ResponseCollectorService@646575d] took [244ms], [org.elasticsearch.snapshots.SnapshotShardsService@65842e7f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@7abfc32b] took [72ms], [org.elasticsearch.shutdown.PluginShutdownService@1fd8d76a] took [403ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@618bed45] took [361ms], [org.elasticsearch.indices.store.IndicesStore@69114a6] took [285ms], [org.elasticsearch.persistent.PersistentTasksNodeService@75a47474] took [73ms], [org.elasticsearch.license.LicenseService@2ac70849] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1aa23b90] took [144ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1a9f4da] took [0ms], [org.elasticsearch.gateway.GatewayService@51b4899b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1f4695b7] took [83ms]
[2022-03-26T08:00:10,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:00:26,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29437657975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:00:36,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:00:41,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3a8ccb31, interval=5s}] took [26028ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:00:54,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26028903841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:01:11,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:01:12,620][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.5s/20530ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@aad4533e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2f59229c], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4ce4d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@681435f5]], which exceeds the warn threshold of [10s]
[2022-03-26T08:01:30,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35023054128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:01:59,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.7s/41709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:02:51,716][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.2m/313067ms] ago, timed out [0s/0ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1822]
[2022-03-26T08:02:38,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.7s/41708295779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:03:10,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:03:23,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75352055542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:03:28,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [75352ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:03:39,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28791ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:03:22,030][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1822] timed out after [313067ms]
[2022-03-26T08:03:59,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28791271607ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:04:13,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:03:46,280][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1823] timed out after [266198ms]
[2022-03-26T08:04:27,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35001679214ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:04:42,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:04:56,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28257203091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:05:08,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:04:52,526][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28258ms] which is above the warn threshold of [5s]
[2022-03-26T08:05:22,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26983935803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:05:36,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:05:58,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27370789426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:06:13,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:06:32,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36803930863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:06:33,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [36803ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:06:48,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:07:06,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34679814372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:07:27,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:07:40,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38615579339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:07:49,606][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [23.7m/1423214ms] ago, timed out [23.4m/1407206ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1787]
[2022-03-26T08:07:56,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29522ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:08:17,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29521866619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:08:31,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:08:53,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35306978255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:09:08,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:09:23,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36302607487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:09:36,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:10:03,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27229840963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:10:18,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:10:22,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [71007ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:15:07,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43777287058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:15:31,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:15:37,261][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [313272ms] which is above the warn threshold of [5s]
[2022-03-26T08:15:38,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313271613960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:15:48,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:15:55,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16357829694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:15:41,822][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [713123ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [28] unchanged indices
[2022-03-26T08:15:52,149][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1843] timed out after [472246ms]
[2022-03-26T08:16:02,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:16:06,490][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.2m] publication of cluster state version [3485] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T08:16:10,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14934104368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:16:18,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:16:07,920][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1846] timed out after [472246ms]
[2022-03-26T08:16:29,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15423135079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:16:46,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:17:04,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27796005661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:17:29,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:18:00,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34108045469ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:18:15,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:18:33,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55373841221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:18:37,969][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][515][57] duration [4.2m], collections [1]/[7.1m], total [4.2m]/[7.6m], memory [257mb]->[179.8mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [166.8mb]->[167.8mb]/[2gb]}{[survivor] [10.2mb]->[8mb]/[0b]}
[2022-03-26T08:18:54,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:19:00,639][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][515] overhead, spent [4.2m] collecting in the last [7.1m]
[2022-03-26T08:19:06,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39080714847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:19:12,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [156358ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:19:21,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:19:36,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26264408310ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:19:58,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@3719a5d2] took [33935ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:19:56,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:20:12,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33935129557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:20:23,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:20:34,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28799057077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:20:44,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:20:59,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22011495405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:21:05,048][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [12.8m/769972ms] ago, timed out [4.9m/297726ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1843]
[2022-03-26T08:21:12,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@13556185] took [27305ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:21:13,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:21:26,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27305092739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:21:40,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27646ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:21:50,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27646206267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:22:03,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:22:15,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21785883320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:22:14,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [49432ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:22:25,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:22:37,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23812673554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:22:48,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:22:59,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22068517967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:23:10,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:23:22,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21393177231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:23:36,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:23:47,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26287376994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:23:24,620][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1878] timed out after [144011ms]
[2022-03-26T08:24:02,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:24:10,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [53058ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:24:15,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26770875666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:24:28,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:24:31,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2324de62, interval=5s}] took [25784ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:24:40,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25784413565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:24:37,379][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=121, version=3485}] took [8.1m] which is above the warn threshold of [30s]: [running task [Publication{term=121, version=3485}]] took [339ms], [connecting to new nodes] took [2340ms], [applying settings] took [80ms], [org.elasticsearch.repositories.RepositoriesService@ebed2d1] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@68154492] took [164312ms], [org.elasticsearch.script.ScriptService@5c5c0db8] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [0ms], [org.elasticsearch.snapshots.RestoreService@f36d607] took [0ms], [org.elasticsearch.ingest.IngestService@4dc532a7] took [10624ms], [org.elasticsearch.action.ingest.IngestActionForwarder@71c8d76a] took [435ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d7740@33bf5699] took [303ms], [org.elasticsearch.indices.TimestampFieldMapperService@a802807] took [737ms], [org.elasticsearch.tasks.TaskManager@6e3473fa] took [169ms], [org.elasticsearch.snapshots.SnapshotsService@59f09ad1] took [100ms], [org.elasticsearch.cluster.InternalClusterInfoService@588efbbb] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@22fabf86] took [294ms], [org.elasticsearch.indices.SystemIndexManager@783a0c4f] took [10870ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@521458b7] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x000000080130ee58@37d27d8d] took [1673ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@4df7bd65] took [461ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7c4cc338] took [186ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801408c08@796240e1] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@7b9a36c6] took [83ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@108f95af] took [149707ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@61685f7c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1351775d] took [75ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@306159f4] took [24896ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@29dd9588] took [1065ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2b7ff798] took [964ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@330c8635] took [25715ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7766aa60] took [3553ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@918c759] took [28680ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4cac6227] took [311ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@bcabad] took [72ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@70ce1e88] took [31187ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@415e345f] took [16988ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@714dcd21] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@360a610f] took [3722ms], [org.elasticsearch.node.ResponseCollectorService@646575d] took [252ms], [org.elasticsearch.snapshots.SnapshotShardsService@65842e7f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@7abfc32b] took [222ms], [org.elasticsearch.shutdown.PluginShutdownService@1fd8d76a] took [154ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@618bed45] took [316ms], [org.elasticsearch.indices.store.IndicesStore@69114a6] took [289ms], [org.elasticsearch.persistent.PersistentTasksNodeService@75a47474] took [215ms], [org.elasticsearch.license.LicenseService@2ac70849] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1aa23b90] took [84ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1a9f4da] took [0ms], [org.elasticsearch.gateway.GatewayService@51b4899b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1f4695b7] took [0ms]
[2022-03-26T08:24:26,381][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1879] timed out after [116706ms]
[2022-03-26T08:24:55,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25748ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:25:11,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25747756791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:25:27,455][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [27.4m/1646151ms] ago, timed out [22.9m/1379953ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{J-jsF4MwSOSM_0ouvZHieA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1823]
[2022-03-26T08:25:29,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:25:38,728][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.5s/14545ms] to notify listeners on successful publication of cluster state (version: 3485, uuid: YW1Mj0iZSHuCyQSxx3bg0Q) for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@aad4533e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2f59229c], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4ce4d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@681435f5]], which exceeds the warn threshold of [10s]
[2022-03-26T08:25:45,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35136636663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:25:55,353][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [24.8m/1491820ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [28m/1685261ms]
[2022-03-26T08:25:59,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:26:14,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29962098417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:26:55,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:27:08,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55150329733ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:26:28,526][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [29962ms] which is above the warn threshold of [5s]
[2022-03-26T08:27:21,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26195ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:27:27,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@253d6925, interval=1s}] took [81346ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:27:31,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26195692146ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:27:43,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21536ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:27:49,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2324de62, interval=5s}] took [21535ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:27:54,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21535344479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:28:04,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:28:18,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21883200466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:28:36,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T08:28:44,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5092/0x00000008017defb8@55dd332a] took [32617ms] which is above the warn threshold of [5000ms]
[2022-03-26T08:28:48,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32617224989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T08:29:02,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:23:50,596][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T10:23:50,627][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T10:23:50,630][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T10:24:03,921][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T10:24:03,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T10:24:03,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T10:24:03,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T10:24:03,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T10:24:03,949][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T10:24:03,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T10:24:03,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T10:24:03,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T10:24:03,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T10:24:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T10:24:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T10:24:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T10:24:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T10:24:03,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T10:24:03,979][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T10:24:03,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T10:24:03,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T10:24:03,984][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T10:24:03,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T10:24:03,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T10:24:03,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T10:24:04,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T10:24:04,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T10:24:04,035][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T10:24:04,049][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T10:24:04,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T10:24:04,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T10:24:04,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T10:24:04,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T10:24:04,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T10:24:04,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T10:24:04,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T10:24:04,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T10:24:04,093][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T10:24:04,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T10:24:04,095][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T10:24:04,097][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T10:24:04,101][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T10:24:04,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T10:24:04,109][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T10:24:04,112][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T10:24:04,114][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T10:24:04,115][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T10:24:04,116][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T10:24:04,118][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T10:24:04,119][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T10:24:04,120][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T10:24:04,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T10:24:04,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T10:24:04,124][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T10:24:04,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T10:24:04,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T10:24:04,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T10:24:04,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T10:24:04,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T10:24:04,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T10:24:04,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T10:24:04,143][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T10:24:04,345][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.7gb], net total_space [125.8gb], types [ext4]
[2022-03-26T10:24:04,349][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T10:24:05,204][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T10:24:23,051][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T10:24:23,064][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-26T10:24:23,066][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T10:24:23,070][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-26T10:24:23,072][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T10:24:23,074][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-26T10:24:23,076][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-26T10:24:23,078][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T10:24:23,079][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-26T10:24:23,083][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-26T10:24:23,084][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-26T10:24:23,085][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T10:24:23,087][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T10:24:23,089][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-26T10:24:23,090][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T10:24:25,082][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T10:24:25,336][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T10:24:27,891][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T10:25:48,566][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T10:25:48,580][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T10:25:48,855][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T10:25:48,864][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T10:25:49,248][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T10:25:52,212][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T10:25:52,404][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t3ftSv1TQa-bKSYcskTdrg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 122, version: 3486, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t3ftSv1TQa-bKSYcskTdrg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T10:25:52,642][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t3ftSv1TQa-bKSYcskTdrg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 122, version: 3486, reason: Publication{term=122, version=3486}
[2022-03-26T10:25:52,846][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T10:25:52,847][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T10:25:55,033][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T10:25:55,080][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [29] indices into cluster_state
[2022-03-26T10:26:28,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [8233ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:26:42,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:28:07,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6974346426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:28:17,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108626ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:28:21,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5d6b4b3e, interval=5s}] took [108626ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:28:27,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108626253475ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:28:39,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:28:47,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21330027501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:28:59,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:29:11,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20266037111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:29:39,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39923ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:29:49,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39923315895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:29:55,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [60189ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:30:01,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:30:12,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21856200893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:30:25,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:30:37,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23544098574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:30:49,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:30:59,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24026718209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:31:12,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24104ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:31:18,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24103457988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:31:27,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:31:40,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14869717710ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:31:49,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22104ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:31:57,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22104209233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:04,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:10,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14507964511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:18,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:21,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14284305377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:27,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:33,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10130359393ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:43,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14626ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:51,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14625650241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:32:59,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:03,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16093643303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:10,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11705ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:16,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11705703011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:21,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:26,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11090456706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:31,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10142ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:35,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10142375925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:41,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:44,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10102344270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:48,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:53,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7514169659ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:33:50,390][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [469870ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [29] unchanged indices
[2022-03-26T10:33:57,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:02,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8300288123ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:07,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:09,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9686104118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:13,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6346ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:09,675][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [8.1m] publication of cluster state version [3489] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t3ftSv1TQa-bKSYcskTdrg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-26T10:34:16,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6346335539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:19,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:22,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5857751911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:26,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6913ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:31,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6912653026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:37,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:45,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10796016860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:54,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:34:58,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16379545261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:06,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:13,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12562951984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:29,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:36,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23131337993ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:43,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:50,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14409913744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:35:56,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:03,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13093121250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:10,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:16,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13503663725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:23,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:31,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12689353705ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:37,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:43,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14627003019ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:48,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:36:55,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10721125277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:00,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:06,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12097352778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:11,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:15,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10918925387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:20,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:23,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9136010842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:37:42,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:38:08,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20784167139ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:38:19,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:38:28,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37181885243ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:38:39,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:39:01,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19911596122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:39:07,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:39:13,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28873042854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:39:18,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:39:24,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11374925133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:39:34,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:40:16,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15963338016ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:40:25,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.8s/49869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:40:31,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.8s/49869062497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:40:44,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:40:55,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17304589233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:40:59,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18354ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:03,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18354639420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:06,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:09,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6881289163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:11,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:14,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5397106319ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:18,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:22,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5727965165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:26,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8454ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:29,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8453895362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:42,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:46,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16370817337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:53,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:41:58,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10658466209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:03,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:10,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10047264487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:15,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:19,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12253126163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:22,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:25,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7388902764ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:31,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:35,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7219039931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:39,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9282ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:44,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9281804861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:48,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8840ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:53,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8839747717ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:42:54,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e3730@7af4502b] took [768151ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:42:56,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7803ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:00,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7802618000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:05,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:31,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8579946499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:37,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:44,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32365053522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:50,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:43:57,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12124017232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:10,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:10,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@504de2c3, interval=1m}] took [20680ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:44:14,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20680026382ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:19,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:24,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9051243620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:29,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [9051ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:44:31,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:35,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11943133741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:39,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8567ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:27,881][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9052ms] which is above the warn threshold of [5s]
[2022-03-26T10:44:43,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8566633680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:48,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:53,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8253598672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:44:59,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5d6b4b3e, interval=5s}] took [11152ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:44:59,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:45:08,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11152703955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:45:15,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:45:24,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15993962859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:45:35,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:45:39,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [18620ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:45:46,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18620312823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:45:57,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22845ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:46:08,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22845273723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:46:21,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:46:26,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5d6b4b3e, interval=5s}] took [24812ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:46:31,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24812933938ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:46:40,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:46:40,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@504de2c3, interval=1m}] took [18931ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:46:53,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18931781719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:06,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:15,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26096019711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:19,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [26096ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:47:21,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:28,271][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [46] timed out after [119366ms]
[2022-03-26T10:47:27,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14782859365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:31,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:37,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10737868112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:27,789][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T10:47:43,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11939ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:48,108][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T10:47:50,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11938786424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:58,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:47:55,273][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=122, version=3489}] took [13m] which is above the warn threshold of [30s]: [running task [Publication{term=122, version=3489}]] took [158ms], [connecting to new nodes] took [1135ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@18449599] took [97ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@47b4efb8] took [409111ms], [org.elasticsearch.script.ScriptService@13e972f9] took [1ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5d2f661a] took [212ms], [org.elasticsearch.snapshots.RestoreService@617bc87b] took [0ms], [org.elasticsearch.ingest.IngestService@1d222c3] took [790ms], [org.elasticsearch.action.ingest.IngestActionForwarder@59f4e6ba] took [63ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016c9320@7f4d4428] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@40ca73aa] took [62ms], [org.elasticsearch.tasks.TaskManager@509e854a] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@2ea168d5] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@5ebd9fe4] took [8085ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@20b02eb6] took [533ms], [org.elasticsearch.indices.SystemIndexManager@769e07c2] took [4361ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@17d601cd] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@616abb78] took [11120ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@1afd390] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@32a18574] took [1ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@5717c1f6] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@6c7f7a2e] took [263ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@7e91b3f0] took [75313ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@e4d1cff] took [56ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@762624fe] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@10ea6421] took [43817ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@17f0b41d] took [831ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@4aba88dc] took [681ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@dfc44a1] took [26602ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5d2f661a] took [737ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4e74f253] took [17248ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@5cb6e7dc] took [62ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@446d6cdc] took [1ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@21bcc08f] took [14508ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@1a0f7192] took [6971ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@10cf9c2c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@3ed0109e] took [1539ms], [org.elasticsearch.node.ResponseCollectorService@5fb26537] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@10b4b67] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@6dc98996] took [42ms], [org.elasticsearch.shutdown.PluginShutdownService@53a33051] took [48ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@3be9d55b] took [155ms], [org.elasticsearch.indices.store.IndicesStore@5b648162] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6354090] took [150988ms], [org.elasticsearch.license.LicenseService@6f8296f4] took [147ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@652438e7] took [232ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@520d89ae] took [364ms], [org.elasticsearch.gateway.GatewayService@15f3d93b] took [20ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@5662796] took [0ms]
[2022-03-26T10:48:03,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14487542207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:03,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [14487ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:48:10,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:15,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11909279406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:23,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:29,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12598478816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:39,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:41,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [28886ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:48:49,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16288489201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:48:53,047][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.8s/35839ms] to notify listeners on successful publication of cluster state (version: 3489, uuid: l-I-5hJ1TKWl1qLR2Awl6Q) for [reassign persistent tasks], which exceeds the warn threshold of [10s]
[2022-03-26T10:48:55,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:01,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15819624035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:06,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:11,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11126014678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:25,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:31,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19573164940ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:37,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:43,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12012658487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:51,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:49:57,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13629949799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:05,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:11,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12907021650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:21,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16361ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:32,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16361505547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:40,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20040ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:46,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20039848148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:52,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:50:56,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11578698772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:00,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:04,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8422390883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:10,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e3730@3e3bb723] took [125651ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:51:11,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:18,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10951164185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:24,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:30,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [13053ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:51:31,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13053326423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:43,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17341ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:51:53,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17341346776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:52:25,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.3s/42300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:52:46,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.3s/42300010916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:53:08,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:53:23,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42468347671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:53:41,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:53:56,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.4s/33495395721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:54:12,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:54:27,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30326100353ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:54:49,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37591ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:55:20,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [67916ms] which is above the warn threshold of [5000ms]
[2022-03-26T10:55:25,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37590774327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:54:24,021][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30326ms] which is above the warn threshold of [5s]
[2022-03-26T10:56:05,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:55:21,781][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11.3m/681969ms] ago, timed out [9.3m/562603ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t3ftSv1TQa-bKSYcskTdrg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [46]
[2022-03-26T10:56:27,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71002824525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:56:57,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.3s/52319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:57:18,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.3s/52319169522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:57:42,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:58:02,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46789758024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:58:33,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:58:58,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49438352260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:59:19,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T10:59:40,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49183228112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T10:59:47,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [98621ms] which is above the warn threshold of [5000ms]
[2022-03-26T11:00:05,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:00:29,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46898680025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:00:54,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:01:10,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48279174009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:01:24,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:01:41,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31226959322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:01:56,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32408ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:02:11,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32408477114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:02:25,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:02:41,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28840893676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:02:58,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:03:17,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33284724064ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:03:35,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:03:50,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36649982767ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:04:04,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29480ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:04:18,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29480050881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:04:30,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:04:41,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26208104353ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:04:51,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:05:16,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20997989995ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:05:29,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:05:39,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36931409554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:05:53,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:06:05,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24330940994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:06:19,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26147ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:06:30,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26147458829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:06:38,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e3730@7e24ee8f] took [374786ms] which is above the warn threshold of [5000ms]
[2022-03-26T11:06:39,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:06:47,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21068122590ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:06:55,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:07:00,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15311096489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:07:09,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3c5b56d1, interval=5s}] took [14400ms] which is above the warn threshold of [5000ms]
[2022-03-26T11:07:09,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:07:14,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14400934460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:07:26,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:07:27,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5d6b4b3e, interval=5s}] took [14148ms] which is above the warn threshold of [5000ms]
[2022-03-26T11:07:42,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14148400787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:07:56,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:08:11,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30391456886ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:08:29,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:08:47,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33171188186ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:09:03,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:09:25,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35580813781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:08:47,035][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [33171ms] which is above the warn threshold of [5s]
[2022-03-26T11:09:40,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36653ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:09:45,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@575f9587, interval=1s}] took [72234ms] which is above the warn threshold of [5000ms]
[2022-03-26T11:09:59,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36653255812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:10:20,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:10:38,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38887020061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:10:58,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:11:23,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36940070416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:11:43,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.3s/47362ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:12:51,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.3s/47361717489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:17:44,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353823ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:25:19,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353822476453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:29:18,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5m/694066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:32:55,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5m/693609000993ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:36:06,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/409460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:39:04,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/409570305163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:41:09,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:42:02,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306359030663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T11:43:59,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T11:47:38,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166846439555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:07:35,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428642ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:11:13,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428479193574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:14:53,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7m/1422385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:18:17,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7m/1422285670560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:22:05,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/430962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:25:28,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431033767209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:29:00,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/409070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:32:31,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408887559620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:35:54,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:39:24,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422429206632ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:42:47,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/412752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:46:41,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/412727766442ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:46:40,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017e3730@524ceb1e] took [5540352ms] which is above the warn threshold of [5000ms]
[2022-03-26T12:51:47,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T12:52:17,298][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.4h/5290086ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e5d89159]], which exceeds the warn threshold of [10s]
[2022-03-26T12:54:39,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/525793296433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T12:57:44,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:00:24,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372087760915ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:00:27,607][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.6m/156940ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e5d89159]], which exceeds the warn threshold of [10s]
[2022-03-26T13:03:06,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:05:44,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323231033587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:06:10,673][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.3m/81153ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6a5d60b7], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@42c90c68], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a3187410]], which exceeds the warn threshold of [10s]
[2022-03-26T13:08:49,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:09:54,356][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.8s/21849ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6a5d60b7], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@42c90c68], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a3187410]], which exceeds the warn threshold of [10s]
[2022-03-26T13:11:50,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333964004221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:14:39,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:17:30,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360451529213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:20:23,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342606ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:23:11,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342491246166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:26:20,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/355729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:27:23,391][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [5.5m/333964ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [57.4m/3448135ms]
[2022-03-26T13:30:03,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/355588620890ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:32:12,795][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [23.2m/1392496ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [1h/3672093ms]
[2022-03-26T13:38:48,822][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-26T13:38:48,885][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-26T13:38:48,886][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-26T13:38:57,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-26T13:38:57,111][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-26T13:38:57,112][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-26T13:38:57,113][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-26T13:38:57,113][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-26T13:38:57,114][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-26T13:38:57,114][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-26T13:38:57,115][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-26T13:38:57,116][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-26T13:38:57,116][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-26T13:38:57,117][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-26T13:38:57,118][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-26T13:38:57,118][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-26T13:38:57,119][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-26T13:38:57,120][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-26T13:38:57,120][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-26T13:38:57,121][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-26T13:38:57,121][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-26T13:38:57,122][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-26T13:38:57,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-26T13:38:57,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-26T13:38:57,123][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-26T13:38:57,124][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-26T13:38:57,124][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-26T13:38:57,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-26T13:38:57,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-26T13:38:57,125][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-26T13:38:57,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-26T13:38:57,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-26T13:38:57,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-26T13:38:57,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-26T13:38:57,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-26T13:38:57,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-26T13:38:57,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-26T13:38:57,129][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-26T13:38:57,129][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-26T13:38:57,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-26T13:38:57,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-26T13:38:57,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-26T13:38:57,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-26T13:38:57,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-26T13:38:57,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-26T13:38:57,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-26T13:38:57,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-26T13:38:57,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-26T13:38:57,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-26T13:38:57,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-26T13:38:57,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-26T13:38:57,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-26T13:38:57,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-26T13:38:57,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-26T13:38:57,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-26T13:38:57,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-26T13:38:57,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-26T13:38:57,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-26T13:38:57,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-26T13:38:57,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-26T13:38:57,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-26T13:38:57,143][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-26T13:38:57,349][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.7gb], net total_space [125.8gb], types [ext4]
[2022-03-26T13:38:57,354][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-26T13:38:58,228][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-26T13:42:03,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [5491ms] which is above the warn threshold of [5000ms]
[2022-03-26T13:43:13,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5851ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:44:36,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5850876978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:44:47,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:44:52,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122810036106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:01,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:05,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12928574719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:10,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9801ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:16,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9801313280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:26,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:26,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [15577ms] which is above the warn threshold of [5000ms]
[2022-03-26T13:45:29,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15577170007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:34,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:35,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [8367ms] which is above the warn threshold of [5000ms]
[2022-03-26T13:45:37,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8367531395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:40,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T13:45:41,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6132538936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T13:54:27,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [5143ms] which is above the warn threshold of [5000ms]
[2022-03-26T13:57:14,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [5130ms] which is above the warn threshold of [5000ms]
[2022-03-26T13:58:43,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [6272ms] which is above the warn threshold of [5000ms]
[2022-03-26T13:59:23,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7368a3b7, interval=30s}] took [6412ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:00:05,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [6620ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:01:13,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c19cbdf, interval=1m}] took [6777ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:02:43,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7368a3b7, interval=30s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:03:55,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [5548ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:04:59,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7368a3b7, interval=30s}] took [9099ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:06:47,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [5882ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:07:26,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:11:59,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:12:23,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c19cbdf, interval=1m}] took [275966ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:16:06,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275966789081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:21:12,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/544526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:21:18,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [544525ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:27:14,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/544525573880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:28:49,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:28:49,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483717833502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:29:11,274][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-26T14:29:11,281][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-26T14:29:14,030][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-26T14:29:14,326][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-26T14:29:16,323][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-26T14:29:18,221][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-26T14:29:18,223][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-26T14:29:18,319][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-26T14:29:18,321][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-26T14:29:18,801][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-26T14:29:24,806][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-26T14:29:25,043][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WLLrCzihQD2WApsd9e3Znw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 123, version: 3490, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WLLrCzihQD2WApsd9e3Znw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-26T14:29:25,508][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WLLrCzihQD2WApsd9e3Znw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 123, version: 3490, reason: Publication{term=123, version=3490}
[2022-03-26T14:29:25,727][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-26T14:29:25,729][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-26T14:29:27,873][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-26T14:29:27,888][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [29] indices into cluster_state
[2022-03-26T14:29:31,832][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-26T14:29:31,842][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-26T14:29:34,409][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][16] overhead, spent [280ms] collecting in the last [1s]
[2022-03-26T14:29:34,680][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-26T14:29:36,549][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-26T14:29:38,618][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-26T14:29:38,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-26T14:29:38,663][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-26T14:29:39,438][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][21] overhead, spent [423ms] collecting in the last [1s]
[2022-03-26T14:29:40,170][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-26T14:29:46,718][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-26T14:29:47,127][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-26T14:30:01,681][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002/FOWGJKYJT02TQ7PmDdYqPQ] update_mapping [_doc]
[2022-03-26T14:30:03,040][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.26][0]]]).
[2022-03-26T14:30:04,324][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-26T14:31:29,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:31:30,008][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:31:56,308][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:31:58,341][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:31:58,766][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:31:59,147][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:31:59,596][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:00,049][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:00,379][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:01,431][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:01,719][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:02,398][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:02,881][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:03,210][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:03,566][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:04,103][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:04,218][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:04,362][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:04,508][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:04,841][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:05,111][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:05,308][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:05,685][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:06,037][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:06,616][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:06,821][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:09,078][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:09,812][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:10,165][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:10,432][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:10,613][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:16,406][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:19,719][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:20,134][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:27,091][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:27,791][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:28,207][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.26/HPB6fZf2RRqxlSCxE9k2Cg] update_mapping [_doc]
[2022-03-26T14:32:49,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2e569cb, interval=1s}] took [13350ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:33:15,394][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46660}] took [6147ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:34:40,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2e569cb, interval=1s}] took [51841ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:36:27,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7361e3c9, interval=5s}] took [81682ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:37:30,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@348a79f8, interval=1m}] took [6028ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:37:34,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:37:42,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6028230925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:37:50,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:01,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20438482845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:07,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:16,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2e569cb, interval=1s}] took [37341ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:38:18,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16902888673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:26,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:05,594][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4m/240376ms] ago, timed out [3.7m/223867ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{WLLrCzihQD2WApsd9e3Znw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1902]
[2022-03-26T14:38:30,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [19638ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:38:32,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19638491000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:46,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:38:52,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19134857173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:00,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14191ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:10,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14191318346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:18,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:08,951][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14191ms] which is above the warn threshold of [5s]
[2022-03-26T14:39:31,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18344473656ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:37,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:42,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18525071770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:52,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:39:59,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15148123756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:40:06,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:40:22,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12609260149ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:40:00,769][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1902] timed out after [16509ms]
[2022-03-26T14:40:37,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:40:39,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2e569cb, interval=1s}] took [45060ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:40:49,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32451587676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:40:58,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21538ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:41:01,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [21537ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:41:04,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21537481571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:41:11,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:41:24,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12088985075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:41:40,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:41:52,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29605321421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:42:07,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:41:51,022][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46654}] took [29605ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:42:15,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25381457327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:42:39,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:42:46,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33081407287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:42:53,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:43:01,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14007403191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:43:10,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:43:20,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17126142369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:43:28,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:54:25,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18375294370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:54:38,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/669413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:54:58,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/669413306087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:55:52,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:57:01,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73401155001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:58:06,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135001ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:58:20,239][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43138}] took [208401ms] which is above the warn threshold of [5000ms]
[2022-03-26T14:58:30,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135000053985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:58:59,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:59:18,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53430252486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T14:59:34,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T14:59:51,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34329507136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:00:02,205][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:43154}] took [34329ms] which is above the warn threshold of [5000ms]
[2022-03-26T15:00:05,653][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][231][32] duration [10.2m], collections [1]/[3.3m], total [10.2m]/[10.3m], memory [1.3gb]->[1.3gb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [146.9mb]->[146.9mb]/[2gb]}{[survivor] [26mb]->[24mb]/[0b]}
[2022-03-26T15:00:10,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35191ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:01:15,212][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][231] overhead, spent [10.2m] collecting in the last [3.3m]
[2022-03-26T15:01:17,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35190782277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:01:51,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2e569cb, interval=1s}] took [1019140ms] which is above the warn threshold of [5000ms]
[2022-03-26T15:01:57,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:02:14,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107563532242ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:02:38,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:03:51,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40310004335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:04:26,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1c7f8c76, interval=5s}] took [40310ms] which is above the warn threshold of [5000ms]
[2022-03-26T15:04:32,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/115023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:04:50,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/115023623850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:04:58,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26040ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:05:07,975][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/config%3A7.17.0][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46648}] took [26039ms] which is above the warn threshold of [5000ms]
[2022-03-26T15:05:10,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26039392979ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:05:19,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:05:28,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21426771142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:05:37,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15803ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:06:35,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15802151946ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:06:54,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:06:50,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017dc000@256cdea0] took [37228ms] which is above the warn threshold of [5000ms]
[2022-03-26T15:07:30,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77898431217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:06:50,133][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [15802ms] which is above the warn threshold of [5s]
[2022-03-26T15:07:53,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:08:03,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59940870673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:08:14,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:08:25,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20382871503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:08:45,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:09:41,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30135115317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:10:19,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:10:35,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73578202482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:12:04,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:14:33,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125907967737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:16:14,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:18:18,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/248578035056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:20:11,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:21:46,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/237153569988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:23:36,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/205626ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:24:41,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/205763637754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:28:58,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-26T15:30:36,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321543557696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-26T15:32:24,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/206316ms] on absolute clock which is above the warn threshold of [5000ms]
