[2022-04-04T16:08:07,309][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-04T16:08:07,331][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-04T16:08:07,331][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-04T16:08:14,761][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-04T16:08:14,762][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-04T16:08:14,763][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-04T16:08:14,763][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-04T16:08:14,764][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-04T16:08:14,764][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-04T16:08:14,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-04T16:08:14,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-04T16:08:14,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-04T16:08:14,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-04T16:08:14,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-04T16:08:14,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-04T16:08:14,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-04T16:08:14,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-04T16:08:14,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-04T16:08:14,770][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-04T16:08:14,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-04T16:08:14,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-04T16:08:14,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-04T16:08:14,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-04T16:08:14,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-04T16:08:14,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-04T16:08:14,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-04T16:08:14,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-04T16:08:14,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-04T16:08:14,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-04T16:08:14,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-04T16:08:14,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-04T16:08:14,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-04T16:08:14,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-04T16:08:14,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-04T16:08:14,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-04T16:08:14,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-04T16:08:14,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-04T16:08:14,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-04T16:08:14,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-04T16:08:14,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-04T16:08:14,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-04T16:08:14,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-04T16:08:14,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-04T16:08:14,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-04T16:08:14,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-04T16:08:14,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-04T16:08:14,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-04T16:08:14,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-04T16:08:14,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-04T16:08:14,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-04T16:08:14,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-04T16:08:14,792][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-04T16:08:14,792][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-04T16:08:14,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-04T16:08:14,793][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-04T16:08:14,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-04T16:08:14,794][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-04T16:08:14,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-04T16:08:14,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-04T16:08:14,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-04T16:08:14,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-04T16:08:14,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-04T16:08:14,888][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.6gb], net total_space [125.8gb], types [ext4]
[2022-04-04T16:08:14,889][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-04T16:08:15,192][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-04T16:08:26,100][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-04T16:08:26,104][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-04T16:08:27,051][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-04T16:08:27,162][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-04T16:08:27,862][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-04T16:08:28,632][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-04T16:08:28,633][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-04T16:08:28,670][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-04T16:08:28,672][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-04T16:08:28,884][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-04T16:08:30,971][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-04T16:08:31,103][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 201, version: 7170, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-04T16:08:31,269][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 201, version: 7170, reason: Publication{term=201, version=7170}
[2022-04-04T16:08:31,377][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-04T16:08:31,378][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-04T16:08:32,300][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-04T16:08:32,306][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [37] indices into cluster_state
[2022-04-04T16:08:33,014][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-04T16:08:33,015][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-04T16:08:33,629][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-04T16:08:34,081][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-04T16:08:34,084][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-04T16:08:34,090][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-04T16:08:34,830][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T16:08:34,904][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T16:08:36,548][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-04T16:08:36,577][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-04T16:08:36,589][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-04T16:08:36,965][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T16:08:36,989][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T16:08:37,890][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T16:08:39,592][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-04-04T16:08:41,313][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-04T16:08:41,325][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-04T16:08:41,327][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-04T16:08:41,976][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-04T16:08:41,999][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-04T16:08:42,167][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T16:08:42,169][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T16:08:42,805][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T16:08:42,806][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T16:08:52,684][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-04T16:08:52,870][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-04T16:09:36,267][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 871 finished with response BulkByScrollResponse[took=585.6ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-04T16:09:39,061][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 891 finished with response BulkByScrollResponse[took=3s,timed_out=false,sliceId=null,updated=1028,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-04T16:09:46,349][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-04T16:10:17,352][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.04] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-04T16:10:17,615][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.04][0]]]).
[2022-04-04T16:10:17,941][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,037][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,047][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,063][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:18,320][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:33,766][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:36,995][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:47,732][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:48,014][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:48,173][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:49,378][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:10:50,810][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:06,547][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:06,737][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:07,856][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:08,039][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:32,023][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:32,107][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:33,046][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:33,125][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,075][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,173][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,184][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,305][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:34,375][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:40,638][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:11:56,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:14:08,831][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:16:03,249][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:16:18,264][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,059][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,280][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,401][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,480][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,543][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:49,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:17:50,448][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:19:49,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:21:40,555][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:22:15,587][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:28:00,585][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:18,785][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:18,884][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:18,960][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:19,065][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:33:19,089][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:36:34,190][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T16:37:11,846][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_all/_settings?expand_wildcards=open%2Cclosed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:40528}] took [7401ms] which is above the warn threshold of [5000ms]
[2022-04-04T16:38:34,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [5931ms] which is above the warn threshold of [5000ms]
[2022-04-04T16:39:22,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:43:07,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9741431358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:43:35,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:44:41,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269455526717ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:45:12,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:46:43,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101901173230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:48:47,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:50:00,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216264378815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:50:20,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:50:37,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93096028806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:51:51,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:52:06,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90380217583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:52:17,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:53:05,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26716455966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:53:35,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:55:13,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76659168792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:53:44,171][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26717ms] which is above the warn threshold of [5s]
[2022-04-04T16:55:54,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:56:16,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139879703406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:56:35,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:56:57,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38828504887ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:16,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:33,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42976796494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:46,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:57:57,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30737680945ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:58:17,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:58:32,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30514317768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T16:58:47,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30356ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T16:59:19,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30355938894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:00:53,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:17,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126193552490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:16,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [126193ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:01:20,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [27982ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:01:20,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:26,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27982990803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:30,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:01:44,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9475555569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:09,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:14,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [38493ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:17,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38493638085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:39,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:12,912][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38493ms] which is above the warn threshold of [5s]
[2022-04-04T17:02:43,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30554748514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:44,651][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cluster/state/metadata/.apm-agent-configuration,.apm-custom-link,.async-search,.kibana_7.16.2_001,.kibana_7.17.0_001,.kibana_task_manager_7.16.2_001,.kibana_task_manager_7.17.0_001,.tasks,logstash-1970.01.01,logstash-2022.03.13,logstash-2022.03.14,logstash-2022.03.15,logstash-2022.03.16,logstash-2022.03.17,logstash-2022.03.18,logstash-2022.03.19,logstash-2022.03.20,logstash-2022.03.21,logstash-2022.03.22,logstash-2022.03.23,logstash-2022.03.24,logstash-2022.03.25,logstash-2022.03.26,logstash-2022.03.27,logstash-2022.03.28,logstash-2022.03.29,logstash-2022.03.30,logstash-2022.03.31,logstash-2022.04.01,logstash-2022.04.02,logstash-2022.04.03,logstash-2022.04.04][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:40528}] took [1522956ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:45,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:46,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6118231182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:56,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:56,845][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:44774}] took [11125ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:57,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [6229ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:02:58,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6229365615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:02:49,782][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [23.7m/1426584ms] ago, timed out [1.4m/84642ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13137]
[2022-04-04T17:03:07,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:22,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [11108ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:03:22,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11108278030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:26,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [17876ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:03:25,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:29,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17876592731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:33,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:37,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8023543515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:52,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:56,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19229398331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:03:56,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [19229ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:01,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@451d2502, interval=1m}] took [8668ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:01,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:06,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8668875824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:10,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:10,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [9264ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:12,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9264159509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:03,289][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.apm-agent-configuration,.apm-custom-link,.async-search,.kibana_7.16.2_001,.kibana_7.17.0_001,.kibana_task_manager_7.16.2_001,.kibana_task_manager_7.17.0_001,.tasks,logstash-1970.01.01,logstash-2022.03.13,logstash-2022.03.14,logstash-2022.03.15,logstash-2022.03.16,logstash-2022.03.17,logstash-2022.03.18,logstash-2022.03.19,logstash-2022.03.20,logstash-2022.03.21,logstash-2022.03.22,logstash-2022.03.23,logstash-2022.03.24,logstash-2022.03.25,logstash-2022.03.26,logstash-2022.03.27,logstash-2022.03.28,logstash-2022.03.29,logstash-2022.03.30,logstash-2022.03.31,logstash-2022.04.01,logstash-2022.04.02,logstash-2022.04.03,logstash-2022.04.04/_stats/store,docs][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.1:40540}] took [8669ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:12,711][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13137] timed out after [1341942ms]
[2022-04-04T17:04:34,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:38,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [11966ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:04:38,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9364396127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:04:57,359][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:57,822][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:58,000][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:58,009][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:58,482][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:59,176][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:04:59,305][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:05:05,312][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:05:05,895][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T17:05:25,076][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1768][36] duration [1.1s], collections [1]/[2.8s], total [1.1s]/[2.4s], memory [1.3gb]->[207.9mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [197.7mb]->[197.7mb]/[2gb]}{[survivor] [5mb]->[10.1mb]/[0b]}
[2022-04-04T17:05:26,084][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1768] overhead, spent [1.1s] collecting in the last [2.8s]
[2022-04-04T17:05:27,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [9578ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:05:29,868][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1770] overhead, spent [646ms] collecting in the last [1.1s]
[2022-04-04T17:05:32,316][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1771][38] duration [1s], collections [1]/[2.6s], total [1s]/[4.1s], memory [220.6mb]->[207.2mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [2.6mb]->[1.3mb]/[0b]}
[2022-04-04T17:05:32,612][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1771] overhead, spent [1s] collecting in the last [2.6s]
[2022-04-04T17:05:41,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [6030ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:06:13,893][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1778][39] duration [2.2s], collections [1]/[3.5s], total [2.2s]/[6.4s], memory [295.2mb]->[210.4mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [1.3mb]->[4.4mb]/[0b]}
[2022-04-04T17:06:15,061][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1778] overhead, spent [2.2s] collecting in the last [3.5s]
[2022-04-04T17:06:25,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [5423ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:07:12,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [28004ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:11:22,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@19ed9812] took [188780ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:12:25,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:12:24,787][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45584}] took [22415ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:11:43,633][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45580}] took [12526ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:12:53,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10746530555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:05,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.9s/47956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:18,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.9s/47956186367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:32,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:40,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26980603717ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:13:52,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:09,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19565791434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:10,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [19565ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:13:36,615][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45582}] took [26980ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:14:21,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:38,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29145893445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:15:31,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:14:23,070][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [29146ms] which is above the warn threshold of [5s]
[2022-04-04T17:15:51,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68637139438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:16:07,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:16:50,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37189761097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:17:48,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101536ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:18:02,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [138725ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:18:17,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101535969927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:19:08,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:20:22,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78630037614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:21:08,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:23:33,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119489165939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:23:51,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/164774ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:24:08,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/164774132379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:10,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:17,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43649999898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:27,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:26:35,972][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33118}] took [439911ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:26:36,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111998260661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:27:11,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.2s/44244ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:27:52,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.2s/44244655187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:01,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:06,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50195498363ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:08,560][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:33110}] took [50195ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:28:11,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:04,047][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13628] timed out after [915415ms]
[2022-04-04T17:28:13,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984729677ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:29,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:36,640][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:45586}] took [18634ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:28:42,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18633661913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:28:54,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:29:26,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25447638287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:29:46,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52s/52099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:29:52,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@451d2502, interval=1m}] took [52099ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:29:59,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52s/52099364439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:30:13,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:30:28,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26513162573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:30:56,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.6s/41680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:19,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.6s/41679339936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:36,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:09,897][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [41679ms] which is above the warn threshold of [5s]
[2022-04-04T17:31:40,287][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1783][40] duration [1m], collections [1]/[14.3m], total [1m]/[1.2m], memory [242.4mb]->[243.3mb]/[2gb], all_pools {[young] [36mb]->[32mb]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [4.4mb]->[5.3mb]/[0b]}
[2022-04-04T17:31:49,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40541648713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:31:58,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [82220ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:32:03,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:19,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28179574852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:34,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31142ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:44,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31141572596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:32:53,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:09,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19118258501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:21,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:29,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27054436662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:31,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@1e5c2a57] took [27054ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:33:34,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:43,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14356930488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:33:56,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:06,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20725969807ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:13,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:23,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17206508767ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:28,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [17206ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:34:31,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:36,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18678964660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:46,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:34:57,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14442756600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:06,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:09,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [19632ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:35:13,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19632716029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:25,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19434ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:38,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19434009833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:35:58,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:36:25,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33309814725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:36:36,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:36:40,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [71680ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:36:13,288][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [33310ms] which is above the warn threshold of [5s]
[2022-04-04T17:36:52,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38370449888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:18,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6m/941849ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:21,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6m/941848721409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:18,545][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13726] timed out after [85411ms]
[2022-04-04T17:52:24,290][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [41.2m/2472792ms] ago, timed out [25.9m/1557377ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13628]
[2022-04-04T17:52:23,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:24,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5536665331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:52:33,860][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1786][41] duration [14.9m], collections [1]/[16.5m], total [14.9m]/[16.1m], memory [259.3mb]->[258.8mb]/[2gb], all_pools {[young] [52mb]->[48mb]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [5.3mb]->[4.8mb]/[0b]}
[2022-04-04T17:52:36,760][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1786] overhead, spent [14.9m] collecting in the last [16.5m]
[2022-04-04T17:52:38,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [12807ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:52:39,326][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [19.3m/1158817ms] ago, timed out [17.8m/1073406ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13726]
[2022-04-04T17:53:28,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47287ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:38,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47287531328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:45,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:53,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17673451824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:53:40,367][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [47888ms] which is above the warn threshold of [5s]
[2022-04-04T17:54:01,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:04,675][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1787][42] duration [36.3s], collections [1]/[13.8s], total [36.3s]/[16.7m], memory [258.8mb]->[298.8mb]/[2gb], all_pools {[young] [48mb]->[0b]/[0b]}{[old] [205.9mb]->[205.9mb]/[2gb]}{[survivor] [4.8mb]->[4.7mb]/[0b]}
[2022-04-04T17:54:08,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16034082805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:15,228][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1787] overhead, spent [36.3s] collecting in the last [13.8s]
[2022-04-04T17:54:18,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:23,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [98252ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:54:26,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17057006150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:36,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:38,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [17900ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:54:45,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17900061666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:54:54,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:02,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18352383537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:12,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:19,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17293979736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:25,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:34,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13308033099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:42,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:55:56,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [16667ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:55:56,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16667219280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:06,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:18,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23932195655ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:37,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:56:56,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31708916148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:57:14,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35782ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:57:26,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35782489547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:57:49,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:06,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35320749451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:14,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [35320ms] which is above the warn threshold of [5000ms]
[2022-04-04T17:58:23,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34333ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:36,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34332731637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:58:47,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:01,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24063289769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:13,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26166903554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:31,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:09,156][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13842] timed out after [85616ms]
[2022-04-04T17:59:42,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18570769955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:58,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:00:10,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26403012443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T17:59:38,230][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44737ms] which is above the warn threshold of [5s]
[2022-04-04T18:00:25,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:00:42,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27322542441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:00:55,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:06,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [57214ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:01:10,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29891720142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:24,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:37,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28980055345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:01:54,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:06,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28866437565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:23,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:38,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29662917778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:54,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:02:56,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@4915e650] took [28813ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:03:06,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28813755871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:03:19,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:03:33,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27438881572ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:03:45,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25753ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:03,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25753250949ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:09,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [25753ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:04:16,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:28,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30833156211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:39,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:04:52,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23210481390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:06,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:20,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26711820219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:29,061][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.8m/593741ms] ago, timed out [8.4m/508125ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13842]
[2022-04-04T18:05:43,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:05:23,622][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26712ms] which is above the warn threshold of [5s]
[2022-04-04T18:06:11,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37035349720ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:19,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:27,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36881779119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:37,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:50,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17941337358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:06:57,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:08,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19997909080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@12b63aa8] took [23784ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:07:32,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23784077858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:40,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:49,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18515982589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:07:58,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:01,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [36188ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:08:05,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17672636680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:15,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:17,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@26d47e70, interval=5s}] took [17579ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:08:26,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17579313899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:37,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21554ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:40,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [21553ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:08:48,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21553878090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:08:57,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:12,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21031219501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:23,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:35,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25310303968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:09:51,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:02,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27758301906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:18,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:32,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27265617257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:10:53,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:11:15,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27121327091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:11:24,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [27121ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:11:33,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:11:48,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47787048396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:04,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30463ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:05,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67aa83d3, interval=5s}] took [30463ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:12:21,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30463017791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:39,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:23,899][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [14007] timed out after [173205ms]
[2022-04-04T18:12:54,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35018464866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:12:25,385][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30463ms] which is above the warn threshold of [5s]
[2022-04-04T18:13:20,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:13:36,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41017708560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:13:57,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:14:13,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37997529966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:14:33,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:14:58,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34758291466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:15:16,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:15:34,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43511446866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:15:40,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [43511ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:16:01,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:16:28,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43155212334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:16:48,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:17:10,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48165740181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:17:28,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:17:43,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40553338740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:00,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:17,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32401898022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:42,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:19:01,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42459924965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:18:59,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@72dbb473] took [42459ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:19:23,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:19:40,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40373965500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:03,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:25,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39104617058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:48,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:21:14,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40761932024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:21:45,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:20:41,471][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [13m/784360ms] ago, timed out [10.1m/611155ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{fQ3QzbzCT3q6oRtMWvm2JQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [14007]
[2022-04-04T18:22:05,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61643464930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:22:30,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:22:48,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.7s/45701637386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:23:17,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:23:36,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46352956835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:24:21,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:25:01,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63735067184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:25:47,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:26:04,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [79184ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:26:32,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79184662515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:27:47,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:28:45,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127215336118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:29:23,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:30:04,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96088589917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:31:04,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:32:14,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/99821815689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:33:01,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:34:05,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118319182518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:36:23,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/201398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:37:51,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/201397994181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:39:24,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/180532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:40:12,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/180532665910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:40:58,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:42:53,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88134721466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:43:37,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166231ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:44:01,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/166231049372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:44:34,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.2s/56225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:46:19,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.2s/56225232434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:46:49,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/130120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:47:14,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/130119701001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:47:34,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:48:35,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50427959823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:49:45,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:50:42,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126814198694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:50:39,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017ea020@4b82d07d] took [126814ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:51:37,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:52:07,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116046972337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:53:14,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:53:55,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96915002772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:54:33,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:55:20,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78325864501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:56:08,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:56:02,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@300797df, interval=1s}] took [78325ms] which is above the warn threshold of [5000ms]
[2022-04-04T18:57:06,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95624450037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T18:58:55,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T18:59:45,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165676870689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:01:51,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:03:57,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176378460972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:05:43,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232004ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:02:09,138][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [342055ms] which is above the warn threshold of [5s]
[2022-04-04T19:07:48,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231744533697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:10:05,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/261139ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:12:43,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260871492204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:15:19,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:18:31,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314523540219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:22:57,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:27:18,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456717467346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:32:52,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9m/595769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:37:19,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9m/595770995381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:41:21,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/509180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:45:01,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/509146794334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:47:55,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:49:50,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394408344863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:53:00,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T19:56:05,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304634990104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T19:58:45,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T20:01:31,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344855409527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T20:00:04,898][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [52s/52034ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@470bfe7f]], which exceeds the warn threshold of [10s]
[2022-04-04T20:04:58,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/356421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T20:07:57,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/356149644022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T20:07:59,303][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [27.6s/27666ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ea1b8370], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ea052ce], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a75b6627]], which exceeds the warn threshold of [10s]
[2022-04-04T20:10:57,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T20:11:34,298][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.8s/26895ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ea1b8370], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6ea052ce], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a75b6627]], which exceeds the warn threshold of [10s]
[2022-04-04T20:20:52,409][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-04T20:20:52,434][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-04T20:20:52,435][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-04T21:21:33,762][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-04T21:21:33,790][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-04T21:21:33,793][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-04T21:21:39,534][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-04T21:21:39,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-04T21:21:39,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-04T21:21:39,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-04T21:21:39,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-04T21:21:39,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-04T21:21:39,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-04T21:21:39,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-04T21:21:39,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-04T21:21:39,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-04T21:21:39,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-04T21:21:39,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-04T21:21:39,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-04T21:21:39,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-04T21:21:39,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-04T21:21:39,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-04T21:21:39,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-04T21:21:39,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-04T21:21:39,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-04T21:21:39,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-04T21:21:39,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-04T21:21:39,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-04T21:21:39,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-04T21:21:39,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-04T21:21:39,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-04T21:21:39,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-04T21:21:39,545][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-04T21:21:39,545][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-04T21:21:39,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-04T21:21:39,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-04T21:21:39,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-04T21:21:39,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-04T21:21:39,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-04T21:21:39,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-04T21:21:39,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-04T21:21:39,549][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-04T21:21:39,549][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-04T21:21:39,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-04T21:21:39,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-04T21:21:39,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-04T21:21:39,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-04T21:21:39,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-04T21:21:39,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-04T21:21:39,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-04T21:21:39,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-04T21:21:39,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-04T21:21:39,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-04T21:21:39,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-04T21:21:39,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-04T21:21:39,556][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-04T21:21:39,630][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-04T21:21:39,632][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-04T21:21:40,086][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-04T21:21:51,800][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-04T21:21:51,809][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-04T21:21:51,810][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T21:21:51,812][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-04T21:21:51,813][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-04T21:21:51,814][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-04T21:21:51,815][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-04T21:21:51,816][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-04T21:21:51,817][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-04T21:21:51,818][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-04T21:21:51,819][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-04T21:21:51,820][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T21:21:51,822][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T21:21:51,823][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-04T21:21:51,826][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-04T21:21:53,780][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-04T21:21:53,966][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-04T21:21:55,194][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-04T21:21:56,075][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-04T21:21:56,076][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-04T21:21:56,155][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-04T21:21:56,157][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-04T21:21:56,405][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-04T21:21:59,066][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-04T21:21:59,201][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 202, version: 7270, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-04T21:21:59,365][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 202, version: 7270, reason: Publication{term=202, version=7270}
[2022-04-04T21:21:59,494][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-04T21:21:59,495][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-04T21:22:00,811][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-04T21:22:00,820][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [38] indices into cluster_state
[2022-04-04T21:22:02,243][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-04T21:22:02,259][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-04T21:22:03,929][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-04T21:22:03,937][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-04T21:22:03,938][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-04T21:22:04,858][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-04T21:22:04,878][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-04T21:22:06,197][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][10] overhead, spent [339ms] collecting in the last [1s]
[2022-04-04T21:22:07,441][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-04T21:22:08,113][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-04T21:22:11,445][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [.kibana-event-log-7.16.2-000001][0]]]).
[2022-04-04T21:22:26,723][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:27,570][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:29,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:31,799][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:22:54,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5975ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:22:57,701][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16432ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [37] unchanged indices
[2022-04-04T21:23:06,431][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [30s] publication of cluster state version [7310] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-04T21:23:16,300][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:23:32,164][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:23:35,604][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:23:36,806][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:24:08,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5703ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:16,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [5087ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:17,412][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46120}] took [9088ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:17,412][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [9088ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:30,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5031ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:24:37,197][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:24:42,433][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:24:58,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@28cee0ee] took [5892ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:25:28,860][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.04/wEdvpaoYSXeRKrjiLgl3Ig] update_mapping [_doc]
[2022-04-04T21:28:27,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [22200ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:28:27,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21855ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:28:28,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21855099369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:29:13,544][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48840}] took [12270ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:30:43,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:45,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7977872114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:49,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:52,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7226746666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:30:58,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:01,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9246177365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:07,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:11,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8872781864ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:13,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:15,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6224912526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:31:16,311][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][325][18] duration [5.5s], collections [1]/[9.1s], total [5.5s]/[6.9s], memory [1.3gb]->[200.5mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [191.4mb]->[191.4mb]/[2gb]}{[survivor] [8.6mb]->[9mb]/[0b]}
[2022-04-04T21:31:26,960][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][325] overhead, spent [5.5s] collecting in the last [9.1s]
[2022-04-04T21:31:31,881][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [6262ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:31:36,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [62817ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:31:51,819][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [5928ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:03,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@15c7a3ae] took [10065ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:10,858][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [6261ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:09,720][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46124}] took [5260ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:20,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [8004ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:32:57,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:32:58,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [30962ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:02,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29761402099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:04,152][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [31367ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:04,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:05,637][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48854}] took [42274ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:05,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8375843755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:07,624][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8376ms] which is above the warn threshold of [5s]
[2022-04-04T21:33:26,075][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][327][19] duration [19.6s], collections [1]/[1m], total [19.6s]/[26.6s], memory [232.5mb]->[242.8mb]/[2gb], all_pools {[young] [36mb]->[40mb]/[0b]}{[old] [191.4mb]->[191.4mb]/[2gb]}{[survivor] [9mb]->[11.3mb]/[0b]}
[2022-04-04T21:33:32,396][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][327] overhead, spent [19.6s] collecting in the last [1m]
[2022-04-04T21:33:41,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:46,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [23412ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:51,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8468554913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:33:45,831][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [15207ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:33:54,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:00,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13079030213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:03,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9328ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:04,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [9328ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:06,599][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:34486}] took [22407ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:07,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9328044536ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:10,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@699488cc, interval=1m}] took [5385ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:09,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5386ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:19,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5385981144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:25,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:25,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@a2855c7] took [16079ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:32,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16079675490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:48,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:56,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [23376ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:34:57,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23376511838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:34:59,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:01,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10487589592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:00,377][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46078}] took [33864ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:35:17,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:17,919][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][329][20] duration [5.2s], collections [1]/[44.9s], total [5.2s]/[31.8s], memory [274.8mb]->[239mb]/[2gb], all_pools {[young] [72mb]->[72mb]/[0b]}{[old] [191.4mb]->[195.1mb]/[2gb]}{[survivor] [11.3mb]->[7.9mb]/[0b]}
[2022-04-04T21:35:17,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13487358761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:24,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:25,616][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][330][21] duration [4.3s], collections [1]/[1.1s], total [4.3s]/[36.2s], memory [239mb]->[287mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [7.9mb]->[7mb]/[0b]}
[2022-04-04T21:35:25,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5283815124ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:29,286][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][330] overhead, spent [4.3s] collecting in the last [1.1s]
[2022-04-04T21:35:39,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [13424ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:35:40,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:41,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7624588175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:35:45,231][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46204}] took [5150ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:35:57,446][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][338][22] duration [1s], collections [1]/[2.7s], total [1s]/[37.2s], memory [262.1mb]->[205.8mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [7mb]->[6.7mb]/[0b]}
[2022-04-04T21:35:58,098][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][338] overhead, spent [1s] collecting in the last [2.7s]
[2022-04-04T21:36:26,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@7c78f1e] took [5565ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:36:43,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:45,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16160869698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:46,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][350][23] duration [10.3s], collections [1]/[26.8s], total [10.3s]/[47.6s], memory [285.8mb]->[254.6mb]/[2gb], all_pools {[young] [84mb]->[52mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [6.7mb]->[7.5mb]/[0b]}
[2022-04-04T21:36:48,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:48,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5246687026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:36:48,954][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][350] overhead, spent [10.3s] collecting in the last [26.8s]
[2022-04-04T21:36:49,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5246ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:36:53,280][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][351][24] duration [897ms], collections [1]/[7s], total [897ms]/[48.5s], memory [254.6mb]->[271.1mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [7.5mb]->[8mb]/[0b]}
[2022-04-04T21:36:53,377][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [2036] timed out after [29279ms]
[2022-04-04T21:36:53,698][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2037] timed out after [26477ms]
[2022-04-04T21:36:58,189][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [31.9s/31998ms] ago, timed out [5.5s/5521ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2037]
[2022-04-04T21:36:58,101][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [34.8s/34800ms] ago, timed out [5.5s/5521ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2036]
[2022-04-04T21:37:40,142][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][382][27] duration [1.5s], collections [1]/[3.2s], total [1.5s]/[50.5s], memory [278.6mb]->[207.2mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [3.5mb]->[4.1mb]/[0b]}
[2022-04-04T21:37:41,511][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][382] overhead, spent [1.5s] collecting in the last [3.2s]
[2022-04-04T21:37:44,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7180ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:28,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@2c7d6a86] took [6651ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:40,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6617ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:40,502][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [7218ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:38:54,649][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [25.8s/25879ms] ago, timed out [2.5s/2547ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2206]
[2022-04-04T21:38:53,483][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2206] timed out after [23332ms]
[2022-04-04T21:39:18,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:39:32,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5309ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:39:43,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6533ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:07,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [18292ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:07,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:39:42,937][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6134ms] which is above the warn threshold of [5s]
[2022-04-04T21:40:07,788][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [18893ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:08,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16891727782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:40:09,110][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][399][28] duration [11.4s], collections [1]/[31.7s], total [11.4s]/[1m], memory [275.2mb]->[241.3mb]/[2gb], all_pools {[young] [76mb]->[64mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [4.1mb]->[6.2mb]/[0b]}
[2022-04-04T21:40:12,117][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][399] overhead, spent [11.4s] collecting in the last [31.7s]
[2022-04-04T21:40:14,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7403ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:40:25,368][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][401][29] duration [2s], collections [1]/[4.4s], total [2s]/[1m], memory [281.3mb]->[200.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [6.2mb]->[5.5mb]/[0b]}
[2022-04-04T21:40:25,706][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][401] overhead, spent [2s] collecting in the last [4.4s]
[2022-04-04T21:40:59,941][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][423][30] duration [1.4s], collections [1]/[3.1s], total [1.4s]/[1m], memory [280.6mb]->[203.8mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [5.5mb]->[4.7mb]/[0b]}
[2022-04-04T21:41:00,650][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][423] overhead, spent [1.4s] collecting in the last [3.1s]
[2022-04-04T21:42:17,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:20,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [14512ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:20,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11896490423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:24,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:24,619][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [12297ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:28,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8036769285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:34,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9619ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:37,747][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][447][31] duration [7.3s], collections [1]/[23.1s], total [7.3s]/[1.2m], memory [283.8mb]->[203.1mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [4.7mb]->[8mb]/[0b]}
[2022-04-04T21:42:41,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9619017568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:44,278][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][447] overhead, spent [7.3s] collecting in the last [23.1s]
[2022-04-04T21:42:47,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:50,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [31277ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:52,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13621711709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:55,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:42:59,421][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46122}] took [7696ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:42:59,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7695668594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:43:02,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@710cc29a] took [7451ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:43:02,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:43:05,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7451315763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:43:03,045][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7451ms] which is above the warn threshold of [5s]
[2022-04-04T21:43:12,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [9315ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:43:59,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:04,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37504541565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:08,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:09,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [9227ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:44:15,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9227140480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:21,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:25,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][449][32] duration [25.5s], collections [1]/[1.1m], total [25.5s]/[1.6m], memory [255.1mb]->[242.5mb]/[2gb], all_pools {[young] [56mb]->[56mb]/[0b]}{[old] [195.1mb]->[195.1mb]/[2gb]}{[survivor] [8mb]->[7.4mb]/[0b]}
[2022-04-04T21:44:27,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12392676343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:32,378][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][449] overhead, spent [25.5s] collecting in the last [1.1m]
[2022-04-04T21:44:35,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:39,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [26612ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:44:39,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14219764236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:44,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:45,828][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46204}] took [8662ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:44:49,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8661880585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:44:55,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:00,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11035757354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:14,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:14,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [18993ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:45:21,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18993924564ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:00,641][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2500] timed out after [100988ms]
[2022-04-04T21:45:31,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:37,497][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46204}] took [16224ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:45:40,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16224438112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:45:48,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [16224ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:45:50,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:46:53,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20305212381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:46:57,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:00,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67108307721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:46:58,398][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.7m/167548ms] ago, timed out [1.1m/66560ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2500]
[2022-04-04T21:47:00,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [67108ms] which is above the warn threshold of [5s]
[2022-04-04T21:47:02,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:04,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5116869710ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:07,041][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][451][33] duration [46.5s], collections [1]/[1.6m], total [46.5s]/[2.4m], memory [286.5mb]->[206.9mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [195.1mb]->[195.2mb]/[2gb]}{[survivor] [7.4mb]->[7.6mb]/[0b]}
[2022-04-04T21:47:09,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:14,072][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][451] overhead, spent [46.5s] collecting in the last [1.6m]
[2022-04-04T21:47:14,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6832855751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:21,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11949ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:47:21,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:29,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627329739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:33,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:33,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@4efe2eed] took [12182ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:47:34,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12182815116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:47:50,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6420ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:48:03,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5127ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:48:29,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:33,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22309355948ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:41,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:49,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12377372285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:48:55,525][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][455][34] duration [15.4s], collections [1]/[33.6s], total [15.4s]/[2.6m], memory [278.9mb]->[222.1mb]/[2gb], all_pools {[young] [76mb]->[20mb]/[0b]}{[old] [195.2mb]->[195.2mb]/[2gb]}{[survivor] [7.6mb]->[6.8mb]/[0b]}
[2022-04-04T21:49:01,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:04,683][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][455] overhead, spent [15.4s] collecting in the last [33.6s]
[2022-04-04T21:49:10,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19502549747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:14,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [31879ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:49:17,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:23,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17108805960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:33,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:40,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15032179324ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:48,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:49:59,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14359911396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:06,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:13,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19501640697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:23,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [14940ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:50:22,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:30,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14940888758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:41,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:24,127][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14941ms] which is above the warn threshold of [5s]
[2022-04-04T21:50:47,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20070632899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:50:58,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:04,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17371606502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:10,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [17371ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:50:58,984][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2661] timed out after [48802ms]
[2022-04-04T21:51:15,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:17,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15557537127ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:19,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:21,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5599444142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:51:26,152][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:34588}] took [5959ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:51:30,633][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/117971ms] ago, timed out [1.1m/69169ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2661]
[2022-04-04T21:51:48,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:52:46,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7993ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:52:29,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7753898691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:38,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [107846ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:53:38,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:53,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107846637336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:55,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20181ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:53:57,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20181937776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:01,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:05,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5465054091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:10,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@75186ca9] took [9502ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:10,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:12,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9502803814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:21,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:21,274][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][475][35] duration [4.6s], collections [1]/[2.5m], total [4.6s]/[2.7m], memory [278.1mb]->[282.1mb]/[2gb], all_pools {[young] [76mb]->[84mb]/[0b]}{[old] [195.2mb]->[195.2mb]/[2gb]}{[survivor] [6.8mb]->[6.8mb]/[0b]}
[2022-04-04T21:54:21,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10947405366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:54:21,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [10947ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:30,633][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][482][36] duration [944ms], collections [1]/[2s], total [944ms]/[2.7m], memory [248.7mb]->[199.7mb]/[2gb], all_pools {[young] [51.9mb]->[0b]/[0b]}{[old] [195.4mb]->[195.4mb]/[2gb]}{[survivor] [5.2mb]->[4.2mb]/[0b]}
[2022-04-04T21:54:30,887][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][482] overhead, spent [944ms] collecting in the last [2s]
[2022-04-04T21:54:46,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6947ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:56,503][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46244}] took [14566ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:56,503][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46240}] took [8721ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:54:59,560][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46242}] took [10709ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:04,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [5192ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:46,180][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46244}] took [7067ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:46,910][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46240}] took [7067ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:46,180][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46242}] took [10675ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:55:53,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [27894ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:56:28,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:56:32,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@6694988f] took [18189ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:56:46,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5007815499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:57:57,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T21:57:59,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90055161178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T21:57:59,216][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46244}] took [90055ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:58:00,687][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][490][38] duration [52.7s], collections [1]/[2.5m], total [52.7s]/[3.6m], memory [265.9mb]->[244.1mb]/[2gb], all_pools {[young] [64mb]->[40mb]/[0b]}{[old] [195.5mb]->[195.5mb]/[2gb]}{[survivor] [6.4mb]->[8.5mb]/[0b]}
[2022-04-04T21:58:02,239][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][490] overhead, spent [52.7s] collecting in the last [2.5m]
[2022-04-04T21:58:04,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [6314ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:58:15,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][496][39] duration [1.2s], collections [1]/[2.2s], total [1.2s]/[3.6m], memory [276.1mb]->[205.4mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [195.5mb]->[195.5mb]/[2gb]}{[survivor] [8.5mb]->[5.9mb]/[0b]}
[2022-04-04T21:58:15,831][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][496] overhead, spent [1.2s] collecting in the last [2.2s]
[2022-04-04T21:58:30,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7942ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:58:57,886][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46250}] took [9972ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:10,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5086ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:25,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@4fcd556d] took [8897ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:37,410][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46254}] took [8625ms] which is above the warn threshold of [5000ms]
[2022-04-04T21:59:58,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [14164ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:06,432][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46256}] took [12141ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:14,331][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [12098ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:24,414][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46250}] took [15542ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:00:43,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [29884ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:08,026][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46256}] took [14795ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:12,712][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [10201ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:10,766][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [15154ms] which is above the warn threshold of [5s]
[2022-04-04T22:01:35,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [33313ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:01:39,305][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [2974] timed out after [87008ms]
[2022-04-04T22:01:59,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [7770ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:02:54,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [9244ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:03:16,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@5c13cc89] took [8106ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:03:39,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11372ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:03:59,965][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [11044ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:04:42,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11959ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:04:51,128][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16706ms] which is above the warn threshold of [5s]
[2022-04-04T22:04:55,761][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46252}] took [6523ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:04:38,060][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3067] timed out after [74392ms]
[2022-04-04T22:05:09,716][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:46256}] took [14191ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:05:21,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [6113ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:05:40,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [9247ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:07:23,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:27,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [71954ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:07:29,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69764931177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:39,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:47,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16758865775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:07:58,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:04,866][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][508][40] duration [55.1s], collections [1]/[2.1m], total [55.1s]/[4.5m], memory [269.4mb]->[231.9mb]/[2gb], all_pools {[young] [72mb]->[28mb]/[0b]}{[old] [195.5mb]->[195.5mb]/[2gb]}{[survivor] [5.9mb]->[8.4mb]/[0b]}
[2022-04-04T22:08:07,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19099492570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:16,654][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][508] overhead, spent [55.1s] collecting in the last [2.1m]
[2022-04-04T22:08:19,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:24,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [56953ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:08:24,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21094733653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:31,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:40,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12148014411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:49,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@f49fd32] took [17846ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:08:49,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:51,537][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.1m/547127ms] ago, timed out [7.6m/460119ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [2974]
[2022-04-04T22:08:54,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17846164305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:08:59,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:02,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9908595250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:07,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:09,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@22b0b54, interval=30s}] took [8402ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:09:13,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8402753944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:19,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:24,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11751ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:09:25,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11751141849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:33,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:28,269][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11751ms] which is above the warn threshold of [5s]
[2022-04-04T22:09:43,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14075381371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:44,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [14075ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:09:49,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:53,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15547642529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:00,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:04,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10870218465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:09:57,965][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3147] timed out after [30063ms]
[2022-04-04T22:10:17,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:26,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16239333629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:37,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:38,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [36777ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:10:47,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20537715519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:10:59,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:11,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21503090877ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:20,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:29,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21794797389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:35,128][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.5m/150631ms] ago, timed out [2m/120568ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3147]
[2022-04-04T22:11:39,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:11:40,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@6888a970] took [18712ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:11:50,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18712842696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:04,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:12,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24659884278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:18,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [24659ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:12:19,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15877ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:28,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15877300346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:37,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:41,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@23f18c76] took [16246ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:12:45,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246309898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:12:59,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:11,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23227842713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:23,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19828766215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:41,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [22179ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:13:41,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:23,138][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19829ms] which is above the warn threshold of [5s]
[2022-04-04T22:13:47,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22179064702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:13:55,407][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [3193] timed out after [65236ms]
[2022-04-04T22:13:56,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15261459858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:12,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:19,352][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.6m/96212ms] ago, timed out [30.9s/30976ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3193]
[2022-04-04T22:14:22,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15715071633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:14:34,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:15:41,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [37844ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:15:44,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22129186581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:15:48,109][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3194] timed out after [65236ms]
[2022-04-04T22:15:55,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:05,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81111481528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:08,504][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [12.6m/759349ms] ago, timed out [11.4m/684957ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3067]
[2022-04-04T22:16:13,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:24,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18381155958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:34,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:43,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20959715292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:16:54,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19356ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:03,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19355907736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:14,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:26,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19721859001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:35,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:44,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21530937150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:17:46,551][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][515][41] duration [52.6s], collections [1]/[2.8m], total [52.6s]/[5.4m], memory [283.9mb]->[211.3mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [195.5mb]->[195.6mb]/[2gb]}{[survivor] [8.4mb]->[7.6mb]/[0b]}
[2022-04-04T22:17:55,949][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][515] overhead, spent [52.6s] collecting in the last [2.8m]
[2022-04-04T22:17:55,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:05,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [60510ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:18:09,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19257728093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:22,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:22,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [27450ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:18:32,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27450776175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:41,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:56,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19736962297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:08,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:18:44,074][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19737ms] which is above the warn threshold of [5s]
[2022-04-04T22:19:22,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@78f6c272] took [26059ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:19:22,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26059075172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:33,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25854ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:43,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25853904079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:19:56,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:09,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21916724990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:25,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:35,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29642521505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:45,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:20:51,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [50028ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:20:54,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20385664294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:05,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:15,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20053894360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:26,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:32,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21010499360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:39,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:20,834][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [3233] timed out after [117853ms]
[2022-04-04T22:21:46,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13272462839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:21:56,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:06,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16075109078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:16,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [16075ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:22:19,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:22,769][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.3m/560118ms] ago, timed out [8.2m/494882ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3194]
[2022-04-04T22:22:28,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23597941295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:10,223][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3234] timed out after [117853ms]
[2022-04-04T22:22:36,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:41,510][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/208969ms] ago, timed out [1.5m/91116ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3233]
[2022-04-04T22:22:46,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17159778182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:22:46,247][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17160ms] which is above the warn threshold of [5s]
[2022-04-04T22:23:00,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:06,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20260062255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:12,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:19,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16011784996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:23:20,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [16011ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:23:55,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:05,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41445972636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:19,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:23,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [25178ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:24:37,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25178135099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:51,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:24:50,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [30936ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:25:03,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30936518134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:25:14,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:25:28,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24468637524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:25:42,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:01,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27165321559ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:10,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:20,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207655369701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:31,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:33,485][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][519][42] duration [2.8m], collections [1]/[2.4m], total [2.8m]/[8.3m], memory [279.3mb]->[287.3mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [195.6mb]->[195.8mb]/[2gb]}{[survivor] [7.6mb]->[7.8mb]/[0b]}
[2022-04-04T22:29:38,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21555543113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:42,281][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][519] overhead, spent [2.8m] collecting in the last [2.4m]
[2022-04-04T22:29:50,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:29:51,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [273461ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:29:58,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17085261205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:09,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:19,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20740837273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:35,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25860ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:30:41,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@699488cc, interval=1m}] took [25860ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:30:54,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25860315536ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:31:13,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:31:33,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37454910155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:31:53,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:32:14,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40368561618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:32:30,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:32:45,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37640683062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:03,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:20,460][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.7m/465796ms] ago, timed out [30.2s/30269ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3317]
[2022-04-04T22:33:17,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30269580044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:34,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:50,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33332286844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:33:27,411][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30269ms] which is above the warn threshold of [5s]
[2022-04-04T22:34:06,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:34:25,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31379423861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:34:42,458][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.9m/897778ms] ago, timed out [12.9m/779925ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3234]
[2022-04-04T22:34:42,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:34:59,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36411097570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:35:16,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:35:19,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [70644ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:35:33,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34233472134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:35:11,387][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3317] timed out after [435527ms]
[2022-04-04T22:35:46,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:03,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30420920688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:18,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:31,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31679717438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:36:49,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:10,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30071183404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:19,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:27,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30907423774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:39,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:37:44,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [18366ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:37:48,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18366219967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:01,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22829ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:07,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22829045427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:15,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:22,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14900631560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:25,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@1c1fa413] took [14900ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:38:12,893][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [22829ms] which is above the warn threshold of [5s]
[2022-04-04T22:38:34,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:44,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18921253362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:38:56,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22547ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:39:14,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22546509987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:39:29,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:39:45,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [53601ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:39:48,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31055287066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:40:05,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:40:19,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.6s/37647648786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:40:45,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:02,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39674179728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:25,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:39,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38879229091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:41:58,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:13,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33888983880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:28,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:30,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [64486ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:42:41,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30597016728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:53,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:43:04,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24779594615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:43:18,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:42:57,220][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3400] timed out after [149844ms]
[2022-04-04T22:43:41,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24886054820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:43:55,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:44:09,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36901028175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:44:28,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:44:44,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33319325103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:03,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [33319ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:45:04,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:13,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34462188023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:26,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:35,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23664432958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:45,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:45:55,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17458558709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:04,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:13,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20676231077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:23,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:36,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18135343578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:45,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:46:51,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@1126dfed] took [22084ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:46:52,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22084088038ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:00,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:10,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15840895290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:18,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:24,567][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9m/542620ms] ago, timed out [6.5m/392776ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3400]
[2022-04-04T22:47:25,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [33043ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:47:26,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17202498208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:44,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:47:50,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [24120ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:48:04,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24120359859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:48:38,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:49:04,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53423123623ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:49:27,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:49:42,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52409280078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:50:03,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:50:55,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35498709367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:51:21,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:51:36,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [75637ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:51:53,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75637033503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:52:12,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:52:30,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52634908852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:52:54,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:03,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [41321ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:53:15,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41321054048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:36,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.8s/42895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:56,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.8s/42895440169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:53:17,393][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3520] timed out after [198494ms]
[2022-04-04T22:54:17,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:54:47,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39315723304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:54:18,687][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15s/15093ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3db32d6]], which exceeds the warn threshold of [10s]
[2022-04-04T22:55:19,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:55:47,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60969356159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:56:32,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:04,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70164925930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:24,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.5s/56558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:34,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.5s/56557642657ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:45,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:57:26,702][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [56558ms] which is above the warn threshold of [5s]
[2022-04-04T22:57:57,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22534452175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:08,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [22534ms] which is above the warn threshold of [5000ms]
[2022-04-04T22:58:08,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:19,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23318872903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:34,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:47,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25250698566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:58:57,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:06,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22956932481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:21,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:33,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24613723541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:35,470][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [12.6m/756665ms] ago, timed out [9.3m/558171ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3520]
[2022-04-04T22:59:43,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:55,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22135634804ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T22:59:55,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@446047b4] took [22135ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:00:04,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:16,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20948706466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:26,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:39,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21914790108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:00:58,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:01:12,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [29709ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:01:17,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29709342501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:01:37,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:01:39,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [40592ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:01:48,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40592948488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:00,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:17,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22187321731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:33,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33677ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:02:50,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33676567702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:03:13,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:03:37,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40660407227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:02,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:23,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48518255481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:41,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:04:56,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38897946006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:05:20,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:05:40,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [38215ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:05:46,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38215845809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:05:01,341][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38898ms] which is above the warn threshold of [5s]
[2022-04-04T23:06:23,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:06:24,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [59607ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:06:44,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59607993168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:07:09,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:06:51,121][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3640] timed out after [258208ms]
[2022-04-04T23:07:40,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48419132392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:08:28,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:09:00,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81223781150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:09:51,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:10:22,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79745094271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:10:51,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:11:13,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60189232509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:11:51,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:12:19,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62027755219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:12:41,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:12:56,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51262741400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:07,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:23,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26071253942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:39,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:13:52,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [31488ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:13:59,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31488761223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:23,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:39,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43501481654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:51,823][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.6m/878859ms] ago, timed out [10.3m/620651ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3640]
[2022-04-04T23:14:54,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:14:56,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31696952325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:15:00,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:15:01,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@8d28e73] took [5896ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:15:01,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5896063865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:15:00,439][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5896ms] which is above the warn threshold of [5s]
[2022-04-04T23:15:18,952][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3726] timed out after [17237ms]
[2022-04-04T23:16:26,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:31,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d094857, interval=5s}] took [61771ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:16:36,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59569760262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:46,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19367ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:51,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19366663933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:16:30,625][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [24.4s/24411ms] ago, timed out [7.1s/7174ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3726]
[2022-04-04T23:16:59,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:07,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13045409526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:19,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:32,538][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][536][43] duration [51.7s], collections [1]/[1.3m], total [51.7s]/[9.1m], memory [287.6mb]->[203.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [195.8mb]->[195.8mb]/[2gb]}{[survivor] [7.8mb]->[7.5mb]/[0b]}
[2022-04-04T23:17:43,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19934313273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:17:55,792][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][536] overhead, spent [51.7s] collecting in the last [1.3m]
[2022-04-04T23:17:58,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:05,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [91833ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:18:10,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39486621965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:19,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:30,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20679284443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:40,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:18:49,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20633272573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:02,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:14,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22651602337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:26,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:37,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@5cc22377] took [23685ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:19:39,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23685843549ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:19:48,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21685ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:00,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21685023969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:14,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:17,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [45882ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:19:54,583][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21685ms] which is above the warn threshold of [5s]
[2022-04-04T23:20:24,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24197756662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:32,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:40,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19115545240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:49,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:20:57,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18125060316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:05,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:04,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [15226ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:21:14,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15226825422ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:31,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:42,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26923812905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:21:53,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21977ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:00,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [21976ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:22:07,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21976737238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:15,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:23,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22012613530ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:34,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:45,554][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.9s/16997ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b6f8725]], which exceeds the warn threshold of [10s]
[2022-04-04T23:22:49,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300448360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:57,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:22:41,040][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3804] timed out after [98350ms]
[2022-04-04T23:23:11,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24059187678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:29,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:39,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30027684729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:54,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:23:56,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [56088ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:24:08,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26061026401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:22,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:43,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28488721009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:59,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:25:16,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36399531021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:24:45,949][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28488ms] which is above the warn threshold of [5s]
[2022-04-04T23:25:29,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:25:42,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30124124956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:25:58,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:11,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28860278981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:27,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:41,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29063809368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:26:39,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [57924ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:26:57,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:27:16,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27540803613ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:27:36,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39842ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:27:49,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39841339072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:11,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:36,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37017358838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:57,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:28:44,098][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.7m/525048ms] ago, timed out [7.1m/426698ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3804]
[2022-04-04T23:29:07,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46015049522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:29:27,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:29:38,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30104843637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:29:50,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:01,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22118185209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:10,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:22,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20632077155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:33,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [20632ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:30:35,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:45,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24820120030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:53,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:30:59,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18242684878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:05,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:09,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11995220568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:13,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:02,756][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3924] timed out after [220549ms]
[2022-04-04T23:31:02,766][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18243ms] which is above the warn threshold of [5s]
[2022-04-04T23:31:15,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [8644ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:17,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8644739260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:22,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:24,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [8706ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:27,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8706469542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:34,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:35,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [11833ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:39,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11833759835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:46,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:47,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [11923ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:31:52,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11923666599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:31:57,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:01,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10998244224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:05,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:06,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [7919ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:32:10,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7919322076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:12,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:12,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@774ccb83] took [7908ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:32:13,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7908052195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:32:18,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [5069ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:32:35,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [10709ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:34:36,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [26054ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:35:26,729][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.5m/510410ms] ago, timed out [4.8m/289861ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3924]
[2022-04-04T23:35:28,806][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [22313ms] which is above the warn threshold of [5s]
[2022-04-04T23:35:11,736][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [4044] timed out after [69297ms]
[2022-04-04T23:36:57,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [35247ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:37:17,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [6093ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:39:09,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@6888a970] took [26279ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:39:24,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:39:10,090][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.7m/405187ms] ago, timed out [5.5m/335890ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{xWR9qJP8QjCXpAw0N2A1ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [4044]
[2022-04-04T23:42:26,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5439868730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:44:16,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:45:36,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291876189532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:47:20,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:48:59,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184907767311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:49:48,668][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.4s/12412ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a6eab7c7]], which exceeds the warn threshold of [10s]
[2022-04-04T23:50:52,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:51:19,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211392600087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:51:42,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:51:57,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017e4fb8@43f594c2] took [260851ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:52:14,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49458794669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:52:58,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:53:32,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76251689235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:53:59,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:54:34,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@53eabb5a, interval=1s}] took [61740ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:54:53,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61740760667ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:55:37,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:56:08,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97277007972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:57:07,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-04T23:57:15,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2cb2e6aa, interval=5s}] took [91020ms] which is above the warn threshold of [5000ms]
[2022-04-04T23:58:12,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91020999537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-04T23:59:31,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:21:33,703][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T01:21:33,721][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T01:21:33,724][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T01:21:39,673][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T01:21:39,678][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T01:21:39,680][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T01:21:39,681][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T01:21:39,682][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T01:21:39,682][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T01:21:39,683][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T01:21:39,684][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T01:21:39,684][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T01:21:39,685][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T01:21:39,685][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T01:21:39,686][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T01:21:39,686][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T01:21:39,687][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T01:21:39,687][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T01:21:39,688][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T01:21:39,688][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T01:21:39,689][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T01:21:39,689][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T01:21:39,690][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T01:21:39,690][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T01:21:39,691][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T01:21:39,691][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T01:21:39,691][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T01:21:39,692][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T01:21:39,692][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T01:21:39,692][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T01:21:39,693][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T01:21:39,693][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T01:21:39,693][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T01:21:39,694][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T01:21:39,694][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T01:21:39,694][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T01:21:39,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T01:21:39,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T01:21:39,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T01:21:39,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T01:21:39,696][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T01:21:39,696][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T01:21:39,697][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T01:21:39,697][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T01:21:39,697][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T01:21:39,698][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T01:21:39,698][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T01:21:39,699][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T01:21:39,699][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T01:21:39,699][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T01:21:39,700][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T01:21:39,700][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T01:21:39,700][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T01:21:39,701][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T01:21:39,701][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T01:21:39,702][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T01:21:39,702][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T01:21:39,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T01:21:39,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T01:21:39,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T01:21:39,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T01:21:39,705][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T01:21:39,805][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-05T01:21:39,808][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T01:21:40,261][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T01:21:57,635][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T01:21:57,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-05T01:21:57,678][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T01:21:57,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-05T01:21:57,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T01:21:57,686][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-05T01:21:57,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-05T01:21:57,689][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T01:21:57,689][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-05T01:21:57,690][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-05T01:21:57,692][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-05T01:21:57,693][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T01:21:57,696][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T01:21:57,697][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T01:21:57,698][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T01:22:00,187][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T01:22:00,474][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T01:22:02,758][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T01:22:04,477][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T01:22:04,501][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T01:22:04,943][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T01:22:04,973][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T01:22:05,734][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T01:22:56,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [20241ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:23:37,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:24:21,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8755595943ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:24:34,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:24:45,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67397247309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:24:56,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:25:07,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23680452909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:25:24,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:25:39,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26165674343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:25:48,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:25:48,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [143144ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:25:57,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25901012045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:26:06,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:26:15,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190156224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:26:26,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:26:27,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6d71bd80, interval=1m}] took [19191ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:26:36,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19191870064ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:26:45,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:26:53,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19228793734ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:27:04,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:27:14,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19007984459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:27:23,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:27:32,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19771503113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:27:44,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:27:55,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20189475549ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:28:06,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:28:17,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22285529942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:28:28,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:28:42,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21181281302ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:29:04,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:29:27,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35258135570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:29:45,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:30:00,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39503337641ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:30:19,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35596ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:30:38,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35596092109ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:31:03,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:31:26,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43762432132ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:31:47,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:31:55,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45017313985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:04,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:13,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17629487796ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:22,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:30,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18342809637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:33,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [337545ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:32:38,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:45,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16308952637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:52,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:32:59,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13389634106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:08,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:18,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16579653306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:27,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:35,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19465722989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:43,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:45,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [51792ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:33:50,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15746927592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:33:59,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:07,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15263601696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:12,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:17,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15125747968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:12,971][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [50476ms] which is above the warn threshold of [5s]
[2022-04-05T01:34:23,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:24,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [10443ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:34:27,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10443649222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:30,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:31,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [6727ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:34:34,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6727689298ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:38,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8453ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:49,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8453024931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:34:59,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:35:07,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19245318878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:35:13,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [27698ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:35:16,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:35:24,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17658593870ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:35:33,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:35:37,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [17208ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:35:44,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17208024012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:35:56,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:36:07,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22157814459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:36:19,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23845ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:36:34,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23845194689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:36:49,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:37:00,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [75159ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:37:08,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29156183202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:37:25,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6d71bd80, interval=1m}] took [35133ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:37:24,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:37:35,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35133254146ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:37:48,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:38:00,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23812247003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:38:12,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:38:27,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24265684191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:38:42,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:38:58,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.7s/29716371555ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:39:12,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:38:39,332][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24266ms] which is above the warn threshold of [5s]
[2022-04-05T01:39:26,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30253803737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:39:36,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:39:35,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [84235ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:39:52,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23695826196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:07,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:09,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [30994ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:40:16,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30994686960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:29,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20414ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:38,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20413694230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:45,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18522ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:54,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18522234255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:40:57,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [18522ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:41:01,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:09,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16258068310ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:18,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:27,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16282591168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:34,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:43,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16982432359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:51,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:57,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15782729008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:41:58,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [49047ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:42:03,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:10,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12591953880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:19,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:26,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16630160988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:33,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:37,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13295319673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:40,645][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13296ms] which is above the warn threshold of [5s]
[2022-04-05T01:42:53,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:42:58,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [34536ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:42:59,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21241351580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:07,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:18,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12269099323ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:28,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:33,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20830424481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:40,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:46,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13380532267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:43:47,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [34210ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:43:53,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:01,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12745657007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:08,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:15,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14732569344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:22,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:27,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14105098035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:32,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [28837ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:44:33,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:37,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10948852403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:41,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [8405ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:44:41,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:43,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8405187347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:47,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:51,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6732551441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:52,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [6732ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:44:57,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:44:59,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9175817085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:02,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:04,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5845678494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:04,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [5845ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:45:08,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:09,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@42ac6072, interval=1m}] took [5379ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:45:10,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5379098549ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:15,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [6433ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:45:15,722][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6433ms] which is above the warn threshold of [5s]
[2022-04-05T01:45:15,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6434ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:18,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6433493200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:21,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:25,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6826843353ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:33,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:35,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [17758ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:45:44,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10931520291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:53,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:45:52,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [19790ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:45:59,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19790401800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:46:11,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:46:19,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17735823316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:46:30,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:46:41,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19813319944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:46:48,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [37549ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:46:51,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:02,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20492670509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:08,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:19,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17681629359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:28,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:37,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18317325823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:46,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:47:49,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [36247ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:47:54,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17930439408ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:02,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [17180ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:48:01,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:08,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17180756112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:17,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:20,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [14806ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:48:27,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14806574209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:33,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:38,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16744555519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:46,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:52,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12881931474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:48:56,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [29626ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:48:58,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:05,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11969423845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:12,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14175ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:23,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14175563608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:34,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:42,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21870065326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:51,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:49:55,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [38298ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:49:56,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16428458140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:50:01,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:50:03,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@53eabb5a, interval=30s}] took [10784ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:50:11,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10784731929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:50:21,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:50:31,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19365053054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:50:41,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:50:50,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20395431865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:51:04,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22007ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:51:16,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22006906810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:51:37,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31907ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:51:40,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [74308ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:51:58,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31906528305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:52:16,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:52:24,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [39171ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:52:33,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39171884294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:52:49,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:52:54,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [32304ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:53:09,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32304712192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:53:26,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:53:44,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37757948761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:54:03,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:54:24,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35801683722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:56:07,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125362ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:54:08,540][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35801ms] which is above the warn threshold of [5s]
[2022-04-05T01:56:25,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125362341472ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:56:30,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [161164ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:56:42,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:56:59,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34693925758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:57:14,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.5s/32516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:57:15,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6d71bd80, interval=1m}] took [32515ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:42:29,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@396ad594] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T01:57:32,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.5s/32515947208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:57:56,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:58:09,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40962705605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:58:21,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25040ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:58:35,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25040188888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:58:48,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:58:55,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [95315ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:58:58,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29312974178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:59:09,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [20638ms] which is above the warn threshold of [5000ms]
[2022-04-05T01:59:09,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:59:19,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20638013339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:59:30,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:59:40,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20983921922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T01:59:51,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T01:59:59,728][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21040ms] which is above the warn threshold of [5s]
[2022-04-05T02:00:00,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21039310254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:00:10,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:00:20,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19740501545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:00:32,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:00:39,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [61321ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:00:49,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20541789086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:01:10,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:01:17,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [37354ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:01:37,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37354231157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:01:56,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:02:18,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45513382335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:02:39,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:03:07,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42695160778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:03:39,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.2s/59253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:04:07,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.2s/59253076239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:04:39,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [157913ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:04:36,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.9s/55966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:05:12,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.9s/55965393307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:05:42,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [64286ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:05:39,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64286ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:06:06,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64286112106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:06:32,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@6d71bd80, interval=1m}] took [53201ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:06:32,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:07:03,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53201035312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:07:29,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [103904ms] which is above the warn threshold of [5s]
[2022-04-05T02:07:23,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.7s/50702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:07:50,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.7s/50702423937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:08:15,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:08:38,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51710998986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:08:57,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:09:13,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [147155ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:09:13,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44742020597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:09:29,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:09:36,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [32325ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:09:48,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32325874733ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:10:03,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:10:03,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@53eabb5a, interval=30s}] took [33716ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:10:17,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33716467515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:10:35,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:10:48,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30460478711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:11:00,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:11:10,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25470793541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:11:20,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:11:42,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [77226ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:11:37,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21295556671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:11:59,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:12:09,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [36915ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:12:16,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36915612572ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:12:40,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:13:06,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37165342279ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:13:42,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66053ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:14:16,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66053065888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:14:43,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:15:14,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [128526ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:15:13,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62473578926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:15:42,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.6s/54660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:15:49,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [54659ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:16:07,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.6s/54659793056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:16:31,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.3s/52332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:16:35,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@53eabb5a, interval=30s}] took [52332ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:16:49,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.3s/52332002713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:17:06,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:17:06,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [34349ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:17:20,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34349744165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:17:35,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:17:48,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30382857148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:18:04,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29044ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:18:16,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29043725712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:18:28,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:16:39,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@396ad594] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T02:18:35,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [83335ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:18:37,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23908808424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:18:47,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:18:50,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [19541ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:18:56,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19541215326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:19:03,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [17063ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:19:03,786][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17063ms] which is above the warn threshold of [5s]
[2022-04-05T02:19:03,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:19:23,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17063224982ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:19:42,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36671ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:19:44,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [36670ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:20:05,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36670561666ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:20:29,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:20:50,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47161096255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:21:08,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:21:23,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39643164883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:21:37,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [86804ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:21:37,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29453ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:21:54,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29452646229ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:22:17,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:22:45,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39609333531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:23:06,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:23:27,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48732490283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:23:47,445][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [48732ms] which is above the warn threshold of [5s]
[2022-04-05T02:23:58,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50546ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:24:23,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50546587357ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:24:41,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [99279ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:24:43,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:25:09,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46792205584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:25:34,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:25:58,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.6s/48626436469ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:26:12,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40687ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:26:24,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40686760257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:26:37,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:26:47,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23553252437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:27:05,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:27:08,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [94145ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:27:14,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29905175756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:27:26,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19424ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:27:28,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [19424ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:27:37,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19424220398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:27:55,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:28:09,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29633712168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:28:20,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:28:22,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26391ms] which is above the warn threshold of [5s]
[2022-04-05T02:28:42,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26391295327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:29:28,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:30:00,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64412934270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:30:33,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [90804ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:30:36,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66890ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:31:06,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66889910619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:31:42,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:31:45,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@53eabb5a, interval=30s}] took [66571ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:32:10,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66571776455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:33:05,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:33:37,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83894628943ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:34:09,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63004ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:34:41,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63004329392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:35:47,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:35:50,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [243102ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:36:35,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96203701604ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:37:15,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [91582ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:37:16,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:37:55,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91582799416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:38:35,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77936ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:38:52,673][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [169519ms] which is above the warn threshold of [5s]
[2022-04-05T02:38:39,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@472e0680] took [77935ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:39:48,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77935833322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:40:14,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:40:42,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95398728855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:41:04,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.7s/55755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:41:36,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.7s/55754706237ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:41:54,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [151153ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:42:01,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58s/58000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:42:20,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.9s/57999966106ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:42:38,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:43:16,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35457927769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:40:05,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@396ad594] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T02:43:41,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:44:09,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63082311083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:44:24,215][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [63082ms] which is above the warn threshold of [5s]
[2022-04-05T02:44:34,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.7s/52779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:44:56,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.7s/52779073334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:45:20,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.6s/44655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:45:33,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [160516ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:45:47,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.6s/44654638564ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:46:09,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:46:10,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [51047ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:46:34,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51047021313ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:46:57,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@42ac6072, interval=1m}] took [45209ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:46:57,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:47:31,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45209355043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:48:41,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:49:53,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101073853201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:50:38,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120339ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:51:18,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [221413ms] which is above the warn threshold of [5000ms]
[2022-04-05T02:51:27,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120339162441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:52:38,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:54:07,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120425809946ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:55:49,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T02:57:16,781][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [187075ms] which is above the warn threshold of [5s]
[2022-04-05T02:57:38,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/187074642642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T02:59:29,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/221627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:01:28,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [408808ms] which is above the warn threshold of [5000ms]
[2022-04-05T03:02:11,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/221733888183ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:05:18,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:06:17,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [324721ms] which is above the warn threshold of [5000ms]
[2022-04-05T03:08:09,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324721926855ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:11:55,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:16:08,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419395239787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:20:00,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:21:25,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [483680ms] which is above the warn threshold of [5000ms]
[2022-04-05T03:23:27,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/483680624184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:26:38,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@68ef0e24, interval=5s}] took [387303ms] which is above the warn threshold of [5000ms]
[2022-04-05T03:26:42,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:29:48,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/387303445630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:31:55,117][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [387303ms] which is above the warn threshold of [5s]
[2022-04-05T03:33:14,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367502ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:36:09,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367528836781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:38:53,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/371627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:41:00,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [738978ms] which is above the warn threshold of [5000ms]
[2022-04-05T03:41:38,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/371449439211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:15:40,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@396ad594] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T03:44:29,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:44:49,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@75817244, interval=5s}] took [337319ms] which is above the warn threshold of [5000ms]
[2022-04-05T03:47:23,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337319470665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:50:08,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:52:49,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337412679724ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:55:54,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T03:57:23,688][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [333820ms] which is above the warn threshold of [5s]
[2022-04-05T03:58:43,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333820315541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T03:58:42,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [333820ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:01:27,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T04:04:04,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/344582487518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T04:07:51,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T04:11:18,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379500706818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T04:15:32,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T04:16:03,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1fd2d36d, interval=1s}] took [843242ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:19:11,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463741516414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T04:23:48,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/489769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T04:23:47,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@53eabb5a, interval=30s}] took [489706ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:31:33,616][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T04:31:33,658][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T04:31:33,660][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T04:31:42,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T04:31:42,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T04:31:42,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T04:31:42,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T04:31:42,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T04:31:42,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T04:31:42,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T04:31:42,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T04:31:42,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T04:31:42,945][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T04:31:42,945][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T04:31:42,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T04:31:42,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T04:31:42,947][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T04:31:42,947][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T04:31:42,948][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T04:31:42,949][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T04:31:42,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T04:31:42,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T04:31:42,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T04:31:42,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T04:31:42,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T04:31:42,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T04:31:42,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T04:31:42,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T04:31:42,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T04:31:42,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T04:31:42,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T04:31:42,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T04:31:42,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T04:31:42,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T04:31:42,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T04:31:42,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T04:31:42,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T04:31:42,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T04:31:42,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T04:31:42,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T04:31:42,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T04:31:42,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T04:31:42,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T04:31:42,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T04:31:42,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T04:31:42,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T04:31:42,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T04:31:42,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T04:31:42,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T04:31:42,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T04:31:42,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T04:31:42,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T04:31:42,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T04:31:42,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T04:31:42,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T04:31:42,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T04:31:42,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T04:31:42,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T04:31:42,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T04:31:42,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T04:31:42,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T04:31:42,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T04:31:43,174][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.5gb], net total_space [125.8gb], types [ext4]
[2022-04-05T04:31:43,178][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T04:31:43,869][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T04:32:03,125][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T04:32:03,141][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T04:32:05,246][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T04:32:05,613][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T04:32:07,183][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T04:32:08,905][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T04:32:08,906][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T04:32:08,970][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T04:32:08,972][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T04:32:09,265][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T04:32:14,006][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T04:32:14,281][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I3j6xUT7RZiP6I5lR5ox3g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 203, version: 7318, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I3j6xUT7RZiP6I5lR5ox3g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T04:32:14,583][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I3j6xUT7RZiP6I5lR5ox3g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 203, version: 7318, reason: Publication{term=203, version=7318}
[2022-04-05T04:32:14,844][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T04:32:14,846][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T04:32:15,581][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:929) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T04:32:16,492][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=100, index=.kibana_7.17.0, from=0}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:929) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T04:32:17,146][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T04:32:17,163][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [38] indices into cluster_state
[2022-04-05T04:32:19,052][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T04:32:19,053][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T04:32:20,645][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-05T04:32:22,276][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T04:32:22,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T04:32:22,299][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T04:32:23,422][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T04:32:24,373][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T04:32:28,592][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T04:32:28,650][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-05T04:32:28,660][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-05T04:32:29,008][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T04:32:29,014][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T04:32:31,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T04:32:34,488][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-05T04:32:40,453][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T04:32:40,568][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-05T04:32:40,570][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-05T04:32:41,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T04:32:41,791][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-05T04:32:41,980][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T04:32:41,984][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T04:32:43,795][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T04:32:43,796][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T04:33:21,637][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.05] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-05T04:33:21,863][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.05][0]]]).
[2022-04-05T04:33:22,160][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:22,399][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:22,565][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:22,716][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:22,878][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:23,078][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:23,094][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:23,380][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:23,564][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:25,197][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:25,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:25,708][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:26,063][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:26,522][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:26,966][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:27,329][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:27,599][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:27,861][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:27,873][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:28,179][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:28,388][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:28,422][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:28,600][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:29,162][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:29,345][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:31,307][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:31,660][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:31,931][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:32,073][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:32,209][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:32,498][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:32,848][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:32,980][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:32,987][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:33,144][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:33,280][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:33,487][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:33,592][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:33,783][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:33,943][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:33:43,369][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:34:45,406][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:35:05,155][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:35:06,176][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:36:49,347][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:14,470][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:14,597][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:22,475][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:27,334][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:27,761][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:28,329][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:28,753][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:56,526][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:39:56,668][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:40:01,516][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T04:44:03,155][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][713] overhead, spent [379ms] collecting in the last [1.2s]
[2022-04-05T04:45:02,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5ec9ca37, interval=1s}] took [5549ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:45:59,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5ec9ca37, interval=1s}] took [11250ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:46:29,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5ec9ca37, interval=1s}] took [12569ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:47:18,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5ec9ca37, interval=1s}] took [23388ms] which is above the warn threshold of [5000ms]
[2022-04-05T04:46:52,800][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5816ms] which is above the warn threshold of [5s]
[2022-04-05T04:49:39,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:01:45,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9m/836093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:06:15,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9m/836152158135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:07:47,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:09:50,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382403182107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:12:16,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:14:44,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266022065079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:17:51,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:20:45,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314573449230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:23:53,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:26:55,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372445850751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:24:45,949][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.4m/145164ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3d9a502e]], which exceeds the warn threshold of [10s]
[2022-04-05T05:32:23,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:34:35,207][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.9s/13982ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3d9a502e]], which exceeds the warn threshold of [10s]
[2022-04-05T05:35:49,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358193708392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:38:19,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/510448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:38:26,416][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.6s/13656ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e0a9d51f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@652ea47d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9de9b7d6]], which exceeds the warn threshold of [10s]
[2022-04-05T05:41:02,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/510659935022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:44:39,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/369162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:48:06,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/369060170578ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T05:50:27,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@789e6a69, interval=5s}] took [369060ms] which is above the warn threshold of [5000ms]
[2022-04-05T05:46:26,477][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35224}] took [730640ms] which is above the warn threshold of [5000ms]
[2022-04-05T05:53:03,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/510701ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T05:56:28,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/510579094443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:01:24,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2m/495056ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:07:52,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2m/495524424802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:10:40,294][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [14.6m/879720ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [20.7m/1244460ms]
[2022-04-05T06:12:03,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8m/651111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:15:11,276][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [42.2m/2536934ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [46.8m/2811022ms]
[2022-04-05T06:15:26,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8m/651110968960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:09:37,867][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1h/4048537ms] ago, timed out [1.1h/3985987ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{I3j6xUT7RZiP6I5lR5ox3g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [5710]
[2022-04-05T06:19:00,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:20:31,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5ec9ca37, interval=1s}] took [400366ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:20:38,069][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.7s/15784ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e0a9d51f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@652ea47d], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3d9a502e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9de9b7d6]], which exceeds the warn threshold of [10s]
[2022-04-05T06:22:41,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400366017013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:29:53,079][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T06:29:53,094][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T06:29:53,095][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T06:30:00,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T06:30:00,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T06:30:00,058][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T06:30:00,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T06:30:00,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T06:30:00,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T06:30:00,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T06:30:00,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T06:30:00,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T06:30:00,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T06:30:00,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T06:30:00,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T06:30:00,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T06:30:00,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T06:30:00,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T06:30:00,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T06:30:00,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T06:30:00,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T06:30:00,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T06:30:00,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T06:30:00,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T06:30:00,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T06:30:00,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T06:30:00,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T06:30:00,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T06:30:00,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T06:30:00,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T06:30:00,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T06:30:00,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T06:30:00,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T06:30:00,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T06:30:00,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T06:30:00,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T06:30:00,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T06:30:00,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T06:30:00,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T06:30:00,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T06:30:00,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T06:30:00,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T06:30:00,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T06:30:00,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T06:30:00,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T06:30:00,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T06:30:00,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T06:30:00,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T06:30:00,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T06:30:00,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T06:30:00,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T06:30:00,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T06:30:00,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T06:30:00,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T06:30:00,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T06:30:00,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T06:30:00,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T06:30:00,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T06:30:00,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T06:30:00,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T06:30:00,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T06:30:00,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T06:30:00,177][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-05T06:30:00,177][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T06:30:00,644][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T06:30:12,677][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T06:30:12,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-05T06:30:12,687][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T06:30:12,689][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-05T06:30:12,690][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T06:30:12,690][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-05T06:30:12,691][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-05T06:30:12,692][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T06:30:12,693][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-05T06:30:12,694][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-05T06:30:12,694][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-05T06:30:12,695][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T06:30:12,696][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T06:30:12,698][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T06:30:12,699][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T06:30:15,901][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T06:30:16,174][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T06:30:17,603][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T06:30:18,544][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T06:30:18,553][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T06:30:18,686][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T06:30:18,688][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T06:30:19,206][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T06:30:24,567][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T06:30:24,841][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lMTvObquQ_yU_X_DCN_RtQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 204, version: 7412, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lMTvObquQ_yU_X_DCN_RtQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T06:30:25,065][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lMTvObquQ_yU_X_DCN_RtQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 204, version: 7412, reason: Publication{term=204, version=7412}
[2022-04-05T06:30:25,271][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T06:30:25,276][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T06:30:53,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [5242ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:31:31,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [25623ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:32:11,430][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:47552}] took [5645ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:32:11,498][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35926}] took [5846ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:32:11,498][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35924}] took [5846ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:32:11,558][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:929) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T06:32:11,824][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35928}] took [6046ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:32:19,984][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:47570}] took [6674ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:33:16,771][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T06:36:04,493][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=204, version=7413}] took [5.4m] which is above the warn threshold of [30s]: [running task [Publication{term=204, version=7413}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@61f6ff72] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@dec96] took [31ms], [org.elasticsearch.script.ScriptService@39b70973] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@42199b92] took [221ms], [org.elasticsearch.snapshots.RestoreService@1442e1cc] took [0ms], [org.elasticsearch.ingest.IngestService@7bc4c68e] took [106302ms], [org.elasticsearch.action.ingest.IngestActionForwarder@7a39a414] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d0880@74f265ca] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@773da619] took [38ms], [org.elasticsearch.tasks.TaskManager@2a90fec6] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1d3ef684] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@5f265ad2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e4296] took [0ms], [org.elasticsearch.indices.SystemIndexManager@21ff4b2c] took [166ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15728459] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@19d19fcb] took [59ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2bcea365] took [19ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5beb19fa] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@47211f6c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@426ab664] took [25ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@a447e85] took [5245ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@206eaf7e] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@40df03e2] took [21ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@159f41d8] took [5864ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2e6d7229] took [1055ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@70b5d15d] took [110ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@48cd0cba] took [2399ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@42199b92] took [4779ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@27b305fa] took [2757ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@628ac71f] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2340f462] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2e0e2543] took [2741ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@28938f19] took [195ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2301e992] took [785ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@7c7af48c] took [1366ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4825575d] took [27ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@727415a8] took [78ms], [org.elasticsearch.node.ResponseCollectorService@5e152f0d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@75606ed7] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@171d5099] took [2165ms], [org.elasticsearch.shutdown.PluginShutdownService@4516de87] took [34ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@41cb290c] took [609ms], [org.elasticsearch.indices.store.IndicesStore@44ff8af4] took [2586ms], [org.elasticsearch.persistent.PersistentTasksNodeService@23517507] took [158ms], [org.elasticsearch.license.LicenseService@5549701f] took [73206ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@4dd2b0f3] took [89928ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@75c23b97] took [203ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1dd85db] took [20800ms], [org.elasticsearch.gateway.GatewayService@547ebfce] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@71ce531d] took [0ms]
[2022-04-05T06:36:06,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e5dd8@7dc1d6b7] took [211667ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:36:08,376][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T06:36:09,505][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:50234}] took [208440ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:36:31,678][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/config%3A7.17.0, params: {index=.kibana_7.17.0, id=config:7.17.0}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][config:7.17.0]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T06:36:32,577][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T06:36:35,418][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=100, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-05T06:36:35,418][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=100, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-05T06:36:59,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39303ced, interval=5s}] took [11522ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:36:59,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:37:02,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6941078085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:40:54,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68554ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:03,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68554193446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:11,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:20,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17599478346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:27,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:34,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16216739116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:41,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e5dd8@74cb564c] took [258179ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:41:42,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:48,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15906143778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:41:56,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39303ced, interval=5s}] took [11318ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:41:54,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:42:05,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11318260142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:42:15,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:42:24,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19294734910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:42:31,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [19294ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:42:33,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:42:43,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18643615249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:05,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:11,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33311379447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:15,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:21,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9477784467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:26,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:29,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11526883773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:33,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6927ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:30,767][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11527ms] which is above the warn threshold of [5s]
[2022-04-05T06:43:36,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6927219633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:40,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:44,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6840792598ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:48,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:52,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8029696746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:56,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:43:59,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7953060757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:44:03,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:44:08,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7459006382ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:44:12,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:44:48,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8459577793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:44:54,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:03,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42160080287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:10,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:16,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15374331340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:22,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:33,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12036640630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:43,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:45:56,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21606303587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:46:10,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:46:22,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26046696293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:46:38,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:46:52,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26549645054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:47:07,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:47:24,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30400764131ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:47:46,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:47:57,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e5dd8@490536bd] took [268800ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:48:00,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37429447085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:48:21,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:48:35,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35126218543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:48:42,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [35126ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:48:44,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:48:56,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24762267647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:49:05,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:49:17,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21025595831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:49:18,964][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=204, version=7414}] took [12.7m] which is above the warn threshold of [30s]: [running task [Publication{term=204, version=7414}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@61f6ff72] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@dec96] took [375095ms], [org.elasticsearch.script.ScriptService@39b70973] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@42199b92] took [0ms], [org.elasticsearch.snapshots.RestoreService@1442e1cc] took [0ms], [org.elasticsearch.ingest.IngestService@7bc4c68e] took [1911ms], [org.elasticsearch.action.ingest.IngestActionForwarder@7a39a414] took [399ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d0880@74f265ca] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@773da619] took [452ms], [org.elasticsearch.tasks.TaskManager@2a90fec6] took [84ms], [org.elasticsearch.snapshots.SnapshotsService@1d3ef684] took [2633ms], [org.elasticsearch.cluster.InternalClusterInfoService@5f265ad2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@6e4296] took [16605ms], [org.elasticsearch.indices.SystemIndexManager@21ff4b2c] took [5371ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@15728459] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@19d19fcb] took [1842ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2bcea365] took [313ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5beb19fa] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@47211f6c] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@426ab664] took [219ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@a447e85] took [55314ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@206eaf7e] took [73ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@40df03e2] took [128ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@159f41d8] took [73370ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2e6d7229] took [2448ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@70b5d15d] took [2194ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@48cd0cba] took [43777ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@42199b92] took [6733ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@27b305fa] took [58907ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@628ac71f] took [202ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2340f462] took [111ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2e0e2543] took [70227ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@2301e992] took [27741ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4825575d] took [318ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@727415a8] took [1290ms], [org.elasticsearch.node.ResponseCollectorService@5e152f0d] took [87ms], [org.elasticsearch.snapshots.SnapshotShardsService@75606ed7] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@171d5099] took [6218ms], [org.elasticsearch.shutdown.PluginShutdownService@4516de87] took [358ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@41cb290c] took [1241ms], [org.elasticsearch.indices.store.IndicesStore@44ff8af4] took [1958ms], [org.elasticsearch.persistent.PersistentTasksNodeService@23517507] took [0ms], [org.elasticsearch.license.LicenseService@5549701f] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@75c23b97] took [160ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@1dd85db] took [352ms], [org.elasticsearch.gateway.GatewayService@547ebfce] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@71ce531d] took [0ms]
[2022-04-05T06:49:27,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21609ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:49:33,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21608945985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:50:03,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:50:17,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34692375575ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:50:31,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:50:35,643][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [13.8m/831454ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [17.7m/1062707ms]
[2022-04-05T06:50:44,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27862835708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:50:33,733][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27863ms] which is above the warn threshold of [5s]
[2022-04-05T06:50:53,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:51:00,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [22183ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:50:59,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22183966011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:51:10,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:51:32,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18328722005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:51:41,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:51:53,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30694675499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:03,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:10,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21647808992ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:25,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:33,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [22486ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:52:33,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22486261360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:38,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:44,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13202343639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:52:45,270][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/112450ms] to compute cluster state update for [create persistent task], which exceeds the warn threshold of [10s]
[2022-04-05T06:52:53,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:53:02,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12946429492ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:53:25,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:53:24,785][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [23.6s/23618ms] to compute cluster state update for [reassign persistent tasks], which exceeds the warn threshold of [10s]
[2022-04-05T06:53:35,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34063473674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:53:41,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@10dcb311, interval=1m}] took [15381ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:53:41,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:53:22,656][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [60] timed out after [187262ms]
[2022-04-05T06:53:48,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15381389348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:54:18,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:54:54,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38432025954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:55:37,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:54:47,530][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38432ms] which is above the warn threshold of [5s]
[2022-04-05T06:55:50,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60322414000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:56:15,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.5s/55570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:56:23,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [115892ms] which is above the warn threshold of [5000ms]
[2022-04-05T06:56:27,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.5s/55570108574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T06:56:43,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:59:09,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28464756935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:00:02,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/198503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T06:53:47,519][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A05042022%3Acount%3Adelay_threshold_exceeded, params: {require_alias=true, refresh=wait_for, index=.kibana_7.17.0, _source=true, id=usage-counters:eventLoop:05042022:count:delay_threshold_exceeded}
org.elasticsearch.action.UnavailableShardsException: [.kibana_7.17.0_001][0] [1] shardIt, [0] active : Timeout waiting for [1m], request: indices:data/write/update
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.retry(TransportInstanceSingleOperationAction.java:231) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.doStart(TransportInstanceSingleOperationAction.java:181) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$2.onTimeout(TransportInstanceSingleOperationAction.java:254) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:345) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:263) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:660) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T07:00:35,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/198503060595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:00:44,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:00:45,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@5a1792c] took [43256ms] which is above the warn threshold of [5000ms]
[2022-04-05T07:01:13,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43256171376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:01:23,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38312ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:01:37,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38312270318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:01:48,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:01:57,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25001324947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:02:07,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:02:21,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19507309981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:02:36,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:02:50,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26742365126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:03:07,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:03:21,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31422860008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:03:34,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:03:48,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28411383761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:04:05,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:04:22,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28095505076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:04:40,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:05:00,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36799548040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:05:20,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:06:17,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38873042942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:06:36,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:07:28,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76129881440ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:07:55,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:08:25,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78232236698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:08:51,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.3s/53320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:10:06,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.3s/53320279474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:10:39,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:11:09,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108933724694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:11:42,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65775ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:12:00,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65996976932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:12:37,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.3s/54395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:12:47,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e5dd8@682ab764] took [710173ms] which is above the warn threshold of [5000ms]
[2022-04-05T07:12:59,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.3s/54395009782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:13:41,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:15:24,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57625388350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:15:56,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:16:52,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139730307465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:18:21,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/143951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:19:20,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/143950550058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:19:48,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88298ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:20:07,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88297716473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:20:39,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:21:04,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39303ced, interval=5s}] took [44566ms] which is above the warn threshold of [5000ms]
[2022-04-05T07:21:30,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44566534026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:21:59,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:21:09,498][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44567ms] which is above the warn threshold of [5s]
[2022-04-05T07:22:41,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85902381722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:23:23,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:23:45,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83698829156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:24:02,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:24:17,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.4s/41492848317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:24:29,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:24:49,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29362395797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:25:05,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:26:19,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32198308080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:26:52,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [32198ms] which is above the warn threshold of [5000ms]
[2022-04-05T07:27:23,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:26:12,058][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [63] timed out after [769022ms]
[2022-04-05T07:27:45,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138948296867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:28:14,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.3s/47310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:28:47,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.3s/47309642757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:29:12,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62993ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:29:29,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62993335845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:29:47,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36612ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:29:51,238][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1706250ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [39] unchanged indices
[2022-04-05T07:30:03,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36611329513ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:30:33,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36902ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:30:56,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36902845378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:30:22,760][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [36611ms] which is above the warn threshold of [5s]
[2022-04-05T07:31:04,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:31:15,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39558182426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:31:07,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [39558ms] which is above the warn threshold of [5000ms]
[2022-04-05T07:30:52,508][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [34.4m] publication of cluster state version [7415] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lMTvObquQ_yU_X_DCN_RtQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T07:31:30,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23851ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:32:02,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23850904310ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:32:29,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:31:37,309][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [43.3m/2599884ms] ago, timed out [40.2m/2412622ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lMTvObquQ_yU_X_DCN_RtQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [60]
[2022-04-05T07:32:43,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60607306437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:33:02,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:33:24,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33073178385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:33:58,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.3s/52358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:34:33,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.3s/52358314180ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:35:03,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:35:37,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65968899753ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:34:31,266][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [22.2m/1333434ms] ago, timed out [9.4m/564412ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{lMTvObquQ_yU_X_DCN_RtQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [63]
[2022-04-05T07:36:07,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:36:44,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63727326647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:37:03,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58317ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:37:27,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58317187002ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:37:47,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43123ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:38:32,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.1s/43123000630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:39:16,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:39:44,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83036070293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:40:05,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.8s/55811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:40:24,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.8s/55811260491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:41:38,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:48:04,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49079482847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:48:27,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/451914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:48:54,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/451914694119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:49:11,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:49:39,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42995594731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:50:25,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:50:50,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75405336845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:51:21,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.5s/54521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:51:45,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.5s/54521236841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:52:26,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65007ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:52:53,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e5dd8@4264cfa1] took [1254945ms] which is above the warn threshold of [5000ms]
[2022-04-05T07:53:11,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65006799187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:53:51,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79952ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:54:36,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79951745779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:55:11,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:55:53,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82703907859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:56:16,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67822ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:56:51,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67821864702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T07:59:16,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/178130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T07:59:49,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/178130194206ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:00:13,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:00:42,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61113314488ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:01:15,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.7s/59778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:10:01,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.7s/59777659009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:10:35,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [59777ms] which is above the warn threshold of [5000ms]
[2022-04-05T08:13:03,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/603867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:16:44,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/603653790641ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:18:41,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/437301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:20:40,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/437126315955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:23:43,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/277482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:25:56,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/277349073956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:28:42,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:31:24,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324946241984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:34:16,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:37:25,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332072480503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:40:32,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:43:37,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376771315271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:46:56,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:49:31,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/383794352496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:44:04,155][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [376772ms] which is above the warn threshold of [5s]
[2022-04-05T08:52:34,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T08:55:40,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338056056447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T08:58:28,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:01:22,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353736082189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:04:24,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:07:07,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354276398842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:10:03,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:12:50,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340781549495ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:15:56,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:18:55,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/342216896742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:21:47,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363302ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:24:44,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363350230080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:27:27,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:30:33,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339107714059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:33:45,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/377261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:36:43,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376850133141ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:36:41,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@23aa0df7, interval=1s}] took [376850ms] which is above the warn threshold of [5000ms]
[2022-04-05T09:39:46,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:42:58,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351906372750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T09:45:37,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T09:51:50,012][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T09:51:50,083][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T09:51:50,088][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T09:52:05,606][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T09:52:05,607][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T09:52:05,608][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T09:52:05,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T09:52:05,609][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T09:52:05,610][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T09:52:05,611][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T09:52:05,612][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T09:52:05,613][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T09:52:05,614][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T09:52:05,615][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T09:52:05,616][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T09:52:05,617][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T09:52:05,618][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T09:52:05,619][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T09:52:05,620][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T09:52:05,621][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T09:52:05,621][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T09:52:05,622][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T09:52:05,624][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T09:52:05,625][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T09:52:05,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T09:52:05,627][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T09:52:05,628][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T09:52:05,629][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T09:52:05,630][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T09:52:05,631][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T09:52:05,632][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T09:52:05,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T09:52:05,634][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T09:52:05,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T09:52:05,636][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T09:52:05,636][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T09:52:05,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T09:52:05,638][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T09:52:05,639][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T09:52:05,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T09:52:05,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T09:52:05,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T09:52:05,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T09:52:05,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T09:52:05,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T09:52:05,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T09:52:05,647][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T09:52:05,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T09:52:05,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T09:52:05,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T09:52:05,650][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T09:52:05,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T09:52:05,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T09:52:05,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T09:52:05,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T09:52:05,655][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T09:52:05,656][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T09:52:05,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T09:52:05,658][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T09:52:05,658][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T09:52:05,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T09:52:05,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T09:52:05,811][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-05T09:52:05,814][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T09:52:06,504][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T09:52:34,844][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T09:52:34,849][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T09:52:37,554][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T09:52:37,815][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T09:52:39,651][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T09:52:41,250][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T09:52:41,251][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T09:52:41,312][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T09:52:41,313][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T09:52:41,637][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T09:52:45,821][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T09:52:46,076][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hjLbyJANTw6fTgK6GIzd5g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 205, version: 7416, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hjLbyJANTw6fTgK6GIzd5g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T09:52:46,593][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hjLbyJANTw6fTgK6GIzd5g}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 205, version: 7416, reason: Publication{term=205, version=7416}
[2022-04-05T09:52:46,959][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T09:52:46,961][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T09:52:51,915][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T09:52:51,922][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T09:52:52,525][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-05T09:52:53,708][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_doc/space%3Adefault, params: {index=.kibana_7.17.0, id=space:default}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_7.17.0][_doc][space:default]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:202) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1100(TransportSingleShardAction.java:130) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:258) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:2195) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:1256) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:194) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:84) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:118) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:35) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:104) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 3 more
[2022-04-05T09:52:54,265][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T09:52:54,273][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T09:52:54,747][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:55,480][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:55,553][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:55,612][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:55,619][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:55,655][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:55,690][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:56,621][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-05T09:52:57,219][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T09:52:57,242][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T09:52:57,246][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T09:52:57,682][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-05T09:52:57,839][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:57,903][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:57,936][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:57,986][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:52:58,268][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T09:52:58,645][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T09:52:58,825][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-05T09:53:00,833][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:53:00,873][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:53:00,910][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:53:00,942][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-05T09:53:05,181][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T09:53:15,888][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.05][0]]]).
[2022-04-05T09:54:01,626][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:01,820][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:01,835][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:01,857][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:02,462][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:04,238][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:07,486][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:08,039][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:08,317][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:54:08,392][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T09:59:30,616][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48246}] took [10587ms] which is above the warn threshold of [5000ms]
[2022-04-05T09:59:30,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [11360ms] which is above the warn threshold of [5000ms]
[2022-04-05T09:59:30,616][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48262}] took [11783ms] which is above the warn threshold of [5000ms]
[2022-04-05T09:59:30,616][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48260}] took [11783ms] which is above the warn threshold of [5000ms]
[2022-04-05T09:59:49,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [7961ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:00:04,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [7603ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:00:32,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017ddbb8@12c69513] took [5815ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:00:41,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [5080ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:02:06,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [37772ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:02:38,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6df867ee, interval=5s}] took [8004ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:02:56,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:08:12,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8004308381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:08:34,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:08:49,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361941647802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:08:59,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:09:08,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25749265576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:09:48,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:10:02,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.3s/48310209441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:10:23,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:10:26,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [79782ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:10:47,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31472658248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:02,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42452ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:13,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42452054665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:27,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24216ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:34,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24215503781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:37,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:43,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11887045418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:33,607][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24216ms] which is above the warn threshold of [5s]
[2022-04-05T10:11:33,767][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3165] timed out after [15518ms]
[2022-04-05T10:11:56,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:11:59,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15113094909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:12:05,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [12672ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:12:05,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:12:13,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12672546777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:12:23,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@41116094, interval=5s}] took [18179ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:12:23,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:12:33,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18179660905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:12:45,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:12:46,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [20290ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:12:54,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20290656662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:13:05,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:13:15,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21742236102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:13:28,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:13:39,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22282422735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:13:48,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:13:58,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20225900272ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:14:09,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:14:20,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20579244266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:14:36,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:14:51,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [24996ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:14:52,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24996261364ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:16:09,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94444ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:16:44,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94444162314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:16:43,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@41116094, interval=5s}] took [94444ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:18:02,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:18:30,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112308027300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:19:07,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:20:45,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66267765956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:24:43,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:28:44,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/333374620619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:34:02,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:35:49,673][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48246}] took [537743ms] which is above the warn threshold of [5000ms]
[2022-04-05T10:36:56,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/537743003572ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:41:50,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485169ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:45:31,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485017909327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:50:04,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2m/497163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:53:01,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2m/497017414314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:56:08,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T10:59:57,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364614555888ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T10:53:01,097][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [497018ms] which is above the warn threshold of [5s]
[2022-04-05T11:03:11,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T11:08:08,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/422245499410ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T11:14:26,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/667129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T11:17:26,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1m/667033066702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T11:28:06,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8m/828447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T11:29:18,024][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=100][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36592}] took [828188ms] which is above the warn threshold of [5000ms]
[2022-04-05T11:35:36,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8m/828188691815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T11:38:17,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9m/599680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T11:43:34,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/600399817307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T12:14:15,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22m/1322644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T11:30:43,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=100][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36588}] took [828188ms] which is above the warn threshold of [5000ms]
[2022-04-05T12:19:41,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22m/1322309002177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T12:23:30,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3m/1401277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T12:23:56,290][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36576}] took [1401613ms] which is above the warn threshold of [5000ms]
[2022-04-05T12:26:29,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3m/1401612451559ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T12:29:52,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T12:30:47,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f00e177, interval=1s}] took [381969ms] which is above the warn threshold of [5000ms]
[2022-04-05T12:33:10,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/381969794130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T12:36:53,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419454ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T12:39:47,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419971820307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T12:42:26,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T12:48:37,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320876541552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T12:51:52,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6m/578436ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T12:59:33,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6m/578817041362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:07:34,848][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T13:07:34,871][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T13:07:34,872][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T13:07:41,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T13:07:41,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T13:07:41,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T13:07:41,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T13:07:41,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T13:07:41,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T13:07:41,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T13:07:41,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T13:07:41,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T13:07:41,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T13:07:41,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T13:07:41,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T13:07:41,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T13:07:41,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T13:07:41,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T13:07:41,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T13:07:41,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T13:07:41,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T13:07:41,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T13:07:41,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T13:07:41,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T13:07:41,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T13:07:41,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T13:07:41,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T13:07:41,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T13:07:41,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T13:07:41,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T13:07:41,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T13:07:41,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T13:07:41,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T13:07:41,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T13:07:41,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T13:07:41,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T13:07:41,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T13:07:41,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T13:07:41,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T13:07:41,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T13:07:41,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T13:07:41,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T13:07:41,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T13:07:41,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T13:07:41,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T13:07:41,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T13:07:41,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T13:07:41,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T13:07:41,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T13:07:41,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T13:07:41,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T13:07:41,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T13:07:41,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T13:07:41,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T13:07:41,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T13:07:41,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T13:07:41,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T13:07:41,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T13:07:41,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T13:07:41,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T13:07:41,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T13:07:41,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T13:07:41,484][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-05T13:07:41,486][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T13:07:42,063][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T13:08:12,383][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T13:08:12,394][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-05T13:08:12,396][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T13:08:12,398][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-05T13:08:12,399][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T13:08:12,400][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-05T13:08:12,401][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-05T13:08:12,402][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T13:08:12,403][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-05T13:08:12,403][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-05T13:08:12,404][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-05T13:08:12,404][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T13:08:12,405][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T13:08:12,406][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T13:08:12,406][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T13:08:14,211][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T13:08:14,523][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T13:08:17,273][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T13:08:19,747][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T13:08:19,762][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T13:08:20,728][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T13:08:21,104][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T13:08:59,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:09:29,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [13008ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:09:54,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3a6eea81, interval=1m}] took [6604ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:10:44,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [32358ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:11:17,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [21889ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:12:04,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [25212ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:12:20,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [7167ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:13:09,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [34313ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:12:56,533][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [54403ms] which is above the warn threshold of [5s]
[2022-04-05T13:13:53,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [18411ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:15:01,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [45996ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:15:42,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [20212ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:16:14,209][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11608ms] which is above the warn threshold of [5s]
[2022-04-05T13:16:23,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [25816ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:17:10,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [32260ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:17:44,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [15066ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:18:22,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [20812ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:19:06,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [28218ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:18:59,879][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16211ms] which is above the warn threshold of [5s]
[2022-04-05T13:19:29,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [9205ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:19:49,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:20:08,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5626ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:20:19,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:20:28,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:20:46,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5030ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:21:02,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:21:33,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:21:48,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6807ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:22:08,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [9405ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:22:29,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:22:50,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [11006ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:23:11,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [10519ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:23:27,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [8205ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:23:50,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [10240ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:23:46,123][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10440ms] which is above the warn threshold of [5s]
[2022-04-05T13:23:46,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:23:57,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5232994803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:05,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [21177ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:24:05,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:10,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21177387634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:17,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:25,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11309596207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:30,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [11309ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:24:34,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:41,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17215382300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:50,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:24:57,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15681573293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:02,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [15681ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:25:03,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:09,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14037246185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:13,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [10192ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:25:13,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:18,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10192759722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:23,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:26,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [9445ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:25:26,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9445171675ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:29,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:29,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@82435a1, interval=1m}] took [6151ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:25:31,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6151301265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:33,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [5133ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:25:33,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5134ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:35,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5133445932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:25:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6640ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:25:59,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:26:08,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5737ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:24:19,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T13:26:28,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6640ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:26:42,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [9046ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:27:03,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [8605ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:27:32,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5004ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:28:54,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6278ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:30:11,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [55923ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:31:25,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [43626ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:31:44,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [5346ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:31:58,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:31:56,064][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7475ms] which is above the warn threshold of [5s]
[2022-04-05T13:32:16,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:32:32,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6422ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:32:53,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:33:31,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6284ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:33:45,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [8004ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:34:06,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [11006ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:34:33,087][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14431ms] which is above the warn threshold of [5s]
[2022-04-05T13:34:33,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [16431ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:34:54,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [9706ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:35:16,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [11671ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:35:58,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5632ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:36:23,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [8134ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:36:54,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [18192ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:37:23,902][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7805ms] which is above the warn threshold of [5s]
[2022-04-05T13:37:24,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [9806ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:37:44,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:38:08,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [11663ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:38:33,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:38:50,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [7105ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:39:10,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [7604ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:39:31,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6404ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:39:53,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:40:19,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [8446ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:40:35,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:40:56,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [6725ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:41:11,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [7004ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:41:25,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [5593ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:41:48,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [10057ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:42:33,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:42:37,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [25901ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:43:00,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25501508554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:43:24,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:43:45,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43691849669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:43:48,385][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [43692ms] which is above the warn threshold of [5s]
[2022-04-05T13:44:03,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:44:16,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [43756ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:44:23,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43756484839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:44:47,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:44:49,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [42998ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:42:38,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T13:45:03,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42998953036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:46:03,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:46:03,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [35343ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:46:21,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35343282831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:47:07,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:47:20,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107912444901ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:47:23,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [107912ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:47:35,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:47:56,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26712688650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:48:21,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:48:43,877][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [45005ms] which is above the warn threshold of [5s]
[2022-04-05T13:48:43,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45005138156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:49:10,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [45005ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:49:19,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.3s/57398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:49:45,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.3s/57397590027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:50:07,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:50:38,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49932353693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:51:04,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [103463ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:51:03,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:51:33,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53530651123ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:51:58,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.8s/56837ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:52:29,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.8s/56837593446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:52:54,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56s/56000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:53:01,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [55999ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:53:15,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.9s/55999612502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:53:36,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [37510ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:53:32,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:53:56,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37510895451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:53:59,920][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [37511ms] which is above the warn threshold of [5s]
[2022-04-05T13:54:19,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:54:40,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46130257875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:55:05,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47126ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:55:15,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [93255ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:55:27,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47125596939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:55:50,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:56:05,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [44857ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:56:08,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44857208584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:56:25,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36979ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:56:25,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@82435a1, interval=1m}] took [36979ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:56:37,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36979157276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:56:52,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:57:05,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24533977361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:57:19,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:57:33,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28533705288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:57:33,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [53067ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:57:47,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27936ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:58:05,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27935656179ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:58:35,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44851ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:59:04,772][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44851ms] which is above the warn threshold of [5s]
[2022-04-05T13:59:03,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44851075134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T13:59:25,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.5s/52515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T13:59:29,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [97366ms] which is above the warn threshold of [5000ms]
[2022-04-05T13:59:55,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.5s/52515375049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:00:37,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:00:37,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [70455ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:01:17,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70455677970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:02:00,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:02:24,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83451224281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:02:28,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [83451ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:02:55,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:03:21,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52852284730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:03:41,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:03:53,841][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [50551ms] which is above the warn threshold of [5s]
[2022-04-05T14:04:08,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50551140175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:04:16,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [50551ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:04:28,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:04:49,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45s/45097193642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:03:04,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T14:05:06,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:05:26,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38444421541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:05:44,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:05:58,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [76368ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:06:01,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37924112332ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:06:21,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:06:37,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37329463096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:06:52,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:07:27,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31892082293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:07:35,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [31892ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:07:40,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47824ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:07:54,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47824756976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:08:07,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:08:21,838][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [26617ms] which is above the warn threshold of [5s]
[2022-04-05T14:08:24,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26617101920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:08:40,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:08:49,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [59361ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:09:01,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32744284360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:09:25,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.9s/45918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:09:37,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.9s/45918260934ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:09:53,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:09:59,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [28889ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:10:07,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28889517883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:10:30,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:10:43,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.6s/35673530327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:10:56,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:11:09,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [62048ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:11:10,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26374551694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:11:30,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [31513ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:11:31,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:11:54,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31513763110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:12:14,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:12:25,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46919756821ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:12:30,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [46919ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:12:37,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:12:47,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23130036417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:13:01,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:13:15,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23276447608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:13:30,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [23276ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:13:35,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:13:51,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33656739875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:14:09,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:14:27,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34265129441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:14:44,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:14:57,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [68682ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:15:04,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34417762391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:15:21,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:15:22,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [37945ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:15:41,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37945718762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:16:03,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [37810ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:16:01,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:16:31,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.8s/37810595199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:17:05,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:17:30,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [63030ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:17:29,391][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [63031ms] which is above the warn threshold of [5s]
[2022-04-05T14:17:32,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63030572209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:18:00,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.7s/57723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:18:06,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [57723ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:18:21,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.7s/57723234945ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:18:40,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:18:40,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [39750ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:19:06,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39750961699ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:19:45,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:20:01,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65425459420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:20:08,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [65425ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:20:19,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:20:39,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7s/32715746830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:20:54,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:20:54,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [36572ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:21:13,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36572808954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:21:29,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:21:46,050][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [33967ms] which is above the warn threshold of [5s]
[2022-04-05T14:21:46,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33966738365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:21:45,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [33966ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:22:20,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:22:36,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51498819365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:22:49,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:23:03,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29100079917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:23:26,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [29100ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:23:32,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.6s/41660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:23:58,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.6s/41659698634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:24:27,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.3s/56342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:24:44,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.3s/56342006994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:24:58,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:22:35,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T14:24:58,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [56342ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:25:09,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31529691587ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:25:26,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [26326ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:25:26,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26327ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:25:40,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26326837283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:25:54,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29309ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:26:13,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29308904471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:26:26,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [29308ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:26:28,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34510ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:26:45,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34510254118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:26:58,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:27:11,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29429056860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:27:28,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [56115ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:27:28,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26687ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:27:41,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26686415547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:27:52,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:28:00,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28205292913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:28:03,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [28205ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:28:07,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:28:25,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14756295124ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:28:42,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35524ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:28:44,963][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35524ms] which is above the warn threshold of [5s]
[2022-04-05T14:28:49,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [35523ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:28:52,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35523620870ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:29:03,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:29:06,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [21045ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:29:16,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21045441812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:29:28,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:29:44,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24040971672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:29:59,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:01,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [56670ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:30:10,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32629966256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:18,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@2c9b7df1, interval=30s}] took [18730ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:30:18,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18731ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:24,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18730630925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:33,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:39,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [13955ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:30:45,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13955940046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:54,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:30:54,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [21659ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:31:14,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21659576094ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:31:24,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:31:34,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30145206789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:31:43,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:31:43,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [30145ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:31:59,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19030774672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:32:12,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:32:18,187][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28558ms] which is above the warn threshold of [5s]
[2022-04-05T14:32:20,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28557941801ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:32:29,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:32:29,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [17677ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:32:40,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17677595940ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:32:54,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:33:07,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22474051359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:33:24,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30524ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:33:37,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [52998ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:33:38,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30524261688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:33:56,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:34:00,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [31523ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:34:19,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31523103079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:34:41,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:35:04,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44957736813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:35:21,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [44957ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:35:22,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:35:50,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41881345680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:36:10,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:36:39,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47669436023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:37:01,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:37:12,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [97855ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:37:48,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50186190844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:38:13,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:38:15,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [73671ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:38:27,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73671536107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:38:48,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:39:07,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34534171218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:39:07,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [34534ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:39:29,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:39:48,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40177859134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:40:09,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:40:32,829][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40270ms] which is above the warn threshold of [5s]
[2022-04-05T14:40:49,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40269423109ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:41:00,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [40269ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:41:11,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:41:34,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.6s/59610016957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:41:59,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.5s/49559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:42:19,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.5s/49559242380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:42:44,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:43:01,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [95005ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:43:16,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45446571881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:43:58,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4091e5d4, interval=5s}] took [62795ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:43:48,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:44:37,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62795787900ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:45:03,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:45:32,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75215640634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:41:54,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T14:45:54,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.9s/51949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:45:58,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [127164ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:46:22,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.9s/51948463742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:46:45,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [51645ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:46:46,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:47:09,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51645201743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:47:31,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:48:00,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [44772ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:48:00,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44772429515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:48:27,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [57615ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:48:27,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:48:48,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57615061640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:49:01,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:49:13,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.6s/33665589714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:49:25,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:49:33,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [58830ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:49:35,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25165116013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:49:45,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@82435a1, interval=1m}] took [19104ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:49:44,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:49:56,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19104882606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:50:08,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:50:23,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23676294565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:50:36,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:50:36,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [23676ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:50:56,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.1s/29151710178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:51:17,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:51:34,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38157151491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:51:51,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:52:07,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [34528ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:52:12,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34528414729ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:52:36,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:52:35,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@82435a1, interval=1m}] took [44786ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:53:06,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44786565568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:53:27,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:53:30,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [51782ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:53:41,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51782275423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:53:55,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:54:18,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28198388499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:54:39,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [69969ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:54:37,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.7s/41771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:55:08,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.7s/41771412665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:55:27,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [48929ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:55:29,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:55:47,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48929030747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:56:24,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:56:49,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48704032451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:57:20,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:57:03,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [48704ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:57:47,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60658938252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:58:34,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@82435a1, interval=1m}] took [72283ms] which is above the warn threshold of [5000ms]
[2022-04-05T14:58:31,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:59:11,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72283357213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T14:59:47,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T14:59:56,120][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [70522ms] which is above the warn threshold of [5s]
[2022-04-05T15:00:40,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70521627558ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:01:02,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [70521ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:01:02,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/84697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:01:28,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/84696281506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:02:56,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:06:30,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105894515913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:07:32,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:08:37,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [269870ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:08:27,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269870190726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:09:54,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/148268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:10:06,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [148267ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:11:02,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/148267790983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:12:06,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:12:03,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3a6eea81, interval=1m}] took [127072ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:13:14,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/127072384409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:05:26,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T15:14:51,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/171798ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:16:06,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [171798ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:16:36,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/171798092672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:18:29,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/214637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:18:38,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [214637ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:20:44,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/214637248000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:22:53,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/254345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:25:33,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/254345518688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:26:05,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [254345ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:27:00,966][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [254346ms] which is above the warn threshold of [5s]
[2022-04-05T15:28:38,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:31:48,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347418054029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:35:04,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:36:16,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [384760ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:38:35,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/384760730062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:42:17,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/418272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:42:21,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@82435a1, interval=1m}] took [418491ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:46:00,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/418491523672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:49:01,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:51:48,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [425976ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:52:37,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/425976477044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:56:06,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/423171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T15:55:40,714][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [425977ms] which is above the warn threshold of [5s]
[2022-04-05T15:56:12,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@47f692aa, interval=5s}] took [423542ms] which is above the warn threshold of [5000ms]
[2022-04-05T15:59:30,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/423542990242ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T16:03:10,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416731ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T16:06:37,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416731036256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T16:06:51,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [416731ms] which is above the warn threshold of [5000ms]
[2022-04-05T16:10:38,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T16:14:28,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448953172878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T15:46:09,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@70507e41] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-05T16:18:00,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/441874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T16:19:35,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@16d5727b, interval=1s}] took [442164ms] which is above the warn threshold of [5000ms]
[2022-04-05T16:23:05,319][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [442164ms] which is above the warn threshold of [5s]
[2022-04-05T16:21:57,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/442164131056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T16:26:57,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/538448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T16:28:42,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@7b8bf216] took [538442ms] which is above the warn threshold of [5000ms]
[2022-04-05T16:30:29,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/538442885542ns] on relative clock which is above the warn threshold of [5000ms]
