[2022-03-27T17:46:52,777][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T17:46:52,797][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T17:46:52,800][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T17:47:09,675][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T17:47:09,687][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T17:47:09,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T17:47:09,696][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T17:47:09,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T17:47:09,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T17:47:09,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T17:47:09,705][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T17:47:09,717][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T17:47:09,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T17:47:09,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T17:47:09,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T17:47:09,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T17:47:09,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T17:47:09,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T17:47:09,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T17:47:09,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T17:47:09,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T17:47:09,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T17:47:09,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T17:47:09,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T17:47:09,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T17:47:09,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T17:47:09,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T17:47:09,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T17:47:09,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T17:47:09,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T17:47:09,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T17:47:09,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T17:47:09,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T17:47:09,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T17:47:09,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T17:47:09,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T17:47:09,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T17:47:09,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T17:47:09,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T17:47:09,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T17:47:09,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T17:47:09,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T17:47:09,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T17:47:09,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T17:47:09,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T17:47:09,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T17:47:09,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T17:47:09,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T17:47:09,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T17:47:09,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T17:47:09,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T17:47:09,812][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T17:47:09,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T17:47:09,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T17:47:09,822][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T17:47:09,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T17:47:09,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T17:47:09,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T17:47:09,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T17:47:09,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T17:47:09,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T17:47:09,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T17:47:10,020][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.7gb], net total_space [125.8gb], types [ext4]
[2022-03-27T17:47:10,021][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T17:47:11,013][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T17:47:31,741][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T17:47:31,744][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T17:47:33,753][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T17:47:34,013][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T17:47:35,529][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T17:47:36,991][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T17:47:36,992][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T17:47:37,040][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T17:47:37,042][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T17:47:37,441][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T17:47:41,679][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T17:47:41,855][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 131, version: 3734, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T17:47:42,182][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 131, version: 3734, reason: Publication{term=131, version=3734}
[2022-03-27T17:47:42,341][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T17:47:42,362][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T17:47:44,026][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T17:47:44,033][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T17:47:45,468][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T17:47:45,469][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T17:47:47,134][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:47,324][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:48,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-27T17:47:48,061][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-27T17:47:48,065][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-27T17:47:48,364][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:50,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T17:47:50,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T17:47:58,958][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T17:48:15,184][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][38] overhead, spent [270ms] collecting in the last [1s]
[2022-03-27T17:48:19,121][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-27T17:48:19,603][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-27T17:48:50,578][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.26][0]]]).
[2022-03-27T17:49:27,559][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 574 finished with response BulkByScrollResponse[took=327.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-27T17:49:30,368][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 599 finished with response BulkByScrollResponse[took=2.5s,timed_out=false,sliceId=null,updated=1039,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-27T17:49:42,455][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-27T17:50:41,627][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:42,207][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:42,572][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:46,125][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:49,220][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:53,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,592][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,745][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,987][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:51,574][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:54,651][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:54,892][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:52:03,704][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:07,779][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:08,711][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:45,784][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:55:21,835][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:57:33,083][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:59:32,216][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:26:08,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:26:08,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:29:35,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@50f1bee5, interval=1s}] took [31305ms] which is above the warn threshold of [5000ms]
[2022-03-27T18:29:51,496][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59330}] took [12865ms] which is above the warn threshold of [5000ms]
[2022-03-27T18:29:35,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T18:33:24,635][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.4s/14468ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T18:40:30,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38043637312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T18:50:37,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/999161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T18:50:39,427][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.3s/21302ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78ac69ee]], which exceeds the warn threshold of [10s]
[2022-03-27T19:01:30,751][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.8s/13860ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78ac69ee]], which exceeds the warn threshold of [10s]
[2022-03-27T19:01:31,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/998758196133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:05:42,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4m/1224665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:06:45,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017ed730@62f2767e] took [1224762ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:07:58,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4m/1224762295911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:06:07,449][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.4s/15422ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f4279a90], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5118159f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T19:09:36,972][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:38120}] took [1224762ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:09:58,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:12:30,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257421582102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:12:31,275][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.3s/22325ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f4279a90], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5118159f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T19:14:46,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/272121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:14:40,518][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:38098}] took [271770ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:16:55,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/271770693812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:18:48,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/256996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:20:07,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257226656122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:15:35,890][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [1753954ms] which is above the warn threshold of [5s]
[2022-03-27T19:21:53,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/181604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:24:07,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/181472293679ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:25:41,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/228160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:31:58,905][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T19:31:58,921][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T19:31:58,922][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T19:32:06,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T19:32:06,558][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T19:32:06,560][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T19:32:06,561][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T19:32:06,563][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T19:32:06,564][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T19:32:06,565][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T19:32:06,567][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T19:32:06,568][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T19:32:06,570][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T19:32:06,571][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T19:32:06,572][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T19:32:06,574][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T19:32:06,575][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T19:32:06,576][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T19:32:06,577][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T19:32:06,577][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T19:32:06,579][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T19:32:06,580][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T19:32:06,581][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T19:32:06,582][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T19:32:06,582][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T19:32:06,583][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T19:32:06,584][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T19:32:06,584][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T19:32:06,585][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T19:32:06,586][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T19:32:06,586][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T19:32:06,587][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T19:32:06,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T19:32:06,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T19:32:06,589][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T19:32:06,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T19:32:06,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T19:32:06,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T19:32:06,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T19:32:06,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T19:32:06,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T19:32:06,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T19:32:06,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T19:32:06,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T19:32:06,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T19:32:06,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T19:32:06,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T19:32:06,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T19:32:06,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T19:32:06,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T19:32:06,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T19:32:06,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T19:32:06,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T19:32:06,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T19:32:06,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T19:32:06,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T19:32:06,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T19:32:06,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T19:32:06,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T19:32:06,721][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.5gb], net total_space [125.8gb], types [ext4]
[2022-03-27T19:32:06,722][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T19:32:07,143][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T19:32:23,141][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T19:32:23,165][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,167][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T19:32:23,170][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,171][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,172][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,173][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,174][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,175][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-27T19:32:23,176][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,177][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,178][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T19:32:23,179][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T19:32:23,180][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,183][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T19:32:26,734][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T19:32:27,025][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T19:32:28,827][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T19:32:30,420][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T19:32:30,422][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T19:32:30,601][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T19:32:30,603][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T19:32:31,093][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T19:32:34,054][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T19:32:34,195][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 132, version: 3812, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T19:32:34,388][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 132, version: 3812, reason: Publication{term=132, version=3812}
[2022-03-27T19:32:34,593][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T19:32:34,594][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T19:32:35,992][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T19:32:36,004][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T19:32:37,480][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T19:32:37,483][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T19:32:39,882][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-27T19:32:39,895][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-27T19:32:39,911][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-27T19:32:42,578][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T19:32:42,641][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T19:32:42,773][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-27T19:32:51,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7033ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:07,958][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13522ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:33:18,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:25,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003156048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:25,736][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [31.9s] publication of cluster state version [3828] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:33:26,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:27,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9339417975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:37,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4fc38b8a] took [31389ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:51,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][19][13] duration [7.2s], collections [2]/[34.8s], total [7.2s]/[8s], memory [152.3mb]->[107.3mb]/[2gb], all_pools {[young] [56mb]->[8mb]/[0b]}{[old] [64.3mb]->[94.6mb]/[2gb]}{[survivor] [32mb]->[4.7mb]/[0b]}
[2022-03-27T19:33:53,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15060ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:54,189][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T19:34:03,984][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [134] timed out after [20550ms]
[2022-03-27T19:34:14,052][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3829}] took [34.6s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3829}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [21663ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [1200ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [39ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [79ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [42ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [1118ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [281ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [106ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [2118ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [2165ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [148ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [4008ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [49ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [1197ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [63ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [112ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [93ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:34:26,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:34:26,581][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][29][14] duration [5.5s], collections [1]/[8.3s], total [5.5s]/[13.6s], memory [175.3mb]->[104mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [94.6mb]->[94.6mb]/[2gb]}{[survivor] [4.7mb]->[9.3mb]/[0b]}
[2022-03-27T19:34:26,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7071082635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:34:26,648][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][29] overhead, spent [5.5s] collecting in the last [8.3s]
[2022-03-27T19:34:29,393][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [50.4s/50467ms] ago, timed out [29.9s/29917ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [134]
[2022-03-27T19:34:35,760][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16834ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:34:37,448][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [19.4s] publication of cluster state version [3830] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:34:57,934][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [164] timed out after [16161ms]
[2022-03-27T19:34:58,792][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [16.7s/16761ms] ago, timed out [600ms/600ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [164]
[2022-03-27T19:35:23,986][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [24532ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [28] unchanged indices
[2022-03-27T19:35:25,176][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.6s] publication of cluster state version [3832] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:35:36,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@667f0d42] took [7381ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:35:42,841][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][52][15] duration [3s], collections [1]/[10.2s], total [3s]/[16.6s], memory [172mb]->[184mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [94.6mb]->[97.9mb]/[2gb]}{[survivor] [9.3mb]->[12mb]/[0b]}
[2022-03-27T19:35:43,063][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [3s] collecting in the last [10.2s]
[2022-03-27T19:35:43,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5137ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:35:43,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60696}] took [98798ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:07,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5805ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:13,016][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3832}] took [45.6s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3832}]] took [0ms], [connecting to new nodes] took [60ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [20017ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [178ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [63ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [66ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [69ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [505ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [26ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [571ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [4522ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [204ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [3760ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [48ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [6903ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [55ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [5378ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [2404ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [244ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [153ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [162ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [57ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:36:21,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@54b3e502] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:44,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8605ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:01,501][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6537ms] which is above the warn threshold of [5s]
[2022-03-27T19:37:02,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [11135ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:10,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@7dcc4038] took [5172ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6662087605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,572][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][69][16] duration [5s], collections [1]/[1.7s], total [5s]/[21.7s], memory [177.9mb]->[113.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [97.9mb]->[103.5mb]/[2gb]}{[survivor] [12mb]->[10.1mb]/[0b]}
[2022-03-27T19:37:43,626][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][69] overhead, spent [5s] collecting in the last [1.7s]
[2022-03-27T19:37:43,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6662ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:45,869][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10813ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:37:46,007][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.4s] publication of cluster state version [3833] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:37:51,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][74][17] duration [861ms], collections [1]/[1.1s], total [861ms]/[22.6s], memory [189.6mb]->[193.6mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [103.5mb]->[107.2mb]/[2gb]}{[survivor] [10.1mb]->[9.8mb]/[0b]}
[2022-03-27T19:37:52,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][74] overhead, spent [861ms] collecting in the last [1.1s]
[2022-03-27T19:38:04,994][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][18] duration [786ms], collections [1]/[1.1s], total [786ms]/[23.4s], memory [177.1mb]->[201.1mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [107.2mb]->[112.7mb]/[2gb]}{[survivor] [9.8mb]->[6.7mb]/[0b]}
[2022-03-27T19:38:05,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [786ms] collecting in the last [1.1s]
[2022-03-27T19:38:11,625][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][82][19] duration [1.1s], collections [1]/[2.3s], total [1.1s]/[24.5s], memory [187.4mb]->[203.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [112.7mb]->[112.7mb]/[2gb]}{[survivor] [6.7mb]->[11.9mb]/[0b]}
[2022-03-27T19:38:11,713][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][82] overhead, spent [1.1s] collecting in the last [2.3s]
[2022-03-27T19:38:12,174][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:39522}] took [19077ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:38:36,069][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][21] duration [1s], collections [1]/[2.1s], total [1s]/[26.1s], memory [192.5mb]->[128.2mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [117.4mb]->[124.3mb]/[2gb]}{[survivor] [11.1mb]->[3.9mb]/[0b]}
[2022-03-27T19:38:36,906][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [1s] collecting in the last [2.1s]
[2022-03-27T19:39:07,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6871ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:39:16,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7472ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:39:25,633][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [37470ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T19:39:30,615][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [49.6s] publication of cluster state version [3838] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:39:41,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@43b9b420] took [15078ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:01,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:17,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6232ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:37,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@32cdb515] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:56,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:40:57,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349957776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:40:58,138][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][109][22] duration [7s], collections [1]/[2.1s], total [7s]/[33.2s], memory [220.2mb]->[220.2mb]/[2gb], all_pools {[young] [92mb]->[4mb]/[0b]}{[old] [124.3mb]->[124.3mb]/[2gb]}{[survivor] [3.9mb]->[7.5mb]/[0b]}
[2022-03-27T19:40:58,980][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][109] overhead, spent [7s] collecting in the last [2.1s]
[2022-03-27T19:41:00,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14983ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:41:29,484][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3838}] took [1.8m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3838}]] took [0ms], [connecting to new nodes] took [27ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [105728ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [178ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [35ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [31ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [486ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [91ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [1028ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [689ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [117ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [2410ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [674ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [1ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [20ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [18ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [1ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [68ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [21ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:41:40,181][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][123][23] duration [3.4s], collections [1]/[1.6s], total [3.4s]/[36.6s], memory [195.8mb]->[219.8mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [124.3mb]->[124.3mb]/[2gb]}{[survivor] [7.5mb]->[12.6mb]/[0b]}
[2022-03-27T19:41:41,320][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][123] overhead, spent [3.4s] collecting in the last [1.6s]
[2022-03-27T19:41:42,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7624ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:41:57,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5143ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:11,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6439ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:21,043][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3839}] took [39.8s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3839}]] took [0ms], [connecting to new nodes] took [287ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [3802ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [727ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [43ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [37ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [33ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [697ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [28ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [12380ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [3267ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [382ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [8676ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [111ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [2454ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [10ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [2836ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [3465ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [48ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [51ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [58ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [47ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:42:31,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@c16e751] took [8631ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:32,649][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60706}] took [18363ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:43:12,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@fc3cb3] took [5898ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:43:12,917][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [37081ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [28] unchanged indices
[2022-03-27T19:43:17,753][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [42.3s] publication of cluster state version [3840] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:43:45,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:43:46,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6382318119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:43:48,223][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][153][24] duration [3s], collections [1]/[8.4s], total [3s]/[39.6s], memory [188.9mb]->[143.5mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [124.3mb]->[129.2mb]/[2gb]}{[survivor] [12.6mb]->[14.2mb]/[0b]}
[2022-03-27T19:43:48,934][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][153] overhead, spent [3s] collecting in the last [8.4s]
[2022-03-27T19:44:16,163][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60708}] took [6557ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:44:23,827][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [29970ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:44:26,175][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [34s] publication of cluster state version [3841] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:45:05,837][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [19s/19087ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T19:45:07,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6123ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:21,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@5f2b336d] took [10113ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:37,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6061ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:56,339][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [34249ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [26] unchanged indices
[2022-03-27T19:46:05,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@261e316d] took [10017ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:46:04,045][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [48.9s] publication of cluster state version [3842] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:46:21,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:46:22,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101079961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:46:22,733][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][25] duration [5.9s], collections [1]/[4.1s], total [5.9s]/[45.6s], memory [211.5mb]->[215.5mb]/[2gb], all_pools {[young] [68mb]->[76mb]/[0b]}{[old] [129.2mb]->[136.3mb]/[2gb]}{[survivor] [14.2mb]->[7.6mb]/[0b]}
[2022-03-27T19:46:23,277][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [5.9s] collecting in the last [4.1s]
[2022-03-27T19:46:23,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [12907ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:47:01,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:47:02,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6884763574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:47:03,519][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][26] duration [4.5s], collections [1]/[7.9s], total [4.5s]/[50.2s], memory [212mb]->[147.2mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [136.3mb]->[136.3mb]/[2gb]}{[survivor] [7.6mb]->[10.9mb]/[0b]}
[2022-03-27T19:47:03,896][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [4.5s] collecting in the last [7.9s]
[2022-03-27T19:47:49,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [20191ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:48:00,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [5749ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:48:08,517][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9864ms] which is above the warn threshold of [5s]
[2022-03-27T19:48:36,154][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3842}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3842}]] took [53ms], [connecting to new nodes] took [45ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [62866ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [278ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [358ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [51ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [43ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [590ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [41ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [45ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [33117ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [17235ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1193ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [981ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [13369ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1052ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [6136ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [3708ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [4129ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [58ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [122ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [55ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:48:40,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4dc16bb] took [11016ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:23,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:49:23,730][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [38.8s/38844ms] to notify listeners on successful publication of cluster state (version: 3842, uuid: sF2-4i7USP28ab9GQa93ig) for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T19:49:24,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37016416254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:49:25,884][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][27] duration [28.1s], collections [1]/[1.2m], total [28.1s]/[1.3m], memory [207.2mb]->[227.2mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [136.3mb]->[140.3mb]/[2gb]}{[survivor] [10.9mb]->[8.9mb]/[0b]}
[2022-03-27T19:49:26,460][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [28.1s] collecting in the last [1.2m]
[2022-03-27T19:49:27,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [43519ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:28,166][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60708}] took [5502ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:49,950][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/69115ms] ago, timed out [5.8s/5849ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [699]
[2022-03-27T19:49:49,148][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [699] timed out after [63266ms]
[2022-03-27T19:50:03,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [9827ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:16,247][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [50.6s/50605ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b6318fb5]], which exceeds the warn threshold of [10s]
[2022-03-27T19:50:17,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5009ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:35,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7719ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:35,358][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6377ms] which is above the warn threshold of [5s]
[2022-03-27T19:51:12,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:50:53,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4f26bc78] took [8508ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:51:12,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300319452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:51:17,426][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][203][28] duration [13.3s], collections [1]/[45.1s], total [13.3s]/[1.5m], memory [213.2mb]->[156.8mb]/[2gb], all_pools {[young] [64mb]->[8mb]/[0b]}{[old] [140.3mb]->[141.2mb]/[2gb]}{[survivor] [8.9mb]->[15.5mb]/[0b]}
[2022-03-27T19:51:20,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][203] overhead, spent [13.3s] collecting in the last [45.1s]
[2022-03-27T19:51:23,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [10841ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:51:55,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6324ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:52:12,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@dba517a] took [8735ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:52:21,935][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [93571ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:52:25,972][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [3843] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:54:36,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:54:37,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118637624293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:54:41,842][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][209][29] duration [1.8m], collections [1]/[2m], total [1.8m]/[3.3m], memory [228.8mb]->[163mb]/[2gb], all_pools {[young] [75.9mb]->[0b]/[0b]}{[old] [141.2mb]->[148.4mb]/[2gb]}{[survivor] [15.5mb]->[14.6mb]/[0b]}
[2022-03-27T19:54:44,670][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][209] overhead, spent [1.8m] collecting in the last [2m]
[2022-03-27T19:54:46,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [10157ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:55:30,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@da53de0] took [18026ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:55:27,020][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10951ms] which is above the warn threshold of [5s]
[2022-03-27T19:56:07,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15885ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:56:33,199][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [796] timed out after [60735ms]
[2022-03-27T19:56:39,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7666ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:56:44,521][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3843}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3843}]] took [48ms], [connecting to new nodes] took [160ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [4333ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [175ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [46ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [119922ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [565ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [67ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [54770ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [73ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [13303ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [740ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [556ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [17900ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1036ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [20453ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [11867ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [3384ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [674ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [67ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [222ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [265ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [173ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [68ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:57:07,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8682ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:57:31,109][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.6m/399019ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [7.5m/451579ms]
[2022-03-27T19:57:36,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [13375ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:57:57,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@13000f0a] took [16055ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:58:31,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15228ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:58:30,248][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.1s/26117ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [57], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [57], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-27T19:58:48,625][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [838] timed out after [50437ms]
[2022-03-27T19:58:55,070][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.3m/203357ms] ago, timed out [2.3m/142622ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [796]
[2022-03-27T19:58:51,094][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9657ms] which is above the warn threshold of [5s]
[2022-03-27T19:59:06,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14722ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:00:01,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [28237ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:00:55,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@44f0025c] took [28562ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:01:33,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24200ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:01:43,017][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.7m/223437ms] ago, timed out [2.8m/173000ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [838]
[2022-03-27T20:02:50,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [16767ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:03:17,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [6228ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:03:20,579][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10406ms] which is above the warn threshold of [5s]
[2022-03-27T20:03:23,315][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [867] timed out after [137504ms]
[2022-03-27T20:04:40,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [51792ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:06:12,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [27097ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:07:22,597][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/383094ms] ago, timed out [4m/245590ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [867]
[2022-03-27T20:07:40,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [35065ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:08:12,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@5552023a] took [24045ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:08:12,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [524113ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T20:08:35,900][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [9.6m] publication of cluster state version [3844] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T20:09:10,804][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12444ms] which is above the warn threshold of [5s]
[2022-03-27T20:09:10,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [5150ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:09:47,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [21795ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:09:59,347][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.7m/106182ms] ago, timed out [6.1s/6180ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [919]
[2022-03-27T20:09:55,134][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [919] timed out after [100002ms]
[2022-03-27T20:10:18,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14027ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:10:42,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [5971ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:11:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [47667ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:12:49,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24250ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:14:07,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@195eb929] took [64812ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:14:38,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10243ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:15:08,357][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3844}] took [6m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3844}]] took [232ms], [connecting to new nodes] took [1275ms], [applying settings] took [55ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [24192ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [1ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [4268ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [230ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [55ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [193ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [273ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [4279ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [81ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [1111ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [55ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [111ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [91601ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [114ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [41732ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1541ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [495ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [47017ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [12415ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [45127ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [51867ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [26758ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [4570ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [4ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [178ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [1328ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [185ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [158ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [175ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [115ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T20:15:31,171][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [25.1m/1510215ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [26m/1562177ms]
[2022-03-27T20:15:58,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [27463ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:15:49,907][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14605ms] which is above the warn threshold of [5s]
[2022-03-27T20:16:43,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [22930ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:17:12,529][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [978] timed out after [176940ms]
[2022-03-27T20:18:23,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [39110ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:18:56,844][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.9m/178736ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T20:19:02,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [23340ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:19:23,130][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.2m/312670ms] ago, timed out [2.2m/135730ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [978]
[2022-03-27T20:19:32,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [5629ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:20:56,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@7e92a380] took [48876ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:21:42,893][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17759ms] which is above the warn threshold of [5s]
[2022-03-27T20:22:15,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [39511ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:22:38,603][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1026] timed out after [103285ms]
[2022-03-27T20:22:48,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5089ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:22:56,178][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/119801ms] ago, timed out [16.5s/16516ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1026]
[2022-03-27T20:23:20,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [21846ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:24:29,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@6772b3ca] took [27915ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:24:46,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5604ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:25:19,186][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7321ms] which is above the warn threshold of [5s]
[2022-03-27T20:25:56,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [18786ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:25:43,874][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1085] timed out after [68711ms]
[2022-03-27T20:26:30,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [20018ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:26:29,347][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [279690ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T20:26:47,393][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6m] publication of cluster state version [3845] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T20:26:58,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [7043ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:27:21,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [13209ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:27:57,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:27:59,436][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/190750ms] ago, timed out [2m/122039ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1085]
[2022-03-27T20:28:05,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16825438317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:14,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17362ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4e73b4c3] took [42731ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:28:22,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17362213066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:29,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:38,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14679117913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:47,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:54,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18117163042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:07,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:24,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19247363113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:24,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [19247ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:29:36,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28993ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:48,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28993809631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:59,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22753ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:11,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22753000999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:27,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:40,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24728472255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:55,145][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][30] duration [11.5s], collections [1]/[2.6m], total [11.5s]/[3.5m], memory [239mb]->[164.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [148.4mb]->[155.3mb]/[2gb]}{[survivor] [14.6mb]->[9.3mb]/[0b]}
[2022-03-27T20:30:55,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:07,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [79581ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:31:10,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32100080869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:21,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:35,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25694212854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:46,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:46,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [25633ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:31:54,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25633141896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:02,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:44,433][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1137] timed out after [186314ms]
[2022-03-27T20:31:50,331][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25633ms] which is above the warn threshold of [5s]
[2022-03-27T20:32:10,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15093939999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:25,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14614823828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:33,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:34,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [31292ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:32:39,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678143508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:47,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:54,822][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.5m/273411ms] ago, timed out [1.4m/87097ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1137]
[2022-03-27T20:32:54,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15077283324ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:02,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:07,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14140ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:33:08,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14140083525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:17,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:22,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260697297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:29,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:37,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11018838527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:46,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:52,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18581165535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:54,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@6cd4f775] took [18581ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:33:56,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:57,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10419ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:00,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10419718030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:05,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:10,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8531ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:10,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8531978994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:14,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:18,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985204318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:24,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:29,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8952649543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:33,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10105ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:33,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:38,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105174178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:45,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:55,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [11627ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:55,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627633758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:40,898][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1195] timed out after [46994ms]
[2022-03-27T20:35:04,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:06,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [19171ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:35:13,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19171286203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:23,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:32,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [18276ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:35:32,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18276305171ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:42,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:50,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18740670765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:00,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:14,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18248094947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:28,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26954798913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:49,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [26954ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:36:53,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:07,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25962274991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:18,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:35,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25526846640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:48,294][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.5m/211502ms] ago, timed out [2.7m/164508ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1195]
[2022-03-27T20:37:50,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:03,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31339587271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:08,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [31339ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:38:14,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:27,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909940241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:41,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:53,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24943892931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:07,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:16,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@136fe03e] took [53165ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:39:16,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28221257233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:25,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@57d50e4f] took [18188ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:39:25,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:37,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18188467646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:47,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:58,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21828073612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:11,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:24,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24310632644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:37,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [49918ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:40:52,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607597041ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:00,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:06,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23438504147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:13,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:18,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12428853891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:24,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:24,309][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1247] timed out after [125802ms]
[2022-03-27T20:41:29,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788367706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:35,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:42,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10305378160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:49,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:53,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14110632368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:55,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [24416ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:41:56,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:02,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7959492081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:07,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:10,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10175928789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:15,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:23,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7761496414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:35,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:37,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [26848ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:42:42,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19087156028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:44,335][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/206991ms] ago, timed out [1.3m/81189ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1247]
[2022-03-27T20:42:48,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:52,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14840324199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:02,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:10,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13296679268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:16,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@1d079082] took [13296ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:43:18,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:25,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16030035027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:34,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:42,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16392333020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:49,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:00,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15139983162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:12,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233696434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:26,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:32,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [37738ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:44:34,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17505077830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:43,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16175ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:52,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16174176456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:04,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:15,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20183898334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:24,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:36,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20300483278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:46,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:35,229][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20300ms] which is above the warn threshold of [5s]
[2022-03-27T20:45:57,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925566309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:11,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24817ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:18,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24816ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:46:25,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24816384157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:35,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:42,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24339864044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:29,635][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1295] timed out after [163885ms]
[2022-03-27T20:46:50,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:58,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15133221280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:07,422][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.8m/228174ms] ago, timed out [1m/64289ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1295]
[2022-03-27T20:47:10,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:15,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [33996ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:47:18,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18863533872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:33,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:47,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24461050421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:56,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:07,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640464718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:14,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:19,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17336224135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:27,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:35,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13439139330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:42,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:51,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15064924266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:00,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:05,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [33540ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:49:08,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18476023291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:19,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:20,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@d7cd0c9] took [14866ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:49:41,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14866100400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:56,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:07,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39709477527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [26198ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:50:36,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26198119200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:53,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:29,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28343756170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:39,897][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3845}] took [23.5m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3845}]] took [202ms], [connecting to new nodes] took [656ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [1103661ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [4866ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [168ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [315ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [163ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [173ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [236ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [6214ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [835ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [199ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [259ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [140986ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [34850ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1281ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [1156ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [17689ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [4016ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [33019ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [383ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [39132ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [26710ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [268ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [8503ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [27ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [598ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [648ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [2139ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [12725ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [880ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [952ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [548ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [480ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T20:51:54,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:57,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [64247ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:52:21,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64247303644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:52:46,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:52:59,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51669305195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:09,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:16,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22508826118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:23,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:29,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15243305737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:36,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:41,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12364733777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:44,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [27608ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:53:47,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:52,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11932884977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:58,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:51,747][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1356] timed out after [272218ms]
[2022-03-27T20:54:07,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10712227548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:07,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [10712ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:54:14,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:21,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16533450647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:27,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:33,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12641232270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:37,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [12641ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:54:43,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:51,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15966779239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:56,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:02,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [13544ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:55:02,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13544893101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:13,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:21,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16063888697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:32,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:58,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17478913240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:04,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [17478ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:56:18,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:33,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46113928980ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:54,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33989ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:16,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33988533773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:17,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@3780334f] took [33988ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:57:30,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:47,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39565754619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:03,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:20,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30818865923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:37,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:00,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34894961639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:23,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:46,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44919082625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:02,296][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [10m/605460ms] ago, timed out [5.5m/333242ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1356]
[2022-03-27T21:00:06,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:26,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41980904773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:45,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:05,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40221138752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:25,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:36,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [80632ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:01:48,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40411213608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:02:15,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:02:57,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49043056224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:03:40,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:04:08,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87288378605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:04:52,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:06:16,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67550365728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:07:39,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168211697494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:23,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:48,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45812378987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:07,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:30,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44851752474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:51,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:10:22,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44156290742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:02,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:47,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71121667643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:47,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [115277ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:12:30,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:14:43,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81980393740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:17:19,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:19:34,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282399642241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:22:03,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292147ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:24:26,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292246678048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:20:13,141][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [282400ms] which is above the warn threshold of [5s]
[2022-03-27T21:24:53,595][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.2m/852960ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [76], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [76], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-27T21:26:58,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:29:39,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294914318924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:32:03,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:35:10,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306725344844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:37:59,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:41:00,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354317640516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:44:12,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:47:13,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334498848209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:49:44,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:51:37,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [372621ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:52:16,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372621398078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:55:21,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:58:09,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330487766639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:01:07,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:03:59,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339610358054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:06:49,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:09:42,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345444829682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:12:21,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:14:57,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331983568281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:17:24,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:19:49,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311551271416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:22:41,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:25:02,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308305704554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:27:27,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:30:27,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293575908107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:33:32,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:00:53,142][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T23:00:53,168][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T23:00:53,170][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T23:01:00,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T23:01:00,349][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T23:01:00,350][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T23:01:00,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T23:01:00,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T23:01:00,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T23:01:00,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T23:01:00,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T23:01:00,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T23:01:00,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T23:01:00,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T23:01:00,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T23:01:00,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T23:01:00,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T23:01:00,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T23:01:00,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T23:01:00,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T23:01:00,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T23:01:00,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T23:01:00,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T23:01:00,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T23:01:00,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T23:01:00,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T23:01:00,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T23:01:00,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T23:01:00,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T23:01:00,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T23:01:00,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T23:01:00,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T23:01:00,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T23:01:00,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T23:01:00,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T23:01:00,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T23:01:00,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T23:01:00,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T23:01:00,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T23:01:00,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T23:01:00,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T23:01:00,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T23:01:00,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T23:01:00,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T23:01:00,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T23:01:00,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T23:01:00,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T23:01:00,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T23:01:00,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T23:01:00,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T23:01:00,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T23:01:00,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T23:01:00,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T23:01:00,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T23:01:00,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T23:01:00,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T23:01:00,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T23:01:00,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T23:01:00,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T23:01:00,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T23:01:00,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T23:01:00,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T23:01:00,474][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-27T23:01:00,475][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T23:01:01,085][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T23:01:14,624][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T23:01:14,630][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,631][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T23:01:14,633][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,633][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,634][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,635][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-27T23:01:14,637][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,638][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T23:01:14,642][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T23:01:14,643][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,644][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T23:01:16,380][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T23:01:16,575][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T23:01:17,977][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T23:01:18,936][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T23:01:18,937][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T23:01:19,036][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T23:01:19,039][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T23:01:19,373][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T23:01:22,045][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T23:01:22,220][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 133, version: 3846, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T23:01:22,452][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 133, version: 3846, reason: Publication{term=133, version=3846}
[2022-03-27T23:01:22,549][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T23:01:22,550][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T23:01:26,100][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T23:01:26,141][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T23:01:27,804][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T23:01:27,806][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T23:01:29,503][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-27T23:01:30,036][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-27T23:06:31,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:15:00,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6431950931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:11:27,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54b4b00c, interval=1s}] took [19754ms] which is above the warn threshold of [5000ms]
[2022-03-27T23:18:06,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9m/958094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:21:29,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9m/958345697704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:23:19,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:24:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318874594432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:27:02,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/208516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:30:33,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207951828223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:34:10,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:38:03,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428727983565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:41:43,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:45:05,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453032585493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:48:18,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:49:20,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@41485283, interval=5s}] took [1812162ms] which is above the warn threshold of [5000ms]
[2022-03-27T23:51:14,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403575013840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:54:51,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:58:47,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388873656035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T00:20:15,986][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T00:20:15,999][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T00:20:16,004][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T00:20:21,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T00:20:21,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T00:20:21,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T00:20:21,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T00:20:21,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T00:20:21,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T00:20:21,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T00:20:21,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T00:20:21,979][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T00:20:21,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T00:20:21,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T00:20:21,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T00:20:21,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T00:20:21,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T00:20:21,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T00:20:21,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T00:20:21,984][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T00:20:21,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T00:20:21,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T00:20:21,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T00:20:21,987][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T00:20:21,987][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T00:20:21,988][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T00:20:21,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T00:20:21,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T00:20:21,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T00:20:21,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T00:20:21,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T00:20:21,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T00:20:21,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T00:20:21,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T00:20:21,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T00:20:22,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T00:20:22,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T00:20:22,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T00:20:22,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T00:20:22,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T00:20:22,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T00:20:22,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T00:20:22,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T00:20:22,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T00:20:22,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T00:20:22,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T00:20:22,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T00:20:22,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T00:20:22,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T00:20:22,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T00:20:22,018][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T00:20:22,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T00:20:22,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T00:20:22,020][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T00:20:22,021][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T00:20:22,022][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T00:20:22,022][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T00:20:22,025][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T00:20:22,025][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T00:20:22,026][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T00:20:22,026][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T00:20:22,028][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T00:20:22,121][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.5gb], net total_space [125.8gb], types [ext4]
[2022-03-28T00:20:22,122][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T00:20:22,508][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T00:20:34,349][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T00:20:34,353][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T00:20:35,793][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T00:20:35,942][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T00:20:36,843][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T00:20:38,149][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T00:20:38,150][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T00:20:38,183][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T00:20:38,185][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T00:20:38,525][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T00:20:43,032][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T00:20:43,306][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 134, version: 3856, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T00:20:43,638][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 134, version: 3856, reason: Publication{term=134, version=3856}
[2022-03-28T00:20:43,826][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T00:20:43,828][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T00:20:48,853][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T00:20:48,873][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-28T00:20:51,647][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-28T00:20:51,649][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-28T00:20:53,693][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-03-28T00:20:56,358][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T00:20:56,372][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T00:20:56,380][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T00:20:58,361][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][20] overhead, spent [293ms] collecting in the last [1s]
[2022-03-28T00:21:00,182][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T00:21:00,377][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][22] overhead, spent [293ms] collecting in the last [1s]
[2022-03-28T00:21:00,555][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T00:21:11,131][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T00:21:11,567][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-03-28T00:21:11,588][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-03-28T00:21:13,938][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T00:21:13,962][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T00:21:17,410][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-28T00:21:20,575][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T00:21:28,970][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T00:21:29,114][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-03-28T00:21:29,118][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-03-28T00:21:30,879][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T00:21:31,019][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-03-28T00:21:31,529][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T00:21:31,537][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T00:21:34,201][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T00:21:34,204][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T00:22:41,945][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.28] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-28T00:22:42,185][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.28][0]]]).
[2022-03-28T00:22:42,488][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:42,850][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,020][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,242][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,315][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,399][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,577][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,800][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,974][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:45,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:45,399][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:45,657][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:47,097][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:22:59,563][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:00,194][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:04,293][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:05,506][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:05,686][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:05,818][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:05,931][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:06,080][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:07,265][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:07,472][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:07,854][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,174][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,316][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,428][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,612][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,824][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,104][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,386][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,688][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,993][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:10,361][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:10,745][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,024][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,238][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,382][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,480][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,066][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,340][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,497][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,628][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,872][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,974][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:13,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:21,936][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,011][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,182][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,525][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,639][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:49,005][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:49,127][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:49,486][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:28:44,793][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:28:45,881][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:28:53,822][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:30:50,367][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:30:50,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:32:07,528][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:33:52,640][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:55:55,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [6529ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:19,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [6649ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:35,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [6152ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:48,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [7294ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:48,995][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5861ms] which is above the warn threshold of [5s]
[2022-03-28T00:57:04,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [5265ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:57:12,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [5308ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:57:44,348][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/83250ms] ago, timed out [57.5s/57502ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [15514]
[2022-03-28T00:58:51,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [54307ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:59:34,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16b41037, interval=5s}] took [12407ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:00:56,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:01:50,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256017084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:02:05,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:02:24,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97365083455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:02:49,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [97365ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:03:04,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:03:04,748][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [154389ms] which is above the warn threshold of [5s]
[2022-03-28T01:03:21,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57023398458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:03:38,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:03:53,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35946928437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:04:21,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:04:54,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42963514276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:05:24,237][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [15514] timed out after [25748ms]
[2022-03-28T01:05:24,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:05:45,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55101931963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:06:03,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:06:24,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47896421118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:06:38,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:07:00,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35004768394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:07:44,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:08:05,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43319118084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:08:44,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73072ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:09:17,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73072007254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:09:43,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:09:57,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66712540833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:10:17,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.9s/34955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:10:39,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.9s/34955156553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:11:00,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:11:36,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [76286ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:11:43,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41331382903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:13:41,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/162347ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:13:41,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16b41037, interval=5s}] took [162347ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:14:37,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/162347257690ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:14:14,276][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.4s/14446ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16]], which exceeds the warn threshold of [10s]
[2022-03-28T01:16:01,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:17:13,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140282102117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:19:34,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:22:05,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212305314009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:24:30,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:27:00,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296682693292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:29:38,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:32:08,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307461637415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:29:29,521][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [296683ms] which is above the warn threshold of [5s]
[2022-03-28T01:34:47,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:37:03,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308163596808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:37:57,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5123/0x00000008017ebb90@265a9f51] took [308163ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:39:16,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269636ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:41:35,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270282336958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:44:16,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/299256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:46:17,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298906880947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:49:05,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:51:37,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268790330150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:54:16,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:54:24,643][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [46.5s/46509ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@58ca7725]], which exceeds the warn threshold of [10s]
[2022-03-28T01:57:11,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330498952372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:00:10,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:58:39,232][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11027ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@805ecb74], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b919decd]], which exceeds the warn threshold of [10s]
[2022-03-28T02:02:44,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/355018253918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:05:23,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312218ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:05:39,449][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.3s/35374ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@805ecb74], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b919decd]], which exceeds the warn threshold of [10s]
[2022-03-28T02:06:34,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [312218ms] which is above the warn threshold of [5000ms]
[2022-03-28T02:07:19,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312218946115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:09:32,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:10:10,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16b41037, interval=5s}] took [249984ms] which is above the warn threshold of [5000ms]
[2022-03-28T02:12:17,801][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.1m/667237ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [13.1m/786703ms]
[2022-03-28T02:12:27,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249984450615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:15:12,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:17:51,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339486621861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:20:33,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:22:36,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296547065866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:25:13,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:26:54,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304686311311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:14:14,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7m/2927402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:18:59,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7m/2927302711993ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:20:10,845][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/93340ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@805ecb74]], which exceeds the warn threshold of [10s]
[2022-03-28T03:25:04,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11m/661715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:30:34,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11m/662271054789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:28:38,792][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.1s/12147ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@58ca7725], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b919decd]], which exceeds the warn threshold of [10s]
[2022-03-28T03:41:53,160][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T03:41:53,207][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T03:41:53,209][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T03:41:59,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T03:41:59,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T03:41:59,723][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T03:41:59,723][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T03:41:59,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T03:41:59,725][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T03:41:59,726][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T03:41:59,727][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T03:41:59,728][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T03:41:59,729][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T03:41:59,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T03:41:59,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T03:41:59,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T03:41:59,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T03:41:59,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T03:41:59,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T03:41:59,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T03:41:59,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T03:41:59,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T03:41:59,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T03:41:59,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T03:41:59,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T03:41:59,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T03:41:59,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T03:41:59,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T03:41:59,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T03:41:59,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T03:41:59,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T03:41:59,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T03:41:59,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T03:41:59,751][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T03:41:59,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T03:41:59,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T03:41:59,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T03:41:59,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T03:41:59,757][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T03:41:59,757][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T03:41:59,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T03:41:59,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T03:41:59,761][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T03:41:59,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T03:41:59,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T03:41:59,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T03:41:59,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T03:41:59,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T03:41:59,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T03:41:59,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T03:41:59,770][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T03:41:59,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T03:41:59,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T03:41:59,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T03:41:59,773][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T03:41:59,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T03:41:59,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T03:41:59,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T03:41:59,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T03:41:59,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T03:41:59,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T03:41:59,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T03:41:59,877][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-28T03:41:59,878][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T03:42:00,323][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T03:42:12,893][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T03:42:12,899][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-28T03:42:12,900][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T03:42:12,902][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-28T03:42:12,903][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T03:42:12,904][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-28T03:42:12,905][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-28T03:42:12,906][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T03:42:12,907][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-28T03:42:12,908][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-28T03:42:12,908][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-28T03:42:12,909][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T03:42:12,911][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T03:42:12,912][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T03:42:12,913][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T03:42:14,184][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T03:42:14,401][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T03:42:15,355][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T03:42:16,691][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T03:42:16,724][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T03:42:17,517][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T03:42:17,540][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T03:42:18,203][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T03:42:20,556][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T03:42:20,775][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{7WjUBEyXSgegYU4N7XIjIw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 135, version: 3956, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{7WjUBEyXSgegYU4N7XIjIw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T03:42:20,971][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{7WjUBEyXSgegYU4N7XIjIw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 135, version: 3956, reason: Publication{term=135, version=3956}
[2022-03-28T03:42:21,123][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T03:42:21,124][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T03:43:51,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5038654427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:46:30,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/200949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:46:53,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/201314949779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:47:21,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73971ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:47:39,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73970329770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:48:04,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:47:15,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@779e2654, interval=1s}] took [16249ms] which is above the warn threshold of [5000ms]
[2022-03-28T03:48:26,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43871945238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:48:48,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:48:48,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@670abd01, interval=5s}] took [45224ms] which is above the warn threshold of [5000ms]
[2022-03-28T03:49:00,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45224584121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:09,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:19,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22663501822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:26,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:31,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17023918975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:35,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:42,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9195430499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:49,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:55,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13640282362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:03,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:11,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13763307321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:17,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15238ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:22,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15237531866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:27,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:32,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9218618207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:39,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:44,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215535017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:49,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:54,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10985838336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:00,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:06,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9993588640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:12,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:25,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11886632481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:34,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:43,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21638021268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:44,945][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T03:52:09,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:52:23,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34121552157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:52:37,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:52:49,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28165843628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:03,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:19,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23474239470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:33,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:05,012][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:34754}] took [139060ms] which is above the warn threshold of [5000ms]
[2022-03-28T03:53:44,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31493945235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:57,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:54:17,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24594203628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:54:34,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:54:53,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35533393924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:55:23,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.3s/50322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:55:36,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.3s/50321884390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:55:49,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:56:00,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25832511399ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:56:16,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:56:31,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26920161500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:09,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:20,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52842811394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:31,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:44,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485271197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:12,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:19,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39062273085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:38,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17011906966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:47,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:56,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19489985453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:07,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:19,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18742029512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:34,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:53,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26615799829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:00:09,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:00:29,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34362518021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:00:49,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:01:07,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39941884421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:01:26,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:01:47,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39012545376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:02:10,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:02:25,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41036568734ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:02:45,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:03:03,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36503940668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:03:23,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:03:40,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36802579439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:05:36,405][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T04:05:36,472][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T04:05:36,473][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T04:05:47,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T04:05:47,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T04:05:47,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T04:05:47,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T04:05:47,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T04:05:47,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T04:05:47,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T04:05:47,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T04:05:47,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T04:05:47,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T04:05:47,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T04:05:47,846][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T04:05:47,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T04:05:47,849][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T04:05:47,851][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T04:05:47,853][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T04:05:47,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T04:05:47,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T04:05:47,855][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T04:05:47,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T04:05:47,859][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T04:05:47,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T04:05:47,861][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T04:05:47,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T04:05:47,865][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T04:05:47,867][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T04:05:47,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T04:05:47,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T04:05:47,869][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T04:05:47,870][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T04:05:47,870][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T04:05:47,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T04:05:47,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T04:05:47,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T04:05:47,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T04:05:47,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T04:05:47,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T04:05:47,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T04:05:47,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T04:05:47,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T04:05:47,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T04:05:47,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T04:05:47,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T04:05:47,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T04:05:47,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T04:05:47,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T04:05:47,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T04:05:47,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T04:05:47,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T04:05:47,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T04:05:47,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T04:05:47,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T04:05:47,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T04:05:47,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T04:05:47,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T04:05:47,918][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T04:05:47,919][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T04:05:47,919][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T04:05:47,921][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T04:05:48,076][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-28T04:05:48,080][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T04:05:48,567][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T04:06:14,953][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T04:06:14,962][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T04:06:18,249][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T04:06:18,567][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T04:06:20,412][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T04:06:22,556][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T04:06:22,571][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T04:06:22,683][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T04:06:22,685][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T04:06:23,115][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T04:06:27,571][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T04:06:27,834][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Md40zX4MSe2wdglOmdyCmQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 136, version: 3958, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Md40zX4MSe2wdglOmdyCmQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T04:06:28,283][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Md40zX4MSe2wdglOmdyCmQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 136, version: 3958, reason: Publication{term=136, version=3958}
[2022-03-28T04:06:28,541][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T04:06:28,560][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T04:06:30,311][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T04:06:30,327][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T04:06:32,681][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-28T04:06:32,682][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-28T04:06:34,833][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-28T04:06:35,437][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-28T04:06:36,762][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][14] overhead, spent [276ms] collecting in the last [1s]
[2022-03-28T04:06:36,812][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-28T04:06:36,925][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T04:06:36,973][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T04:06:36,974][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T04:06:40,566][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T04:06:41,139][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T04:06:57,390][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T04:07:06,404][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.28][0]]]).
[2022-03-28T04:07:51,269][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:51,468][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:51,945][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:53,061][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:55,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:59,977][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:00,309][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:07,202][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:09,853][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:10,254][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:10,539][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:10,947][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:09:33,077][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:10:05,780][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:12:29,925][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:33:56,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6acb1e22, interval=1s}] took [5582ms] which is above the warn threshold of [5000ms]
[2022-03-28T04:52:12,361][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.7s/11770ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7f85232f]], which exceeds the warn threshold of [10s]
[2022-03-28T04:51:48,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:00:08,111][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.2s/11271ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7f85232f]], which exceeds the warn threshold of [10s]
[2022-03-28T05:03:41,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518937326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:05:57,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6m/1777640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:05:20,086][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [57.5s/57507ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c2454078], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@46ca0fd6], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1f35bb87]], which exceeds the warn threshold of [10s]
[2022-03-28T05:08:08,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6m/1777482317120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:11:02,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:12:43,156][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.8s/26828ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c2454078], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@46ca0fd6], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1f35bb87]], which exceeds the warn threshold of [10s]
[2022-03-28T05:12:57,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304558175576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:15:29,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:19:30,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266371588344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:22:36,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426936ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:29:46,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427002153591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:32:20,683][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [34.7m/2082040ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [29m/1741123ms]
[2022-03-28T05:35:36,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8m/769707ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:37:54,024][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [59m/3544662ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [45.9m/2755752ms]
[2022-03-28T05:40:10,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8m/769247855401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:43:00,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/455407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:44:03,407][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [57.1s/57184ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c2454078], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@46ca0fd6], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1f35bb87], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7f85232f]], which exceeds the warn threshold of [10s]
[2022-03-28T05:39:26,489][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [769248ms] which is above the warn threshold of [5s]
[2022-03-28T05:52:55,049][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T05:52:55,078][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T05:52:55,089][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T05:55:05,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T05:55:05,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T05:55:05,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T05:55:05,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T05:55:05,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T05:55:05,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T05:55:05,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T05:55:05,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T05:55:05,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T05:55:05,056][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T05:55:05,056][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T05:55:05,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T05:55:05,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T05:55:05,058][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T05:55:05,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T05:55:05,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T05:55:05,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T05:55:05,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T05:55:05,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T05:55:05,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T05:55:05,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T05:55:05,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T05:55:05,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T05:55:05,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T05:55:05,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T05:55:05,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T05:55:05,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T05:55:05,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T05:55:05,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T05:55:05,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T05:55:05,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T05:55:05,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T05:55:05,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T05:55:05,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T05:55:05,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T05:55:05,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T05:55:05,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T05:55:05,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T05:55:05,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T05:55:05,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T05:55:05,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T05:55:05,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T05:55:05,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T05:55:05,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T05:55:05,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T05:55:05,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T05:55:05,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T05:55:05,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T05:55:05,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T05:55:05,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T05:55:05,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T05:55:05,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T05:55:05,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T05:55:05,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T05:55:05,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T05:55:05,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T05:55:05,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T05:55:05,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T05:55:05,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T05:55:05,181][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.3gb], net total_space [125.8gb], types [ext4]
[2022-03-28T05:55:05,183][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T05:55:05,570][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T05:55:17,640][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T05:55:17,645][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-28T05:55:17,649][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T05:55:17,652][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-28T05:55:17,652][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T05:55:17,653][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-28T05:55:17,655][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-28T05:55:17,664][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T05:55:17,665][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-28T05:55:17,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-28T05:55:17,669][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-28T05:55:17,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T05:55:17,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T05:55:17,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T05:55:17,674][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T05:55:19,122][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T05:55:19,282][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T05:55:20,338][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T05:55:21,329][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T05:55:21,330][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T05:55:21,422][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T05:55:21,424][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T05:55:21,686][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T05:55:23,882][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T05:55:24,037][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{e6eQceSZRIq2OarxKKv8VQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 137, version: 4013, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{e6eQceSZRIq2OarxKKv8VQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T05:55:24,188][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{e6eQceSZRIq2OarxKKv8VQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 137, version: 4013, reason: Publication{term=137, version=4013}
[2022-03-28T05:55:24,299][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T05:55:24,300][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T05:55:26,232][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T05:55:26,261][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T05:56:39,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:57:45,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50818583069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:57:53,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:57:25,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2f9dd8a8, interval=1s}] took [51557ms] which is above the warn threshold of [5000ms]
[2022-03-28T05:58:07,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94210334900ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:58:17,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:58:35,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24073853025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:58:56,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:59:04,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40038781680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:59:34,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:59:56,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36065095439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:03,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:12,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29946835174ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:20,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:26,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16664453178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:51,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:57,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32255822701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:03,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:12,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10590506928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:19,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:26,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16765555346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:32,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:36,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12815377910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:45,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:52,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12478209147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:58,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14323ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:07,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14322125027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:19,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:24,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19922668178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:28,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:36,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10016838638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:43,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:48,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14402139154ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:54,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:58,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11447451832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:03,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:08,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8760325239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:13,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:16,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10050828237ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:18,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:21,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518429255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:04:06,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:04:21,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21691880202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:04:51,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:05:05,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43518163709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:05:21,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:05:38,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30884593564ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:02,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:22,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41313600422ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:42,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:59,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40150814845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:07:20,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:07:53,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36882919486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:09:07,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:10:07,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105048163479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:11:31,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:12:59,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142607261440ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:15:23,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/230693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:17:57,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/230692815253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:20:40,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:23:42,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314206268884ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:26:30,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:29:23,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349908042172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:32:06,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:34:53,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337280655066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:37:43,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:40:39,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331046696731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:43:26,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:46:10,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350010827825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:49:19,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:51:54,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339240130797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:54:43,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:57:12,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336920479337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:59:37,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:02:18,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292472726025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T07:04:52,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:10:54,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315614919986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T07:14:24,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:17:17,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571662878844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T07:20:34,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:26:46,707][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T07:26:46,750][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T07:26:46,751][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T07:26:56,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T07:26:56,330][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T07:26:56,330][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T07:26:56,331][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T07:26:56,332][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T07:26:56,332][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T07:26:56,333][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T07:26:56,333][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T07:26:56,341][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T07:26:56,342][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T07:26:56,342][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T07:26:56,343][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T07:26:56,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T07:26:56,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T07:26:56,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T07:26:56,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T07:26:56,346][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T07:26:56,347][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T07:26:56,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T07:26:56,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T07:26:56,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T07:26:56,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T07:26:56,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T07:26:56,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T07:26:56,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T07:26:56,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T07:26:56,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T07:26:56,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T07:26:56,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T07:26:56,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T07:26:56,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T07:26:56,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T07:26:56,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T07:26:56,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T07:26:56,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T07:26:56,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T07:26:56,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T07:26:56,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T07:26:56,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T07:26:56,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T07:26:56,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T07:26:56,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T07:26:56,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T07:26:56,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T07:26:56,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T07:26:56,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T07:26:56,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T07:26:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T07:26:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T07:26:56,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T07:26:56,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T07:26:56,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T07:26:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T07:26:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T07:26:56,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T07:26:56,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T07:26:56,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T07:26:56,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T07:26:56,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T07:26:56,535][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.3gb], net total_space [125.8gb], types [ext4]
[2022-03-28T07:26:56,536][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T07:26:56,942][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T07:27:22,652][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T07:27:22,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T07:27:25,132][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T07:27:25,457][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T07:27:27,173][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T07:27:28,915][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T07:27:28,918][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T07:27:28,971][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T07:27:28,973][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T07:27:29,371][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T07:27:34,131][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T07:27:34,336][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{m5DZ2fMjStiis9-J7vfJLw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 138, version: 4015, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{m5DZ2fMjStiis9-J7vfJLw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T07:27:34,659][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{m5DZ2fMjStiis9-J7vfJLw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 138, version: 4015, reason: Publication{term=138, version=4015}
[2022-03-28T07:27:34,974][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T07:27:34,975][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T07:27:39,184][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T07:27:39,233][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T07:27:42,577][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-28T07:27:42,581][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-28T07:27:47,134][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T07:27:47,174][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T07:27:47,194][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T07:27:47,905][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-28T07:27:51,373][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T07:27:51,515][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:51,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T07:27:54,308][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:54,369][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:55,715][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:56,345][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:56,407][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:57,151][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:57,266][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,123][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,209][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,271][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,325][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:28:07,364][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T07:28:33,329][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.28][0]]]).
[2022-03-28T07:29:14,840][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T07:29:17,818][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T08:10:50,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@65d702bb, interval=1s}] took [21429ms] which is above the warn threshold of [5000ms]
[2022-03-28T08:21:15,753][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11062ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9]], which exceeds the warn threshold of [10s]
[2022-03-28T08:19:27,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:33:43,630][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12s/12036ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:35:40,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4m/1164533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:40:39,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4m/1164741412986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:40:38,282][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.7s/12770ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:43:16,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/573524ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:45:54,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/573506986520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:45:02,324][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [45.6s/45696ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b8a4b47], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3f5f6f8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:48:56,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:51:29,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329541245921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:51:26,422][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.7s/24791ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b8a4b47], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3f5f6f8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:54:22,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@65d702bb, interval=1s}] took [312799ms] which is above the warn threshold of [5000ms]
[2022-03-28T08:53:59,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:58:44,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312799607002ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:59:35,634][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10.7m/642340ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [16.4m/984857ms]
[2022-03-28T09:01:43,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T09:03:22,451][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [18.1m/1090416ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [21.9m/1318308ms]
[2022-03-28T09:05:05,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448075136156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T09:07:47,961][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [46.8s/46875ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b8a4b47], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3f5f6f8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T09:14:29,385][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T09:14:29,460][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T09:14:29,466][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
