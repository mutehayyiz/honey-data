[2022-04-05T16:41:34,689][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T16:41:34,705][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T16:41:34,706][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T16:41:40,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T16:41:40,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T16:41:40,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T16:41:40,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T16:41:40,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T16:41:40,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T16:41:40,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T16:41:40,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T16:41:40,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T16:41:40,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T16:41:40,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T16:41:40,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T16:41:40,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T16:41:40,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T16:41:40,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T16:41:40,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T16:41:40,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T16:41:40,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T16:41:40,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T16:41:40,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T16:41:40,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T16:41:40,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T16:41:40,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T16:41:40,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T16:41:40,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T16:41:40,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T16:41:40,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T16:41:40,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T16:41:40,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T16:41:40,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T16:41:40,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T16:41:40,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T16:41:40,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T16:41:40,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T16:41:40,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T16:41:40,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T16:41:40,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T16:41:40,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T16:41:40,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T16:41:40,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T16:41:40,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T16:41:40,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T16:41:40,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T16:41:40,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T16:41:40,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T16:41:40,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T16:41:40,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T16:41:40,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T16:41:40,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T16:41:40,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T16:41:40,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T16:41:40,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T16:41:40,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T16:41:40,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T16:41:40,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T16:41:40,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T16:41:40,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T16:41:40,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T16:41:40,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T16:41:40,442][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.5gb], net total_space [125.8gb], types [ext4]
[2022-04-05T16:41:40,443][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T16:41:40,734][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T16:41:51,479][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T16:41:51,483][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T16:41:52,637][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T16:41:52,781][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T16:41:53,970][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T16:41:54,935][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T16:41:54,936][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T16:41:54,981][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T16:41:54,983][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T16:41:55,205][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T16:41:58,244][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T16:41:58,426][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{gaYA5D8dQCWPK49CSMzNGQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 206, version: 7479, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{gaYA5D8dQCWPK49CSMzNGQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T16:41:58,669][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{gaYA5D8dQCWPK49CSMzNGQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 206, version: 7479, reason: Publication{term=206, version=7479}
[2022-04-05T16:41:58,795][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T16:41:58,796][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T16:42:00,593][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T16:42:00,606][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T16:42:01,681][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T16:42:01,683][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T16:42:02,462][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-05T16:42:02,695][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-05T16:42:03,176][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-05T16:42:03,260][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T16:42:03,268][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T16:42:03,272][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T16:42:04,044][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T16:42:04,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T16:42:07,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T16:42:17,867][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.05][0]]]).
[2022-04-05T16:42:21,367][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-05T16:42:21,567][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-05T16:43:05,096][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 565 finished with response BulkByScrollResponse[took=366.8ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-05T16:43:07,497][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 582 finished with response BulkByScrollResponse[took=2.4s,timed_out=false,sliceId=null,updated=1025,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-05T16:43:15,827][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-05T16:43:36,555][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T16:43:37,209][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T16:43:54,151][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:04,371][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:28,450][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:36,866][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:37,494][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:17:46,779][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:17:47,266][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:19:51,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:20:02,020][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:20:11,092][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:24:04,192][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:25:03,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@41484e5c, interval=1s}] took [29543ms] which is above the warn threshold of [5000ms]
[2022-04-05T17:26:22,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16e41a2c, interval=5s}] took [13022ms] which is above the warn threshold of [5000ms]
[2022-04-05T17:30:58,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T17:37:46,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5182468799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T17:41:01,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14m/845726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T17:37:07,808][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.6s/10669ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T17:43:34,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14m/845405294539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T17:45:34,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/274263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T17:45:21,503][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.3s/21374ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T17:47:49,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/274359640101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T17:56:31,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9m/655552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:03:36,106][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [18.6m/1119765ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [13.7m/824405ms]
[2022-04-05T18:02:48,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9m/655730999227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:10:58,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4m/867921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:09:21,927][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [29.5m/1775496ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [33.5m/2011447ms]
[2022-04-05T18:16:15,850][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [44m/2643462ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [31.3m/1878116ms]
[2022-04-05T18:15:57,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4m/867965713071ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:15:50,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b2101c9, interval=5s}] took [867965ms] which is above the warn threshold of [5000ms]
[2022-04-05T18:21:34,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/635723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:22:03,930][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13s/13054ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5c00c5e0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T18:24:27,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/635228745177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:17:05,672][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [867966ms] which is above the warn threshold of [5s]
[2022-04-05T18:26:05,885][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.4s/30492ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5c00c5e0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T18:27:26,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:31:41,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351527032424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:37:28,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/602155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:41:45,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/602474728126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:42:55,744][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3630217ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [43.2m/2595728ms]
[2022-04-05T18:44:56,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:55:43,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/447948718976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:55:27,087][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.3h/4680641ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [58.8m/3528870ms]
[2022-04-05T18:59:10,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14m/844965ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:59:29,891][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.5h/5525469ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [1.1h/4256545ms]
[2022-04-05T19:04:09,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T19:04:09,485][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T19:04:09,486][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T19:04:14,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T19:04:14,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T19:04:14,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T19:04:14,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T19:04:14,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T19:04:14,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T19:04:14,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T19:04:14,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T19:04:14,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T19:04:14,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T19:04:14,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T19:04:14,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T19:04:14,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T19:04:14,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T19:04:14,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T19:04:14,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T19:04:14,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T19:04:14,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T19:04:14,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T19:04:14,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T19:04:14,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T19:04:14,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T19:04:14,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T19:04:14,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T19:04:14,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T19:04:14,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T19:04:14,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T19:04:14,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T19:04:14,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T19:04:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T19:04:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T19:04:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T19:04:14,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T19:04:14,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T19:04:14,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T19:04:14,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T19:04:14,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T19:04:14,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T19:04:14,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T19:04:14,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T19:04:14,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T19:04:14,846][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T19:04:14,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T19:04:14,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T19:04:14,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T19:04:14,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T19:04:14,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T19:04:14,849][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T19:04:14,851][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T19:04:14,852][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T19:04:14,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T19:04:14,855][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T19:04:14,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T19:04:14,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T19:04:14,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T19:04:14,968][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-05T19:04:14,969][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T19:04:16,307][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T19:06:10,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/106544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:06:10,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/106613821073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:06:24,181][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T19:06:24,188][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-05T19:06:24,196][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T19:06:24,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-05T19:06:24,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T19:06:24,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-05T19:06:24,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-05T19:06:24,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T19:06:24,203][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-05T19:06:24,203][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-05T19:06:24,204][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-05T19:06:24,205][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T19:06:24,206][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T19:06:24,208][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T19:06:24,208][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T19:06:25,531][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T19:06:25,714][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T19:06:26,468][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T19:06:27,339][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T19:06:27,340][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T19:06:27,444][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T19:06:27,446][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T19:06:27,737][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T19:06:29,951][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T19:06:30,142][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 207, version: 7546, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T19:06:30,310][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 207, version: 7546, reason: Publication{term=207, version=7546}
[2022-04-05T19:06:30,449][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T19:06:30,450][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T19:06:31,356][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T19:06:31,368][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T19:06:32,221][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T19:06:32,222][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T19:06:33,515][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-05T19:06:34,263][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T19:06:34,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T19:06:34,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T19:08:00,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:39,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5010497828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:48,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/244577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:50,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/245142213515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:48,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7092ae78, interval=5s}] took [245142ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:10:52,930][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [257995ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [39] unchanged indices
[2022-04-05T19:10:55,079][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.3m] publication of cluster state version [7557] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:10:59,233][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][10][9] duration [946ms], collections [1]/[4m], total [946ms]/[1.2s], memory [622.3mb]->[93.9mb]/[2gb], all_pools {[young] [548mb]->[0b]/[0b]}{[old] [54.3mb]->[54.3mb]/[2gb]}{[survivor] [23.9mb]->[39.5mb]/[0b]}
[2022-04-05T19:11:01,568][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-05T19:11:04,353][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53912}] took [12630ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:11:13,166][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][17][11] duration [1.4s], collections [1]/[3.8s], total [1.4s]/[3s], memory [183.1mb]->[104mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [92.5mb]->[92.5mb]/[2gb]}{[survivor] [6.6mb]->[11.5mb]/[0b]}
[2022-04-05T19:11:13,610][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][17] overhead, spent [1.4s] collecting in the last [3.8s]
[2022-04-05T19:11:29,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [13433ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:11:44,346][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T19:12:01,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7092ae78, interval=5s}] took [28514ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:10,499][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T19:12:21,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [6166ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:37,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5922ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:36,873][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53924}] took [7240ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:40,279][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [71264ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [39] unchanged indices
[2022-04-05T19:12:45,474][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.5m] publication of cluster state version [7559] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:12:53,523][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][26][12] duration [1.2s], collections [1]/[2.6s], total [1.2s]/[4.2s], memory [180mb]->[106.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [92.5mb]->[95.7mb]/[2gb]}{[survivor] [11.5mb]->[10.6mb]/[0b]}
[2022-04-05T19:12:53,902][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][26] overhead, spent [1.2s] collecting in the last [2.6s]
[2022-04-05T19:12:57,184][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-05T19:13:09,873][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][34][13] duration [3.3s], collections [1]/[5.6s], total [3.3s]/[7.5s], memory [182.4mb]->[112.8mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [95.7mb]->[102.1mb]/[2gb]}{[survivor] [10.6mb]->[10.6mb]/[0b]}
[2022-04-05T19:13:10,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][34] overhead, spent [3.3s] collecting in the last [5.6s]
[2022-04-05T19:13:26,518][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][42] overhead, spent [629ms] collecting in the last [1.1s]
[2022-04-05T19:13:31,273][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T19:13:34,692][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][47][15] duration [803ms], collections [1]/[2s], total [803ms]/[9s], memory [165.6mb]->[114.4mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [105.6mb]->[105.6mb]/[2gb]}{[survivor] [8mb]->[8.8mb]/[0b]}
[2022-04-05T19:13:35,520][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][47] overhead, spent [803ms] collecting in the last [2s]
[2022-04-05T19:13:42,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5314ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:14:10,973][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][53][16] duration [1.6s], collections [1]/[3.1s], total [1.6s]/[10.6s], memory [198.4mb]->[117.2mb]/[2gb], all_pools {[young] [84mb]->[12mb]/[0b]}{[old] [105.6mb]->[110.9mb]/[2gb]}{[survivor] [8.8mb]->[6.3mb]/[0b]}
[2022-04-05T19:14:20,222][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][53] overhead, spent [1.6s] collecting in the last [3.1s]
[2022-04-05T19:14:22,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [21885ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:14:33,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:14:33,926][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][54][17] duration [4.5s], collections [1]/[25.3s], total [4.5s]/[15.1s], memory [117.2mb]->[193.2mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [110.9mb]->[110.9mb]/[2gb]}{[survivor] [6.3mb]->[11.7mb]/[0b]}
[2022-04-05T19:14:33,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6200901331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:14:34,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [7074ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:15:00,496][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.3s] publication of cluster state version [7564] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:15:10,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][67][18] duration [3.3s], collections [1]/[6.1s], total [3.3s]/[18.5s], memory [206.6mb]->[131mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [110.9mb]->[115.8mb]/[2gb]}{[survivor] [11.7mb]->[11.2mb]/[0b]}
[2022-04-05T19:15:10,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][67] overhead, spent [3.3s] collecting in the last [6.1s]
[2022-04-05T19:15:24,133][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43380}] took [51320ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:15:28,232][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12365ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [37] unchanged indices
[2022-04-05T19:15:29,429][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14s] publication of cluster state version [7565] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:15:46,178][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][19] duration [2s], collections [1]/[5s], total [2s]/[20.5s], memory [203mb]->[211mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [115.8mb]->[121.4mb]/[2gb]}{[survivor] [11.2mb]->[8mb]/[0b]}
[2022-04-05T19:15:46,327][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [2s] collecting in the last [5s]
[2022-04-05T19:15:52,787][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][85] overhead, spent [427ms] collecting in the last [1.3s]
[2022-04-05T19:15:57,745][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][87][21] duration [1.5s], collections [1]/[3.3s], total [1.5s]/[22.5s], memory [199mb]->[134.1mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [121.4mb]->[125.6mb]/[2gb]}{[survivor] [9.5mb]->[8.4mb]/[0b]}
[2022-04-05T19:15:58,048][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][87] overhead, spent [1.5s] collecting in the last [3.3s]
[2022-04-05T19:16:00,887][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [541ms] collecting in the last [1.6s]
[2022-04-05T19:16:07,178][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [489ms] collecting in the last [1.4s]
[2022-04-05T19:16:25,250][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [496ms] collecting in the last [1.4s]
[2022-04-05T19:16:34,527][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][113][28] duration [1.2s], collections [1]/[2.6s], total [1.2s]/[25.6s], memory [229.8mb]->[163.9mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [145.8mb]->[153.3mb]/[2gb]}{[survivor] [12mb]->[10.6mb]/[0b]}
[2022-04-05T19:16:35,957][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][113] overhead, spent [1.2s] collecting in the last [2.6s]
[2022-04-05T19:16:42,748][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][114][30] duration [3s], collections [2]/[3.3s], total [3s]/[28.6s], memory [163.9mb]->[233.2mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [153.3mb]->[156.3mb]/[2gb]}{[survivor] [10.6mb]->[13.2mb]/[0b]}
[2022-04-05T19:16:43,161][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][114] overhead, spent [3s] collecting in the last [3.3s]
[2022-04-05T19:16:43,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5928ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:16:49,940][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][117][31] duration [1s], collections [1]/[1.3s], total [1s]/[29.7s], memory [197.5mb]->[213.5mb]/[2gb], all_pools {[young] [28mb]->[4mb]/[0b]}{[old] [156.3mb]->[161mb]/[2gb]}{[survivor] [13.2mb]->[13.1mb]/[0b]}
[2022-04-05T19:16:50,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [1s] collecting in the last [1.3s]
[2022-04-05T19:16:52,334][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][118][32] duration [792ms], collections [1]/[4.2s], total [792ms]/[30.5s], memory [213.5mb]->[174.7mb]/[2gb], all_pools {[young] [4mb]->[20mb]/[0b]}{[old] [161mb]->[167.7mb]/[2gb]}{[survivor] [13.1mb]->[6.9mb]/[0b]}
[2022-04-05T19:17:07,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:17:07,917][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][125][33] duration [3.3s], collections [1]/[1.6s], total [3.3s]/[33.8s], memory [258.7mb]->[176.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [167.7mb]->[167.7mb]/[2gb]}{[survivor] [6.9mb]->[8.3mb]/[0b]}
[2022-04-05T19:17:07,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5507261345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:17:08,562][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][125] overhead, spent [3.3s] collecting in the last [1.6s]
[2022-04-05T19:17:09,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [8228ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:04,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [8113ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:12,576][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=207, version=7576}] took [45.2s] which is above the warn threshold of [30s]: [running task [Publication{term=207, version=7576}]] took [0ms], [connecting to new nodes] took [70ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@4556b7d] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@5dec30b5] took [767ms], [org.elasticsearch.script.ScriptService@4998b0d] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [0ms], [org.elasticsearch.snapshots.RestoreService@1ba57fc7] took [0ms], [org.elasticsearch.ingest.IngestService@27f08d9e] took [670ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6ff9ab8e] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016cddc0@7ccb2b64] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@4ca0287d] took [0ms], [org.elasticsearch.tasks.TaskManager@6f9fd00a] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@4d18a3ac] took [33ms], [org.elasticsearch.cluster.InternalClusterInfoService@6589f19e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@72403552] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66e3b204] took [113ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@433b486a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@359b7368] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2406de71] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4a00ceae] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@4714c437] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@793b4ed9] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@7641461] took [11440ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3aa9c8e9] took [99ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@653547d6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@11af3519] took [2372ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2a330bae] took [194ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@44aff015] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@beb04a1] took [8553ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [3466ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6304ff38] took [7476ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@51d8f68d] took [67ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7d688122] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@71f3dd86] took [4076ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@10c2a3ab] took [2268ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@334f0aa0] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@68cf110c] took [1021ms], [org.elasticsearch.node.ResponseCollectorService@6d953845] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@540aac] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4b050a0c] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@5b522511] took [56ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@71302f6d] took [152ms], [org.elasticsearch.indices.store.IndicesStore@2029b7d2] took [84ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7529b199] took [0ms], [org.elasticsearch.license.LicenseService@722ebc70] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f6f0ea8] took [193ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@651913b1] took [0ms], [org.elasticsearch.gateway.GatewayService@184d8c89] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2fc7d780] took [0ms]
[2022-04-05T19:18:20,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@4fd9bf2c] took [9633ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:28,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:46,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [6607ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:47,567][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53756}] took [112489ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:47,567][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53758}] took [109535ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:56,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:18:57,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5554829875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:01,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5263ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:19:01,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:02,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5263304178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:10,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:13,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9000252321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:16,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5884ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:20,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5883780781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:23,308][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54000}] took [5884ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:19:23,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:23,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7449067157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:25,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@4d482020] took [23449ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:19:35,896][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [54206ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [35] unchanged indices
[2022-04-05T19:19:35,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:36,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9127127523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:36,111][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1m] publication of cluster state version [7577] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:19:36,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][137][34] duration [7s], collections [1]/[38.7s], total [7s]/[40.8s], memory [252.1mb]->[190.8mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [167.7mb]->[167.7mb]/[2gb]}{[survivor] [8.3mb]->[11mb]/[0b]}
[2022-04-05T19:19:45,380][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53764}] took [55586ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:04,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e7b1b78, interval=5s}] took [10078ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:04,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:05,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478553935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:10,348][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][35] duration [6s], collections [1]/[14s], total [6s]/[46.9s], memory [242.8mb]->[230.4mb]/[2gb], all_pools {[young] [68mb]->[52mb]/[0b]}{[old] [167.7mb]->[171.3mb]/[2gb]}{[survivor] [11mb]->[11.1mb]/[0b]}
[2022-04-05T19:20:13,466][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [6s] collecting in the last [14s]
[2022-04-05T19:20:14,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [9325ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:21,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@66bcf642] took [6137ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:38,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:39,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13764573174ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:46,845][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][145][36] duration [8.2s], collections [1]/[33.9s], total [8.2s]/[55.2s], memory [230.4mb]->[180.7mb]/[2gb], all_pools {[young] [52mb]->[68mb]/[0b]}{[old] [171.3mb]->[175.2mb]/[2gb]}{[survivor] [11.1mb]->[5.4mb]/[0b]}
[2022-04-05T19:20:49,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [10136ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:21:05,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [7934ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:21:36,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:21:37,103][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][37] duration [14.3s], collections [1]/[13.2s], total [14.3s]/[1.1m], memory [256.7mb]->[264.7mb]/[2gb], all_pools {[young] [80mb]->[84mb]/[0b]}{[old] [175.2mb]->[175.2mb]/[2gb]}{[survivor] [5.4mb]->[5.4mb]/[0b]}
[2022-04-05T19:21:37,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20505714014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:21:38,262][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [14.3s] collecting in the last [13.2s]
[2022-04-05T19:21:40,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [29457ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:21:57,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54040}] took [10821ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:08,570][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53760}] took [301131ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:09,277][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53762}] took [301131ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:29,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@57124d1c] took [40439ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:52,344][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43432}] took [21228ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:53,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [7204ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:53,882][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54044}] took [10406ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:23:08,429][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54038}] took [5204ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:23:31,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:32,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421407603ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:32,503][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][38] duration [6.8s], collections [1]/[4.7s], total [6.8s]/[1.2m], memory [228.3mb]->[236.3mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [175.2mb]->[175.2mb]/[2gb]}{[survivor] [5mb]->[5mb]/[0b]}
[2022-04-05T19:23:32,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:33,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14031345950ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:33,027][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [6.8s] collecting in the last [4.7s]
[2022-04-05T19:23:33,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [26454ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:23:33,997][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=207, version=7577}] took [3.9m] which is above the warn threshold of [30s]: [running task [Publication{term=207, version=7577}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@4556b7d] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@5dec30b5] took [185510ms], [org.elasticsearch.script.ScriptService@4998b0d] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [0ms], [org.elasticsearch.snapshots.RestoreService@1ba57fc7] took [0ms], [org.elasticsearch.ingest.IngestService@27f08d9e] took [1920ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6ff9ab8e] took [152ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016cddc0@7ccb2b64] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@4ca0287d] took [158ms], [org.elasticsearch.tasks.TaskManager@6f9fd00a] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@4d18a3ac] took [148ms], [org.elasticsearch.cluster.InternalClusterInfoService@6589f19e] took [1ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@72403552] took [71ms], [org.elasticsearch.indices.SystemIndexManager@66e3b204] took [610ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@433b486a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@359b7368] took [1242ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2406de71] took [78ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4a00ceae] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@4714c437] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@793b4ed9] took [96ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@7641461] took [21960ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3aa9c8e9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@653547d6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@11af3519] took [23603ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2a330bae] took [149ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@44aff015] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@beb04a1] took [440ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [42ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6304ff38] took [92ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@51d8f68d] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7d688122] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@71f3dd86] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@10c2a3ab] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@334f0aa0] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@68cf110c] took [117ms], [org.elasticsearch.node.ResponseCollectorService@6d953845] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@540aac] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4b050a0c] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@5b522511] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@71302f6d] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2029b7d2] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7529b199] took [0ms], [org.elasticsearch.license.LicenseService@722ebc70] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f6f0ea8] took [50ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@651913b1] took [0ms], [org.elasticsearch.gateway.GatewayService@184d8c89] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2fc7d780] took [0ms]
[2022-04-05T19:23:39,036][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.7m/406159ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [6.6m/399807ms]
[2022-04-05T19:23:59,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:24:22,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:24:22,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@79289f93, interval=30s}] took [13100ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:24:25,323][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54054}] took [17703ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:24:25,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13100323452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:24:29,306][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][160][40] duration [7.8s], collections [2]/[20.5s], total [7.8s]/[1.4m], memory [252.7mb]->[272.8mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [175.2mb]->[181.4mb]/[2gb]}{[survivor] [9.4mb]->[8.9mb]/[0b]}
[2022-04-05T19:24:29,631][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][160] overhead, spent [7.8s] collecting in the last [20.5s]
[2022-04-05T19:24:33,867][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][161][41] duration [2.3s], collections [1]/[7.4s], total [2.3s]/[1.4m], memory [272.8mb]->[193mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [181.4mb]->[181.4mb]/[2gb]}{[survivor] [8.9mb]->[11.5mb]/[0b]}
[2022-04-05T19:24:34,200][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [2.3s] collecting in the last [7.4s]
[2022-04-05T19:24:34,729][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [43644ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T19:24:35,119][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [47.5s] publication of cluster state version [7578] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:24:39,495][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][42] duration [1s], collections [1]/[2.7s], total [1s]/[1.4m], memory [253mb]->[196.5mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [181.4mb]->[186mb]/[2gb]}{[survivor] [11.5mb]->[10.5mb]/[0b]}
[2022-04-05T19:24:39,801][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [1s] collecting in the last [2.7s]
[2022-04-05T19:24:42,537][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][166] overhead, spent [445ms] collecting in the last [1.1s]
[2022-04-05T19:24:54,780][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T19:24:56,001][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T19:25:17,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8834739669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:46,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28312ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:52,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28312034660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:59,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:08,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12724460606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:25,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:32,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26810742571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:29,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@33f70c4b] took [85823ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:26:32,735][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54068}] took [67848ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:26:41,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:47,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16033847638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:48,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [16033ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:26:49,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:50,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8749332054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:58,470][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][45] duration [2.9s], collections [1]/[4.5s], total [2.9s]/[1.5m], memory [265.7mb]->[200.8mb]/[2gb], all_pools {[young] [72mb]->[48mb]/[0b]}{[old] [191.5mb]->[191.5mb]/[2gb]}{[survivor] [6.1mb]->[9.2mb]/[0b]}
[2022-04-05T19:26:59,587][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [2.9s] collecting in the last [4.5s]
[2022-04-05T19:27:00,401][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [121886ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T19:27:02,303][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2m] publication of cluster state version [7580] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:27:05,318][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][181][46] duration [1.3s], collections [1]/[3.3s], total [1.3s]/[1.5m], memory [256.8mb]->[200mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [191.5mb]->[194.4mb]/[2gb]}{[survivor] [9.2mb]->[5.5mb]/[0b]}
[2022-04-05T19:27:05,673][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][181] overhead, spent [1.3s] collecting in the last [3.3s]
[2022-04-05T19:27:07,133][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][182] overhead, spent [538ms] collecting in the last [1.8s]
[2022-04-05T19:27:11,356][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][185] overhead, spent [381ms] collecting in the last [1.4s]
[2022-04-05T19:27:15,042][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][187][49] duration [838ms], collections [1]/[1.9s], total [838ms]/[1.5m], memory [215.8mb]->[202.7mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [194.4mb]->[195.6mb]/[2gb]}{[survivor] [9.3mb]->[7.1mb]/[0b]}
[2022-04-05T19:27:15,304][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [838ms] collecting in the last [1.9s]
[2022-04-05T19:27:17,447][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][188][50] duration [795ms], collections [1]/[2.6s], total [795ms]/[1.5m], memory [202.7mb]->[236.6mb]/[2gb], all_pools {[young] [4mb]->[32mb]/[0b]}{[old] [195.6mb]->[195.6mb]/[2gb]}{[survivor] [7.1mb]->[9mb]/[0b]}
[2022-04-05T19:27:17,579][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][188] overhead, spent [795ms] collecting in the last [2.6s]
[2022-04-05T19:27:27,610][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][192][51] duration [1.1s], collections [1]/[3.6s], total [1.1s]/[1.6m], memory [272.6mb]->[207.1mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [195.6mb]->[196.4mb]/[2gb]}{[survivor] [9mb]->[10.7mb]/[0b]}
[2022-04-05T19:27:27,880][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][192] overhead, spent [1.1s] collecting in the last [3.6s]
[2022-04-05T19:31:12,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/210872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:33:44,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/210281672917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:35:21,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/256972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:36:36,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257562235712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:39:23,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/240875ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:39:05,676][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][194][52] duration [2.9m], collections [1]/[1.3s], total [2.9m]/[4.5m], memory [247.1mb]->[291.1mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [196.4mb]->[201.7mb]/[2gb]}{[survivor] [10.7mb]->[11.2mb]/[0b]}
[2022-04-05T19:40:29,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/240875226688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:40:39,766][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][194] overhead, spent [2.9m] collecting in the last [1.3s]
[2022-04-05T19:43:01,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/214513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:44:04,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [922711ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:46:29,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/213792345061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:51:11,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/490270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:57:24,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/490324784844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:00:33,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/562071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:00:41,313][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.3m/1222070ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.19][0]], allocationId [1VOd_d6vRTicjiMR5cSWAg], primary term [75], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.19][0]], allocationId [1VOd_d6vRTicjiMR5cSWAg], primary term [75], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-05T20:04:24,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/562261520926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:08:14,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/458797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:12:16,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/458714447386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:18:35,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7m/586604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:22:43,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7m/586357282371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:27:16,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2m/557623ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:28:45,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7092ae78, interval=5s}] took [2165760ms] which is above the warn threshold of [5000ms]
[2022-04-05T20:30:38,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/558427343380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:33:55,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:37:11,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399718786048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:40:43,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:46:31,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405862338479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:55:01,865][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T20:55:01,880][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T20:55:01,882][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T20:55:07,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T20:55:07,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T20:55:07,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T20:55:07,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T20:55:07,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T20:55:07,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T20:55:07,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T20:55:07,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T20:55:07,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T20:55:07,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T20:55:07,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T20:55:07,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T20:55:07,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T20:55:07,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T20:55:07,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T20:55:07,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T20:55:07,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T20:55:07,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T20:55:07,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T20:55:07,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T20:55:07,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T20:55:07,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T20:55:07,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T20:55:07,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T20:55:07,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T20:55:07,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T20:55:07,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T20:55:07,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T20:55:07,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T20:55:07,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T20:55:07,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T20:55:07,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T20:55:07,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T20:55:07,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T20:55:07,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T20:55:07,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T20:55:07,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T20:55:07,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T20:55:07,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T20:55:07,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T20:55:07,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T20:55:07,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T20:55:07,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T20:55:07,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T20:55:07,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T20:55:07,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T20:55:07,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T20:55:07,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T20:55:07,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T20:55:07,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T20:55:07,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T20:55:07,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T20:55:07,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T20:55:07,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T20:55:07,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T20:55:07,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T20:55:07,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T20:55:07,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T20:55:07,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T20:55:07,477][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-05T20:55:07,478][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T20:55:07,890][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T20:56:48,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:56:53,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@307f13ae, interval=1m}] took [11398ms] which is above the warn threshold of [5000ms]
[2022-04-05T20:57:20,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11398714916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:57:23,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:57:24,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70184289007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:58:01,366][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T20:58:01,374][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-05T20:58:01,376][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T20:58:01,380][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-05T20:58:01,382][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T20:58:01,383][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-05T20:58:01,384][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-05T20:58:01,386][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T20:58:01,387][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-05T20:58:01,388][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-05T20:58:01,389][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-05T20:58:01,389][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T20:58:01,391][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T20:58:01,394][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T20:58:01,397][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T20:58:02,593][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T20:58:02,761][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T20:58:03,581][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T20:58:04,496][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T20:58:04,501][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T20:58:04,618][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T20:58:04,620][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T20:58:04,897][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T20:58:07,196][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T20:58:07,377][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 208, version: 7582, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T20:58:07,518][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 208, version: 7582, reason: Publication{term=208, version=7582}
[2022-04-05T20:58:07,651][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T20:58:07,651][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T20:58:08,347][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T20:58:08,360][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T20:58:16,353][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T20:58:16,895][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T20:58:23,296][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54494}] took [6257ms] which is above the warn threshold of [5000ms]
[2022-04-05T20:58:23,296][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54496}] took [6257ms] which is above the warn threshold of [5000ms]
[2022-04-05T20:58:23,296][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54486}] took [6257ms] which is above the warn threshold of [5000ms]
[2022-04-05T20:58:37,460][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12410ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T20:58:37,885][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.8s] publication of cluster state version [7586] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T21:00:59,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:01:21,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22808500265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:02:10,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:02:20,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85469194278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:02:35,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:02:48,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21941884802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:03:00,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:03:02,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28004639007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:03:04,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@771cf174] took [266963ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:03:15,061][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=208, version=7586}] took [4.6m] which is above the warn threshold of [30s]: [running task [Publication{term=208, version=7586}]] took [18ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@734e3959] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@626ab188] took [810ms], [org.elasticsearch.script.ScriptService@69ad8d26] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [0ms], [org.elasticsearch.snapshots.RestoreService@85409c4] took [0ms], [org.elasticsearch.ingest.IngestService@3f850fb7] took [0ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3a4461dc] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016caca0@236680ff] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@b54b77e] took [0ms], [org.elasticsearch.tasks.TaskManager@1c3bf914] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@5e821ce] took [12ms], [org.elasticsearch.cluster.InternalClusterInfoService@2bd0647a] took [13ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d2b9d6b] took [0ms], [org.elasticsearch.indices.SystemIndexManager@26484ae7] took [55ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@4907ed68] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@44e1896d] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@20807c7c] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4534e39b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@3c227e9d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@141ce361] took [13ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3966256] took [244021ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7151b3cc] took [93ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5d6d26ff] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@21de314d] took [18066ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@53956683] took [63ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7504264c] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@416abccd] took [3106ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [126ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@11eea9f4] took [2781ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@69b77434] took [42ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@4d88fb6f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5413a4fa] took [193ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3b50a6b9] took [44ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@350e292c] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@69d59070] took [1ms], [org.elasticsearch.node.ResponseCollectorService@578e869d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4ab42228] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@e37251b] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@4b2472bd] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6016f0f4] took [44ms], [org.elasticsearch.indices.store.IndicesStore@2540d55a] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7cbac6c2] took [0ms], [org.elasticsearch.license.LicenseService@295958c6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5d567cc5] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@6c64c043] took [0ms], [org.elasticsearch.gateway.GatewayService@6a393eeb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@6876c24] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@6862ee5f]]]] took [4981ms], [ClusterStateObserver[null]] took [0ms]
[2022-04-05T21:03:32,424][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
	at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1715) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
	Suppressed: java.net.SocketException: Broken pipe
		at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
		at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
		at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
		at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
		at sun.security.ssl.SSLSocketOutputRecord.encodeAlert(SSLSocketOutputRecord.java:82) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:400) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:312) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:463) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
		at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
		at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
		at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
		at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
		at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: java.io.EOFException: SSL peer shut down incorrectly
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:483) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:472) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	... 25 more
[2022-04-05T21:03:46,214][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T21:03:46,297][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T21:03:46,298][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T21:03:52,562][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][51] overhead, spent [543ms] collecting in the last [1.3s]
[2022-04-05T21:03:58,104][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][55] overhead, spent [633ms] collecting in the last [1.6s]
[2022-04-05T21:03:58,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T21:03:58,661][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T21:04:02,569][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][59] overhead, spent [438ms] collecting in the last [1s]
[2022-04-05T21:04:05,592][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T21:04:16,667][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][67][15] duration [1.8s], collections [1]/[4.3s], total [1.8s]/[4.6s], memory [207.4mb]->[132.1mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [113.4mb]->[120.1mb]/[2gb]}{[survivor] [13.9mb]->[12mb]/[0b]}
[2022-04-05T21:04:16,860][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][67] overhead, spent [1.8s] collecting in the last [4.3s]
[2022-04-05T21:04:22,975][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][72] overhead, spent [432ms] collecting in the last [1.5s]
[2022-04-05T21:04:34,089][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][79][19] duration [1.3s], collections [1]/[1.2s], total [1.3s]/[6.7s], memory [153mb]->[221mb]/[2gb], all_pools {[young] [12mb]->[84mb]/[0b]}{[old] [130.9mb]->[130.9mb]/[2gb]}{[survivor] [10mb]->[10mb]/[0b]}
[2022-04-05T21:04:34,520][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][79] overhead, spent [1.3s] collecting in the last [1.2s]
[2022-04-05T21:04:44,980][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][21] duration [5.5s], collections [2]/[4.1s], total [5.5s]/[12.3s], memory [221mb]->[230.5mb]/[2gb], all_pools {[young] [84mb]->[80mb]/[0b]}{[old] [130.9mb]->[147.5mb]/[2gb]}{[survivor] [10mb]->[5.3mb]/[0b]}
[2022-04-05T21:04:45,408][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [5.5s] collecting in the last [4.1s]
[2022-04-05T21:04:45,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [9881ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:04:59,109][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][85][23] duration [3.1s], collections [1]/[5s], total [3.1s]/[16s], memory [232.8mb]->[162.2mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [147.5mb]->[147.5mb]/[2gb]}{[survivor] [5.3mb]->[14.7mb]/[0b]}
[2022-04-05T21:04:59,990][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][85] overhead, spent [3.1s] collecting in the last [5s]
[2022-04-05T21:05:00,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [7701ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:05:01,603][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T21:05:09,273][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.4s/21412ms] to compute cluster state update for [put-mapping [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A][_doc]], which exceeds the warn threshold of [10s]
[2022-04-05T21:05:27,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:05:28,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8197625975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:05:30,752][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][92][24] duration [5.2s], collections [1]/[10s], total [5.2s]/[21.3s], memory [206.2mb]->[192.1mb]/[2gb], all_pools {[young] [48mb]->[72mb]/[0b]}{[old] [147.5mb]->[159.5mb]/[2gb]}{[survivor] [14.7mb]->[4.5mb]/[0b]}
[2022-04-05T21:05:31,668][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][92] overhead, spent [5.2s] collecting in the last [10s]
[2022-04-05T21:05:32,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [5212ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:05:36,257][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][25] duration [993ms], collections [1]/[5.7s], total [993ms]/[22.3s], memory [192.1mb]->[252.1mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [159.5mb]->[159.5mb]/[2gb]}{[survivor] [4.5mb]->[5.5mb]/[0b]}
[2022-04-05T21:05:38,465][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [20.5s] publication of cluster state version [7604] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T21:05:52,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [5036ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:05:59,051][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][101][26] duration [1.2s], collections [1]/[3.2s], total [1.2s]/[23.5s], memory [201mb]->[168.7mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [159.5mb]->[159.5mb]/[2gb]}{[survivor] [5.5mb]->[9.2mb]/[0b]}
[2022-04-05T21:05:59,344][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][101] overhead, spent [1.2s] collecting in the last [3.2s]
[2022-04-05T21:06:09,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@4827ed09] took [8669ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:06:20,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:06:21,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8662194907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:06:21,359][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][105][27] duration [2.7s], collections [1]/[9s], total [2.7s]/[26.3s], memory [244.7mb]->[172.1mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [159.5mb]->[162.2mb]/[2gb]}{[survivor] [9.2mb]->[9.8mb]/[0b]}
[2022-04-05T21:06:21,360][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][105] overhead, spent [2.7s] collecting in the last [9s]
[2022-04-05T21:06:25,662][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14033ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [35] unchanged indices
[2022-04-05T21:06:26,353][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15s] publication of cluster state version [7606] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T21:06:29,725][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][110] overhead, spent [563ms] collecting in the last [2.1s]
[2022-04-05T21:06:33,674][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][111][29] duration [1.7s], collections [1]/[4.2s], total [1.7s]/[28.6s], memory [172.5mb]->[203.7mb]/[2gb], all_pools {[young] [0b]->[32mb]/[0b]}{[old] [163.7mb]->[163.7mb]/[2gb]}{[survivor] [8.8mb]->[8mb]/[0b]}
[2022-04-05T21:06:33,919][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][111] overhead, spent [1.7s] collecting in the last [4.2s]
[2022-04-05T21:06:37,488][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][30] duration [1.3s], collections [1]/[1.3s], total [1.3s]/[30s], memory [203.7mb]->[259.7mb]/[2gb], all_pools {[young] [32mb]->[12mb]/[0b]}{[old] [163.7mb]->[163.7mb]/[2gb]}{[survivor] [8mb]->[12.7mb]/[0b]}
[2022-04-05T21:06:37,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [1.3s] collecting in the last [1.3s]
[2022-04-05T21:06:40,593][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][113][31] duration [1.3s], collections [1]/[5.4s], total [1.3s]/[31.3s], memory [259.7mb]->[180.9mb]/[2gb], all_pools {[young] [12mb]->[8mb]/[0b]}{[old] [163.7mb]->[167.9mb]/[2gb]}{[survivor] [12.7mb]->[9mb]/[0b]}
[2022-04-05T21:06:40,759][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T21:06:45,032][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][114][32] duration [1.9s], collections [1]/[4.3s], total [1.9s]/[33.3s], memory [180.9mb]->[185.9mb]/[2gb], all_pools {[young] [8mb]->[8mb]/[0b]}{[old] [167.9mb]->[167.9mb]/[2gb]}{[survivor] [9mb]->[10mb]/[0b]}
[2022-04-05T21:06:45,226][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][114] overhead, spent [1.9s] collecting in the last [4.3s]
[2022-04-05T21:06:51,051][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][116][33] duration [2.4s], collections [1]/[3.8s], total [2.4s]/[35.7s], memory [237.9mb]->[183.5mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [167.9mb]->[170.6mb]/[2gb]}{[survivor] [10mb]->[12.8mb]/[0b]}
[2022-04-05T21:06:53,130][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][116] overhead, spent [2.4s] collecting in the last [3.8s]
[2022-04-05T21:06:54,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [7678ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:06:56,293][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [1.9s] collecting in the last [6s]
[2022-04-05T21:06:59,561][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][118][37] duration [1.2s], collections [1]/[3s], total [1.2s]/[38.9s], memory [192.1mb]->[192.4mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [181.1mb]->[185.5mb]/[2gb]}{[survivor] [11mb]->[6.8mb]/[0b]}
[2022-04-05T21:06:59,892][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][118] overhead, spent [1.2s] collecting in the last [3s]
[2022-04-05T21:07:05,027][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][119][38] duration [2.4s], collections [1]/[1.7s], total [2.4s]/[41.4s], memory [192.4mb]->[276.4mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [185.5mb]->[185.5mb]/[2gb]}{[survivor] [6.8mb]->[7.4mb]/[0b]}
[2022-04-05T21:07:05,141][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][119] overhead, spent [2.4s] collecting in the last [1.7s]
[2022-04-05T21:07:08,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][120][39] duration [1.2s], collections [1]/[6.6s], total [1.2s]/[42.7s], memory [276.4mb]->[192.2mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [185.5mb]->[185.5mb]/[2gb]}{[survivor] [7.4mb]->[6.6mb]/[0b]}
[2022-04-05T21:07:08,946][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T21:07:22,006][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][43] duration [725ms], collections [1]/[1.6s], total [725ms]/[44.5s], memory [224.3mb]->[197.8mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [187.4mb]->[187.4mb]/[2gb]}{[survivor] [8.8mb]->[10.3mb]/[0b]}
[2022-04-05T21:07:22,225][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [725ms] collecting in the last [1.6s]
[2022-04-05T21:07:29,774][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][130][44] duration [1.2s], collections [1]/[3.2s], total [1.2s]/[45.7s], memory [265.8mb]->[196.3mb]/[2gb], all_pools {[young] [72mb]->[40mb]/[0b]}{[old] [187.4mb]->[188.9mb]/[2gb]}{[survivor] [10.3mb]->[7.3mb]/[0b]}
[2022-04-05T21:07:30,653][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][130] overhead, spent [1.2s] collecting in the last [3.2s]
[2022-04-05T21:07:35,321][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][131][46] duration [1.9s], collections [2]/[2.5s], total [1.9s]/[47.7s], memory [196.3mb]->[284.5mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [188.9mb]->[188.9mb]/[2gb]}{[survivor] [7.3mb]->[8.6mb]/[0b]}
[2022-04-05T21:07:35,680][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][131] overhead, spent [1.9s] collecting in the last [2.5s]
[2022-04-05T21:07:40,953][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T21:07:41,356][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][133][48] duration [1s], collections [1]/[3.5s], total [1s]/[49.3s], memory [281.6mb]->[201.7mb]/[2gb], all_pools {[young] [0b]->[16mb]/[0b]}{[old] [188.9mb]->[190.7mb]/[2gb]}{[survivor] [9.8mb]->[10.9mb]/[0b]}
[2022-04-05T21:07:41,592][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][133] overhead, spent [1s] collecting in the last [3.5s]
[2022-04-05T21:07:57,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:07:58,634][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54578}] took [10187ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:07:59,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8491704566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:08:00,125][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][136][49] duration [5.8s], collections [1]/[3.6s], total [5.8s]/[55.2s], memory [241.7mb]->[269.7mb]/[2gb], all_pools {[young] [40mb]->[12mb]/[0b]}{[old] [190.7mb]->[195.5mb]/[2gb]}{[survivor] [10.9mb]->[9.4mb]/[0b]}
[2022-04-05T21:08:02,079][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][136] overhead, spent [5.8s] collecting in the last [3.6s]
[2022-04-05T21:08:03,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [15176ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:08:06,674][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [19874ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T21:08:09,369][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [27.2s] publication of cluster state version [7612] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T21:08:20,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [7377ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:08:36,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:08:38,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12290914199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:08:38,903][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][141][50] duration [8.6s], collections [1]/[8.1s], total [8.6s]/[1m], memory [252.9mb]->[288.9mb]/[2gb], all_pools {[young] [52mb]->[32mb]/[0b]}{[old] [195.5mb]->[196.5mb]/[2gb]}{[survivor] [9.4mb]->[13.5mb]/[0b]}
[2022-04-05T21:08:40,963][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][141] overhead, spent [8.6s] collecting in the last [8.1s]
[2022-04-05T21:08:42,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [19415ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:09:09,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@4a635620] took [12228ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:09:12,088][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][142][51] duration [3.4s], collections [1]/[48.3s], total [3.4s]/[1.1m], memory [288.9mb]->[277.4mb]/[2gb], all_pools {[young] [32mb]->[64mb]/[0b]}{[old] [196.5mb]->[203mb]/[2gb]}{[survivor] [13.5mb]->[10.3mb]/[0b]}
[2022-04-05T21:09:16,129][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:44024}] took [15169ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:09:23,415][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][52] duration [1.8s], collections [1]/[6.7s], total [1.8s]/[1.1m], memory [289.4mb]->[216.8mb]/[2gb], all_pools {[young] [80mb]->[28mb]/[0b]}{[old] [203mb]->[207.8mb]/[2gb]}{[survivor] [10.3mb]->[8.9mb]/[0b]}
[2022-04-05T21:09:23,241][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=208, version=7612}] took [1m] which is above the warn threshold of [30s]: [running task [Publication{term=208, version=7612}]] took [56ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@734e3959] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@626ab188] took [23129ms], [org.elasticsearch.script.ScriptService@69ad8d26] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [0ms], [org.elasticsearch.snapshots.RestoreService@85409c4] took [0ms], [org.elasticsearch.ingest.IngestService@3f850fb7] took [2515ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3a4461dc] took [153ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016caca0@236680ff] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@b54b77e] took [0ms], [org.elasticsearch.tasks.TaskManager@1c3bf914] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@5e821ce] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@2bd0647a] took [52ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d2b9d6b] took [158ms], [org.elasticsearch.indices.SystemIndexManager@26484ae7] took [1082ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@4907ed68] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@44e1896d] took [356ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@20807c7c] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4534e39b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@3c227e9d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@141ce361] took [117ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3966256] took [24568ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7151b3cc] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5d6d26ff] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@21de314d] took [3351ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@53956683] took [203ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7504264c] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@416abccd] took [3818ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [34ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@11eea9f4] took [3067ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@69b77434] took [39ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@4d88fb6f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5413a4fa] took [256ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3b50a6b9] took [34ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@350e292c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@69d59070] took [41ms], [org.elasticsearch.node.ResponseCollectorService@578e869d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4ab42228] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@e37251b] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@4b2472bd] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6016f0f4] took [43ms], [org.elasticsearch.indices.store.IndicesStore@2540d55a] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7cbac6c2] took [0ms], [org.elasticsearch.license.LicenseService@295958c6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5d567cc5] took [41ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@6c64c043] took [0ms], [org.elasticsearch.gateway.GatewayService@6a393eeb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@6876c24] took [0ms]
[2022-04-05T21:09:23,581][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [1.8s] collecting in the last [6.7s]
[2022-04-05T21:09:25,942][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][145][53] duration [727ms], collections [1]/[2.3s], total [727ms]/[1.1m], memory [216.8mb]->[218.1mb]/[2gb], all_pools {[young] [28mb]->[20mb]/[0b]}{[old] [207.8mb]->[211.6mb]/[2gb]}{[survivor] [8.9mb]->[6.5mb]/[0b]}
[2022-04-05T21:09:26,382][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][145] overhead, spent [727ms] collecting in the last [2.3s]
[2022-04-05T21:09:36,103][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][149][54] duration [892ms], collections [1]/[1.4s], total [892ms]/[1.1m], memory [278.1mb]->[306.1mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [211.6mb]->[211.6mb]/[2gb]}{[survivor] [6.5mb]->[7.1mb]/[0b]}
[2022-04-05T21:09:36,662][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][149] overhead, spent [892ms] collecting in the last [1.4s]
[2022-04-05T21:09:41,063][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:54334}] took [112916ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:09:55,620][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][157][55] duration [2.3s], collections [1]/[4.1s], total [2.3s]/[1.2m], memory [266.7mb]->[220.6mb]/[2gb], all_pools {[young] [48mb]->[32mb]/[0b]}{[old] [211.6mb]->[211.6mb]/[2gb]}{[survivor] [7.1mb]->[8.9mb]/[0b]}
[2022-04-05T21:09:56,238][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][157] overhead, spent [2.3s] collecting in the last [4.1s]
[2022-04-05T21:10:18,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [9119ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:10:13,932][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16370ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [35] unchanged indices
[2022-04-05T21:10:26,879][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [30s] publication of cluster state version [7615] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T21:10:48,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:11:18,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7403977482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:11:26,203][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54606}] took [11807ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:11:26,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:11:28,103][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54582}] took [47720ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:11:27,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39315115084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:11:28,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@1541a17b] took [59751ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:11:39,182][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54578}] took [9451ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:11:46,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:11:47,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5469843683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:11:47,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][56] duration [17.2s], collections [1]/[1.2m], total [17.2s]/[1.5m], memory [276.6mb]->[258.1mb]/[2gb], all_pools {[young] [56mb]->[64mb]/[0b]}{[old] [211.6mb]->[215.9mb]/[2gb]}{[survivor] [8.9mb]->[6.2mb]/[0b]}
[2022-04-05T21:11:48,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [6070ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:12:09,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11403ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:12:11,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11403433973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:12:11,927][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][169][57] duration [8s], collections [1]/[12.6s], total [8s]/[1.6m], memory [306.1mb]->[222.8mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [215.9mb]->[215.9mb]/[2gb]}{[survivor] [6.2mb]->[6.8mb]/[0b]}
[2022-04-05T21:12:12,187][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][169] overhead, spent [8s] collecting in the last [12.6s]
[2022-04-05T21:12:55,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:12:57,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8742114085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:12:56,905][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][180][58] duration [5.9s], collections [1]/[12.6s], total [5.9s]/[1.7m], memory [306.8mb]->[225.4mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [215.9mb]->[215.9mb]/[2gb]}{[survivor] [6.8mb]->[9.5mb]/[0b]}
[2022-04-05T21:12:57,580][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][180] overhead, spent [5.9s] collecting in the last [12.6s]
[2022-04-05T21:13:28,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@3a3feb20] took [19011ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:13:55,727][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54610}] took [12207ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:14:01,197][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54608}] took [13207ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:14:50,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:14:51,665][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][185][59] duration [26.4s], collections [1]/[35.5s], total [26.4s]/[2.1m], memory [273.4mb]->[301.4mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [215.9mb]->[216.8mb]/[2gb]}{[survivor] [9.5mb]->[9.5mb]/[0b]}
[2022-04-05T21:14:51,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36832991737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:14:55,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:14:55,544][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][185] overhead, spent [26.4s] collecting in the last [35.5s]
[2022-04-05T21:14:56,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5871969736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:14:56,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [48708ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:14:56,085][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54578}] took [5872ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:15:07,735][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=208, version=7615}] took [4.5m] which is above the warn threshold of [30s]: [running task [Publication{term=208, version=7615}]] took [240ms], [connecting to new nodes] took [996ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@734e3959] took [159ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@626ab188] took [171473ms], [org.elasticsearch.script.ScriptService@69ad8d26] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [0ms], [org.elasticsearch.snapshots.RestoreService@85409c4] took [0ms], [org.elasticsearch.ingest.IngestService@3f850fb7] took [2267ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3a4461dc] took [107ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016caca0@236680ff] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@b54b77e] took [72ms], [org.elasticsearch.tasks.TaskManager@1c3bf914] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@5e821ce] took [142ms], [org.elasticsearch.cluster.InternalClusterInfoService@2bd0647a] took [152ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d2b9d6b] took [146ms], [org.elasticsearch.indices.SystemIndexManager@26484ae7] took [2253ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@4907ed68] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@44e1896d] took [390ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@20807c7c] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4534e39b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@3c227e9d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@141ce361] took [139ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3966256] took [83476ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7151b3cc] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5d6d26ff] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@21de314d] took [1804ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@53956683] took [165ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7504264c] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@416abccd] took [1650ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [53ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@11eea9f4] took [2712ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@69b77434] took [71ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@4d88fb6f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5413a4fa] took [2595ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3b50a6b9] took [1801ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@350e292c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@69d59070] took [261ms], [org.elasticsearch.node.ResponseCollectorService@578e869d] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4ab42228] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@e37251b] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@4b2472bd] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6016f0f4] took [126ms], [org.elasticsearch.indices.store.IndicesStore@2540d55a] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7cbac6c2] took [0ms], [org.elasticsearch.license.LicenseService@295958c6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5d567cc5] took [66ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@6c64c043] took [0ms], [org.elasticsearch.gateway.GatewayService@6a393eeb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@6876c24] took [0ms]
[2022-04-05T21:15:12,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [5730ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:15:28,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:15:28,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [13281ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:15:29,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11280155194ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:15:40,399][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][191][60] duration [3.6s], collections [1]/[17.1s], total [3.6s]/[2.2m], memory [306.3mb]->[248.2mb]/[2gb], all_pools {[young] [80mb]->[20mb]/[0b]}{[old] [216.8mb]->[217.8mb]/[2gb]}{[survivor] [9.5mb]->[14.4mb]/[0b]}
[2022-04-05T21:15:45,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [12614ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:15:53,462][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54578}] took [8102ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:16:08,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [12642ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:16:14,409][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [58.7s/58736ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@10f42159], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9578f0b7], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6de49c68], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce340410]], which exceeds the warn threshold of [10s]
[2022-04-05T21:18:15,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [6071ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:18:16,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:18:32,031][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.9m/119249ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [92], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [92], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-05T21:18:33,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93624707466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:18:56,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:19:13,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41832437567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:19:21,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25837ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:19:27,436][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][61] duration [1.1m], collections [1]/[2.4m], total [1.1m]/[3.4m], memory [304.2mb]->[247.8mb]/[2gb], all_pools {[young] [72mb]->[52mb]/[0b]}{[old] [217.8mb]->[224.8mb]/[2gb]}{[survivor] [14.4mb]->[11mb]/[0b]}
[2022-04-05T21:22:49,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25837668932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:22:50,655][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [1.1m] collecting in the last [2.4m]
[2022-04-05T21:22:56,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/214571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:22:56,463][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54578}] took [282241ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:22:59,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [240408ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:23:00,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/214570788409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:23:07,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:23:16,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11527403001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:23:32,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:23:41,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23749169418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:23:52,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:24:04,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20711235724ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:24:17,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:24:30,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22549760411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:24:41,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:24:24,238][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [824] timed out after [39806ms]
[2022-04-05T21:24:51,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25913970517ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:04,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:17,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23039400655ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:29,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:36,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72877c45, interval=5s}] took [24551ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:25:18,146][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23039ms] which is above the warn threshold of [5s]
[2022-04-05T21:25:42,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24551637466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:58,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28851ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:59,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [28850ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:26:12,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28850869279ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:25:42,659][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.5m/572243ms] ago, timed out [8.8m/532437ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [824]
[2022-04-05T21:26:25,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:26:37,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27776102672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:27:04,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:27:15,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36896578844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:28:10,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67840ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:29:35,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67839620410ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:29:35,591][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][62] duration [1.9m], collections [1]/[7.6m], total [1.9m]/[5.3m], memory [247.8mb]->[290.7mb]/[2gb], all_pools {[young] [52mb]->[56mb]/[0b]}{[old] [224.8mb]->[230.7mb]/[2gb]}{[survivor] [11mb]->[8mb]/[0b]}
[2022-04-05T21:29:47,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:29:50,308][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [1.9m] collecting in the last [7.6m]
[2022-04-05T21:30:06,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96488418367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:30:11,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [201224ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:30:24,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:30:40,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37350671447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:31:03,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:31:24,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38321504418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:31:44,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:32:05,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40064666326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:32:28,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:32:56,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44917791930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:33:13,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:33:40,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44753930906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:34:01,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:34:22,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46565656444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:34:44,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:35:05,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44110860917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:35:32,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:35:52,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.2s/48284133593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:36:20,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.4s/47423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:36:47,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47566100859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:38:09,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:38:51,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108110122808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:39:19,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:39:40,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69541390302ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:40:04,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42968ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:40:56,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42968064922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:39:49,425][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1194052ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T21:41:21,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:42:24,841][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [23.4m] publication of cluster state version [7616] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T21:42:36,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80893928401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:42:52,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:43:28,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91200151716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:43:45,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.5s/52529ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:43:56,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72877c45, interval=5s}] took [52528ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:43:57,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.5s/52528663047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:44:14,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@67308496] took [28872ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:44:14,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:44:36,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28872833924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:45:15,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.8s/57841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:45:32,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.8s/57840797610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:45:53,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:46:09,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42148296616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:46:59,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:47:07,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@593e4fe8] took [162612ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:47:18,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62623690709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:47:58,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:48:19,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [60951ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:48:24,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60951522893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:48:51,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54s/54062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:49:13,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54s/54061352672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:49:55,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61370ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:50:43,677][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [61371ms] which is above the warn threshold of [5s]
[2022-04-05T21:50:48,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61370662182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:51:22,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:51:53,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89428178213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:52:13,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50524ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:52:39,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.5s/50523155117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:52:43,167][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][197][63] duration [49.6s], collections [1]/[23.3m], total [49.6s]/[6.2m], memory [290.7mb]->[261.7mb]/[2gb], all_pools {[young] [56mb]->[20mb]/[0b]}{[old] [230.7mb]->[230.7mb]/[2gb]}{[survivor] [8mb]->[10.9mb]/[0b]}
[2022-04-05T21:53:07,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.9s/51988ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:53:13,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [191939ms] which is above the warn threshold of [5000ms]
[2022-04-05T21:53:33,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.9s/51988640741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:54:00,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:54:31,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53527271402ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:55:05,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:55:30,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65944205261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:55:44,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39978ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:56:02,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39978769036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:56:58,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:57:15,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75329460637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:57:29,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:57:44,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30090861932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:58:06,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36473ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:58:31,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36473291230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:58:49,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:59:11,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43550288832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T21:59:31,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T21:59:48,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39975142419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:00:05,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:00:26,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34091794128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:00:41,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:00:54,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39010413787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:01:09,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26169ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:01:26,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26168920728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:01:40,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:01:59,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31658494586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:02:21,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41934ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:02:24,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [73592ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:02:44,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41934162483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:02:06,368][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [955] timed out after [787285ms]
[2022-04-05T22:02:56,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35591ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:03:06,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35590312725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:03:18,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:03:32,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21659457802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:03:08,558][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [35590ms] which is above the warn threshold of [5s]
[2022-04-05T22:03:42,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:03:41,391][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=208, version=7616}] took [20.4m] which is above the warn threshold of [30s]: [running task [Publication{term=208, version=7616}]] took [190ms], [connecting to new nodes] took [22365ms], [applying settings] took [80ms], [org.elasticsearch.repositories.RepositoriesService@734e3959] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@626ab188] took [500297ms], [org.elasticsearch.script.ScriptService@69ad8d26] took [582ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [508ms], [org.elasticsearch.snapshots.RestoreService@85409c4] took [515ms], [org.elasticsearch.ingest.IngestService@3f850fb7] took [18995ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3a4461dc] took [786ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016caca0@236680ff] took [317ms], [org.elasticsearch.indices.TimestampFieldMapperService@b54b77e] took [2806ms], [org.elasticsearch.tasks.TaskManager@1c3bf914] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@5e821ce] took [873ms], [org.elasticsearch.cluster.InternalClusterInfoService@2bd0647a] took [756ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d2b9d6b] took [1319ms], [org.elasticsearch.indices.SystemIndexManager@26484ae7] took [22350ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@4907ed68] took [125ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@44e1896d] took [2644ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@20807c7c] took [777ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4534e39b] took [673ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@3c227e9d] took [430ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@141ce361] took [812ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3966256] took [411074ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7151b3cc] took [126ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5d6d26ff] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@21de314d] took [71409ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@53956683] took [1418ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7504264c] took [500ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@416abccd] took [47232ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [7830ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@11eea9f4] took [42037ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@69b77434] took [360ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@4d88fb6f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5413a4fa] took [35826ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3b50a6b9] took [17649ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@350e292c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@69d59070] took [5579ms], [org.elasticsearch.node.ResponseCollectorService@578e869d] took [92ms], [org.elasticsearch.snapshots.SnapshotShardsService@4ab42228] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@e37251b] took [92ms], [org.elasticsearch.shutdown.PluginShutdownService@4b2472bd] took [310ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6016f0f4] took [477ms], [org.elasticsearch.indices.store.IndicesStore@2540d55a] took [938ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7cbac6c2] took [100ms], [org.elasticsearch.license.LicenseService@295958c6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5d567cc5] took [192ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@6c64c043] took [6ms], [org.elasticsearch.gateway.GatewayService@6a393eeb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@6876c24] took [95ms]
[2022-04-05T22:03:51,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23940229520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:04:01,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:04:12,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18137536401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:04:24,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:04:54,925][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [48.7m/2924616ms] which is longer than the warn threshold of [300000ms]; there are currently [14] pending tasks, the oldest of which has age [52m/3121483ms]
[2022-04-05T22:05:01,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22618339138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:05:22,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:05:47,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.6s/56699365787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:06:05,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:06:20,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.3s/44308373487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:06:32,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [101007ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:06:47,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:07:02,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42401510471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:07:31,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:07:49,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45304444778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:08:03,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:08:15,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31070607015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:08:33,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:08:39,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@5f3df8a7] took [105719ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:08:46,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29344827658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:08:58,740][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [21.6m/1297132ms] ago, timed out [8.4m/509847ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [955]
[2022-04-05T22:08:53,606][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/108308ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.kibana-event-log-7.16.2-000001][0]], allocationId [LMpRzKDiR2iyzvl4JQs8Yw], primary term [111], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana-event-log-7.16.2-000001][0]], allocationId [LMpRzKDiR2iyzvl4JQs8Yw], primary term [111], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana-event-log-7.16.2-000001][0]], allocationId [LMpRzKDiR2iyzvl4JQs8Yw], primary term [111], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.kibana-event-log-7.16.2-000001][0]], allocationId [LMpRzKDiR2iyzvl4JQs8Yw], primary term [111], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [98], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [98], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [90], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [90], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [98], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [98], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [90], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [90], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-05T22:09:05,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:09:20,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30813549836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:09:34,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:09:46,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31020739827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:10:11,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:10:22,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.1s/36192307592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:10:47,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72877c45, interval=5s}] took [36192ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:11:01,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:11:17,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46239816013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:12:22,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/84833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:12:49,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [84833ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:12:55,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/84833656926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:13:24,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.8s/59862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:14:13,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.8s/59861753816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:16:24,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:17:03,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182636705026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:20:15,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/230913ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:16:44,820][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [182637ms] which is above the warn threshold of [5s]
[2022-04-05T22:20:48,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/230912743341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:20:53,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [413549ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:21:07,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:21:26,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50440715084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:21:42,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:21:58,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37002191499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:22:09,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26292ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:22:21,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26291175626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:22:30,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21768ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:22:39,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21768599863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:22:47,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:22:48,572][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1037] timed out after [816245ms]
[2022-04-05T22:23:01,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17210931018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:23:08,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:23:21,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19980120443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:23:31,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:23:36,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [23868ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:23:39,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23868369871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:23:48,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:23:57,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16339880908ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:24:07,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:24:16,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19630735894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:24:25,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:24:32,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18024391810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:24:38,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [37655ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:24:43,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:24:59,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17914565980ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:25:11,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:27:56,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23638532758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:28:05,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/178793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:28:17,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/178793452738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:28:29,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:28:40,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22623385378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:29:00,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:28:43,253][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [19.9m/1196038ms] ago, timed out [6.3m/379793ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1037]
[2022-04-05T22:29:20,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31369649103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:29:34,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:29:49,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34713611360ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:29:54,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@35ec368f] took [66083ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:30:03,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27666ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:30:22,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27666541518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:30:43,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:31:04,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39631641060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:31:32,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:31:56,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49174362588ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:32:26,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:32:51,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52135670262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:33:13,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.2s/49203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:33:35,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.2s/49203376809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:33:49,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36007ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:33:51,771][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][202][64] duration [2.1m], collections [1]/[8.1m], total [2.1m]/[8.3m], memory [309.7mb]->[284.5mb]/[2gb], all_pools {[young] [68mb]->[40mb]/[0b]}{[old] [230.7mb]->[235.1mb]/[2gb]}{[survivor] [10.9mb]->[9.4mb]/[0b]}
[2022-04-05T22:34:03,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36006192498ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:34:07,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][202] overhead, spent [2.1m] collecting in the last [8.1m]
[2022-04-05T22:34:19,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:34:18,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [165849ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:34:31,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28504212389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:34:43,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26560ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:34:57,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26559968275ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:35:10,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26554ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:35:21,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26554483716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:35:32,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:35:38,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72877c45, interval=5s}] took [21813ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:35:10,287][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1426738ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [36] unchanged indices
[2022-04-05T22:35:49,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21813938299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:36:02,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30195ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:36:17,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30195009175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:35:59,387][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.3m] publication of cluster state version [7617] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T22:36:34,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:36:39,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [31786ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:36:45,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31786496670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:36:59,523][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [31787ms] which is above the warn threshold of [5s]
[2022-04-05T22:37:03,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:37:17,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28608482341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:37:37,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:37:58,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33352652829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:38:20,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41128ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:38:42,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41127960631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:38:18,297][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1126] timed out after [387445ms]
[2022-04-05T22:39:04,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:39:42,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46372373886ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:39:48,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [46372ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:40:04,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60586ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:40:21,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60585762470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:40:38,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:41:01,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34042048286ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:41:21,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:41:40,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42786477390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:41:59,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:42:16,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38228928090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:42:42,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:43:00,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43282429681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:43:23,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:43:47,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.2s/36255678274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:44:12,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [36255ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:44:16,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57010ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:44:44,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57010339993ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:43:53,678][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [36255ms] which is above the warn threshold of [5s]
[2022-04-05T22:44:53,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:45:01,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38458287721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:45:17,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:45:34,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23770597177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:45:49,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@4a2c33a2] took [55339ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:45:49,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:01,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31568812441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:16,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:27,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23625189352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:34,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:39,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21390658169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:50,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:55,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16475782579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:54,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [16475ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:46:59,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9022ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:46:59,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@54fe44b4, interval=30s}] took [9021ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:47:04,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9021548768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:14,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:17,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [13358ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:47:24,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13358535076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:30,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:39,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17246945236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:49,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:32,095][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1227] timed out after [102082ms]
[2022-04-05T22:47:38,007][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1219] timed out after [125853ms]
[2022-04-05T22:48:01,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18405121718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:48:09,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:47:55,537][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [18.2m/1094206ms] ago, timed out [11.7m/706761ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1126]
[2022-04-05T22:48:13,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [38156ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:48:18,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19751739545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:48:27,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:48:35,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18821004129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:48:38,591][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=208, version=7617}] took [11.8m] which is above the warn threshold of [30s]: [running task [Publication{term=208, version=7617}]] took [391ms], [connecting to new nodes] took [2023ms], [applying settings] took [185ms], [org.elasticsearch.repositories.RepositoriesService@734e3959] took [841ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@626ab188] took [249062ms], [org.elasticsearch.script.ScriptService@69ad8d26] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [0ms], [org.elasticsearch.snapshots.RestoreService@85409c4] took [0ms], [org.elasticsearch.ingest.IngestService@3f850fb7] took [14134ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3a4461dc] took [1692ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016caca0@236680ff] took [264ms], [org.elasticsearch.indices.TimestampFieldMapperService@b54b77e] took [402ms], [org.elasticsearch.tasks.TaskManager@1c3bf914] took [191ms], [org.elasticsearch.snapshots.SnapshotsService@5e821ce] took [901ms], [org.elasticsearch.cluster.InternalClusterInfoService@2bd0647a] took [578ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@3d2b9d6b] took [1023ms], [org.elasticsearch.indices.SystemIndexManager@26484ae7] took [15831ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@4907ed68] took [306ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@44e1896d] took [2538ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@20807c7c] took [715ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4534e39b] took [139ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@3c227e9d] took [186ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@141ce361] took [280ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3966256] took [272538ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7151b3cc] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5d6d26ff] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@21de314d] took [24188ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@53956683] took [814ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7504264c] took [531ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@416abccd] took [22218ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@a527d26] took [2942ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@11eea9f4] took [19980ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@69b77434] took [181ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@4d88fb6f] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@5413a4fa] took [26246ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@3b50a6b9] took [14182ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@350e292c] took [289ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@69d59070] took [4539ms], [org.elasticsearch.node.ResponseCollectorService@578e869d] took [95ms], [org.elasticsearch.snapshots.SnapshotShardsService@4ab42228] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@e37251b] took [202ms], [org.elasticsearch.shutdown.PluginShutdownService@4b2472bd] took [322ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6016f0f4] took [327ms], [org.elasticsearch.indices.store.IndicesStore@2540d55a] took [408ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7cbac6c2] took [97ms], [org.elasticsearch.license.LicenseService@295958c6] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5d567cc5] took [108ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@6c64c043] took [0ms], [org.elasticsearch.gateway.GatewayService@6a393eeb] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@6876c24] took [0ms]
[2022-04-05T22:48:44,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:48:46,514][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16828ms] which is above the warn threshold of [5s]
[2022-04-05T22:48:56,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16827791050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:48:59,688][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.8m/230264ms] ago, timed out [1.7m/104411ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1219]
[2022-04-05T22:49:10,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:49:10,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@72877c45, interval=5s}] took [25553ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:49:20,412][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.5h/5633645ms] which is longer than the warn threshold of [300000ms]; there are currently [13] pending tasks, the oldest of which has age [1.6h/5822275ms]
[2022-04-05T22:49:23,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25553041656ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:49:36,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:49:45,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26377112363ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:49:57,142][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.7s/10753ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@10f42159], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9578f0b7], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6de49c68], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ce340410]], which exceeds the warn threshold of [10s]
[2022-04-05T22:49:57,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:50:09,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22118186604ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:50:24,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25762ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:50:38,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25761537702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:50:54,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:51:10,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [54309ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:51:08,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28548444800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:51:27,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:51:42,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34035243278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:51:58,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30666ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:52:10,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30665244196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:52:27,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:52:37,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29321573437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:52:52,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:03,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24494260691ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:03,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@7c10bc53] took [53815ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:53:10,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:18,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19880115685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:23,414][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [3m/182392ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-05T22:53:24,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13952ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:33,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13952663967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:41,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:50,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17357010394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:53:58,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16428ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:54:06,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [33784ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:54:08,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16427259838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:54:15,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16098ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:54:20,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [16098ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:54:31,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16098433717ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:54:14,461][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [1261] timed out after [121432ms]
[2022-04-05T22:54:41,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:54:55,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26626958031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:55:11,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:55:26,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30221966630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:55:40,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:55:27,654][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1273] timed out after [67617ms]
[2022-04-05T22:55:51,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27384485101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:56:10,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:56:28,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30299724785ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:56:45,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:57:04,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34805065214ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:57:17,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [34805ms] which is above the warn threshold of [5000ms]
[2022-04-05T22:57:25,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:58:01,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40074937403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:58:13,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:58:28,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48703623400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:58:43,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:59:01,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28198460184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:59:14,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T22:59:31,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32437221389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T22:59:51,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:00:11,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.4s/36458158203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:00:18,710][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.8m/472741ms] ago, timed out [5.8m/351309ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1261]
[2022-04-05T23:00:28,325][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [36458ms] which is above the warn threshold of [5s]
[2022-04-05T23:00:34,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43286ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:01:02,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43286860874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:03:30,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:03:41,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176086747800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:03:44,033][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [18.1m/1091667ms] ago, timed out [16.4m/989585ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1227]
[2022-04-05T23:03:55,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25498ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:03:57,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4d89f4ac, interval=1s}] took [201584ms] which is above the warn threshold of [5000ms]
[2022-04-05T23:04:10,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25497331867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:04:25,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:04:42,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28044990752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:04:58,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017e1df8@462e20d5] took [28044ms] which is above the warn threshold of [5000ms]
[2022-04-05T23:05:00,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:05:13,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37959442758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:05:33,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31770ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:05:52,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31770094516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:06:08,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:06:10,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@fa14660, interval=5s}] took [33779ms] which is above the warn threshold of [5000ms]
[2022-04-05T23:06:30,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33779163866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:06:51,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:07:17,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43643644283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:07:42,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.9s/51911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:08:02,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.9s/51910680581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:08:30,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:08:33,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@67308496] took [48188ms] which is above the warn threshold of [5000ms]
[2022-04-05T23:08:51,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48188061087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:09:14,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:09:35,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42732662454ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:09:55,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:10:14,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39765999644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:10:33,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:10:54,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40380876497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:11:19,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:11:43,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46233982864ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:12:08,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:12:55,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48111117750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:13:18,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:12:43,688][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1072602ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [37] unchanged indices
[2022-04-05T23:13:42,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70312247445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:45:40,219][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [50.4m] publication of cluster state version [7618] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Tnbc4LTaROG7UZlXciiYZg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T23:46:11,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7m/1967755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:49:12,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.7m/1967579038554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:52:18,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T23:56:00,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367156011770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T23:59:47,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T00:49:24,987][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-06T00:49:24,999][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-06T00:49:25,000][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-06T00:49:29,729][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-06T00:49:29,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-06T00:49:29,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-06T00:49:29,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-06T00:49:29,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-06T00:49:29,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-06T00:49:29,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-06T00:49:29,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-06T00:49:29,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-06T00:49:29,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-06T00:49:29,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-06T00:49:29,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-06T00:49:29,738][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-06T00:49:29,738][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-06T00:49:29,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-06T00:49:29,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-06T00:49:29,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-06T00:49:29,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-06T00:49:29,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-06T00:49:29,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-06T00:49:29,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-06T00:49:29,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-06T00:49:29,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-06T00:49:29,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-06T00:49:29,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-06T00:49:29,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-06T00:49:29,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-06T00:49:29,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-06T00:49:29,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-06T00:49:29,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-06T00:49:29,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-06T00:49:29,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-06T00:49:29,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-06T00:49:29,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-06T00:49:29,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-06T00:49:29,748][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-06T00:49:29,748][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-06T00:49:29,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-06T00:49:29,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-06T00:49:29,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-06T00:49:29,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-06T00:49:29,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-06T00:49:29,751][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-06T00:49:29,751][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-06T00:49:29,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-06T00:49:29,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-06T00:49:29,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-06T00:49:29,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-06T00:49:29,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-06T00:49:29,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-06T00:49:29,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-06T00:49:29,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-06T00:49:29,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-06T00:49:29,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-06T00:49:29,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-06T00:49:29,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-06T00:49:29,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-06T00:49:29,757][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-06T00:49:29,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-06T00:49:29,833][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-06T00:49:29,834][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-06T00:49:30,258][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-06T00:49:43,279][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-06T00:49:43,289][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-06T00:49:43,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-06T00:49:43,292][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-06T00:49:43,292][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-06T00:49:43,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-06T00:49:43,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-06T00:49:43,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-06T00:49:43,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-06T00:49:43,296][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-06T00:49:43,296][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-06T00:49:43,297][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-06T00:49:43,297][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-06T00:49:43,298][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-06T00:49:43,298][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-06T00:49:46,737][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-06T00:49:47,154][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-06T00:51:45,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [6877ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:53:33,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [5025ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:54:43,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [29413ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:55:13,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [6123ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:56:23,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [7906ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:56:51,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [8904ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:57:20,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [9936ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:57:48,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [5907ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:58:15,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [7540ms] which is above the warn threshold of [5000ms]
[2022-04-06T00:59:52,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [12505ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:00:34,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [11834ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:01:20,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [12768ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:02:14,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [22034ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:05:01,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [6516ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:07:23,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [8992ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:08:04,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [15803ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:10:16,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:11:52,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [8805ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:13:12,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:13:59,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8523408011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:14:12,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:14:19,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74699451108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:14:28,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:14:33,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [15963ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:14:38,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15963193715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:14:50,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21415ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:14:55,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [21414ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:14:57,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21414914401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:15:05,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15824ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:15:05,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [15824ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:15:15,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15824765575ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:15:30,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [24637ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:15:30,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:15:40,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24637913439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:15:49,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:15:53,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [18739ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:16:01,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18739767470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:16:12,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:16:16,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [22826ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:16:23,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22826616783ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:16:31,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:16:31,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [20129ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:16:41,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20129432826ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:16:49,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:16:50,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [17871ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:16:56,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17871680886ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:06,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:09,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [16960ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:17:15,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16960389725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:25,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:27,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [18960ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:17:31,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18960008686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:37,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:42,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [12214ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:17:47,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12214588031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:17:57,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:01,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [19602ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:18:08,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19602113314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:16,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:24,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18390526903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:34,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18058ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:38,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [18057ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:18:44,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18057874196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:55,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20940ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:18:57,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [20939ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:19:02,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20939961320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:19:12,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:19:20,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16793376365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:19:29,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:19:41,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [17509ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:19:45,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17509390752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:19:52,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:19:53,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [23429ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:20:01,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23429541329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:20:07,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15480ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:20:09,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [15480ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:20:13,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15480529571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:20:23,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:20:24,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [14782ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:20:30,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14782421379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:10:57,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@44a6dcad] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T01:20:37,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [15346ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:20:37,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15346ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:20:46,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15346421521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:20:54,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [16703ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:20:54,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16703ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:21:04,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16703261236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:21:14,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:21:14,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [18999ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:21:26,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18999785092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:21:41,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:21:55,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26959498140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:04,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:04,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [22425ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:22:15,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22425592045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:25,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:29,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [21481ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:22:35,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21481523463ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:43,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:51,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18135362586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:22:59,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:01,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [16680ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:23:06,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16680377279ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:14,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14239ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:25,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14238705015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:31,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:35,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [17921ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:23:38,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17921385348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:49,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:23:50,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [17762ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:23:59,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17762258867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:08,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:11,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [19097ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:24:15,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19097249874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:24,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:26,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [16012ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:24:29,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16012684556ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:35,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:38,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11275467527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:45,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:46,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [9252ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:24:51,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9252197680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:24:59,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14424ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:05,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14423761661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:12,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:20,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12996054778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:26,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [14525ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:25:26,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:34,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14525722664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:42,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [15541ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:25:42,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15541ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:25:50,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15541100546ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:00,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17600ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:04,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [17600ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:26:06,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17600516772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:13,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [13288ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:26:13,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:20,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13288084013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:27,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [14390ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:26:27,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:34,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14390374481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:42,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:46,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [15018ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:26:50,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15018463036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:26:55,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:01,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12799129309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:08,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:17,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13027323041ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:24,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:36,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [16613ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:27:38,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16613883248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:48,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:27:49,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [22551ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:27:55,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22551171266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:05,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [16348ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:28:04,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:11,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16348815951ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:19,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15450ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:25,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15450320922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:34,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [15233ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:28:35,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15234ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:43,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15233553406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:53,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:28:53,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [18831ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:28:58,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18831699358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:04,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10386ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:04,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [10385ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:29:09,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10385258334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:15,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [10917ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:29:15,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10917ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:20,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10917024981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:28,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:28,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [12754ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:29:36,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12754754670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:50,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:29:51,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [22522ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:30:00,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22522987078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:30:12,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [20396ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:30:11,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:30:39,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20396092006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:31:10,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [55321ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:31:09,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:31:50,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55321743069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:32:32,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:33:09,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81169836461ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:33:56,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [70471ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:33:43,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:34:42,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70471395128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:35:38,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [104201ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:35:31,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:36:31,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104201942529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:37:21,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:37:46,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [113890ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:37:56,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113890317158ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:38:54,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [89967ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:38:51,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:39:31,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89967095296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:40:06,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:40:30,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [72948ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:40:53,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72948289483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:41:34,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [83382ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:41:28,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:41:59,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83382986605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:42:28,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:42:30,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [63723ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:42:50,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63723055509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:38:54,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@44a6dcad] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T01:43:30,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [60794ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:43:28,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:43:52,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60794126234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:44:14,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:44:17,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [46184ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:44:29,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46184253907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:44:45,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30720ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:45:02,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30719828130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:45:20,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:45:38,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34793829287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:45:57,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [38196ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:45:57,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38197ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:46:17,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38196805751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:46:49,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [47526ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:46:48,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:47:21,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47526639957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:47:48,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:48:16,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62846716952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:48:46,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [57026ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:48:46,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:49:20,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57026304526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:49:56,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:50:35,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67301347556ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:51:08,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:51:15,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [73527ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:51:43,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73527184010ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:52:19,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:52:49,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71012630786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:53:16,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52822ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:53:51,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52822149284ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:54:25,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68022ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:54:36,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [68022ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:55:24,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68022141547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:56:56,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/158244ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:56:58,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [158243ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:57:14,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/158243673330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:57:34,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:57:34,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [38564ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:57:53,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.5s/38564915290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:58:13,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [35232ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:58:09,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:58:29,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35232528702ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:58:43,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:58:47,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [35029ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:58:58,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35029705029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T01:59:19,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:59:24,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [34543ms] which is above the warn threshold of [5000ms]
[2022-04-06T01:59:45,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34543841381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:00:09,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48480ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:00:21,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [48480ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:00:37,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.4s/48480111385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:01:01,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:01:05,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [51482ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:01:30,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51482336079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:02:22,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75273ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:03:04,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75272598250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:03:40,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T01:59:16,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@44a6dcad] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T02:03:57,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [83630ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:04:09,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/83630879716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:04:41,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61088ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:05:07,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61088878159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:05:36,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.5s/56530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:05:35,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [56529ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:06:04,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.5s/56529546190ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:06:29,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:06:33,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [51737ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:06:50,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51737778790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:07:09,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [42013ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:07:09,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42s/42013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:07:33,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42s/42013419286ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:07:57,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:08:05,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [46018ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:08:29,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46018559935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:08:55,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:09:18,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.3s/58377100654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:09:47,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53473ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:09:51,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [53473ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:09:59,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53473378705ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:10:14,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25965ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:10:30,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25964921217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:10:44,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:10:50,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [30981ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:11:03,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30981544883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:11:29,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44164ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:11:32,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [44163ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:11:52,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44163938544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:12:11,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:12:13,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [42127ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:12:30,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42127330136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:13:02,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [47807ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:13:01,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47808ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:13:36,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47807952121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:14:08,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:14:17,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [67099ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:14:46,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67099543727ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:15:35,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [84093ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:15:33,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/84093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:16:31,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/84093118613ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:17:35,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [124529ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:17:35,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/124530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:18:10,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/124529984018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:18:50,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:18:52,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [73783ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:19:19,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73783643419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:19:52,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:19:56,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [56792ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:20:18,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56792754861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:21:36,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/106448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:21:38,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [106447ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:22:00,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/106447963220ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:22:19,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:22:27,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [46895ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:22:34,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.8s/46895363449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:22:50,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:22:56,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [31649ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:23:07,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31649332222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:23:20,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:23:22,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [30562ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:23:32,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30562119705ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:23:49,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:23:49,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [28429ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:22:13,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@44a6dcad] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T02:24:00,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28429725909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:24:10,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:24:22,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [22486ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:24:23,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22486418086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:24:35,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25046ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:24:42,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [25046ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:24:50,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25046071823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:25:21,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:25:40,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [40975ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:25:44,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.9s/40975428843ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:26:08,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.5s/49563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:26:42,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.5s/49562683903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:27:15,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:27:52,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66793383361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:28:23,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:28:32,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [67777ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:29:04,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67777180662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:30:08,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:30:24,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [104472ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:31:11,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104472620044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:31:55,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:32:21,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [105577ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:32:39,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105577180046ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:33:15,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:33:45,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79882274206ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:34:38,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [77993ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:34:33,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:35:25,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77993758038ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:36:15,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:36:26,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=30s}] took [102185ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:36:53,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102185231156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:37:19,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65801ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:37:26,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [65801ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:37:43,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65801558886ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:38:10,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:38:50,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51242410664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:39:24,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:39:40,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [72049ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:39:54,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72049922375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:40:31,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:40:33,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [69563ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:40:58,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69563297420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:41:36,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:41:42,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [62256ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:42:12,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62256527268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:42:41,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:42:45,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [65957ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:43:11,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65957567960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:43:42,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:43:43,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [61176ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:44:09,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61176108354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:44:41,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [60372ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:44:42,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:45:16,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60372268495ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:40:40,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@44a6dcad] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T02:45:47,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:46:24,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64985315233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:47:24,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:47:26,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [91542ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:48:16,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91542313616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:50:29,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:50:41,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [182307ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:52:05,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182307231551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:54:03,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211195ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:53:53,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7a08ab8f, interval=1m}] took [211194ms] which is above the warn threshold of [5000ms]
[2022-04-06T02:56:16,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/211194376303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T02:58:46,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T02:59:15,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [282143ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:01:34,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282143905240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:05:19,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:05:15,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [394749ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:08:35,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394749181108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:12:04,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [400565ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:12:06,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:15:25,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400565698495ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:18:37,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [392478ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:18:33,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:22:35,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392478944625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:25:47,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424560ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:29:09,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424474307835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:32:44,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:35:47,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/419663197806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:39:20,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:39:22,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@36192858, interval=1m}] took [390057ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:12:56,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@44a6dcad] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T03:42:32,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/390057662135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:46:31,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/442424ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:47:36,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [442571ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:50:09,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/442571688043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T03:53:46,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T03:54:17,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7947cf4, interval=5s}] took [427580ms] which is above the warn threshold of [5000ms]
[2022-04-06T03:57:54,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427580067168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T04:02:09,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/487742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T04:02:23,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4cad604e, interval=5s}] took [488174ms] which is above the warn threshold of [5000ms]
[2022-04-06T04:07:04,997][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-06T04:07:05,018][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-06T04:07:05,019][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-06T04:07:10,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-06T04:07:10,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-06T04:07:10,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-06T04:07:10,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-06T04:07:10,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-06T04:07:10,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-06T04:07:10,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-06T04:07:10,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-06T04:07:10,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-06T04:07:10,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-06T04:07:10,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-06T04:07:10,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-06T04:07:10,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-06T04:07:10,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-06T04:07:10,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-06T04:07:10,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-06T04:07:10,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-06T04:07:10,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-06T04:07:10,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-06T04:07:10,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-06T04:07:10,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-06T04:07:10,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-06T04:07:10,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-06T04:07:10,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-06T04:07:10,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-06T04:07:10,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-06T04:07:10,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-06T04:07:10,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-06T04:07:10,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-06T04:07:10,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-06T04:07:10,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-06T04:07:10,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-06T04:07:10,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-06T04:07:10,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-06T04:07:10,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-06T04:07:10,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-06T04:07:10,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-06T04:07:10,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-06T04:07:10,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-06T04:07:10,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-06T04:07:10,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-06T04:07:10,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-06T04:07:10,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-06T04:07:10,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-06T04:07:10,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-06T04:07:10,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-06T04:07:10,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-06T04:07:10,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-06T04:07:10,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-06T04:07:10,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-06T04:07:10,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-06T04:07:10,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-06T04:07:10,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-06T04:07:10,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-06T04:07:10,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-06T04:07:10,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-06T04:07:10,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-06T04:07:10,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-06T04:07:10,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-06T04:07:10,182][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-06T04:07:10,183][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-06T04:07:10,598][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-06T04:13:56,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:23:41,517][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-06T05:23:41,574][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-06T05:23:41,576][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-06T05:23:47,266][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-06T05:23:47,269][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-06T05:23:47,270][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-06T05:23:47,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-06T05:23:47,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-06T05:23:47,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-06T05:23:47,272][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-06T05:23:47,272][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-06T05:23:47,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-06T05:23:47,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-06T05:23:47,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-06T05:23:47,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-06T05:23:47,274][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-06T05:23:47,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-06T05:23:47,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-06T05:23:47,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-06T05:23:47,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-06T05:23:47,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-06T05:23:47,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-06T05:23:47,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-06T05:23:47,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-06T05:23:47,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-06T05:23:47,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-06T05:23:47,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-06T05:23:47,279][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-06T05:23:47,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-06T05:23:47,280][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-06T05:23:47,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-06T05:23:47,281][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-06T05:23:47,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-06T05:23:47,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-06T05:23:47,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-06T05:23:47,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-06T05:23:47,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-06T05:23:47,283][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-06T05:23:47,284][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-06T05:23:47,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-06T05:23:47,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-06T05:23:47,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-06T05:23:47,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-06T05:23:47,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-06T05:23:47,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-06T05:23:47,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-06T05:23:47,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-06T05:23:47,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-06T05:23:47,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-06T05:23:47,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-06T05:23:47,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-06T05:23:47,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-06T05:23:47,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-06T05:23:47,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-06T05:23:47,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-06T05:23:47,290][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-06T05:23:47,290][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-06T05:23:47,290][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-06T05:23:47,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-06T05:23:47,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-06T05:23:47,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-06T05:23:47,292][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-06T05:23:47,402][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-06T05:23:47,404][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-06T05:23:47,893][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-06T05:25:03,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:25:03,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@12936d3c, interval=1m}] took [10765ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:25:05,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10765511350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:25:05,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:25:05,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13861527481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:27:12,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [8044ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:30:20,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:35:22,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:37:09,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [13007ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:37:15,683][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-06T05:37:47,673][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-06T05:44:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6874ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:44:57,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [9961ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:45:38,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [19565ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:46:18,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [9946ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:46:44,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [7277ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:48:01,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:48:41,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39151854792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:48:53,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:49:11,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75166433056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:49:32,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:49:50,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39222190204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:50:08,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [31399ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:50:10,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:50:23,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31399926210ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:50:37,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:50:56,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32617110548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:02,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:03,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [28171ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:51:07,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28171629206ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:10,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:12,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7204724965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:16,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6397ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:51:16,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:18,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6397024274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T05:51:19,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@79a18e52] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T05:51:37,672][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-06T05:51:37,900][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-06T05:52:20,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [6089ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:52:57,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@58a2814, interval=1m}] took [12587ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:53:43,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [11971ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:54:16,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6050ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:54:57,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [9834ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:55:30,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@58a2814, interval=1m}] took [9047ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:56:09,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6091ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:57:08,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [18740ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:57:53,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5115ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:58:22,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5899ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:58:42,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5447ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:59:10,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5062ms] which is above the warn threshold of [5000ms]
[2022-04-06T05:59:52,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [6474ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:00:23,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5479ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:00:57,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6240ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:01:25,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [7142ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:02:14,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [14403ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:03:01,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [15615ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:03:27,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [7307ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:04:00,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6608ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:04:45,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [14593ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:05:22,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5954ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:06:09,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5484ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:06:41,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [8526ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:07:41,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [12637ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:08:30,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5051ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:09:13,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [7399ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:09:34,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [7478ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:07:05,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@79a18e52] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T06:10:05,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6098ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:10:37,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [7552ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:11:08,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [7409ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:11:32,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [8738ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:12:03,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5098ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:12:25,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5213ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:12:57,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6098ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:13:34,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [10754ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:14:00,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [6572ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:14:32,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [5439ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:14:53,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5332ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:15:32,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [6945ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:15:56,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [6918ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:16:19,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [5170ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:16:49,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [8537ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:17:49,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@12936d3c, interval=1m}] took [26294ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:18:29,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [8169ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:19:13,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [13237ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:20:05,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [10015ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:24:15,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [70754ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:24:13,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:25:03,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114102144297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T06:26:03,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/110163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:26:03,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@12936d3c, interval=1m}] took [110162ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:27:02,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/110162555457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T06:28:39,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/153535ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:28:45,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [153534ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:29:14,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/153534967426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T06:29:50,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:30:29,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72760037239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T06:31:14,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:31:17,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [85096ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:31:42,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85096412761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T06:32:06,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.7s/52794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:32:15,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [52794ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:32:32,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.7s/52794067237ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T06:33:10,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T06:33:14,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [62754ms] which is above the warn threshold of [5000ms]
[2022-04-06T06:28:42,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@79a18e52] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T06:35:55,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62754760781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:01:40,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3m/1698263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:01:40,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [1697872ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:04:48,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2m/1697872965030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:08:52,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:09:24,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fdb75e2, interval=5s}] took [427216ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:12:14,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427216142845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:18:22,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [558000ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:18:02,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/558000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:22:26,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/558000467822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:26:31,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:26:46,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@65771487, interval=5s}] took [485421ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:30:12,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485421998236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:33:41,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/438857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:38:51,821][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-06T07:38:51,837][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-06T07:38:51,838][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-06T07:38:56,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-06T07:38:56,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-06T07:38:56,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-06T07:38:56,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-06T07:38:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-06T07:38:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-06T07:38:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-06T07:38:56,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-06T07:38:56,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-06T07:38:56,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-06T07:38:56,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-06T07:38:56,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-06T07:38:56,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-06T07:38:56,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-06T07:38:56,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-06T07:38:56,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-06T07:38:56,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-06T07:38:56,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-06T07:38:56,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-06T07:38:56,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-06T07:38:56,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-06T07:38:56,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-06T07:38:56,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-06T07:38:56,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-06T07:38:56,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-06T07:38:56,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-06T07:38:56,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-06T07:38:56,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-06T07:38:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-06T07:38:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-06T07:38:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-06T07:38:56,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-06T07:38:56,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-06T07:38:56,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-06T07:38:56,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-06T07:38:56,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-06T07:38:56,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-06T07:38:56,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-06T07:38:56,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-06T07:38:56,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-06T07:38:56,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-06T07:38:56,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-06T07:38:56,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-06T07:38:56,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-06T07:38:56,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-06T07:38:56,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-06T07:38:56,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-06T07:38:56,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-06T07:38:56,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-06T07:38:56,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-06T07:38:56,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-06T07:38:56,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-06T07:38:56,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-06T07:38:56,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-06T07:38:56,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-06T07:38:56,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-06T07:38:56,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-06T07:38:56,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-06T07:38:56,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-06T07:38:56,476][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.4gb], net total_space [125.8gb], types [ext4]
[2022-04-06T07:38:56,477][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-06T07:38:56,833][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-06T07:39:07,111][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-06T07:39:07,118][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-06T07:39:08,563][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-06T07:39:08,773][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-06T07:39:09,725][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-06T07:39:10,595][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-06T07:39:10,600][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-06T07:39:10,908][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-06T07:39:10,943][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-06T07:39:51,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:39:57,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [17779ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:40:09,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5172551669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:14,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26540ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:17,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [26539ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:40:19,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26539763420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:25,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:28,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10575424963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:34,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:42,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [20224ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:40:48,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9649265797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:40:55,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20842ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:00,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20842096177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:05,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:12,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9382445014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:19,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:26,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13523320456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:35,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:41,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15321836934ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:49,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:41:59,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14745286665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:10,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:16,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21095552111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:22,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:25,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [76643ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:42:26,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11957709978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:32,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:34,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [9800ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:42:35,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9800834301ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:40,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:43,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [7573ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:42:47,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7573986059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:42:54,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:01,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15347344773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:08,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:13,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13627302248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:19,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:25,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11114384763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:29,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:34,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [34777ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:43:35,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10035313126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:42,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [12834ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:43:42,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:48,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12834288939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:43:57,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [13281ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:43:56,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13281ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:01,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13281018973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:08,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:10,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [12623ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:44:17,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12623723186ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:26,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:35,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17384853163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:40,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [17384ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:44:45,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:52,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18744281371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:44:38,943][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [43289ms] which is above the warn threshold of [5s]
[2022-04-06T07:45:00,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:08,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16089742260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:14,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:21,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14042016677ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:30,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:42,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [29385ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:45:44,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15343475202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:53,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23650ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:45:55,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@5a208fdf, interval=1m}] took [23649ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:46:01,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23649922249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:46:10,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:46:28,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16961819272ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:46:37,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:46:46,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26255868105ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:46:55,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:02,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18081441534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:11,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [61503ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:47:12,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:17,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17166675416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:22,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [10242ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:47:29,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10242195890ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:34,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:35,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [12325ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:47:40,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12325318352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:46,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:47:53,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12025805999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:00,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:06,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13675425267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:10,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [25701ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:48:10,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11286ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:17,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11286438836ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:04,732][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13675ms] which is above the warn threshold of [5s]
[2022-04-06T07:48:26,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:28,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [14584ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:48:33,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14584430531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:39,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:48:40,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [13117ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:48:47,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13117896025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:49:00,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20630ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:49:07,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20629134687ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:49:23,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [40937ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:49:20,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20308ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:49:31,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20308203182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:49:43,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22115ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:49:43,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@5a208fdf, interval=1m}] took [22115ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:49:51,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22115620603ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:10,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [26746ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:50:08,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:16,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26746358609ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:24,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15602ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:29,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15602506584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:36,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:43,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11073881804ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:51,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:59,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15552276468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:50:59,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [42228ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:51:10,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:51:21,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18893892556ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:51:37,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:51:36,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [25052ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:51:49,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25052221690ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:02,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:12,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.5s/26580696052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:23,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:32,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [47539ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:52:31,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20959303929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:39,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:41,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [16268ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:52:45,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16268307458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:51,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:51,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [12067ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:52:55,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12067221173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:52:59,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:03,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8207792338ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:07,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:09,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [16600ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:53:10,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8392399672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:15,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:19,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [7884ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:53:20,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7884622872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:25,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:30,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9370865001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:35,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:37,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10935759363ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:53:40,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [15780ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:54:10,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [10447ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:54:22,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:54:49,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [12207ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:55:18,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [16810ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:55:47,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [8798ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:54:34,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@69f2de2] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T07:56:42,995][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10050ms] which is above the warn threshold of [5s]
[2022-04-06T07:56:43,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [11049ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:56:43,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:56:45,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9248955416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:56:49,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:56:50,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [6667ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:56:53,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6667131273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:56:57,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:01,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7885590869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:05,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:09,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [15793ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:57:10,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7908212318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:17,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:17,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [11661ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:57:24,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11661282132ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:38,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:44,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20487705787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:57:53,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:58:00,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15112648500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:58:09,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [35600ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:58:10,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:58:21,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16679330611ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:58:30,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:58:30,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [21374ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:58:38,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21374669056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:58:58,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:08,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26920436931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:12,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14567ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:16,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [41487ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:59:18,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14567067659ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:22,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:23,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [10539ms] which is above the warn threshold of [5000ms]
[2022-04-06T07:59:26,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10539232431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:28,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:30,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6054938297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T07:59:30,863][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6055ms] which is above the warn threshold of [5s]
[2022-04-06T07:59:31,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [6054ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:00:45,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [9271ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:01:16,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [15111ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:02:09,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [5669ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:02:46,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [9874ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:05:16,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [118748ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:05:41,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:06:22,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10337024148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:06:57,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:07:09,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [75634ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:08:01,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75634284714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:08:26,337][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [75635ms] which is above the warn threshold of [5s]
[2022-04-06T08:08:35,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/100709ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:09:13,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/100709297957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:09:47,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:09:49,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [171173ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:10:22,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70463996050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:10:53,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [66628ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:11:00,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:11:33,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66628290379ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:12:10,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:12:39,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77733549145ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:13:11,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:13:08,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [77733ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:13:33,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62607617706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:13:54,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:13:55,415][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [41043ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:14:11,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41043544755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:14:38,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:14:39,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@65d6abb2, interval=1m}] took [43563ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:14:52,558][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [84608ms] which is above the warn threshold of [5s]
[2022-04-06T08:15:19,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43563477198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:15:51,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:16:35,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73258199163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:16:58,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [73258ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:17:06,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:17:44,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74186790345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:18:12,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68346ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:18:14,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [68346ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:18:36,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68346622164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:19:03,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:19:54,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.2s/47282935311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:20:19,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [47282ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:20:34,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:21:16,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93426377283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:14:59,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@69f2de2] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T08:21:49,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:22:25,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73742940653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:22:27,379][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [73743ms] which is above the warn threshold of [5s]
[2022-04-06T08:22:56,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [133046ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:22:48,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.3s/59303ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:23:19,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.3s/59303629297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:23:37,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.8s/51811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:23:40,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [51811ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:24:01,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.8s/51811291545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:24:19,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:24:42,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.3s/40382382487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:25:02,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43646ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:25:15,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [84028ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:25:20,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43645920372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:25:40,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [35170ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:25:38,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:26:12,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35170450620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:26:52,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:27:35,823][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [68723ms] which is above the warn threshold of [5s]
[2022-04-06T08:27:32,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68723436921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:27:57,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [68723ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:27:59,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:28:28,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74513880212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:28:57,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [53456ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:28:55,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:29:25,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53456087204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:29:57,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:30:30,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60188249349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:30:59,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:31:02,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [60188ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:31:26,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63265677969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:31:54,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:32:27,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53733678027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:32:52,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:33:25,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.1s/59160187153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:33:40,659][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [59160ms] which is above the warn threshold of [5s]
[2022-04-06T08:33:55,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:34:09,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [121268ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:34:36,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62108084806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:35:56,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/123863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:36:43,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/123863569916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:36:59,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:37:16,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64945146159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:37:12,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [188808ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:37:30,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:37:43,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29995607368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:38:00,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:38:14,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28869183159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:38:17,812][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28869ms] which is above the warn threshold of [5s]
[2022-04-06T08:38:31,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:38:47,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32456285300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:38:46,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [61325ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:39:00,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:39:13,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30069709892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:39:30,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:39:43,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27400946781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:40:01,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:40:18,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [58412ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:40:18,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31011470230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:40:37,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:40:39,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [35593ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:40:54,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35593235490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:41:19,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.3s/42342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:39:12,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@69f2de2] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T08:41:26,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [42342ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:41:36,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.3s/42342180773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:41:56,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:42:18,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35150964983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:42:39,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:43:10,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45208028684ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:43:15,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [80358ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:43:45,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.4s/59421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:44:17,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.4s/59420799881ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:44:38,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:44:38,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [61231ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:44:46,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61231860303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:44:50,029][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [61232ms] which is above the warn threshold of [5s]
[2022-04-06T08:44:57,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:45:09,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18051305083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:45:22,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:45:27,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [41387ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:45:33,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23336028107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:45:42,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:45:42,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [23464ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:45:50,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23464602406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:45:57,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:03,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14262697780ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:02,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [14262ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:46:10,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [10981ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:46:09,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:15,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10981983426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:19,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:22,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [12042ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:46:23,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12042069386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:26,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:29,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6957528191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:32,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5731ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:32,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [12689ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:46:36,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5731628617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:40,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7354ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:41,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [7353ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:46:42,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7353260956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:46,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5842ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:51,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5842019900ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:46:55,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [5842ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:46:56,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10877ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:01,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10877593067ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:06,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:12,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9276199297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:17,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [9276ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:47:18,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:25,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12699935163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:33,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:34,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [14075ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:47:40,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14075144464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:46,794][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14075ms] which is above the warn threshold of [5s]
[2022-04-06T08:47:49,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:47:57,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16019109863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:08,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18768ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:14,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [34787ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:48:18,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18768405234ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:24,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:25,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [16271ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:48:31,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16271355908ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:39,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:41,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [14969ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:48:47,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14969180569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:48:59,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:15,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19915789665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:14,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [19915ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:49:22,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:24,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [23806ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:49:27,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23806922462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:32,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [9732ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:49:32,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:36,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9732175420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:40,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:43,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8475202147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:48,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7770ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:49:50,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [16245ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:49:53,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7770582142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:01,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:02,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [11914ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:50:04,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11914996351ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:11,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10334ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:18,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10334241110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:19,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [10334ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:50:23,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:28,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12734230247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:32,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:34,221][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8580ms] which is above the warn threshold of [5s]
[2022-04-06T08:50:36,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8580488897ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:37,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [8580ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:50:39,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7436ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:43,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7435898604ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:46,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7554ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:49,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [7553ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:50:49,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7553917214ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T08:50:53,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [7155ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:51:03,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [6020ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:51:26,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [14258ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:51:43,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [7405ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:52:06,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [11771ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:52:30,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [10608ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:52:44,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:52:54,468][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5218ms] which is above the warn threshold of [5s]
[2022-04-06T08:52:57,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [8220ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:53:13,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [11625ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:53:39,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [7825ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:53:55,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [8706ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:54:09,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [6596ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:54:23,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [5404ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:54:38,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [6492ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:55:06,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [16117ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:55:29,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [6161ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:56:04,267][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [15812ms] which is above the warn threshold of [5s]
[2022-04-06T08:56:05,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [22217ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:56:25,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [6426ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:57:35,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [52014ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:58:05,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [10163ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:58:35,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [12199ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:58:46,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@69f2de2] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T08:58:52,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [7104ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:59:09,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [11209ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:59:27,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [8606ms] which is above the warn threshold of [5000ms]
[2022-04-06T08:59:54,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [9190ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:00:21,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [10607ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:00:39,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [5404ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:01:07,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [11008ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:01:23,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [9006ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:01:37,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [9392ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:02:13,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7840ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:02:26,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7840140219ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:03:33,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [45028ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:02:39,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:04:34,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32450719265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:04:51,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/129845ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:04:52,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [129844ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:05:07,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/129844979562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:05:35,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@5a208fdf, interval=1m}] took [46537ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:05:38,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:06:03,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46537230184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:06:21,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44902ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:08:20,339][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [44902ms] which is above the warn threshold of [5s]
[2022-04-06T09:08:26,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44901900810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:08:47,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [44901ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:08:43,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:10:45,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/139998997138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:12:15,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/193316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:12:29,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [193315ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:14:09,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/193315782949ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:16:59,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [229287ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:15:53,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/229287ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:16:44,382][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [229288ms] which is above the warn threshold of [5s]
[2022-04-06T09:17:47,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/229287864848ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:18:45,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/179366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:19:18,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/179365354092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:18:54,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@65d6abb2, interval=1m}] took [179365ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:19:54,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:20:43,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69483981734ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:21:32,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [75856ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:21:10,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:21:58,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75856825792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:22:14,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:22:16,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [66574ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:22:28,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66574961728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:18:12,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@69f2de2] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T09:22:41,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:24:00,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27504862880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:24:43,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [130503ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:24:28,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:24:39,769][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [102999ms] which is above the warn threshold of [5s]
[2022-04-06T09:25:12,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/102999048144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:25:52,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@241bcef2, interval=30s}] took [80506ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:25:48,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:26:40,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80506622751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:27:23,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:28:06,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/95472285625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:28:44,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [95472ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:28:50,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85010ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:30:11,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/85009908879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:31:09,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [120589ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:30:49,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:31:55,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120589740599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:32:32,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:33:23,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103032963350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:33:51,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78473ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:34:14,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [181506ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:34:06,178][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [181506ms] which is above the warn threshold of [5s]
[2022-04-06T09:34:47,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78473050501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:36:02,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/132550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:36:08,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@633b8e5f] took [132550ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:36:33,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/132550275687ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:37:23,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:38:18,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79219995967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:38:15,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [79219ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:38:43,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:39:18,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81326240503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:39:47,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:40:14,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64457480441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:40:28,144][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [64458ms] which is above the warn threshold of [5s]
[2022-04-06T09:41:43,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/117251ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:41:54,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [117251ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:42:06,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/117251437925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:42:36,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:42:37,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [52647ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:42:56,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52647111879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:43:16,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@65d6abb2, interval=1m}] took [40091ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:43:17,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40091ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:43:36,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40091979342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:43:55,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:40:07,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] failed to run scheduled task [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker@69f2de2] on thread pool [generic]
java.lang.NullPointerException: Cannot invoke "org.elasticsearch.cluster.ClusterState.metadata()" because "state" is null
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.hasSearchableSnapshotsIndices(SearchableSnapshotsUsageTracker.java:37) ~[?:?]
	at org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsUsageTracker.run(SearchableSnapshotsUsageTracker.java:31) ~[?:?]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-06T09:44:13,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37962167314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:44:41,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [80522ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:44:42,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42560ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:45:03,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42560602238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:45:29,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:45:31,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@65d6abb2, interval=1m}] took [53713ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:46:09,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53713283268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:46:44,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67088ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:47:06,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67088497186ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:47:00,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [67088ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:47:30,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:48:04,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51649441544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:48:29,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:49:05,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59507107412ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:50:00,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:50:33,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [141917ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:50:48,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82410785348ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:51:35,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b705ac0, interval=5s}] took [97647ms] which is above the warn threshold of [5000ms]
[2022-04-06T09:51:39,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:52:33,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97647941139ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:53:03,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T09:54:20,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92909886776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T09:55:22,036][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [92910ms] which is above the warn threshold of [5s]
[2022-04-06T09:57:20,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:00:23,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [328958ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:02:30,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236048795447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T10:06:48,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6m/576522ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:07:27,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [576482ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:11:55,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6m/576482948757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T10:17:32,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7m/645108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:22:10,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36192858, interval=1s}] took [645426ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:23:18,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7m/645426283037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T10:27:58,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2m/617132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:28:47,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4666500, interval=5s}] took [617127ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:33:12,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2m/617127778604ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T10:37:35,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6m/581424ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:49:57,568][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-06T10:49:57,600][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-06T10:49:57,601][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-06T10:50:03,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-06T10:50:03,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-06T10:50:03,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-06T10:50:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-06T10:50:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-06T10:50:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-06T10:50:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-06T10:50:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-06T10:50:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-06T10:50:03,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-06T10:50:03,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-06T10:50:03,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-06T10:50:03,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-06T10:50:03,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-06T10:50:03,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-06T10:50:03,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-06T10:50:03,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-06T10:50:03,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-06T10:50:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-06T10:50:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-06T10:50:03,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-06T10:50:03,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-06T10:50:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-06T10:50:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-06T10:50:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-06T10:50:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-06T10:50:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-06T10:50:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-06T10:50:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-06T10:50:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-06T10:50:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-06T10:50:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-06T10:50:03,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-06T10:50:03,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-06T10:50:03,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-06T10:50:03,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-06T10:50:03,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-06T10:50:03,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-06T10:50:03,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-06T10:50:03,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-06T10:50:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-06T10:50:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-06T10:50:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-06T10:50:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-06T10:50:03,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-06T10:50:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-06T10:50:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-06T10:50:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-06T10:50:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-06T10:50:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-06T10:50:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-06T10:50:03,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-06T10:50:03,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-06T10:50:03,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-06T10:50:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-06T10:50:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-06T10:50:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-06T10:50:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-06T10:50:03,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-06T10:50:03,562][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-06T10:50:03,563][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-06T10:50:04,197][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-06T10:52:06,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [57833ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:56:40,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [5299ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:57:55,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19224ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:59:15,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19223812047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T10:59:36,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T10:59:41,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [118766ms] which is above the warn threshold of [5000ms]
[2022-04-06T10:59:50,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118766194989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:00:04,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:00:07,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [29858ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:00:14,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29858502899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:00:26,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:00:35,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22220493531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:00:45,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:00:54,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18424434257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:01:04,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19968ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:01:14,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19968426519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:01:25,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:01:28,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [21079ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:01:33,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21079839769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:01:48,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:01:52,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [21660ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:02:03,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21660245045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:02:21,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:02:24,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [32636ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:02:35,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32636475557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:02:51,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@64ed27ac, interval=1m}] took [28581ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:02:50,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:03:03,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28581918003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:03:20,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30359ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:03:21,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [30358ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:03:31,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30358265263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:03:42,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:03:54,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23275998108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:03:59,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:04:03,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17374278561ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:04:07,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:04:13,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8575469910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:04:15,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:04:18,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8265886595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:06:54,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:07:14,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:07:33,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8057277700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:07:57,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:08:14,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43572554638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:08:42,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.9s/45924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:08:42,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [45923ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:08:57,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.9s/45923803776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:09:18,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:09:35,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31624847416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:09:58,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.9s/43923ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:10:20,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.9s/43922777094ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:10:36,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:11:04,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39750659343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:11:34,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@64ed27ac, interval=1m}] took [55048ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:11:32,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55s/55048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:11:54,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55s/55048021714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:12:20,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44961ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:12:19,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [44961ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:12:41,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44961505637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:12:58,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.7s/41730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:13:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [41729ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:13:18,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.7s/41729722815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:13:53,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51086ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:13:53,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [51085ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:14:31,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51085900223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:14:58,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:15:21,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67663586688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:15:41,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:15:59,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43737545318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:16:20,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:16:34,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38715621026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:16:51,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30652ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:16:54,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [30652ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:17:10,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30652318628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:17:34,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [41326ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:17:32,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:17:56,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41326083686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:18:15,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:18:17,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [42952ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:18:32,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42952459381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:18:47,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:18:52,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [31843ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:19:08,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31843579462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:19:23,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:19:25,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [36753ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:19:36,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.7s/36753911299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:19:47,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@64ed27ac, interval=1m}] took [25294ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:19:48,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:20:01,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25294540900ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:20:12,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:20:15,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [24166ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:20:27,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24166207773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:20:43,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [27743ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:20:42,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:20:56,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27743515144ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:21:07,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@64ed27ac, interval=1m}] took [27370ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:21:07,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:21:20,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27370400782ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:21:31,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24010ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:21:40,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24010653657ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:21:48,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:21:55,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17426451728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:22:07,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18778ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:22:17,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18778356697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:22:30,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:22:43,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21604132110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:23:04,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:23:19,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34181181529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:23:30,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:23:30,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [27533ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:23:37,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27533362149ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:23:49,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18848ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:23:49,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [18847ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:24:00,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18847720490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:24:17,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:24:26,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26639232877ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:24:34,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:24:42,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17521115470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:24:56,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:25:08,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20846164669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:25:47,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48044ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:26:34,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48043795395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:26:08,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [48043ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:27:16,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:28:02,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89469983882ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:28:43,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [81867ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:28:40,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81868ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:29:32,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81867815024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:30:23,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:30:45,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103942155003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:31:16,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:31:32,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.7s/56703970089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:31:53,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32040ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:31:54,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [32040ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:31:59,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32040038681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:32:10,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:32:22,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23212654024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:32:32,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:32:40,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21335896311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:32:53,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:33:07,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21467356752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:33:19,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:33:30,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25773226852ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:33:41,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:33:49,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23012594017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:34:00,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:34:11,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18887725390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:34:22,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:34:41,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20748636541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:35:00,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:35:14,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.6s/38696072478ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:35:35,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:35:34,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [33969ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:35:45,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33969120422ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:35:58,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23848ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:36:08,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23847529739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:36:21,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [23410ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:36:24,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:36:36,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23410490835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:36:49,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:36:49,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [27965ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:37:06,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27965652020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:37:21,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:37:37,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31733154052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:37:56,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:38:37,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32404036682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:39:46,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:39:49,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [108508ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:40:13,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108508343192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:40:46,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:40:54,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [62614ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:41:20,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62614708393ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:41:56,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:42:23,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70529580352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:42:48,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.6s/50634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:43:11,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.6s/50634350140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:43:31,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44119ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:43:53,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44118533450ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:44:14,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:44:17,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@64ed27ac, interval=1m}] took [43279ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:44:36,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43279206025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:44:55,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:45:19,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40812072526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:45:41,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47s/47062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:46:04,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47s/47062380355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:46:30,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:47:03,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48039074636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:47:42,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:48:12,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71653332499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:48:52,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:49:08,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [69019ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:49:24,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69019381299ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:50:02,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71050ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:50:48,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71049487693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:52:10,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:52:16,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [126579ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:52:46,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/126579927786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:53:16,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:53:20,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [67825ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:53:37,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67825701920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:53:57,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:53:59,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [40773ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:54:15,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.7s/40773262187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:54:26,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@52edb833, interval=30s}] took [30100ms] which is above the warn threshold of [5000ms]
[2022-04-06T11:54:25,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:54:28,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30100990756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:54:31,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-06T11:54:31,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6093290872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-06T11:59:50,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [8499ms] which is above the warn threshold of [5000ms]
[2022-04-06T12:00:25,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [8742ms] which is above the warn threshold of [5000ms]
[2022-04-06T12:00:50,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@64ed27ac, interval=1m}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-04-06T12:02:48,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [9569ms] which is above the warn threshold of [5000ms]
[2022-04-06T12:03:31,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [9531ms] which is above the warn threshold of [5000ms]
[2022-04-06T12:04:37,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@70e8268f, interval=5s}] took [5002ms] which is above the warn threshold of [5000ms]
