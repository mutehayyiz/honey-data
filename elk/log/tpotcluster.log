[2022-03-27T17:46:52,777][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T17:46:52,797][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T17:46:52,800][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T17:47:09,675][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T17:47:09,687][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T17:47:09,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T17:47:09,696][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T17:47:09,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T17:47:09,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T17:47:09,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T17:47:09,705][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T17:47:09,717][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T17:47:09,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T17:47:09,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T17:47:09,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T17:47:09,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T17:47:09,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T17:47:09,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T17:47:09,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T17:47:09,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T17:47:09,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T17:47:09,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T17:47:09,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T17:47:09,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T17:47:09,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T17:47:09,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T17:47:09,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T17:47:09,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T17:47:09,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T17:47:09,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T17:47:09,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T17:47:09,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T17:47:09,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T17:47:09,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T17:47:09,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T17:47:09,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T17:47:09,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T17:47:09,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T17:47:09,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T17:47:09,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T17:47:09,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T17:47:09,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T17:47:09,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T17:47:09,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T17:47:09,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T17:47:09,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T17:47:09,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T17:47:09,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T17:47:09,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T17:47:09,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T17:47:09,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T17:47:09,812][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T17:47:09,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T17:47:09,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T17:47:09,822][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T17:47:09,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T17:47:09,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T17:47:09,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T17:47:09,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T17:47:09,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T17:47:09,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T17:47:09,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T17:47:10,020][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.7gb], net total_space [125.8gb], types [ext4]
[2022-03-27T17:47:10,021][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T17:47:11,013][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T17:47:31,741][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T17:47:31,744][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T17:47:33,753][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T17:47:34,013][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T17:47:35,529][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T17:47:36,991][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T17:47:36,992][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T17:47:37,040][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T17:47:37,042][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T17:47:37,441][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T17:47:41,679][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T17:47:41,855][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 131, version: 3734, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T17:47:42,182][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 131, version: 3734, reason: Publication{term=131, version=3734}
[2022-03-27T17:47:42,341][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T17:47:42,362][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T17:47:44,026][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T17:47:44,033][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T17:47:45,468][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T17:47:45,469][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T17:47:47,134][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:47,324][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:48,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-27T17:47:48,061][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-27T17:47:48,065][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-27T17:47:48,364][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:50,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T17:47:50,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T17:47:58,958][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T17:48:15,184][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][38] overhead, spent [270ms] collecting in the last [1s]
[2022-03-27T17:48:19,121][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-27T17:48:19,603][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-27T17:48:50,578][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.26][0]]]).
[2022-03-27T17:49:27,559][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 574 finished with response BulkByScrollResponse[took=327.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-27T17:49:30,368][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 599 finished with response BulkByScrollResponse[took=2.5s,timed_out=false,sliceId=null,updated=1039,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-27T17:49:42,455][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-27T17:50:41,627][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:42,207][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:42,572][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:46,125][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:49,220][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:53,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,592][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,745][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,987][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:51,574][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:54,651][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:54,892][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:52:03,704][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:07,779][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:08,711][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:45,784][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:55:21,835][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:57:33,083][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:59:32,216][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:26:08,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:26:08,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:29:35,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@50f1bee5, interval=1s}] took [31305ms] which is above the warn threshold of [5000ms]
[2022-03-27T18:29:51,496][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59330}] took [12865ms] which is above the warn threshold of [5000ms]
[2022-03-27T18:29:35,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T18:33:24,635][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.4s/14468ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T18:40:30,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38043637312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T18:50:37,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/999161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T18:50:39,427][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.3s/21302ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78ac69ee]], which exceeds the warn threshold of [10s]
[2022-03-27T19:01:30,751][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.8s/13860ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78ac69ee]], which exceeds the warn threshold of [10s]
[2022-03-27T19:01:31,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/998758196133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:05:42,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4m/1224665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:06:45,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017ed730@62f2767e] took [1224762ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:07:58,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4m/1224762295911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:06:07,449][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.4s/15422ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f4279a90], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5118159f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T19:09:36,972][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:38120}] took [1224762ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:09:58,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:12:30,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257421582102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:12:31,275][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.3s/22325ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f4279a90], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5118159f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T19:14:46,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/272121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:14:40,518][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:38098}] took [271770ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:16:55,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/271770693812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:18:48,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/256996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:20:07,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257226656122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:15:35,890][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [1753954ms] which is above the warn threshold of [5s]
[2022-03-27T19:21:53,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/181604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:24:07,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/181472293679ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:25:41,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/228160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:31:58,905][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T19:31:58,921][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T19:31:58,922][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T19:32:06,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T19:32:06,558][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T19:32:06,560][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T19:32:06,561][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T19:32:06,563][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T19:32:06,564][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T19:32:06,565][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T19:32:06,567][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T19:32:06,568][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T19:32:06,570][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T19:32:06,571][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T19:32:06,572][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T19:32:06,574][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T19:32:06,575][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T19:32:06,576][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T19:32:06,577][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T19:32:06,577][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T19:32:06,579][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T19:32:06,580][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T19:32:06,581][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T19:32:06,582][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T19:32:06,582][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T19:32:06,583][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T19:32:06,584][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T19:32:06,584][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T19:32:06,585][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T19:32:06,586][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T19:32:06,586][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T19:32:06,587][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T19:32:06,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T19:32:06,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T19:32:06,589][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T19:32:06,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T19:32:06,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T19:32:06,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T19:32:06,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T19:32:06,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T19:32:06,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T19:32:06,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T19:32:06,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T19:32:06,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T19:32:06,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T19:32:06,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T19:32:06,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T19:32:06,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T19:32:06,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T19:32:06,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T19:32:06,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T19:32:06,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T19:32:06,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T19:32:06,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T19:32:06,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T19:32:06,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T19:32:06,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T19:32:06,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T19:32:06,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T19:32:06,721][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.5gb], net total_space [125.8gb], types [ext4]
[2022-03-27T19:32:06,722][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T19:32:07,143][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T19:32:23,141][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T19:32:23,165][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,167][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T19:32:23,170][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,171][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,172][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,173][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,174][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,175][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-27T19:32:23,176][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,177][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,178][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T19:32:23,179][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T19:32:23,180][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,183][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T19:32:26,734][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T19:32:27,025][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T19:32:28,827][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T19:32:30,420][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T19:32:30,422][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T19:32:30,601][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T19:32:30,603][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T19:32:31,093][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T19:32:34,054][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T19:32:34,195][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 132, version: 3812, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T19:32:34,388][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 132, version: 3812, reason: Publication{term=132, version=3812}
[2022-03-27T19:32:34,593][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T19:32:34,594][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T19:32:35,992][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T19:32:36,004][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T19:32:37,480][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T19:32:37,483][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T19:32:39,882][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-27T19:32:39,895][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-27T19:32:39,911][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-27T19:32:42,578][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T19:32:42,641][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T19:32:42,773][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-27T19:32:51,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7033ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:07,958][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13522ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:33:18,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:25,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003156048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:25,736][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [31.9s] publication of cluster state version [3828] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:33:26,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:27,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9339417975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:37,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4fc38b8a] took [31389ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:51,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][19][13] duration [7.2s], collections [2]/[34.8s], total [7.2s]/[8s], memory [152.3mb]->[107.3mb]/[2gb], all_pools {[young] [56mb]->[8mb]/[0b]}{[old] [64.3mb]->[94.6mb]/[2gb]}{[survivor] [32mb]->[4.7mb]/[0b]}
[2022-03-27T19:33:53,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15060ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:54,189][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T19:34:03,984][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [134] timed out after [20550ms]
[2022-03-27T19:34:14,052][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3829}] took [34.6s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3829}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [21663ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [1200ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [39ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [79ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [42ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [1118ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [281ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [106ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [2118ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [2165ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [148ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [4008ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [49ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [1197ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [63ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [112ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [93ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:34:26,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:34:26,581][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][29][14] duration [5.5s], collections [1]/[8.3s], total [5.5s]/[13.6s], memory [175.3mb]->[104mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [94.6mb]->[94.6mb]/[2gb]}{[survivor] [4.7mb]->[9.3mb]/[0b]}
[2022-03-27T19:34:26,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7071082635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:34:26,648][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][29] overhead, spent [5.5s] collecting in the last [8.3s]
[2022-03-27T19:34:29,393][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [50.4s/50467ms] ago, timed out [29.9s/29917ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [134]
[2022-03-27T19:34:35,760][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16834ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:34:37,448][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [19.4s] publication of cluster state version [3830] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:34:57,934][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [164] timed out after [16161ms]
[2022-03-27T19:34:58,792][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [16.7s/16761ms] ago, timed out [600ms/600ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [164]
[2022-03-27T19:35:23,986][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [24532ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [28] unchanged indices
[2022-03-27T19:35:25,176][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.6s] publication of cluster state version [3832] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:35:36,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@667f0d42] took [7381ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:35:42,841][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][52][15] duration [3s], collections [1]/[10.2s], total [3s]/[16.6s], memory [172mb]->[184mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [94.6mb]->[97.9mb]/[2gb]}{[survivor] [9.3mb]->[12mb]/[0b]}
[2022-03-27T19:35:43,063][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [3s] collecting in the last [10.2s]
[2022-03-27T19:35:43,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5137ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:35:43,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60696}] took [98798ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:07,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5805ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:13,016][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3832}] took [45.6s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3832}]] took [0ms], [connecting to new nodes] took [60ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [20017ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [178ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [63ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [66ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [69ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [505ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [26ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [571ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [4522ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [204ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [3760ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [48ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [6903ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [55ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [5378ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [2404ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [244ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [153ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [162ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [57ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:36:21,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@54b3e502] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:44,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8605ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:01,501][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6537ms] which is above the warn threshold of [5s]
[2022-03-27T19:37:02,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [11135ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:10,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@7dcc4038] took [5172ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6662087605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,572][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][69][16] duration [5s], collections [1]/[1.7s], total [5s]/[21.7s], memory [177.9mb]->[113.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [97.9mb]->[103.5mb]/[2gb]}{[survivor] [12mb]->[10.1mb]/[0b]}
[2022-03-27T19:37:43,626][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][69] overhead, spent [5s] collecting in the last [1.7s]
[2022-03-27T19:37:43,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6662ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:45,869][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10813ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:37:46,007][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.4s] publication of cluster state version [3833] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:37:51,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][74][17] duration [861ms], collections [1]/[1.1s], total [861ms]/[22.6s], memory [189.6mb]->[193.6mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [103.5mb]->[107.2mb]/[2gb]}{[survivor] [10.1mb]->[9.8mb]/[0b]}
[2022-03-27T19:37:52,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][74] overhead, spent [861ms] collecting in the last [1.1s]
[2022-03-27T19:38:04,994][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][18] duration [786ms], collections [1]/[1.1s], total [786ms]/[23.4s], memory [177.1mb]->[201.1mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [107.2mb]->[112.7mb]/[2gb]}{[survivor] [9.8mb]->[6.7mb]/[0b]}
[2022-03-27T19:38:05,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [786ms] collecting in the last [1.1s]
[2022-03-27T19:38:11,625][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][82][19] duration [1.1s], collections [1]/[2.3s], total [1.1s]/[24.5s], memory [187.4mb]->[203.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [112.7mb]->[112.7mb]/[2gb]}{[survivor] [6.7mb]->[11.9mb]/[0b]}
[2022-03-27T19:38:11,713][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][82] overhead, spent [1.1s] collecting in the last [2.3s]
[2022-03-27T19:38:12,174][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:39522}] took [19077ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:38:36,069][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][21] duration [1s], collections [1]/[2.1s], total [1s]/[26.1s], memory [192.5mb]->[128.2mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [117.4mb]->[124.3mb]/[2gb]}{[survivor] [11.1mb]->[3.9mb]/[0b]}
[2022-03-27T19:38:36,906][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [1s] collecting in the last [2.1s]
[2022-03-27T19:39:07,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6871ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:39:16,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7472ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:39:25,633][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [37470ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T19:39:30,615][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [49.6s] publication of cluster state version [3838] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:39:41,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@43b9b420] took [15078ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:01,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:17,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6232ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:37,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@32cdb515] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:56,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:40:57,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349957776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:40:58,138][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][109][22] duration [7s], collections [1]/[2.1s], total [7s]/[33.2s], memory [220.2mb]->[220.2mb]/[2gb], all_pools {[young] [92mb]->[4mb]/[0b]}{[old] [124.3mb]->[124.3mb]/[2gb]}{[survivor] [3.9mb]->[7.5mb]/[0b]}
[2022-03-27T19:40:58,980][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][109] overhead, spent [7s] collecting in the last [2.1s]
[2022-03-27T19:41:00,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14983ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:41:29,484][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3838}] took [1.8m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3838}]] took [0ms], [connecting to new nodes] took [27ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [105728ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [178ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [35ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [31ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [486ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [91ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [1028ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [689ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [117ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [2410ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [674ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [1ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [20ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [18ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [1ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [68ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [21ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:41:40,181][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][123][23] duration [3.4s], collections [1]/[1.6s], total [3.4s]/[36.6s], memory [195.8mb]->[219.8mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [124.3mb]->[124.3mb]/[2gb]}{[survivor] [7.5mb]->[12.6mb]/[0b]}
[2022-03-27T19:41:41,320][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][123] overhead, spent [3.4s] collecting in the last [1.6s]
[2022-03-27T19:41:42,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7624ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:41:57,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5143ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:11,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6439ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:21,043][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3839}] took [39.8s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3839}]] took [0ms], [connecting to new nodes] took [287ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [3802ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [727ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [43ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [37ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [33ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [697ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [28ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [12380ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [3267ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [382ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [8676ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [111ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [2454ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [10ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [2836ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [3465ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [48ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [51ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [58ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [47ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:42:31,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@c16e751] took [8631ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:32,649][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60706}] took [18363ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:43:12,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@fc3cb3] took [5898ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:43:12,917][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [37081ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [28] unchanged indices
[2022-03-27T19:43:17,753][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [42.3s] publication of cluster state version [3840] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:43:45,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:43:46,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6382318119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:43:48,223][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][153][24] duration [3s], collections [1]/[8.4s], total [3s]/[39.6s], memory [188.9mb]->[143.5mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [124.3mb]->[129.2mb]/[2gb]}{[survivor] [12.6mb]->[14.2mb]/[0b]}
[2022-03-27T19:43:48,934][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][153] overhead, spent [3s] collecting in the last [8.4s]
[2022-03-27T19:44:16,163][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60708}] took [6557ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:44:23,827][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [29970ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:44:26,175][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [34s] publication of cluster state version [3841] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:45:05,837][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [19s/19087ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T19:45:07,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6123ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:21,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@5f2b336d] took [10113ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:37,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6061ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:56,339][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [34249ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [26] unchanged indices
[2022-03-27T19:46:05,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@261e316d] took [10017ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:46:04,045][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [48.9s] publication of cluster state version [3842] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:46:21,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:46:22,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101079961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:46:22,733][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][25] duration [5.9s], collections [1]/[4.1s], total [5.9s]/[45.6s], memory [211.5mb]->[215.5mb]/[2gb], all_pools {[young] [68mb]->[76mb]/[0b]}{[old] [129.2mb]->[136.3mb]/[2gb]}{[survivor] [14.2mb]->[7.6mb]/[0b]}
[2022-03-27T19:46:23,277][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [5.9s] collecting in the last [4.1s]
[2022-03-27T19:46:23,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [12907ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:47:01,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:47:02,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6884763574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:47:03,519][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][26] duration [4.5s], collections [1]/[7.9s], total [4.5s]/[50.2s], memory [212mb]->[147.2mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [136.3mb]->[136.3mb]/[2gb]}{[survivor] [7.6mb]->[10.9mb]/[0b]}
[2022-03-27T19:47:03,896][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [4.5s] collecting in the last [7.9s]
[2022-03-27T19:47:49,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [20191ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:48:00,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [5749ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:48:08,517][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9864ms] which is above the warn threshold of [5s]
[2022-03-27T19:48:36,154][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3842}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3842}]] took [53ms], [connecting to new nodes] took [45ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [62866ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [278ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [358ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [51ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [43ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [590ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [41ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [45ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [33117ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [17235ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1193ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [981ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [13369ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1052ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [6136ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [3708ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [4129ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [58ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [122ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [55ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:48:40,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4dc16bb] took [11016ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:23,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:49:23,730][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [38.8s/38844ms] to notify listeners on successful publication of cluster state (version: 3842, uuid: sF2-4i7USP28ab9GQa93ig) for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T19:49:24,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37016416254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:49:25,884][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][27] duration [28.1s], collections [1]/[1.2m], total [28.1s]/[1.3m], memory [207.2mb]->[227.2mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [136.3mb]->[140.3mb]/[2gb]}{[survivor] [10.9mb]->[8.9mb]/[0b]}
[2022-03-27T19:49:26,460][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [28.1s] collecting in the last [1.2m]
[2022-03-27T19:49:27,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [43519ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:28,166][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60708}] took [5502ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:49,950][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/69115ms] ago, timed out [5.8s/5849ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [699]
[2022-03-27T19:49:49,148][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [699] timed out after [63266ms]
[2022-03-27T19:50:03,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [9827ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:16,247][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [50.6s/50605ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b6318fb5]], which exceeds the warn threshold of [10s]
[2022-03-27T19:50:17,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5009ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:35,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7719ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:35,358][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6377ms] which is above the warn threshold of [5s]
[2022-03-27T19:51:12,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:50:53,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4f26bc78] took [8508ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:51:12,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300319452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:51:17,426][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][203][28] duration [13.3s], collections [1]/[45.1s], total [13.3s]/[1.5m], memory [213.2mb]->[156.8mb]/[2gb], all_pools {[young] [64mb]->[8mb]/[0b]}{[old] [140.3mb]->[141.2mb]/[2gb]}{[survivor] [8.9mb]->[15.5mb]/[0b]}
[2022-03-27T19:51:20,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][203] overhead, spent [13.3s] collecting in the last [45.1s]
[2022-03-27T19:51:23,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [10841ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:51:55,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6324ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:52:12,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@dba517a] took [8735ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:52:21,935][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [93571ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:52:25,972][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [3843] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:54:36,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:54:37,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118637624293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:54:41,842][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][209][29] duration [1.8m], collections [1]/[2m], total [1.8m]/[3.3m], memory [228.8mb]->[163mb]/[2gb], all_pools {[young] [75.9mb]->[0b]/[0b]}{[old] [141.2mb]->[148.4mb]/[2gb]}{[survivor] [15.5mb]->[14.6mb]/[0b]}
[2022-03-27T19:54:44,670][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][209] overhead, spent [1.8m] collecting in the last [2m]
[2022-03-27T19:54:46,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [10157ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:55:30,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@da53de0] took [18026ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:55:27,020][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10951ms] which is above the warn threshold of [5s]
[2022-03-27T19:56:07,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15885ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:56:33,199][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [796] timed out after [60735ms]
[2022-03-27T19:56:39,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7666ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:56:44,521][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3843}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3843}]] took [48ms], [connecting to new nodes] took [160ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [4333ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [175ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [46ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [119922ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [565ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [67ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [54770ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [73ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [13303ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [740ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [556ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [17900ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1036ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [20453ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [11867ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [3384ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [674ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [67ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [222ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [265ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [173ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [68ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:57:07,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8682ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:57:31,109][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.6m/399019ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [7.5m/451579ms]
[2022-03-27T19:57:36,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [13375ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:57:57,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@13000f0a] took [16055ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:58:31,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15228ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:58:30,248][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.1s/26117ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [57], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [57], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-27T19:58:48,625][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [838] timed out after [50437ms]
[2022-03-27T19:58:55,070][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.3m/203357ms] ago, timed out [2.3m/142622ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [796]
[2022-03-27T19:58:51,094][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9657ms] which is above the warn threshold of [5s]
[2022-03-27T19:59:06,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14722ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:00:01,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [28237ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:00:55,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@44f0025c] took [28562ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:01:33,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24200ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:01:43,017][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.7m/223437ms] ago, timed out [2.8m/173000ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [838]
[2022-03-27T20:02:50,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [16767ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:03:17,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [6228ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:03:20,579][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10406ms] which is above the warn threshold of [5s]
[2022-03-27T20:03:23,315][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [867] timed out after [137504ms]
[2022-03-27T20:04:40,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [51792ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:06:12,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [27097ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:07:22,597][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/383094ms] ago, timed out [4m/245590ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [867]
[2022-03-27T20:07:40,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [35065ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:08:12,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@5552023a] took [24045ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:08:12,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [524113ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T20:08:35,900][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [9.6m] publication of cluster state version [3844] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T20:09:10,804][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12444ms] which is above the warn threshold of [5s]
[2022-03-27T20:09:10,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [5150ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:09:47,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [21795ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:09:59,347][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.7m/106182ms] ago, timed out [6.1s/6180ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [919]
[2022-03-27T20:09:55,134][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [919] timed out after [100002ms]
[2022-03-27T20:10:18,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14027ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:10:42,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [5971ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:11:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [47667ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:12:49,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24250ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:14:07,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@195eb929] took [64812ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:14:38,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10243ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:15:08,357][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3844}] took [6m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3844}]] took [232ms], [connecting to new nodes] took [1275ms], [applying settings] took [55ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [24192ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [1ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [4268ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [230ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [55ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [193ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [273ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [4279ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [81ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [1111ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [55ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [111ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [91601ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [114ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [41732ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1541ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [495ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [47017ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [12415ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [45127ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [51867ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [26758ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [4570ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [4ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [178ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [1328ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [185ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [158ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [175ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [115ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T20:15:31,171][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [25.1m/1510215ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [26m/1562177ms]
[2022-03-27T20:15:58,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [27463ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:15:49,907][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14605ms] which is above the warn threshold of [5s]
[2022-03-27T20:16:43,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [22930ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:17:12,529][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [978] timed out after [176940ms]
[2022-03-27T20:18:23,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [39110ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:18:56,844][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.9m/178736ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T20:19:02,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [23340ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:19:23,130][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.2m/312670ms] ago, timed out [2.2m/135730ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [978]
[2022-03-27T20:19:32,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [5629ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:20:56,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@7e92a380] took [48876ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:21:42,893][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17759ms] which is above the warn threshold of [5s]
[2022-03-27T20:22:15,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [39511ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:22:38,603][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1026] timed out after [103285ms]
[2022-03-27T20:22:48,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5089ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:22:56,178][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/119801ms] ago, timed out [16.5s/16516ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1026]
[2022-03-27T20:23:20,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [21846ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:24:29,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@6772b3ca] took [27915ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:24:46,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5604ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:25:19,186][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7321ms] which is above the warn threshold of [5s]
[2022-03-27T20:25:56,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [18786ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:25:43,874][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1085] timed out after [68711ms]
[2022-03-27T20:26:30,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [20018ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:26:29,347][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [279690ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T20:26:47,393][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6m] publication of cluster state version [3845] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T20:26:58,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [7043ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:27:21,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [13209ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:27:57,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:27:59,436][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/190750ms] ago, timed out [2m/122039ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1085]
[2022-03-27T20:28:05,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16825438317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:14,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17362ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4e73b4c3] took [42731ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:28:22,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17362213066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:29,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:38,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14679117913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:47,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:54,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18117163042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:07,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:24,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19247363113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:24,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [19247ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:29:36,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28993ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:48,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28993809631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:59,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22753ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:11,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22753000999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:27,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:40,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24728472255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:55,145][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][30] duration [11.5s], collections [1]/[2.6m], total [11.5s]/[3.5m], memory [239mb]->[164.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [148.4mb]->[155.3mb]/[2gb]}{[survivor] [14.6mb]->[9.3mb]/[0b]}
[2022-03-27T20:30:55,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:07,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [79581ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:31:10,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32100080869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:21,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:35,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25694212854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:46,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:46,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [25633ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:31:54,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25633141896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:02,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:44,433][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1137] timed out after [186314ms]
[2022-03-27T20:31:50,331][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25633ms] which is above the warn threshold of [5s]
[2022-03-27T20:32:10,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15093939999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:25,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14614823828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:33,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:34,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [31292ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:32:39,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678143508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:47,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:54,822][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.5m/273411ms] ago, timed out [1.4m/87097ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1137]
[2022-03-27T20:32:54,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15077283324ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:02,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:07,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14140ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:33:08,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14140083525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:17,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:22,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260697297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:29,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:37,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11018838527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:46,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:52,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18581165535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:54,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@6cd4f775] took [18581ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:33:56,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:57,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10419ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:00,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10419718030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:05,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:10,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8531ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:10,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8531978994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:14,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:18,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985204318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:24,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:29,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8952649543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:33,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10105ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:33,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:38,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105174178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:45,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:55,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [11627ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:55,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627633758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:40,898][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1195] timed out after [46994ms]
[2022-03-27T20:35:04,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:06,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [19171ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:35:13,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19171286203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:23,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:32,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [18276ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:35:32,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18276305171ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:42,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:50,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18740670765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:00,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:14,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18248094947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:28,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26954798913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:49,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [26954ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:36:53,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:07,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25962274991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:18,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:35,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25526846640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:48,294][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.5m/211502ms] ago, timed out [2.7m/164508ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1195]
[2022-03-27T20:37:50,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:03,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31339587271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:08,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [31339ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:38:14,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:27,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909940241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:41,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:53,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24943892931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:07,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:16,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@136fe03e] took [53165ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:39:16,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28221257233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:25,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@57d50e4f] took [18188ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:39:25,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:37,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18188467646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:47,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:58,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21828073612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:11,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:24,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24310632644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:37,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [49918ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:40:52,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607597041ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:00,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:06,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23438504147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:13,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:18,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12428853891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:24,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:24,309][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1247] timed out after [125802ms]
[2022-03-27T20:41:29,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788367706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:35,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:42,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10305378160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:49,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:53,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14110632368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:55,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [24416ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:41:56,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:02,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7959492081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:07,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:10,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10175928789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:15,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:23,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7761496414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:35,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:37,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [26848ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:42:42,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19087156028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:44,335][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/206991ms] ago, timed out [1.3m/81189ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1247]
[2022-03-27T20:42:48,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:52,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14840324199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:02,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:10,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13296679268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:16,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@1d079082] took [13296ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:43:18,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:25,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16030035027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:34,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:42,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16392333020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:49,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:00,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15139983162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:12,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233696434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:26,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:32,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [37738ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:44:34,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17505077830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:43,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16175ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:52,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16174176456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:04,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:15,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20183898334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:24,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:36,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20300483278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:46,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:35,229][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20300ms] which is above the warn threshold of [5s]
[2022-03-27T20:45:57,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925566309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:11,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24817ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:18,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24816ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:46:25,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24816384157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:35,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:42,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24339864044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:29,635][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1295] timed out after [163885ms]
[2022-03-27T20:46:50,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:58,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15133221280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:07,422][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.8m/228174ms] ago, timed out [1m/64289ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1295]
[2022-03-27T20:47:10,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:15,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [33996ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:47:18,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18863533872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:33,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:47,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24461050421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:56,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:07,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640464718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:14,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:19,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17336224135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:27,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:35,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13439139330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:42,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:51,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15064924266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:00,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:05,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [33540ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:49:08,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18476023291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:19,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:20,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@d7cd0c9] took [14866ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:49:41,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14866100400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:56,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:07,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39709477527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [26198ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:50:36,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26198119200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:53,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:29,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28343756170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:39,897][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3845}] took [23.5m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3845}]] took [202ms], [connecting to new nodes] took [656ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [1103661ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [4866ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [168ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [315ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [163ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [173ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [236ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [6214ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [835ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [199ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [259ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [140986ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [34850ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1281ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [1156ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [17689ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [4016ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [33019ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [383ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [39132ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [26710ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [268ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [8503ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [27ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [598ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [648ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [2139ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [12725ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [880ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [952ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [548ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [480ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T20:51:54,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:57,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [64247ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:52:21,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64247303644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:52:46,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:52:59,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51669305195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:09,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:16,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22508826118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:23,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:29,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15243305737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:36,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:41,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12364733777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:44,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [27608ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:53:47,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:52,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11932884977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:58,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:51,747][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1356] timed out after [272218ms]
[2022-03-27T20:54:07,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10712227548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:07,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [10712ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:54:14,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:21,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16533450647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:27,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:33,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12641232270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:37,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [12641ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:54:43,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:51,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15966779239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:56,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:02,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [13544ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:55:02,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13544893101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:13,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:21,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16063888697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:32,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:58,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17478913240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:04,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [17478ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:56:18,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:33,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46113928980ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:54,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33989ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:16,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33988533773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:17,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@3780334f] took [33988ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:57:30,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:47,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39565754619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:03,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:20,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30818865923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:37,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:00,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34894961639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:23,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:46,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44919082625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:02,296][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [10m/605460ms] ago, timed out [5.5m/333242ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1356]
[2022-03-27T21:00:06,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:26,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41980904773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:45,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:05,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40221138752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:25,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:36,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [80632ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:01:48,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40411213608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:02:15,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:02:57,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49043056224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:03:40,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:04:08,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87288378605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:04:52,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:06:16,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67550365728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:07:39,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168211697494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:23,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:48,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45812378987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:07,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:30,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44851752474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:51,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:10:22,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44156290742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:02,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:47,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71121667643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:47,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [115277ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:12:30,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:14:43,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81980393740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:17:19,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:19:34,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282399642241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:22:03,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292147ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:24:26,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292246678048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:20:13,141][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [282400ms] which is above the warn threshold of [5s]
[2022-03-27T21:24:53,595][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.2m/852960ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [76], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [76], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-27T21:26:58,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:29:39,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294914318924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:32:03,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:35:10,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306725344844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:37:59,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:41:00,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354317640516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:44:12,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:47:13,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334498848209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:49:44,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:51:37,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [372621ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:52:16,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372621398078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:55:21,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:58:09,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330487766639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:01:07,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:03:59,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339610358054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:06:49,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:09:42,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345444829682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:12:21,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:14:57,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331983568281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:17:24,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:19:49,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311551271416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:22:41,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:25:02,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308305704554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:27:27,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:30:27,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293575908107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:33:32,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:00:53,142][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T23:00:53,168][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T23:00:53,170][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T23:01:00,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T23:01:00,349][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T23:01:00,350][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T23:01:00,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T23:01:00,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T23:01:00,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T23:01:00,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T23:01:00,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T23:01:00,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T23:01:00,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T23:01:00,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T23:01:00,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T23:01:00,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T23:01:00,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T23:01:00,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T23:01:00,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T23:01:00,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T23:01:00,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T23:01:00,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T23:01:00,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T23:01:00,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T23:01:00,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T23:01:00,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T23:01:00,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T23:01:00,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T23:01:00,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T23:01:00,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T23:01:00,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T23:01:00,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T23:01:00,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T23:01:00,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T23:01:00,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T23:01:00,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T23:01:00,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T23:01:00,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T23:01:00,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T23:01:00,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T23:01:00,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T23:01:00,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T23:01:00,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T23:01:00,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T23:01:00,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T23:01:00,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T23:01:00,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T23:01:00,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T23:01:00,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T23:01:00,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T23:01:00,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T23:01:00,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T23:01:00,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T23:01:00,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T23:01:00,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T23:01:00,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T23:01:00,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T23:01:00,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T23:01:00,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T23:01:00,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T23:01:00,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T23:01:00,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T23:01:00,474][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-27T23:01:00,475][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T23:01:01,085][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T23:01:14,624][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T23:01:14,630][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,631][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T23:01:14,633][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,633][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,634][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,635][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-27T23:01:14,637][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,638][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T23:01:14,642][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T23:01:14,643][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,644][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T23:01:16,380][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T23:01:16,575][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T23:01:17,977][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T23:01:18,936][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T23:01:18,937][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T23:01:19,036][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T23:01:19,039][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T23:01:19,373][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T23:01:22,045][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T23:01:22,220][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 133, version: 3846, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T23:01:22,452][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 133, version: 3846, reason: Publication{term=133, version=3846}
[2022-03-27T23:01:22,549][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T23:01:22,550][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T23:01:26,100][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T23:01:26,141][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T23:01:27,804][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T23:01:27,806][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T23:01:29,503][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-27T23:01:30,036][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-27T23:06:31,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:15:00,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6431950931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:11:27,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54b4b00c, interval=1s}] took [19754ms] which is above the warn threshold of [5000ms]
[2022-03-27T23:18:06,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9m/958094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:21:29,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9m/958345697704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:23:19,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:24:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318874594432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:27:02,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/208516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:30:33,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207951828223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:34:10,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:38:03,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428727983565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:41:43,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:45:05,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453032585493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:48:18,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:49:20,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@41485283, interval=5s}] took [1812162ms] which is above the warn threshold of [5000ms]
[2022-03-27T23:51:14,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403575013840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:54:51,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:58:47,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388873656035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T00:20:15,986][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T00:20:15,999][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T00:20:16,004][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T00:20:21,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T00:20:21,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T00:20:21,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T00:20:21,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T00:20:21,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T00:20:21,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T00:20:21,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T00:20:21,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T00:20:21,979][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T00:20:21,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T00:20:21,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T00:20:21,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T00:20:21,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T00:20:21,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T00:20:21,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T00:20:21,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T00:20:21,984][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T00:20:21,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T00:20:21,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T00:20:21,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T00:20:21,987][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T00:20:21,987][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T00:20:21,988][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T00:20:21,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T00:20:21,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T00:20:21,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T00:20:21,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T00:20:21,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T00:20:21,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T00:20:21,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T00:20:21,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T00:20:21,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T00:20:22,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T00:20:22,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T00:20:22,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T00:20:22,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T00:20:22,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T00:20:22,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T00:20:22,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T00:20:22,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T00:20:22,014][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T00:20:22,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T00:20:22,015][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T00:20:22,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T00:20:22,016][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T00:20:22,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T00:20:22,017][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T00:20:22,018][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T00:20:22,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T00:20:22,019][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T00:20:22,020][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T00:20:22,021][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T00:20:22,022][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T00:20:22,022][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T00:20:22,025][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T00:20:22,025][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T00:20:22,026][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T00:20:22,026][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T00:20:22,028][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T00:20:22,121][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.5gb], net total_space [125.8gb], types [ext4]
[2022-03-28T00:20:22,122][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T00:20:22,508][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T00:20:34,349][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T00:20:34,353][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T00:20:35,793][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T00:20:35,942][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T00:20:36,843][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T00:20:38,149][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T00:20:38,150][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T00:20:38,183][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T00:20:38,185][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T00:20:38,525][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T00:20:43,032][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T00:20:43,306][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 134, version: 3856, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T00:20:43,638][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 134, version: 3856, reason: Publication{term=134, version=3856}
[2022-03-28T00:20:43,826][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T00:20:43,828][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T00:20:48,853][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T00:20:48,873][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-28T00:20:51,647][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-28T00:20:51,649][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-28T00:20:53,693][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-03-28T00:20:56,358][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T00:20:56,372][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T00:20:56,380][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T00:20:58,361][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][20] overhead, spent [293ms] collecting in the last [1s]
[2022-03-28T00:21:00,182][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T00:21:00,377][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][22] overhead, spent [293ms] collecting in the last [1s]
[2022-03-28T00:21:00,555][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T00:21:11,131][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T00:21:11,567][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-03-28T00:21:11,588][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-03-28T00:21:13,938][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T00:21:13,962][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T00:21:17,410][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-28T00:21:20,575][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T00:21:28,970][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T00:21:29,114][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-03-28T00:21:29,118][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-03-28T00:21:30,879][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T00:21:31,019][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-03-28T00:21:31,529][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T00:21:31,537][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T00:21:34,201][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T00:21:34,204][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T00:22:41,945][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.28] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-28T00:22:42,185][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.28][0]]]).
[2022-03-28T00:22:42,488][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:42,850][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,020][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,242][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,315][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,399][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,577][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,800][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:43,974][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:45,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:45,399][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:45,657][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:22:47,097][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:22:59,563][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:00,194][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:04,293][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:05,506][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:05,686][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:05,818][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:05,931][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:06,080][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:07,265][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:07,472][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-28T00:23:07,854][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,174][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,316][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,428][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,612][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:08,824][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,104][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,386][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,688][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:09,993][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:10,361][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:10,745][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,024][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,238][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,382][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,480][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:11,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,066][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,340][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,497][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,628][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,872][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:12,974][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:13,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:21,936][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,011][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,182][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,525][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:48,639][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:49,005][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:49,127][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:23:49,486][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:28:44,793][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:28:45,881][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:28:53,822][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:30:50,367][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:30:50,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:32:07,528][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:33:52,640][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T00:55:55,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [6529ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:19,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [6649ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:35,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [6152ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:48,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [7294ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:56:48,995][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5861ms] which is above the warn threshold of [5s]
[2022-03-28T00:57:04,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [5265ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:57:12,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [5308ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:57:44,348][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3m/83250ms] ago, timed out [57.5s/57502ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{a8hjpCxCQu6LQxoKzWNlpQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [15514]
[2022-03-28T00:58:51,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [54307ms] which is above the warn threshold of [5000ms]
[2022-03-28T00:59:34,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16b41037, interval=5s}] took [12407ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:00:56,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:01:50,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256017084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:02:05,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:02:24,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/97365083455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:02:49,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [97365ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:03:04,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:03:04,748][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [154389ms] which is above the warn threshold of [5s]
[2022-03-28T01:03:21,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57023398458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:03:38,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:03:53,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35946928437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:04:21,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:04:54,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42963514276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:05:24,237][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [15514] timed out after [25748ms]
[2022-03-28T01:05:24,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:05:45,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55101931963ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:06:03,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:06:24,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.8s/47896421118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:06:38,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:07:00,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35004768394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:07:44,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:08:05,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.3s/43319118084ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:08:44,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73072ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:09:17,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73072007254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:09:43,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:09:57,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66712540833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:10:17,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.9s/34955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:10:39,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.9s/34955156553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:11:00,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:11:36,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [76286ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:11:43,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41331382903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:13:41,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/162347ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:13:41,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16b41037, interval=5s}] took [162347ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:14:37,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/162347257690ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:14:14,276][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.4s/14446ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16]], which exceeds the warn threshold of [10s]
[2022-03-28T01:16:01,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:17:13,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/140282102117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:19:34,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:22:05,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212305314009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:24:30,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:27:00,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296682693292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:29:38,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:32:08,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307461637415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:29:29,521][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [296683ms] which is above the warn threshold of [5s]
[2022-03-28T01:34:47,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:37:03,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308163596808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:37:57,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5123/0x00000008017ebb90@265a9f51] took [308163ms] which is above the warn threshold of [5000ms]
[2022-03-28T01:39:16,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269636ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:41:35,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270282336958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:44:16,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/299256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:46:17,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298906880947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:49:05,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:51:37,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268790330150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T01:54:16,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:54:24,643][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [46.5s/46509ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@58ca7725]], which exceeds the warn threshold of [10s]
[2022-03-28T01:57:11,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330498952372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:00:10,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T01:58:39,232][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11027ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@805ecb74], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b919decd]], which exceeds the warn threshold of [10s]
[2022-03-28T02:02:44,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/355018253918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:05:23,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312218ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:05:39,449][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.3s/35374ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@805ecb74], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b919decd]], which exceeds the warn threshold of [10s]
[2022-03-28T02:06:34,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@648536f1, interval=1s}] took [312218ms] which is above the warn threshold of [5000ms]
[2022-03-28T02:07:19,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312218946115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:09:32,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:10:10,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16b41037, interval=5s}] took [249984ms] which is above the warn threshold of [5000ms]
[2022-03-28T02:12:17,801][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.1m/667237ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [13.1m/786703ms]
[2022-03-28T02:12:27,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249984450615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:15:12,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339487ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:17:51,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339486621861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:20:33,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:22:36,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/296547065866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T02:25:13,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T02:26:54,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304686311311ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:14:14,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7m/2927402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:18:59,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7m/2927302711993ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:20:10,845][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/93340ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@805ecb74]], which exceeds the warn threshold of [10s]
[2022-03-28T03:25:04,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11m/661715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:30:34,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11m/662271054789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:28:38,792][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.1s/12147ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@fbd9fc16], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@58ca7725], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b919decd]], which exceeds the warn threshold of [10s]
[2022-03-28T03:41:53,160][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T03:41:53,207][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T03:41:53,209][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T03:41:59,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T03:41:59,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T03:41:59,723][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T03:41:59,723][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T03:41:59,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T03:41:59,725][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T03:41:59,726][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T03:41:59,727][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T03:41:59,728][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T03:41:59,729][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T03:41:59,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T03:41:59,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T03:41:59,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T03:41:59,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T03:41:59,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T03:41:59,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T03:41:59,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T03:41:59,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T03:41:59,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T03:41:59,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T03:41:59,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T03:41:59,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T03:41:59,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T03:41:59,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T03:41:59,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T03:41:59,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T03:41:59,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T03:41:59,749][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T03:41:59,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T03:41:59,750][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T03:41:59,751][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T03:41:59,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T03:41:59,754][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T03:41:59,755][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T03:41:59,756][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T03:41:59,757][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T03:41:59,757][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T03:41:59,758][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T03:41:59,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T03:41:59,761][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T03:41:59,765][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T03:41:59,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T03:41:59,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T03:41:59,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T03:41:59,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T03:41:59,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T03:41:59,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T03:41:59,770][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T03:41:59,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T03:41:59,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T03:41:59,772][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T03:41:59,773][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T03:41:59,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T03:41:59,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T03:41:59,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T03:41:59,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T03:41:59,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T03:41:59,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T03:41:59,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T03:41:59,877][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-28T03:41:59,878][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T03:42:00,323][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T03:42:12,893][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T03:42:12,899][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-28T03:42:12,900][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T03:42:12,902][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-28T03:42:12,903][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T03:42:12,904][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-28T03:42:12,905][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-28T03:42:12,906][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T03:42:12,907][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-28T03:42:12,908][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-28T03:42:12,908][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-28T03:42:12,909][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T03:42:12,911][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T03:42:12,912][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T03:42:12,913][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T03:42:14,184][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T03:42:14,401][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T03:42:15,355][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T03:42:16,691][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T03:42:16,724][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T03:42:17,517][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T03:42:17,540][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T03:42:18,203][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T03:42:20,556][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T03:42:20,775][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{7WjUBEyXSgegYU4N7XIjIw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 135, version: 3956, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{7WjUBEyXSgegYU4N7XIjIw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T03:42:20,971][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{7WjUBEyXSgegYU4N7XIjIw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 135, version: 3956, reason: Publication{term=135, version=3956}
[2022-03-28T03:42:21,123][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T03:42:21,124][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T03:43:51,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5038654427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:46:30,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/200949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:46:53,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/201314949779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:47:21,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73971ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:47:39,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73970329770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:48:04,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:47:15,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@779e2654, interval=1s}] took [16249ms] which is above the warn threshold of [5000ms]
[2022-03-28T03:48:26,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43871945238ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:48:48,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:48:48,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@670abd01, interval=5s}] took [45224ms] which is above the warn threshold of [5000ms]
[2022-03-28T03:49:00,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45224584121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:09,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:19,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22663501822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:26,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17023ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:31,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17023918975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:35,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:42,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9195430499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:49,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:49:55,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13640282362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:03,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:11,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13763307321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:17,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15238ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:22,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15237531866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:27,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:32,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9218618207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:39,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:44,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12215535017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:49,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:50:54,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10985838336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:00,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:06,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9993588640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:12,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:25,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11886632481ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:34,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:43,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21638021268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:51:44,945][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T03:52:09,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:52:23,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34121552157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:52:37,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:52:49,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28165843628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:03,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:19,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23474239470ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:33,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:05,012][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:34754}] took [139060ms] which is above the warn threshold of [5000ms]
[2022-03-28T03:53:44,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31493945235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:53:57,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:54:17,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24594203628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:54:34,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:54:53,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.5s/35533393924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:55:23,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.3s/50322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:55:36,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.3s/50321884390ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:55:49,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:56:00,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25832511399ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:56:16,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:56:31,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26920161500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:09,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:20,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52842811394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:31,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:57:44,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24485271197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:12,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:19,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39062273085ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:38,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17011906966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:47,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:58:56,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19489985453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:07,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:19,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18742029512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:34,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T03:59:53,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26615799829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:00:09,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:00:29,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34362518021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:00:49,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:01:07,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.9s/39941884421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:01:26,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:01:47,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39012545376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:02:10,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:02:25,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41036568734ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:02:45,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:03:03,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36503940668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:03:23,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T04:03:40,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36802579439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T04:05:36,405][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T04:05:36,472][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T04:05:36,473][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T04:05:47,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T04:05:47,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T04:05:47,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T04:05:47,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T04:05:47,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T04:05:47,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T04:05:47,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T04:05:47,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T04:05:47,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T04:05:47,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T04:05:47,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T04:05:47,846][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T04:05:47,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T04:05:47,849][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T04:05:47,851][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T04:05:47,853][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T04:05:47,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T04:05:47,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T04:05:47,855][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T04:05:47,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T04:05:47,859][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T04:05:47,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T04:05:47,861][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T04:05:47,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T04:05:47,865][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T04:05:47,867][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T04:05:47,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T04:05:47,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T04:05:47,869][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T04:05:47,870][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T04:05:47,870][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T04:05:47,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T04:05:47,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T04:05:47,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T04:05:47,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T04:05:47,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T04:05:47,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T04:05:47,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T04:05:47,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T04:05:47,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T04:05:47,893][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T04:05:47,894][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T04:05:47,895][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T04:05:47,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T04:05:47,896][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T04:05:47,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T04:05:47,897][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T04:05:47,898][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T04:05:47,899][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T04:05:47,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T04:05:47,903][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T04:05:47,904][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T04:05:47,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T04:05:47,905][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T04:05:47,910][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T04:05:47,918][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T04:05:47,919][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T04:05:47,919][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T04:05:47,921][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T04:05:48,076][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-28T04:05:48,080][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T04:05:48,567][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T04:06:14,953][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T04:06:14,962][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T04:06:18,249][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T04:06:18,567][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T04:06:20,412][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T04:06:22,556][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T04:06:22,571][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T04:06:22,683][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T04:06:22,685][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T04:06:23,115][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T04:06:27,571][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T04:06:27,834][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Md40zX4MSe2wdglOmdyCmQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 136, version: 3958, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Md40zX4MSe2wdglOmdyCmQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T04:06:28,283][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{Md40zX4MSe2wdglOmdyCmQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 136, version: 3958, reason: Publication{term=136, version=3958}
[2022-03-28T04:06:28,541][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T04:06:28,560][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T04:06:30,311][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T04:06:30,327][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T04:06:32,681][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-28T04:06:32,682][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-28T04:06:34,833][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-28T04:06:35,437][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-28T04:06:36,762][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][14] overhead, spent [276ms] collecting in the last [1s]
[2022-03-28T04:06:36,812][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-28T04:06:36,925][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T04:06:36,973][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T04:06:36,974][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T04:06:40,566][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T04:06:41,139][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T04:06:57,390][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T04:07:06,404][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.28][0]]]).
[2022-03-28T04:07:51,269][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:51,468][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:51,945][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:53,061][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:55,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:07:59,977][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:00,309][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:07,202][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:09,853][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:10,254][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:10,539][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:08:10,947][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:09:33,077][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:10:05,780][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:12:29,925][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T04:33:56,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6acb1e22, interval=1s}] took [5582ms] which is above the warn threshold of [5000ms]
[2022-03-28T04:52:12,361][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.7s/11770ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7f85232f]], which exceeds the warn threshold of [10s]
[2022-03-28T04:51:48,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:00:08,111][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.2s/11271ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7f85232f]], which exceeds the warn threshold of [10s]
[2022-03-28T05:03:41,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518937326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:05:57,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6m/1777640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:05:20,086][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [57.5s/57507ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c2454078], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@46ca0fd6], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1f35bb87]], which exceeds the warn threshold of [10s]
[2022-03-28T05:08:08,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6m/1777482317120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:11:02,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:12:43,156][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.8s/26828ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c2454078], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@46ca0fd6], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1f35bb87]], which exceeds the warn threshold of [10s]
[2022-03-28T05:12:57,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304558175576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:15:29,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:19:30,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266371588344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:22:36,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/426936ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:29:46,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427002153591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:32:20,683][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [34.7m/2082040ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [29m/1741123ms]
[2022-03-28T05:35:36,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8m/769707ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:37:54,024][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [59m/3544662ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [45.9m/2755752ms]
[2022-03-28T05:40:10,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8m/769247855401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:43:00,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/455407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:44:03,407][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [57.1s/57184ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c2454078], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@46ca0fd6], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1f35bb87], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7f85232f]], which exceeds the warn threshold of [10s]
[2022-03-28T05:39:26,489][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [769248ms] which is above the warn threshold of [5s]
[2022-03-28T05:52:55,049][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T05:52:55,078][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T05:52:55,089][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T05:55:05,046][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T05:55:05,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T05:55:05,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T05:55:05,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T05:55:05,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T05:55:05,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T05:55:05,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T05:55:05,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T05:55:05,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T05:55:05,056][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T05:55:05,056][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T05:55:05,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T05:55:05,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T05:55:05,058][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T05:55:05,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T05:55:05,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T05:55:05,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T05:55:05,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T05:55:05,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T05:55:05,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T05:55:05,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T05:55:05,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T05:55:05,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T05:55:05,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T05:55:05,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T05:55:05,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T05:55:05,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T05:55:05,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T05:55:05,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T05:55:05,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T05:55:05,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T05:55:05,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T05:55:05,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T05:55:05,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T05:55:05,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T05:55:05,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T05:55:05,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T05:55:05,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T05:55:05,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T05:55:05,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T05:55:05,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T05:55:05,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T05:55:05,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T05:55:05,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T05:55:05,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T05:55:05,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T05:55:05,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T05:55:05,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T05:55:05,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T05:55:05,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T05:55:05,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T05:55:05,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T05:55:05,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T05:55:05,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T05:55:05,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T05:55:05,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T05:55:05,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T05:55:05,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T05:55:05,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T05:55:05,181][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.3gb], net total_space [125.8gb], types [ext4]
[2022-03-28T05:55:05,183][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T05:55:05,570][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T05:55:17,640][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T05:55:17,645][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-28T05:55:17,649][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T05:55:17,652][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-28T05:55:17,652][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T05:55:17,653][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-28T05:55:17,655][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-28T05:55:17,664][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T05:55:17,665][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-28T05:55:17,667][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-28T05:55:17,669][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-28T05:55:17,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T05:55:17,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T05:55:17,672][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T05:55:17,674][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T05:55:19,122][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T05:55:19,282][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T05:55:20,338][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T05:55:21,329][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T05:55:21,330][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T05:55:21,422][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T05:55:21,424][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T05:55:21,686][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T05:55:23,882][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T05:55:24,037][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{e6eQceSZRIq2OarxKKv8VQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 137, version: 4013, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{e6eQceSZRIq2OarxKKv8VQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T05:55:24,188][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{e6eQceSZRIq2OarxKKv8VQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 137, version: 4013, reason: Publication{term=137, version=4013}
[2022-03-28T05:55:24,299][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T05:55:24,300][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T05:55:26,232][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T05:55:26,261][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T05:56:39,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:57:45,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50818583069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:57:53,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:57:25,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2f9dd8a8, interval=1s}] took [51557ms] which is above the warn threshold of [5000ms]
[2022-03-28T05:58:07,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94210334900ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:58:17,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:58:35,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24073853025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:58:56,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:59:04,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40038781680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T05:59:34,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T05:59:56,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36s/36065095439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:03,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:12,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29946835174ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:20,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:26,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16664453178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:51,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32256ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:00:57,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32255822701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:03,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:12,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10590506928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:19,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:26,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16765555346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:32,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12815ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:36,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12815377910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:45,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:52,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12478209147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:01:58,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14323ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:07,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14322125027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:19,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:24,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19922668178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:28,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:36,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10016838638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:43,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:48,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14402139154ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:54,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11448ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:02:58,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11447451832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:03,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:08,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8760325239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:13,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:16,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10050828237ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:18,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:03:21,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518429255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:04:06,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:04:21,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21691880202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:04:51,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:05:05,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43518163709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:05:21,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:05:38,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30884593564ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:02,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:22,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41313600422ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:42,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:06:59,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40150814845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:07:20,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:07:53,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36882919486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:09:07,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:10:07,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/105048163479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:11:31,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:12:59,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142607261440ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:15:23,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/230693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:17:57,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/230692815253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:20:40,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:23:42,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314206268884ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:26:30,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:29:23,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349908042172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:32:06,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:34:53,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337280655066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:37:43,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:40:39,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331046696731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:43:26,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349998ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:46:10,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350010827825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:49:19,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:51:54,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339240130797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:54:43,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T06:57:12,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336920479337ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T06:59:37,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:02:18,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292472726025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T07:04:52,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:10:54,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315614919986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T07:14:24,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:17:17,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/571662878844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T07:20:34,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T07:26:46,707][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T07:26:46,750][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T07:26:46,751][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T07:26:56,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T07:26:56,330][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T07:26:56,330][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T07:26:56,331][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T07:26:56,332][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T07:26:56,332][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T07:26:56,333][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T07:26:56,333][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T07:26:56,341][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T07:26:56,342][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T07:26:56,342][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T07:26:56,343][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T07:26:56,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T07:26:56,344][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T07:26:56,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T07:26:56,345][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T07:26:56,346][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T07:26:56,347][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T07:26:56,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T07:26:56,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T07:26:56,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T07:26:56,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T07:26:56,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T07:26:56,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T07:26:56,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T07:26:56,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T07:26:56,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T07:26:56,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T07:26:56,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T07:26:56,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T07:26:56,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T07:26:56,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T07:26:56,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T07:26:56,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T07:26:56,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T07:26:56,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T07:26:56,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T07:26:56,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T07:26:56,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T07:26:56,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T07:26:56,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T07:26:56,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T07:26:56,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T07:26:56,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T07:26:56,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T07:26:56,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T07:26:56,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T07:26:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T07:26:56,388][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T07:26:56,389][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T07:26:56,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T07:26:56,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T07:26:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T07:26:56,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T07:26:56,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T07:26:56,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T07:26:56,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T07:26:56,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T07:26:56,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T07:26:56,535][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.3gb], net total_space [125.8gb], types [ext4]
[2022-03-28T07:26:56,536][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T07:26:56,942][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T07:27:22,652][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T07:27:22,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T07:27:25,132][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T07:27:25,457][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T07:27:27,173][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T07:27:28,915][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T07:27:28,918][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T07:27:28,971][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T07:27:28,973][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T07:27:29,371][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T07:27:34,131][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T07:27:34,336][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{m5DZ2fMjStiis9-J7vfJLw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 138, version: 4015, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{m5DZ2fMjStiis9-J7vfJLw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T07:27:34,659][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{m5DZ2fMjStiis9-J7vfJLw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 138, version: 4015, reason: Publication{term=138, version=4015}
[2022-03-28T07:27:34,974][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T07:27:34,975][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T07:27:39,184][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T07:27:39,233][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T07:27:42,577][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-28T07:27:42,581][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-28T07:27:47,134][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-28T07:27:47,174][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-28T07:27:47,194][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-28T07:27:47,905][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-28T07:27:51,373][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T07:27:51,515][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:51,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T07:27:54,308][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:54,369][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:55,715][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:56,345][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:56,407][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:57,151][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:57,266][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,123][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,209][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,271][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:27:58,325][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-28T07:28:07,364][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T07:28:33,329][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.28][0]]]).
[2022-03-28T07:29:14,840][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T07:29:17,818][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.28/7uXba1rNQCCUquvsNbvH5A] update_mapping [_doc]
[2022-03-28T08:10:50,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@65d702bb, interval=1s}] took [21429ms] which is above the warn threshold of [5000ms]
[2022-03-28T08:21:15,753][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11062ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9]], which exceeds the warn threshold of [10s]
[2022-03-28T08:19:27,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:33:43,630][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12s/12036ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:35:40,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4m/1164533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:40:39,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4m/1164741412986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:40:38,282][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.7s/12770ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:43:16,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/573524ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:45:54,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5m/573506986520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:45:02,324][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [45.6s/45696ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b8a4b47], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3f5f6f8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:48:56,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:51:29,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329541245921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:51:26,422][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.7s/24791ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b8a4b47], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3f5f6f8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T08:54:22,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@65d702bb, interval=1s}] took [312799ms] which is above the warn threshold of [5000ms]
[2022-03-28T08:53:59,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T08:58:44,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312799607002ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T08:59:35,634][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10.7m/642340ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [16.4m/984857ms]
[2022-03-28T09:01:43,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T09:03:22,451][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [18.1m/1090416ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [21.9m/1318308ms]
[2022-03-28T09:05:05,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448075136156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T09:07:47,961][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [46.8s/46875ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a7057be9], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@2b8a4b47], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3f5f6f8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@64455ea0]], which exceeds the warn threshold of [10s]
[2022-03-28T09:14:29,385][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T09:14:29,460][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T09:14:29,466][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T12:28:13,566][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T12:28:13,578][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T12:28:13,580][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T12:28:19,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T12:28:19,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T12:28:19,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T12:28:19,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T12:28:19,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T12:28:19,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T12:28:19,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T12:28:19,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T12:28:19,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T12:28:19,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T12:28:19,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T12:28:19,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T12:28:19,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T12:28:19,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T12:28:19,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T12:28:19,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T12:28:19,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T12:28:19,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T12:28:19,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T12:28:19,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T12:28:19,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T12:28:19,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T12:28:19,846][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T12:28:19,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T12:28:19,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T12:28:19,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T12:28:19,849][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T12:28:19,850][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T12:28:19,851][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T12:28:19,851][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T12:28:19,852][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T12:28:19,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T12:28:19,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T12:28:19,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T12:28:19,858][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T12:28:19,858][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T12:28:19,859][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T12:28:19,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T12:28:19,861][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T12:28:19,863][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T12:28:19,866][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T12:28:19,867][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T12:28:19,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T12:28:19,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T12:28:19,869][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T12:28:19,869][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T12:28:19,869][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T12:28:19,870][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T12:28:19,871][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T12:28:19,871][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T12:28:19,872][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T12:28:19,872][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T12:28:19,873][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T12:28:19,873][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T12:28:19,876][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T12:28:19,876][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T12:28:19,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T12:28:19,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T12:28:19,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T12:28:19,969][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.2gb], net total_space [125.8gb], types [ext4]
[2022-03-28T12:28:19,970][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T12:28:20,329][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T12:28:33,900][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T12:28:33,909][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-28T12:28:33,910][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-28T12:28:33,914][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-28T12:28:33,915][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T12:28:33,916][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-28T12:28:33,918][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-28T12:28:33,919][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T12:28:33,920][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-28T12:28:33,928][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-28T12:28:33,929][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-28T12:28:33,931][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-28T12:28:33,932][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-28T12:28:33,933][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-28T12:28:33,934][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T12:28:35,336][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T12:28:35,564][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T12:28:36,783][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T12:28:38,352][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T12:28:38,357][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T12:28:38,472][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T12:28:38,482][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T12:28:38,782][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T12:28:41,522][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T12:28:41,746][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{-LvtWXuXRI69XFv-vB-1tA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 139, version: 4065, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{-LvtWXuXRI69XFv-vB-1tA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T12:28:41,982][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{-LvtWXuXRI69XFv-vB-1tA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 139, version: 4065, reason: Publication{term=139, version=4065}
[2022-03-28T12:28:42,136][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T12:28:42,145][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T12:31:09,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T12:41:15,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6m/698959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T12:43:28,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6m/699229852094ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T12:40:15,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ceaf757, interval=1s}] took [16125ms] which is above the warn threshold of [5000ms]
[2022-03-28T12:46:39,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T12:49:48,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305211500405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T12:52:39,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398991ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T12:55:46,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399328230048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T12:58:23,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:00:30,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/348280725491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:02:42,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/258761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:05:30,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/258984772225ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:08:34,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:12:13,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347001893744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:15:02,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:17:59,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388690023603ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:21:03,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364670ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:23:49,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364504702872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:26:52,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:29:40,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328819916505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:32:44,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:35:43,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366902746889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:47:35,348][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-28T13:47:35,900][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-28T13:47:35,902][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-28T13:54:33,043][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-28T13:54:33,049][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-28T13:54:33,050][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-28T13:54:33,051][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-28T13:54:33,052][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-28T13:54:33,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-28T13:54:33,053][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-28T13:54:33,054][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-28T13:54:33,055][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-28T13:54:33,056][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-28T13:54:33,056][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-28T13:54:33,057][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-28T13:54:33,058][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-28T13:54:33,059][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-28T13:54:33,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-28T13:54:33,060][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-28T13:54:33,061][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-28T13:54:33,062][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-28T13:54:33,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-28T13:54:33,063][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-28T13:54:33,064][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-28T13:54:33,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-28T13:54:33,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-28T13:54:33,066][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-28T13:54:33,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-28T13:54:33,067][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-28T13:54:33,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-28T13:54:33,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-28T13:54:33,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-28T13:54:33,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-28T13:54:33,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-28T13:54:33,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-28T13:54:33,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-28T13:54:33,073][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-28T13:54:33,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-28T13:54:33,074][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-28T13:54:33,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-28T13:54:33,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-28T13:54:33,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-28T13:54:33,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-28T13:54:33,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-28T13:54:33,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-28T13:54:33,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-28T13:54:33,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-28T13:54:33,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-28T13:54:33,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-28T13:54:33,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-28T13:54:33,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-28T13:54:33,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-28T13:54:33,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-28T13:54:33,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-28T13:54:33,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-28T13:54:33,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-28T13:54:33,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-28T13:54:33,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-28T13:54:33,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-28T13:54:33,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-28T13:54:33,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-28T13:54:33,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-28T13:54:33,180][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.3gb], net total_space [125.8gb], types [ext4]
[2022-03-28T13:54:33,181][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-28T13:54:33,567][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-28T13:54:47,201][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-28T13:54:47,209][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-28T13:54:48,594][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-28T13:54:48,774][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-28T13:54:49,771][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-28T13:54:50,666][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-28T13:54:50,667][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-28T13:54:50,761][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-28T13:54:50,763][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-28T13:54:51,072][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-28T13:54:53,630][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-28T13:54:53,897][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{SbckOJiqSNejtEM8OnUibg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 140, version: 4067, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{SbckOJiqSNejtEM8OnUibg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-28T13:54:54,141][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{SbckOJiqSNejtEM8OnUibg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 140, version: 4067, reason: Publication{term=140, version=4067}
[2022-03-28T13:54:54,306][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-28T13:54:54,308][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-28T13:54:57,295][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-03-28T13:55:18,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [10683ms] which is above the warn threshold of [5000ms]
[2022-03-28T13:55:44,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58752304, interval=5s}] took [6768ms] which is above the warn threshold of [5000ms]
[2022-03-28T13:56:36,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [34495ms] which is above the warn threshold of [5000ms]
[2022-03-28T13:56:34,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5927ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:02,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5927399954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:09,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:15,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.2s/41220637602ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:21,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:26,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12371609513ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:31,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9768ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:35,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9768379338ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:41,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:47,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10035521991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:49,069][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-28T13:57:51,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9491ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:57:55,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9491043173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:04,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12719ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:12,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12719053551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:20,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16279ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:28,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16279224535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:36,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:44,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15995808514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:50,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15006ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:58:55,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15005831663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:00,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:04,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9761888891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:09,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:15,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8587476334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:20,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:25,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10856122110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:29,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:36,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9983695382ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:43,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:48,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12499748212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:53,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T13:59:59,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10617200040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:06,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:13,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12984358451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:19,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:23,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13133224078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:29,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9191ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:35,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9191164296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:40,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11840ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:44,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11839749435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:51,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:00:57,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10794696840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:03,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11372ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:12,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11371915039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:18,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:23,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15845837625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:27,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:31,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9070339525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:43,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:45,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14739432089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:48,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:49,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@3ab0d7e6] took [320485ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:01:49,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6321521326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:01:52,238][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=140, version=4068}] took [6.5m] which is above the warn threshold of [30s]: [running task [Publication{term=140, version=4068}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@73ecd356] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@7ae05ef7] took [1ms], [org.elasticsearch.script.ScriptService@2c296533] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3cf0bcdc] took [81ms], [org.elasticsearch.snapshots.RestoreService@2993be77] took [0ms], [org.elasticsearch.ingest.IngestService@51fc5a54] took [572ms], [org.elasticsearch.action.ingest.IngestActionForwarder@369bbda5] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d2018@fb4e1f6] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@3be09671] took [1ms], [org.elasticsearch.tasks.TaskManager@8fd55a3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@63e0a78c] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@42a8f8ce] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@5d8f5efb] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4af8b65c] took [4ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@3f3f5ef3] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801302e58@778edf64] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@34aeecf3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@b56741a] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013bec08@6df4c43e] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@409daff1] took [1ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@3c30917c] took [100ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7c9a07c7] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@38ab5994] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1423c579] took [16ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@3cc4c68] took [3ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@47bc1b7e] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@25b99de4] took [10ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3cf0bcdc] took [47ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2fbb8c27] took [37ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4e753ab4] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@73a32702] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@3f391f0e] took [14ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@4fa62264] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@16b15adc] took [5ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@6107f66b] took [48ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@1f4c4d2d] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@145ddaf5] took [2ms], [org.elasticsearch.node.ResponseCollectorService@8a61518] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@564c0ec9] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2b940408] took [22ms], [org.elasticsearch.shutdown.PluginShutdownService@5164e9fd] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6829a0ea] took [1ms], [org.elasticsearch.indices.store.IndicesStore@f0b826] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@3105418a] took [0ms], [org.elasticsearch.license.LicenseService@1c295e1c] took [246059ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@39228383] took [148264ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@3c163d09] took [91ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@70e2d939] took [2233ms], [org.elasticsearch.gateway.GatewayService@10b647f6] took [103ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7109e534] took [0ms]
[2022-03-28T14:01:59,831][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [31] indices into cluster_state
[2022-03-28T14:02:06,071][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5804ms] which is above the warn threshold of [5s]
[2022-03-28T14:02:29,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@175372a5] took [7831ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:02:52,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [5050ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:03:21,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@280acce6] took [18780ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:03:31,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [5068ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:03:47,415][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.7m/104384ms] to compute cluster state update for [cluster_reroute(async_shard_fetch)], which exceeds the warn threshold of [10s]
[2022-03-28T14:04:03,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@11b3bdf4] took [10250ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:04:28,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [9866ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:04:25,148][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6664ms] which is above the warn threshold of [5s]
[2022-03-28T14:04:48,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [12566ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:06:01,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5600ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:09,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5599726563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:16,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:21,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19773723153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:27,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10848ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:31,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10847519865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:35,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8611ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:41,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8610919871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:47,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:06:52,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11269456251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:00,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:06,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12834512682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:12,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:18,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@2d8057c3] took [139872ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:07:18,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12740527034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:24,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:25,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@214ace8c, interval=5s}] took [10672ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:07:29,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10672396293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:33,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9540ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:37,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9540726116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:42,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:47,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8729016445ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:48,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [18269ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:07:52,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:07:57,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10911648073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:01,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:08,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9015461371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:16,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:21,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14225478119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:12,845][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9016ms] which is above the warn threshold of [5s]
[2022-03-28T14:08:29,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:34,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12634441474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:38,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10092ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:42,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10091277636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:46,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:52,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7264985403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:55,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:08:59,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10083330630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:05,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:12,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9735596020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:20,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:28,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14578166134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:37,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:44,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15739701371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:51,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:09:55,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13748695895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:00,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:02,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@5f595bbf] took [128027ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:10:05,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10909885117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:12,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:17,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11479145542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:22,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9913ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:27,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9912885905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:30,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [21392ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:10:36,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:42,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13758834590ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:47,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@538911bd, interval=1m}] took [11764ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:10:48,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:51,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11764571907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:10:59,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:03,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10073934036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:13,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:15,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [23698ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:11:28,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13624878599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:34,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22904ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:38,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22903487838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:44,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:49,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9471129510ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:54,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:11:57,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11383082504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:12:01,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:12:05,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6929376779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:12:09,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:12:11,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7198532747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:12:24,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@7c987a40] took [73249ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:12:43,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [10158ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:13:34,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@bd1f5f1] took [35813ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:13:38,838][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [47.2s] publication of cluster state version [4069] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{SbckOJiqSNejtEM8OnUibg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-28T14:14:22,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:14:23,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [29075ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:14:29,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24627728165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:14:37,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58752304, interval=5s}] took [14536ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:14:37,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14536ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:14:47,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14536634546ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:14:50,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:14:53,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13632106989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:15:11,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:15:20,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8268633768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:15:41,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42965ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:15:47,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.9s/42964891347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:15:53,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:15:58,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11909651912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:07,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12969ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:23,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12969702532ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:28,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:34,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22262263043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:42,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:47,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13060037294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:52,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:16:58,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10102458305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:05,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:14,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13232883285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:20,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:28,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14981052541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:38,039][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:46,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@55f3d0b] took [179773ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:17:46,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16389469235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:17:54,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:04,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17385656477ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:12,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:20,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17647689953ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:29,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:37,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16995239680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:38,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [34642ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:18:42,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:47,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13120933829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:50,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:53,006][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8477ms] which is above the warn threshold of [5s]
[2022-03-28T14:18:53,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8477283123ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:18:56,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:00,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5793652302ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:04,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:11,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945570652ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:18,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:24,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14565195066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:31,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12441ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:35,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12440938315ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:39,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:44,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8658482497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:50,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:19:54,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10941243493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:02,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:08,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11227236177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:17,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:25,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15148900835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:29,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:35,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12276662657ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:37,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@3a2a1d9a] took [107475ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:20:41,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:44,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11517235784ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:47,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:51,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [7355ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:20:52,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7355111696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:20:57,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:02,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8649585356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:09,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12882ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:18,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12881517290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:25,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:30,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [28264ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:21:36,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15382990093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:43,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:49,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18422128007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:50,341][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18422ms] which is above the warn threshold of [5s]
[2022-03-28T14:21:54,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:21:59,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10740790296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:04,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:13,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9564699755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:22,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:30,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18786277972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:40,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17626ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:48,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17626209128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:22:57,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:02,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16925678147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:09,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:18,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12162940166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:26,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:33,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16499660600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:40,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:46,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14733179454ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:23:52,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:01,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11950677659ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:11,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:19,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18178961660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:27,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:27,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@3e500953] took [165591ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:24:32,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16867502921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:37,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:41,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10200323842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:45,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:49,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7912181579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:49,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [18112ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:24:52,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:24:57,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7298939553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:08,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:15,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15389759661ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:19,187][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [15390ms] which is above the warn threshold of [5s]
[2022-03-28T14:25:21,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:26,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [28167ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:25:26,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12777453162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:35,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12968ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:41,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12968181899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:49,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:25:57,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15116751175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:06,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:16,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15941262051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:24,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:32,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19477856744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:42,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:50,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17036034969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:26:56,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:27:06,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15051688270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:27:15,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:27:22,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18208521210ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:27:33,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:27:44,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17829162911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:27:53,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20838ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:04,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20838836847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:10,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:16,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15766383591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:24,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:28,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@2694b975] took [183166ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:28:31,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14932174367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:40,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:42,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58752304, interval=5s}] took [15164ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:28:48,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15164556141ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:53,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14303ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:57,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14303768922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:28:58,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [14303ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:29:02,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:07,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8536921673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:10,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:14,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7993906856ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:16,085][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7994ms] which is above the warn threshold of [5s]
[2022-03-28T14:29:17,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:22,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7615934288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:27,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:31,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8919806065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:36,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:43,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9620359082ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:48,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:29:54,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12287442048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:02,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:11,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12655162257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:21,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:32,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18588020370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:39,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19394ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:44,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19394532642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:49,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10685ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:30:51,483][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1008683ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [27] unchanged indices
[2022-03-28T14:30:54,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10685096423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:00,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:07,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10470406523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:18,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:24,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@5a5f7fc9] took [134985ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:31:26,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16755089095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:36,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17720ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:47,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17719880880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:31:55,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:03,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20734990423ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:04,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [38454ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:32:10,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:17,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15043620277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:23,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:22,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@30736fd5] took [12225ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:32:26,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12225731176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:31,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:38,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8240482029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:46,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:51,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15241642577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:32:56,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:01,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9715892674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:08,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:16,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12134326246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:22,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:27,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13912601856ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:32,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9902ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:40,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9901691036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:49,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16878ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:33:56,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16877709831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:03,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14497ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:11,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14497188921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:17,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14044ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:24,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14044611804ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:31,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:35,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13639714386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:42,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:42,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@e57e219] took [138720ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:34:48,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10514382602ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:34:56,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14281ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:00,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14280853433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:07,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:09,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [25213ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:35:15,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10932399516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:23,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d0badb7, interval=30s}] took [15702ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:35:23,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:30,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15702291431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:38,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:45,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15525416431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:52,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:35:52,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [29335ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:35:54,243][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13810ms] which is above the warn threshold of [5s]
[2022-03-28T14:35:58,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13810042764ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:04,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:11,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12919074079ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:18,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:24,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13357507817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:29,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:35,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11751353223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:43,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:49,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13454720364ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:53,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:36:59,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10047751020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:37:08,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:37:18,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14374680249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:37:28,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:37:38,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19391437402ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:37:46,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:37:54,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18720582465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:38:05,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:38:18,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19582464971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:38:26,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20542ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:38:38,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20541592013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:38:49,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@2c8b3ae8] took [176385ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:38:49,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22244ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:38:55,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22244688894ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:39:04,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:39:12,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15678662704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:39:21,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:39:33,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16914864431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:39:44,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [32593ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:39:47,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:39:57,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25465042616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:40:10,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:40:12,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58752304, interval=5s}] took [22378ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:40:19,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22378543383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:40:30,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:40:40,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21040202757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:40:50,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:40:59,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19678646806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:41:08,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:41:18,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18677572535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:41:28,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:41:37,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19757823401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:41:46,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:41:55,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17896349635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:42:08,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:42:18,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21328953755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:42:31,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21556ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:42:39,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21555704903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:42:47,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:42:55,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19061374227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:08,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:18,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19153401968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:25,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:32,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18613233138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:40,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:46,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14481643139ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:52,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:43:57,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@72df56b] took [223303ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:43:58,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12058659122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:03,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:10,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11049525489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:16,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:22,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13016543745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:27,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [13016ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:44:30,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:40,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14069680825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:44:50,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@30736fd5] took [19815ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:44:50,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:01,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19815908002ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:10,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:16,753][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [19917ms] which is above the warn threshold of [5s]
[2022-03-28T14:45:16,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19916865493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:20,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [19916ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:45:27,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:39,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16831867709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:49,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21359ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:45:59,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21358684396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:09,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20681ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:16,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20681239160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:24,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:33,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15417711965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:43,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:52,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18488230948ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:46:58,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:09,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15523037646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:18,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:28,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19247777816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:37,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:46,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19516130649ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:54,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17626ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:47:59,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17625408464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:48:09,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:48:17,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@36db88bd] took [160436ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:48:20,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12577880770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:48:28,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20908ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:48:32,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58752304, interval=5s}] took [20907ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:48:37,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.9s/20907907800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:48:47,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:48:55,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18763567196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:04,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17098ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:13,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17097848051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:24,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:30,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [55556ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:49:35,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19694660000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:44,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21027ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:45,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@30736fd5] took [21026ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:49:51,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21026185152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:49:58,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13184ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:08,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13184148591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:13,142][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13184ms] which is above the warn threshold of [5s]
[2022-03-28T14:50:19,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:25,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21320144313ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:35,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:44,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15657920644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:50,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:50:57,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15270888339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:05,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:09,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15482075985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:15,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9303ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:19,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9302803134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:27,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:34,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12190671156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:42,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:50,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14288704570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:51:55,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:00,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14031836654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:10,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:19,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13529892644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:26,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17197ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:31,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@66edfdc4] took [161456ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:52:33,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17197741797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:40,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14218ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:47,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14217922397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:54,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:52:59,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13648328604ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:03,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [27866ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:53:05,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11168ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:12,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11168526021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:19,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14172ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:21,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@538911bd, interval=1m}] took [14171ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:53:28,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14171906764ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:38,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18498ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:47,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18497437938ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:53:55,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:01,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17129056231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:12,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:20,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16810458631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:30,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:40,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17803578332ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:47,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:54:53,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18253782948ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:01,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:09,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13002404689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:17,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:23,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15956720182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:31,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:42,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13937977371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:49,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:55:57,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18513662108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:05,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:12,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15801751386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:13,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@13c4d78b] took [165706ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:56:20,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:28,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15208485590ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:37,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:45,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16627912017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:49,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [16627ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:56:51,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:54,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13930665754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:56:58,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6774ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:02,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6773979163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:06,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:08,854][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8730ms] which is above the warn threshold of [5s]
[2022-03-28T14:57:11,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8729976844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:15,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:20,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9819572048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:25,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:31,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9599218760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:39,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:47,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13218881464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:57:56,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:04,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17288758025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:12,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:23,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15389078289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:30,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:41,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18591881558ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:48,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18323ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:58:54,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18322744264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:01,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12657ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:10,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12656676580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:19,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17603ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:25,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17603669305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:35,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:39,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@7baafe35] took [156662ms] which is above the warn threshold of [5000ms]
[2022-03-28T14:59:45,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15441910838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T14:59:52,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:01,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18305524847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:12,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:22,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18517106340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:25,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [36822ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:00:31,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:40,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19748718839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:49,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:00:55,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18229958118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:01:02,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:01:04,537][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12807ms] which is above the warn threshold of [5s]
[2022-03-28T15:01:10,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12806951143ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:01:20,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17535ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:01:30,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17535355749ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:01:41,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:01:52,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21875526724ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:02:03,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:02:15,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20557933113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:02:23,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:02:33,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21533585391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:02:42,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:03:30,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18352946628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:03:35,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:03:41,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53721732456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:03:48,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:03:53,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12756808928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:03,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:12,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13826017265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:20,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:27,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18219259124ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:43,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@18d4c3bf] took [233075ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:04:42,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21889ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:50,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21889026961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:04:57,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:05:06,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15357408261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:05:16,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:05:25,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18637103259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:05:33,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [33994ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:05:36,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:05:47,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19902296493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:05:57,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:06:08,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20089690103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:06:22,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:06:33,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25826405191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:06:37,167][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25826ms] which is above the warn threshold of [5s]
[2022-03-28T15:06:45,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22237ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:06:55,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22237573529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:07:11,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24883ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:07:29,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24882195385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:07:43,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:07:54,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33130968034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:08:06,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23461ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:08:19,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23460504421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:08:31,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:08:50,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25477005983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:08:59,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:09:08,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.3s/27300688956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:09:21,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:09:30,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22294941610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:09:41,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:09:48,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19240962402ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:09:59,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:10:09,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17805660435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:10:14,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@2d9937a1] took [241656ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:10:21,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23510ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:10:32,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23509516197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:10:42,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:10:50,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19505726277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:10:56,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:03,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15558214447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:09,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [35063ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:11:12,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:17,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15124239649ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:23,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11923ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:28,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11922608914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:34,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:39,139][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11426ms] which is above the warn threshold of [5s]
[2022-03-28T15:11:39,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11425805677ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:11:49,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:12:00,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13890239153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:12:11,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:12:21,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21729856096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:12:34,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:12:46,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21787805045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:12:56,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:13:09,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23629219586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:13:21,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24670ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:13:31,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24669048414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:13:45,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:13:57,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21732365989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:14:27,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:14:51,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42625117780ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:15:12,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:15:31,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.5s/44500613876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:15:53,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:16:10,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40688572449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:16:31,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39612ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:16:42,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@16e3f71] took [306290ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:16:49,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39612069540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:17:08,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:17:25,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.9s/35986530133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:17:44,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35334ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:18:02,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35334009012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:18:27,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45166ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:19:12,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45165873474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:19:06,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [116486ms] which is above the warn threshold of [5000ms]
[2022-03-28T15:19:59,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:21:13,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86573933480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:22:26,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/143651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:23:34,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/143650989990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:24:58,843][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [143651ms] which is above the warn threshold of [5s]
[2022-03-28T15:25:22,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173452ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:27:01,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173327154783ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:28:48,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/213228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:30:45,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/213117864846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:33:27,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/255668ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:36:14,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/255651217668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:39:11,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:42:36,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347980517800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:45:16,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/381894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:47:54,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382161965397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:50:40,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:52:54,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322256838429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:55:02,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/262481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T15:57:18,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/262742297001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T15:59:32,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:01:55,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265144822660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:04:09,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/278398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:08:04,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/278064872938ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:10:47,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:13:02,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374835535726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:16:20,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:20:04,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288821990630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:24:08,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:27:19,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540436948725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:29:53,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:30:37,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5099/0x00000008017d9960@60adfa36] took [4186663ms] which is above the warn threshold of [5000ms]
[2022-03-28T16:32:18,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338470099358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:34:59,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:37:30,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298644725451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:40:01,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307119ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:41:32,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79b28a1b, interval=1s}] took [605798ms] which is above the warn threshold of [5000ms]
[2022-03-28T16:42:26,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307153404684ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:44:47,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287038ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:44:40,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58752304, interval=5s}] took [286910ms] which is above the warn threshold of [5000ms]
[2022-03-28T16:46:59,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/286910500853ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:49:29,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:51:50,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273289093435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:54:01,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/285480ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-28T16:54:36,749][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [558342ms] which is above the warn threshold of [5s]
[2022-03-28T16:57:10,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/285053351296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-28T16:59:49,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340371ms] on absolute clock which is above the warn threshold of [5000ms]
