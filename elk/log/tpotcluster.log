[2022-04-09T15:39:17,821][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-09T15:39:17,848][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-09T15:39:17,850][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-09T15:39:26,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-09T15:39:26,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-09T15:39:26,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-09T15:39:26,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-09T15:39:26,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-09T15:39:26,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-09T15:39:26,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-09T15:39:26,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-09T15:39:26,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-09T15:39:26,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-09T15:39:26,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-09T15:39:26,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-09T15:39:26,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-09T15:39:26,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-09T15:39:26,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-09T15:39:26,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-09T15:39:26,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-09T15:39:26,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-09T15:39:26,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-09T15:39:26,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-09T15:39:26,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-09T15:39:26,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-09T15:39:26,093][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-09T15:39:26,093][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-09T15:39:26,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-09T15:39:26,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-09T15:39:26,094][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-09T15:39:26,095][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-09T15:39:26,095][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-09T15:39:26,095][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-09T15:39:26,096][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-09T15:39:26,096][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-09T15:39:26,097][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-09T15:39:26,097][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-09T15:39:26,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-09T15:39:26,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-09T15:39:26,098][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-09T15:39:26,099][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-09T15:39:26,099][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-09T15:39:26,099][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-09T15:39:26,100][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-09T15:39:26,101][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-09T15:39:26,101][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-09T15:39:26,101][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-09T15:39:26,102][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-09T15:39:26,102][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-09T15:39:26,103][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-09T15:39:26,103][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-09T15:39:26,103][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-09T15:39:26,103][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-09T15:39:26,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-09T15:39:26,104][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-09T15:39:26,105][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-09T15:39:26,105][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-09T15:39:26,105][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-09T15:39:26,106][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-09T15:39:26,106][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-09T15:39:26,107][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-09T15:39:26,108][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-09T15:39:26,171][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.2gb], net total_space [125.8gb], types [ext4]
[2022-04-09T15:39:26,173][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-09T15:39:26,371][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-09T15:39:39,130][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-09T15:39:39,133][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-09T15:39:40,071][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-09T15:39:40,201][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-09T15:39:40,924][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-09T15:39:41,749][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-09T15:39:41,750][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-09T15:39:41,784][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-09T15:39:41,786][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-09T15:39:41,984][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-09T15:39:44,161][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-09T15:39:44,275][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t9rKPJjZTxyppVbtj2OTZA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 221, version: 8288, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t9rKPJjZTxyppVbtj2OTZA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-09T15:39:44,430][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{t9rKPJjZTxyppVbtj2OTZA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 221, version: 8288, reason: Publication{term=221, version=8288}
[2022-04-09T15:39:44,533][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-09T15:39:44,534][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-09T15:39:45,183][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-09T15:39:45,189][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [43] indices into cluster_state
[2022-04-09T15:39:45,867][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-09T15:39:45,869][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-09T15:39:46,647][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-09T15:39:47,047][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-09T15:39:47,054][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-09T15:39:47,056][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-09T15:39:47,543][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-09T15:39:47,912][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-09T15:39:50,959][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-09T15:39:51,015][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-09T15:39:51,021][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-09T15:39:51,315][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-09T15:39:51,461][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-09T15:39:51,463][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-09T15:39:56,012][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-04-09T15:39:58,703][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-09T15:39:58,758][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-09T15:39:58,779][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-09T15:39:59,925][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-09T15:39:59,964][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-09T15:40:00,281][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-09T15:40:00,283][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-09T15:40:01,508][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-09T15:40:01,517][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-09T15:40:02,927][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-09T15:40:03,112][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-09T15:40:47,253][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 930 finished with response BulkByScrollResponse[took=215.5ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-09T15:40:48,966][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 951 finished with response BulkByScrollResponse[took=1.7s,timed_out=false,sliceId=null,updated=1020,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-09T15:40:58,275][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-09T15:41:29,810][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:41:35,566][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:41:35,700][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:41:35,716][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:41:35,979][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:41:36,586][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:42:23,821][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:45:28,869][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:46:05,116][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:49:45,037][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] to [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,124][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003] creating index, cause [rollover_data_stream], templates [.deprecation-indexing-template], shards [1]/[1]
[2022-04-09T15:49:45,137][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] updating number_of_replicas to [0] for indices [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003]
[2022-04-09T15:49:45,219][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003] from [null] to [{"phase":"new","action":"complete","name":"complete"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,220][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] to [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,292][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003][0]]]).
[2022-04-09T15:49:45,348][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003] from [{"phase":"new","action":"complete","name":"complete"}] to [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,355][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] to [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,358][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] to [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,494][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003] from [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] to [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:49:45,498][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002] from [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] to [{"phase":"hot","action":"complete","name":"complete"}] in policy [.deprecation-indexing-ilm-policy]
[2022-04-09T15:50:33,414][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:53:44,142][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:53:45,130][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:55:36,601][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:55:39,259][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T15:55:42,616][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:00:03,369][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:04:59,095][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:05:00,103][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:37,185][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:37,321][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:37,329][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:37,429][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:38,188][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:38,339][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:38,352][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:06:38,363][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:07:53,244][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:09:19,312][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:09:19,673][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:11:31,428][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:15:17,573][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:22:13,208][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:23:54,136][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:26:35,121][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:26:35,389][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:26:35,509][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:30:05,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@438b8f1e, interval=1s}] took [27494ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:30:43,770][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.9s/10924ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b435a1a5], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6dd1a709]], which exceeds the warn threshold of [10s]
[2022-04-09T16:30:28,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:31:16,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5236427088ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:31:51,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:33:42,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107189189645ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:33:47,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/117581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:33:47,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d51fa5a, interval=5s}] took [117581ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:33:47,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/117581034043ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:34:10,109][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:34:12,796][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:34:14,716][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:34:25,909][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2962][65] duration [1.5s], collections [1]/[3.4s], total [1.5s]/[3.6s], memory [1.4gb]->[235.7mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [225.5mb]->[225.5mb]/[2gb]}{[survivor] [8mb]->[10.1mb]/[0b]}
[2022-04-09T16:34:26,134][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2962] overhead, spent [1.5s] collecting in the last [3.4s]
[2022-04-09T16:34:42,755][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2971][68] duration [2.4s], collections [1]/[4s], total [2.4s]/[7.1s], memory [288.7mb]->[235.4mb]/[2gb], all_pools {[young] [88mb]->[0b]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [4.9mb]->[3.6mb]/[0b]}
[2022-04-09T16:34:43,368][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2971] overhead, spent [2.4s] collecting in the last [4s]
[2022-04-09T16:34:43,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@438b8f1e, interval=1s}] took [5251ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:34:53,074][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2975][69] duration [2s], collections [1]/[5s], total [2s]/[9.2s], memory [275.4mb]->[255.9mb]/[2gb], all_pools {[young] [40mb]->[40mb]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [3.6mb]->[4.1mb]/[0b]}
[2022-04-09T16:34:53,532][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2975] overhead, spent [2s] collecting in the last [5s]
[2022-04-09T16:34:59,379][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2976][70] duration [3.6s], collections [1]/[5.5s], total [3.6s]/[12.8s], memory [255.9mb]->[235.4mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [4.1mb]->[3.6mb]/[0b]}
[2022-04-09T16:34:59,939][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2976] overhead, spent [3.6s] collecting in the last [5.5s]
[2022-04-09T16:34:59,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@438b8f1e, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:35:20,787][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2991][75] duration [1.2s], collections [1]/[1.2s], total [1.2s]/[14.7s], memory [294.8mb]->[326.8mb]/[2gb], all_pools {[young] [56mb]->[8mb]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [7mb]->[5.2mb]/[0b]}
[2022-04-09T16:35:21,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2991] overhead, spent [1.2s] collecting in the last [1.2s]
[2022-04-09T16:37:02,319][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/TR7XVbYZTaSlS3ZYLHgNVQ] deleting index
[2022-04-09T16:37:05,207][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003/gql1LYRDRDCM8vLep2Y_bw] update_mapping [_doc]
[2022-04-09T16:39:17,715][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.09/5KJd-gq-TVWbUnHd0iTG-A] update_mapping [_doc]
[2022-04-09T16:47:28,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d51fa5a, interval=5s}] took [7209ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:47:40,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@438b8f1e, interval=1s}] took [7390ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:51:46,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34538ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:52:52,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34676832129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:53:19,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:53:44,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142337011267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:54:14,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.5s/54520ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:54:46,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.5s/54519654522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:55:13,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:55:13,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c6f94a1, interval=5s}] took [58996ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:55:25,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58996332595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:55:38,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:55:38,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d51fa5a, interval=5s}] took [25439ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:55:52,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25439088541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:56:09,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:56:25,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30295395329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:57:57,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50461ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T16:58:35,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@438b8f1e, interval=1s}] took [50114ms] which is above the warn threshold of [5000ms]
[2022-04-09T16:58:40,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50114596008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T16:59:25,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/145970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:01:06,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146306794001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:02:02,881][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14s/14003ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b091c452], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@351693b0]], which exceeds the warn threshold of [10s]
[2022-04-09T17:01:40,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:02:32,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134444453598ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:04:23,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/163374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:02:34,494][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [134444ms] which is above the warn threshold of [5s]
[2022-04-09T17:05:48,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/163049139155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:06:16,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113271ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:06:51,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113596499568ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:07:30,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73691ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:09:03,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73691277313ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:11:01,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/202831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:12:51,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/202810741276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:15:19,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:16:41,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265266724306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:23:03,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:24:22,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464028953505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:26:56,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:30:12,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/232624209742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:32:31,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:34:32,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336127922676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:37:18,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/286880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:39:47,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/286464671015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:41:50,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/271585ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:43:01,207][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [52s/52051ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b091c452]], which exceeds the warn threshold of [10s]
[2022-04-09T17:44:08,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/272263044552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:46:51,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:47:24,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c6f94a1, interval=5s}] took [300590ms] which is above the warn threshold of [5000ms]
[2022-04-09T17:49:30,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/300590944119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:51:35,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/283381ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:51:11,909][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15575ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@351693b0], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b435a1a5]], which exceeds the warn threshold of [10s]
[2022-04-09T17:54:10,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/283341634970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T17:55:58,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3d51fa5a, interval=5s}] took [283341ms] which is above the warn threshold of [5000ms]
[2022-04-09T17:56:44,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T17:55:24,531][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.5s/12590ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b091c452], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@351693b0], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b435a1a5], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6dd1a709]], which exceeds the warn threshold of [10s]
[2022-04-09T17:59:12,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309753452330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:00:35,930][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [38s/38050ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b091c452], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@351693b0], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b435a1a5], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6dd1a709]], which exceeds the warn threshold of [10s]
[2022-04-09T18:01:53,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:05:29,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308803985933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:07:59,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:14:55,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366827204922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:18:31,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/632179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:26:00,577][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-09T18:26:00,601][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-09T18:26:00,602][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-09T18:26:06,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-09T18:26:06,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-09T18:26:06,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-09T18:26:06,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-09T18:26:06,146][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-09T18:26:06,146][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-09T18:26:06,146][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-09T18:26:06,147][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-09T18:26:06,147][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-09T18:26:06,148][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-09T18:26:06,148][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-09T18:26:06,148][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-09T18:26:06,149][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-09T18:26:06,149][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-09T18:26:06,150][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-09T18:26:06,150][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-09T18:26:06,151][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-09T18:26:06,151][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-09T18:26:06,152][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-09T18:26:06,152][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-09T18:26:06,153][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-09T18:26:06,153][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-09T18:26:06,153][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-09T18:26:06,154][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-09T18:26:06,154][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-09T18:26:06,155][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-09T18:26:06,155][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-09T18:26:06,155][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-09T18:26:06,156][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-09T18:26:06,156][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-09T18:26:06,156][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-09T18:26:06,157][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-09T18:26:06,157][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-09T18:26:06,158][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-09T18:26:06,158][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-09T18:26:06,158][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-09T18:26:06,159][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-09T18:26:06,159][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-09T18:26:06,159][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-09T18:26:06,160][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-09T18:26:06,160][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-09T18:26:06,161][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-09T18:26:06,162][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-09T18:26:06,162][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-09T18:26:06,162][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-09T18:26:06,163][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-09T18:26:06,163][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-09T18:26:06,163][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-09T18:26:06,164][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-09T18:26:06,164][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-09T18:26:06,165][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-09T18:26:06,165][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-09T18:26:06,165][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-09T18:26:06,166][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-09T18:26:06,166][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-09T18:26:06,167][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-09T18:26:06,167][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-09T18:26:06,167][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-09T18:26:06,168][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-09T18:26:06,252][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.1gb], net total_space [125.8gb], types [ext4]
[2022-04-09T18:26:06,254][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-09T18:26:06,823][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-09T18:28:33,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6606ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:28:47,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6067263508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:28:47,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/141455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:28:47,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/141993656898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:28:57,183][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-09T18:28:57,192][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-09T18:28:57,194][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-09T18:28:57,197][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-09T18:28:57,198][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-09T18:28:57,199][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-09T18:28:57,199][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-09T18:28:57,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-09T18:28:57,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-09T18:28:57,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-09T18:28:57,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-09T18:28:57,203][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-09T18:28:57,204][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-09T18:28:57,205][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-09T18:28:57,207][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-09T18:28:58,412][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-09T18:28:58,592][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-09T18:28:59,444][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-09T18:29:00,427][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-09T18:29:00,429][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-09T18:29:00,578][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-09T18:29:00,584][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-09T18:29:00,925][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-09T18:29:04,215][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-09T18:29:04,416][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{n8mJHbpzSjm3TDf_eaOlww}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 222, version: 8393, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{n8mJHbpzSjm3TDf_eaOlww}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-09T18:29:04,606][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{n8mJHbpzSjm3TDf_eaOlww}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 222, version: 8393, reason: Publication{term=222, version=8393}
[2022-04-09T18:29:04,794][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-09T18:29:04,795][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-09T18:29:07,739][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-09T18:29:08,066][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [43] indices into cluster_state
[2022-04-09T18:29:42,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@249eb157] took [6914ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:30:04,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:30:19,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [5804ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:31:28,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:31:32,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7117499454ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:31:36,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8501ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:31:40,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8500995838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:31:43,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7461ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:31:45,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7461549612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:31:59,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@18d33eca] took [89690ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:32:25,196][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43796}] took [103732ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:32:28,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:50974}] took [30052ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:32:28,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:50960}] took [28651ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:33:04,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [5130ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:33:25,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:33:28,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6083775856ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:33:32,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7469ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:33:35,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7468493504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:33:37,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:33:39,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5644780080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:34:00,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@62ecd60e] took [53259ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:34:20,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [5790ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:34:20,563][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5991ms] which is above the warn threshold of [5s]
[2022-04-09T18:34:28,594][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43840}] took [13009ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:35:01,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [8601ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:35:16,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [5787ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:35:18,220][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51020}] took [6849ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:35:24,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@376758cd] took [6310ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:35:50,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:36:35,677][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43862}] took [27018ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:37:30,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@6d3ffb55] took [88531ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:37:50,250][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51046}] took [5749ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:37:59,376][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=222, version=8395}] took [8.8m] which is above the warn threshold of [30s]: [running task [Publication{term=222, version=8395}]] took [13ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@6815a70] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@43312e15] took [528702ms], [org.elasticsearch.script.ScriptService@6eaddd46] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@6b3b70be] took [0ms], [org.elasticsearch.snapshots.RestoreService@6765f573] took [0ms], [org.elasticsearch.ingest.IngestService@2dee3e0c] took [32ms], [org.elasticsearch.action.ingest.IngestActionForwarder@2da9811a] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016ceec0@1da7aa91] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@55210a3b] took [20ms], [org.elasticsearch.tasks.TaskManager@9e340cd] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@68a3a1ba] took [25ms], [org.elasticsearch.cluster.InternalClusterInfoService@4e15082c] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@d990d21] took [0ms], [org.elasticsearch.indices.SystemIndexManager@6d18a5f] took [97ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@6394ed7e] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@6bcc4b84] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@6b298a47] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@bb2b01b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@6a29955d] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@61eec91d] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@650b4452] took [242ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@6818ad46] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@7730ceb0] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@5a6f2f8d] took [243ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@382dc2ea] took [67ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@41a39877] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@30179bbd] took [25ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@6b3b70be] took [47ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1327d975] took [53ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@454b6369] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@39f11ec5] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@1fdb359b] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@5eb22474] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49ffd08] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@15f00a81] took [20ms], [org.elasticsearch.node.ResponseCollectorService@5aae1dcf] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5f86be5b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@248a7f1e] took [23ms], [org.elasticsearch.shutdown.PluginShutdownService@31bbdd0c] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@7bbfa6f7] took [0ms], [org.elasticsearch.indices.store.IndicesStore@3bde5c0b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@ef8e5a3] took [0ms], [org.elasticsearch.license.LicenseService@1356d9e1] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@5c2f65c6] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@515b6c2c] took [0ms], [org.elasticsearch.gateway.GatewayService@43d565de] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@1a8d4bd6] took [0ms]
[2022-04-09T18:37:59,720][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.8m/531565ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [8.8m/533984ms]
[2022-04-09T18:38:01,157][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-09T18:38:01,416][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-09T18:38:01,041][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-04-09T18:38:32,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:38:33,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [12111ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:38:33,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910246138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:38:39,534][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51046}] took [7849ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:38:45,401][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43906}] took [5799ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:40:29,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:40:34,678][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51088}] took [14332ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:40:39,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6526778096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:40:45,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:40:50,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20320030435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:40:56,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@788df4a4] took [123772ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:40:59,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:07,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13634096726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:37,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:40,502][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51088}] took [52939ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:41:42,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39304595777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:42,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [39304ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:41:48,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:52,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10872353020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:59,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:02,442][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51088}] took [10921ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:42:05,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10920309340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:13,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:13,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@93ae349, interval=5s}] took [14947ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:42:18,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14947916866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:34,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11206ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:37,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [11206ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:42:40,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11206478187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:36,694][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11207ms] which is above the warn threshold of [5s]
[2022-04-09T18:42:47,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22326ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:47,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2cfcb139, interval=5s}] took [22326ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:42:51,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22326023289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:42:59,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12047ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:43:05,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12047364634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:43:18,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:43:27,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [19438ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:43:38,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19438318859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:43:55,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:43:57,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@93ae349, interval=5s}] took [35724ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:44:06,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35724603842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:41:02,503][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketTimeoutException: null
	at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:110) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-09T18:44:17,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:44:23,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22159335511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:44:31,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15083ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:44:43,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15082993965ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:44:53,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:45:05,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21204208557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:45:16,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:45:26,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22985105858ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:45:37,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:45:46,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20556747501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:45:56,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18687ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:46:07,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18687358339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:46:15,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:46:28,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20441433426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:46:40,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:46:52,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24035864654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:05,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:13,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24562271278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:23,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:31,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18870414822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:40,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:48,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15720485249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:47:50,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@1a74d0de] took [224306ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:47:57,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:48:04,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18697546964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:48:12,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15119ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:48:26,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15119304010ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:48:35,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23839ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:48:36,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23838353167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:48:52,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2cfcb139, interval=5s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:49:41,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:49:56,525][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51112}] took [6404ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:50:04,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@4b88004c] took [16210ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:50:08,288][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [47.6s/47622ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [164], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [164], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-09T18:50:09,282][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:51120}] took [5160ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:50:26,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [6167ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:50:59,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:51:20,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5128745114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:51:19,386][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13947ms] which is above the warn threshold of [5s]
[2022-04-09T18:51:42,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [56463ms] which is above the warn threshold of [5000ms]
[2022-04-09T18:51:42,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:51:59,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42516513689ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:52:17,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:52:28,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35733834491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:52:44,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:53:00,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25794024490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:53:18,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:53:48,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33737187860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:54:09,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:54:31,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50865695170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:55:08,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53s/53054ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:55:33,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53s/53053713854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:55:55,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:56:30,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53655742811ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:56:54,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.2s/57284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:57:15,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.2s/57284025526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:57:41,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43s/43028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:58:02,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43s/43028268131ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:58:39,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:58:56,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64017839999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:59:18,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T18:59:36,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38409339446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T18:59:48,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:00:04,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31431662607ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:00:17,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:00:35,166][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26925348061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:00:54,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:01:15,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36367941011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:01:47,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:02:17,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53658513655ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:02:25,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@3802afb9] took [603963ms] which is above the warn threshold of [5000ms]
[2022-04-09T19:02:42,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52652ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:03:06,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52651999110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:03:30,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.3s/51300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:04:17,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.2s/51299351142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:04:48,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:05:08,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@93ae349, interval=5s}] took [77437ms] which is above the warn threshold of [5000ms]
[2022-04-09T19:05:29,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77437811164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:06:24,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:07:55,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71905031682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:09:52,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:10:26,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@30bbf2, interval=1s}] took [231827ms] which is above the warn threshold of [5000ms]
[2022-04-09T19:11:24,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/231827938401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:12:24,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/152179ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:12:41,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/152178960244ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:13:05,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39839ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:13:21,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39839757201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:13:43,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:13:27,200][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [39840ms] which is above the warn threshold of [5s]
[2022-04-09T19:14:07,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38760204167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:14:44,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.7s/57712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:15:12,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.7s/57712769803ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:14:41,934][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1398100ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [42] unchanged indices
[2022-04-09T19:15:35,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.8s/53862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:15:54,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.8s/53861514366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:16:13,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:16:37,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.2s/39284844787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:16:47,451][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.2m] publication of cluster state version [8397] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{n8mJHbpzSjm3TDf_eaOlww}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-09T19:17:05,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:17:29,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.8s/50893322994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:17:49,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45520ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:18:20,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45519673241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:18:48,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.1s/58155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:19:29,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.1s/58155658874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:19:59,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70547ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:20:23,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70546306968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:21:08,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:21:52,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66084068328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:23:15,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/128759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:25:24,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/128758868333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:26:20,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:27:26,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/182427177575ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:28:30,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:30:16,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122383122789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:32:04,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.6m/216070ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:32:34,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d9978@7a9c5c50] took [1033745ms] which is above the warn threshold of [5000ms]
[2022-04-09T19:34:27,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215830500023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:37:18,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:39:55,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311384546746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:42:44,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:45:55,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325289299268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:49:01,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T19:52:37,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374884678016ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T19:57:29,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/513771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T20:05:42,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/513470142620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T20:10:03,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4m/749263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T20:13:17,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4m/749564420632ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T20:16:17,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374088ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T20:19:17,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/373528154755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T20:22:12,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T20:25:11,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353137393904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T20:18:45,535][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [373528ms] which is above the warn threshold of [5s]
[2022-04-09T20:27:56,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345848ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-09T20:30:51,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345988766972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-09T20:33:52,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/346985ms] on absolute clock which is above the warn threshold of [5000ms]
