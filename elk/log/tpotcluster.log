[2022-03-31T17:12:52,786][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T17:12:52,827][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T17:12:52,828][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T17:12:59,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T17:12:59,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T17:12:59,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T17:12:59,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T17:12:59,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T17:12:59,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T17:12:59,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T17:12:59,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T17:12:59,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T17:12:59,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T17:12:59,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T17:12:59,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T17:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T17:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T17:12:59,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T17:12:59,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T17:12:59,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T17:12:59,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T17:12:59,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T17:12:59,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T17:12:59,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T17:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T17:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T17:12:59,409][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T17:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T17:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T17:12:59,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T17:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T17:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T17:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T17:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T17:12:59,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T17:12:59,479][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.2gb], net total_space [125.8gb], types [ext4]
[2022-03-31T17:12:59,480][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T17:12:59,707][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T17:13:09,457][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T17:13:09,459][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T17:13:10,433][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T17:13:10,552][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T17:13:11,275][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T17:13:12,000][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T17:13:12,001][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T17:13:12,024][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T17:13:12,026][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T17:13:12,200][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T17:13:14,014][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T17:13:14,133][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 169, version: 4918, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T17:13:14,304][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 169, version: 4918, reason: Publication{term=169, version=4918}
[2022-03-31T17:13:14,406][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T17:13:14,407][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T17:13:15,087][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T17:13:15,093][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T17:13:15,796][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T17:13:15,797][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T17:13:16,358][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:16,488][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:16,776][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T17:13:16,780][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T17:13:16,783][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T17:13:16,906][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:17,218][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T17:13:17,376][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T17:13:19,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T17:13:20,336][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-31T17:13:39,973][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-31T17:13:40,092][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-31T17:14:19,372][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 527 finished with response BulkByScrollResponse[took=333.6ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-31T17:14:21,900][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 543 finished with response BulkByScrollResponse[took=2.6s,timed_out=false,sliceId=null,updated=1036,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-31T17:14:29,045][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-31T17:14:51,442][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:14:51,569][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:14:51,581][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:10,197][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:11,029][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:11,166][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:12,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:15,029][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:18,202][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:22,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,037][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,116][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,235][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,306][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:07,289][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:16,333][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:16,402][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:36,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:38,285][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:17:44,428][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,358][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,420][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,425][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,759][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:11,391][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:39,477][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:39,537][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:40,472][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:45,769][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:28:36,844][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:34:02,052][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:35:12,088][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:35:12,162][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:41:11,426][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:41:11,503][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:44:26,573][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,585][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,675][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,686][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,790][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:12,577][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,584][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,644][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,650][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:52:51,841][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:56:02,982][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:57:23,034][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:57:23,130][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:04:52,544][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:04:53,475][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:06:00,466][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:06:00,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:12:47,828][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:13:05,830][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:14:24,807][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:31:06,738][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:33:17,155][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:38:29,565][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:47:19,147][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:55:12,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8323ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:55:49,269][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60610}] took [8546ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:56:00,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8662ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:56:26,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:04,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:18,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/469179476688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:33,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:09,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32037024964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:06,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60590}] took [469179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:06,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60612}] took [469179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:24,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:26,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52467198605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:30,819][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.8m/593109ms] ago, timed out [9.3m/558079ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [42739]
[2022-03-31T19:05:41,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6078][104] duration [6.6m], collections [1]/[9.1m], total [6.6m]/[6.6m], memory [1.3gb]->[252.7mb]/[2gb], all_pools {[young] [1.1gb]->[64mb]/[0b]}{[old] [180.1mb]->[180.1mb]/[2gb]}{[survivor] [7.5mb]->[8.6mb]/[0b]}
[2022-03-31T19:05:41,280][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [42739] timed out after [35030ms]
[2022-03-31T19:05:41,834][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6078] overhead, spent [6.6m] collecting in the last [9.1m]
[2022-03-31T19:05:41,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [16994ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:44,299][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6079][105] duration [2.6s], collections [1]/[15.9s], total [2.6s]/[6.6m], memory [252.7mb]->[267.3mb]/[2gb], all_pools {[young] [64mb]->[80mb]/[0b]}{[old] [180.1mb]->[180.3mb]/[2gb]}{[survivor] [8.6mb]->[7mb]/[0b]}
[2022-03-31T19:05:55,479][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6086][106] duration [1s], collections [1]/[2.3s], total [1s]/[6.7m], memory [267.3mb]->[198.6mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [180.3mb]->[180.3mb]/[2gb]}{[survivor] [7mb]->[6.3mb]/[0b]}
[2022-03-31T19:05:55,655][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6086] overhead, spent [1s] collecting in the last [2.3s]
[2022-03-31T19:05:59,540][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T19:06:02,425][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6091][107] duration [742ms], collections [1]/[1.4s], total [742ms]/[6.7m], memory [246.6mb]->[210.3mb]/[2gb], all_pools {[young] [64mb]->[16mb]/[0b]}{[old] [180.3mb]->[180.3mb]/[2gb]}{[survivor] [6.3mb]->[13.9mb]/[0b]}
[2022-03-31T19:06:02,656][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6091] overhead, spent [742ms] collecting in the last [1.4s]
[2022-03-31T19:06:04,387][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T19:06:08,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6094][110] duration [883ms], collections [1]/[2.1s], total [883ms]/[6.7m], memory [196.2mb]->[196.1mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.6mb]->[5.5mb]/[0b]}
[2022-03-31T19:06:08,462][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6094] overhead, spent [883ms] collecting in the last [2.1s]
[2022-03-31T19:06:11,520][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6095][111] duration [1.1s], collections [1]/[3.4s], total [1.1s]/[6.7m], memory [196.1mb]->[195.8mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.5mb]->[5.2mb]/[0b]}
[2022-03-31T19:06:11,722][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6095] overhead, spent [1.1s] collecting in the last [3.4s]
[2022-03-31T19:06:14,195][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6096][112] duration [1.1s], collections [1]/[2.8s], total [1.1s]/[6.7m], memory [195.8mb]->[195.3mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.2mb]->[4.7mb]/[0b]}
[2022-03-31T19:06:14,400][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6096] overhead, spent [1.1s] collecting in the last [2.8s]
[2022-03-31T19:06:21,337][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6101][113] duration [923ms], collections [1]/[1.8s], total [923ms]/[6.8m], memory [243.3mb]->[197.5mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [4.7mb]->[6.9mb]/[0b]}
[2022-03-31T19:06:22,879][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6101] overhead, spent [923ms] collecting in the last [1.8s]
[2022-03-31T19:06:31,947][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6108][115] duration [714ms], collections [1]/[1.2s], total [714ms]/[6.8m], memory [236.5mb]->[194.8mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.9mb]->[4.2mb]/[0b]}
[2022-03-31T19:06:32,291][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6108] overhead, spent [714ms] collecting in the last [1.2s]
[2022-03-31T19:06:42,071][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6116] overhead, spent [538ms] collecting in the last [1.5s]
[2022-03-31T19:06:45,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6118] overhead, spent [578ms] collecting in the last [1.7s]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35248}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,939][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6135][122] duration [3.4s], collections [1]/[5.3s], total [3.4s]/[6.9m], memory [217.3mb]->[202.7mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [3.4mb]->[4.9mb]/[0b]}
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:14,546][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6135] overhead, spent [3.4s] collecting in the last [5.3s]
[2022-03-31T19:07:14,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6259ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35242}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:32,057][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6141][123] duration [2.3s], collections [1]/[5.9s], total [2.3s]/[6.9m], memory [234.7mb]->[200.9mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [4.9mb]->[7.1mb]/[0b]}
[2022-03-31T19:07:32,729][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6141] overhead, spent [2.3s] collecting in the last [5.9s]
[2022-03-31T19:07:42,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:07:42,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [14930ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:43,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389036208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:07:47,726][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6142][124] duration [6.7s], collections [1]/[12.7s], total [6.7s]/[7m], memory [200.9mb]->[224.5mb]/[2gb], all_pools {[young] [8mb]->[24mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [7.1mb]->[6.7mb]/[0b]}
[2022-03-31T19:07:49,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6142] overhead, spent [6.7s] collecting in the last [12.7s]
[2022-03-31T19:07:51,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7308ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:10,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [9177ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:57,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44789ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:08:57,370][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [46990ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:58,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44788663816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:04,942][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5601ms] which is above the warn threshold of [5s]
[2022-03-31T19:09:05,820][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6144][125] duration [38.7s], collections [1]/[59.4s], total [38.7s]/[7.7m], memory [228.5mb]->[222.4mb]/[2gb], all_pools {[young] [32mb]->[24mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [6.7mb]->[8.6mb]/[0b]}
[2022-03-31T19:09:09,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6144] overhead, spent [38.7s] collecting in the last [59.4s]
[2022-03-31T19:09:13,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [13122ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:25,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5802ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,814][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6146][126] duration [10.8s], collections [1]/[6.9s], total [10.8s]/[7.9m], memory [226.4mb]->[286.4mb]/[2gb], all_pools {[young] [24mb]->[32mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [8.6mb]->[8mb]/[0b]}
[2022-03-31T19:09:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15579015529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,958][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6146] overhead, spent [10.8s] collecting in the last [6.9s]
[2022-03-31T19:09:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [16179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:52,011][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6148][127] duration [3.2s], collections [1]/[1.2s], total [3.2s]/[7.9m], memory [249.8mb]->[257.8mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [8mb]->[8.7mb]/[0b]}
[2022-03-31T19:09:52,471][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6148] overhead, spent [3.2s] collecting in the last [1.2s]
[2022-03-31T19:09:52,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,600][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6149][128] duration [4.5s], collections [1]/[12.9s], total [4.5s]/[8m], memory [257.8mb]->[201.8mb]/[2gb], all_pools {[young] [8mb]->[12mb]/[0b]}{[old] [193.8mb]->[194.2mb]/[2gb]}{[survivor] [8.7mb]->[7.6mb]/[0b]}
[2022-03-31T19:09:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6130551581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,730][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6149] overhead, spent [4.5s] collecting in the last [12.9s]
[2022-03-31T19:09:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6130ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:15,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6153][129] duration [5s], collections [1]/[2s], total [5s]/[8.1m], memory [273.8mb]->[289.8mb]/[2gb], all_pools {[young] [72mb]->[52mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.6mb]->[7.5mb]/[0b]}
[2022-03-31T19:10:15,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:10:16,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7102418865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:10:16,106][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6153] overhead, spent [5s] collecting in the last [2s]
[2022-03-31T19:10:16,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7302ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:34,024][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6157][130] duration [3.2s], collections [1]/[2.2s], total [3.2s]/[8.1m], memory [261.8mb]->[289.8mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.5mb]->[7.8mb]/[0b]}
[2022-03-31T19:10:34,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6157] overhead, spent [3.2s] collecting in the last [2.2s]
[2022-03-31T19:10:35,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11017ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:37,386][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6158][131] duration [2.4s], collections [1]/[12s], total [2.4s]/[8.2m], memory [289.8mb]->[209.2mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.8mb]->[7mb]/[0b]}
[2022-03-31T19:10:49,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6162][132] duration [2.1s], collections [1]/[5.4s], total [2.1s]/[8.2m], memory [213.2mb]->[213.5mb]/[2gb], all_pools {[young] [12mb]->[20mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7mb]->[7.2mb]/[0b]}
[2022-03-31T19:10:49,721][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6162] overhead, spent [2.1s] collecting in the last [5.4s]
[2022-03-31T19:11:01,436][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6163][134] duration [6.1s], collections [2]/[7s], total [6.1s]/[8.3m], memory [213.5mb]->[215.8mb]/[2gb], all_pools {[young] [20mb]->[4mb]/[0b]}{[old] [194.2mb]->[195.5mb]/[2gb]}{[survivor] [7.2mb]->[10mb]/[0b]}
[2022-03-31T19:11:02,244][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6163] overhead, spent [6.1s] collecting in the last [7s]
[2022-03-31T19:11:04,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8125ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:07,210][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6165] overhead, spent [582ms] collecting in the last [1.2s]
[2022-03-31T19:11:28,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:29,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6185628504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:29,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6172][136] duration [4.2s], collections [1]/[3.3s], total [4.2s]/[8.4m], memory [282.8mb]->[290.8mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [6.4mb]->[6.5mb]/[0b]}
[2022-03-31T19:11:29,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6172] overhead, spent [4.2s] collecting in the last [3.3s]
[2022-03-31T19:11:29,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6385ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:47,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:48,197][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6178][137] duration [2.6s], collections [1]/[6.8s], total [2.6s]/[8.4m], memory [278.9mb]->[204.3mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [6.5mb]->[8mb]/[0b]}
[2022-03-31T19:11:53,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5424975194ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:54,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:53,920][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6178] overhead, spent [2.6s] collecting in the last [6.8s]
[2022-03-31T19:11:55,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6898208933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:55,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [12323ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:58,820][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6179][140] duration [6.1s], collections [3]/[10.8s], total [6.1s]/[8.5m], memory [204.3mb]->[201.7mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [8mb]->[5.3mb]/[0b]}
[2022-03-31T19:11:59,176][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6179] overhead, spent [6.1s] collecting in the last [10.8s]
[2022-03-31T19:12:11,413][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6184][141] duration [2.4s], collections [1]/[4.6s], total [2.4s]/[8.6m], memory [249.7mb]->[217.8mb]/[2gb], all_pools {[young] [52mb]->[16mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [5.3mb]->[5.4mb]/[0b]}
[2022-03-31T19:12:12,040][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6184] overhead, spent [2.4s] collecting in the last [4.6s]
[2022-03-31T19:12:12,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5124ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:22,888][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6190][142] duration [1.4s], collections [1]/[3.4s], total [1.4s]/[8.6m], memory [237.8mb]->[200.8mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [5.4mb]->[4.4mb]/[0b]}
[2022-03-31T19:12:23,535][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6190] overhead, spent [1.4s] collecting in the last [3.4s]
[2022-03-31T19:12:46,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:46,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6463ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:46,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5663364526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,884][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [9259ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,857][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6200][144] duration [9.1s], collections [2]/[14.9s], total [9.1s]/[8.8m], memory [280.8mb]->[208.4mb]/[2gb], all_pools {[young] [87.9mb]->[36mb]/[0b]}{[old] [196.3mb]->[196.4mb]/[2gb]}{[survivor] [4.4mb]->[8mb]/[0b]}
[2022-03-31T19:12:56,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9258564750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:56,110][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6200] overhead, spent [9.1s] collecting in the last [14.9s]
[2022-03-31T19:12:56,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [9258ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:13:20,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:23,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23855602023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:33,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:40,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13291120682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:46,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:51,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579457648ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:52,686][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6201][145] duration [17.8s], collections [1]/[27.4s], total [17.8s]/[9m], memory [208.4mb]->[210.2mb]/[2gb], all_pools {[young] [36mb]->[12mb]/[0b]}{[old] [196.4mb]->[196.4mb]/[2gb]}{[survivor] [8mb]->[9.7mb]/[0b]}
[2022-03-31T19:13:54,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:59,078][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6201] overhead, spent [17.8s] collecting in the last [27.4s]
[2022-03-31T19:13:59,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8213830658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35242}] took [13734ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35282}] took [13734ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:04,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [34084ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:14,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13734178316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:21,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:28,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13160752307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:40,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:47,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635288522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:56,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:57,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [16505ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:02,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16505714411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:58,707][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16506ms] which is above the warn threshold of [5s]
[2022-03-31T19:15:11,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:12,839][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [14804ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:19,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804030000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:52,236][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6203][146] duration [16.8s], collections [1]/[20.6s], total [16.8s]/[9.3m], memory [222.2mb]->[246.2mb]/[2gb], all_pools {[young] [16mb]->[72mb]/[0b]}{[old] [196.4mb]->[196.4mb]/[2gb]}{[survivor] [9.7mb]->[9.7mb]/[0b]}
[2022-03-31T19:15:52,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6203] overhead, spent [16.8s] collecting in the last [20.6s]
[2022-03-31T19:15:53,614][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [41329ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [56132ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41328448939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:50,319][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [43515] timed out after [113954ms]
[2022-03-31T19:16:10,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [14070ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:16:10,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:10,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14070574936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:14,791][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/208340ms] ago, timed out [1.5m/94386ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [43515]
[2022-03-31T19:16:14,906][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6204][147] duration [9.1s], collections [1]/[1m], total [9.1s]/[9.5m], memory [246.2mb]->[214.3mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [196.4mb]->[198.3mb]/[2gb]}{[survivor] [9.7mb]->[8mb]/[0b]}
[2022-03-31T19:16:26,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:28,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8607236776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:27,905][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6206][148] duration [6.2s], collections [1]/[9.9s], total [6.2s]/[9.6m], memory [246.3mb]->[205.5mb]/[2gb], all_pools {[young] [72mb]->[28mb]/[0b]}{[old] [198.3mb]->[198.3mb]/[2gb]}{[survivor] [8mb]->[7.2mb]/[0b]}
[2022-03-31T19:16:28,567][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6206] overhead, spent [6.2s] collecting in the last [9.9s]
[2022-03-31T19:16:45,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:48,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320004956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:48,744][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6208][149] duration [6.9s], collections [1]/[2.3s], total [6.9s]/[9.7m], memory [281.5mb]->[285.5mb]/[2gb], all_pools {[young] [80mb]->[88mb]/[0b]}{[old] [198.3mb]->[198.3mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-03-31T19:16:51,048][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6208] overhead, spent [6.9s] collecting in the last [2.3s]
[2022-03-31T19:16:50,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:58,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [18679ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:16:58,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158197752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:02,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:03,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [11221ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:06,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11221683414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:12,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:14,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5145/0x00000008017f1d80@6a3a4261] took [10518ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:17,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518083841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:20,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:26,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8425527213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:27,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8425ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:33,641][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35344}] took [10911ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:32,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:36,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910392266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:41,150][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [8859ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [8859ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:43,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859358328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:48,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7245998531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7245ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:18:01,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:11,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13191718366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:19,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18498ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:59,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [18497ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:19:31,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18497778394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:32,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@c12697f, interval=1m}] took [72591ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:19:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72591ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:33,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72591565071ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:40,100][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6213][151] duration [48.4s], collections [2]/[1.3m], total [48.4s]/[10.5m], memory [261.9mb]->[237.8mb]/[2gb], all_pools {[young] [60mb]->[36mb]/[0b]}{[old] [198.3mb]->[200.4mb]/[2gb]}{[survivor] [7.5mb]->[9.3mb]/[0b]}
[2022-03-31T19:19:42,986][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6213] overhead, spent [48.4s] collecting in the last [1.3m]
[2022-03-31T19:19:44,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [12280ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:06,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:08,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10717680333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:08,571][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6218][152] duration [7.8s], collections [1]/[12.6s], total [7.8s]/[10.6m], memory [285.8mb]->[208mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [200.4mb]->[201.5mb]/[2gb]}{[survivor] [9.3mb]->[6.5mb]/[0b]}
[2022-03-31T19:20:15,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:15,838][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6218] overhead, spent [7.8s] collecting in the last [12.6s]
[2022-03-31T19:20:16,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862216387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:16,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [20579ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:18,105][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6219][153] duration [3.5s], collections [1]/[11.3s], total [3.5s]/[10.7m], memory [208mb]->[288.6mb]/[2gb], all_pools {[young] [12mb]->[80mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [6.5mb]->[7mb]/[0b]}
[2022-03-31T19:20:19,279][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6219] overhead, spent [3.5s] collecting in the last [11.3s]
[2022-03-31T19:20:34,982][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6222][154] duration [2.8s], collections [1]/[2.1s], total [2.8s]/[10.7m], memory [288.6mb]->[292.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [7mb]->[3.7mb]/[0b]}
[2022-03-31T19:20:37,254][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6222] overhead, spent [2.8s] collecting in the last [2.1s]
[2022-03-31T19:20:37,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11597ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:57,815][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6232][155] duration [706ms], collections [1]/[1.2s], total [706ms]/[10.8m], memory [225.3mb]->[229.3mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [3.7mb]->[3.1mb]/[0b]}
[2022-03-31T19:20:58,065][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6232] overhead, spent [706ms] collecting in the last [1.2s]
[2022-03-31T19:21:29,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:21:30,421][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6240][156] duration [5.6s], collections [1]/[4.3s], total [5.6s]/[10.9m], memory [276.7mb]->[276.7mb]/[2gb], all_pools {[young] [72mb]->[92mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [3.1mb]->[3.1mb]/[0b]}
[2022-03-31T19:21:30,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9093295596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:21:30,914][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6240] overhead, spent [5.6s] collecting in the last [4.3s]
[2022-03-31T19:21:30,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [10941ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:21:45,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11645ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:16,449][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35376}] took [24149ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:16,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:22:17,297][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6245][157] duration [13.8s], collections [1]/[2.9s], total [13.8s]/[11.1m], memory [250.6mb]->[262.6mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [5mb]->[6.5mb]/[0b]}
[2022-03-31T19:22:17,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19858473482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:22:18,357][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6245] overhead, spent [13.8s] collecting in the last [2.9s]
[2022-03-31T19:22:20,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [27101ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:32,370][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6249][158] duration [1s], collections [1]/[3.1s], total [1s]/[11.1m], memory [224.1mb]->[210.9mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [6.5mb]->[9.3mb]/[0b]}
[2022-03-31T19:22:32,863][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6249] overhead, spent [1s] collecting in the last [3.1s]
[2022-03-31T19:22:35,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5117ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:37,551][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6250][160] duration [1.6s], collections [2]/[4.5s], total [1.6s]/[11.1m], memory [210.9mb]->[295.7mb]/[2gb], all_pools {[young] [0b]->[28mb]/[0b]}{[old] [201.5mb]->[203.9mb]/[2gb]}{[survivor] [9.3mb]->[4.7mb]/[0b]}
[2022-03-31T19:22:38,087][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6250] overhead, spent [1.6s] collecting in the last [4.5s]
[2022-03-31T19:26:17,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/185094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:32:32,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/185435813173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:34:51,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [208966ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:35:13,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:38:30,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540616712550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:42:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:43:56,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [427142ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:45:52,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427142976773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:48:28,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:47:11,064][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.2m/76359ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26]], which exceeds the warn threshold of [10s]
[2022-03-31T19:50:51,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016408265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:53:50,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:53:57,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5c0a69ee, interval=5s}] took [679083ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:56:38,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312067572289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:59:07,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:01:51,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327075915271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:04:47,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:06:39,686][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15526ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7]], which exceeds the warn threshold of [10s]
[2022-03-31T20:07:27,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339996989053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:10:13,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:12:23,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325021196471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:11:38,325][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.5s/11517ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:14:24,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:16:36,737][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.3s/26315ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:17:04,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251476839909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:20:01,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:20:31,204][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.5s/11501ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:22:58,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328540951672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:23:41,486][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.9s/24908ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:20:45,644][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [905039ms] which is above the warn threshold of [5s]
[2022-03-31T20:25:54,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:38:57,673][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T20:38:57,686][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T20:38:57,687][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T20:39:03,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T20:39:03,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T20:39:03,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T20:39:03,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T20:39:03,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T20:39:03,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T20:39:03,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T20:39:03,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T20:39:03,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T20:39:03,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T20:39:03,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T20:39:03,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T20:39:03,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T20:39:03,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T20:39:03,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T20:39:03,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T20:39:03,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T20:39:03,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T20:39:03,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T20:39:03,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T20:39:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T20:39:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T20:39:03,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T20:39:03,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T20:39:03,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T20:39:03,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T20:39:03,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T20:39:03,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T20:39:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T20:39:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T20:39:03,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T20:39:03,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T20:39:03,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T20:39:03,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T20:39:03,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T20:39:03,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T20:39:03,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T20:39:03,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T20:39:03,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T20:39:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T20:39:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T20:39:03,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T20:39:03,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T20:39:03,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T20:39:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T20:39:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T20:39:03,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T20:39:03,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T20:39:03,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T20:39:03,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T20:39:03,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T20:39:03,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T20:39:03,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T20:39:03,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T20:39:03,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T20:39:03,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T20:39:03,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T20:39:03,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T20:39:03,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T20:39:04,064][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103gb], net total_space [125.8gb], types [ext4]
[2022-03-31T20:39:04,064][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T20:39:04,628][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T20:39:20,103][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T20:39:20,111][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-31T20:39:20,113][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T20:39:20,115][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-31T20:39:20,116][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T20:39:20,117][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-31T20:39:20,117][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-31T20:39:20,118][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T20:39:20,119][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-31T20:39:20,119][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-31T20:39:20,120][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-31T20:39:20,120][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T20:39:20,122][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T20:39:20,123][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T20:39:20,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T20:39:21,862][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T20:39:22,037][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T20:39:23,280][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T20:39:24,464][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T20:39:24,465][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T20:39:24,493][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T20:39:24,496][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T20:39:24,751][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T20:39:28,194][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T20:39:28,364][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nQU3MU6hSMmnvawcghyAiA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 170, version: 5020, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nQU3MU6hSMmnvawcghyAiA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T20:39:28,617][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nQU3MU6hSMmnvawcghyAiA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 170, version: 5020, reason: Publication{term=170, version=5020}
[2022-03-31T20:39:28,781][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T20:39:28,784][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T20:39:30,141][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T20:39:30,154][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T20:39:31,430][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T20:39:31,431][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T20:39:32,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-31T20:39:33,084][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-31T20:39:33,468][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T20:39:33,472][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T20:39:33,483][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T20:39:33,662][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-31T20:39:34,453][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T20:39:34,748][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T20:39:39,003][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T20:39:50,973][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.31][0]]]).
[2022-03-31T20:40:14,661][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:14,833][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:15,557][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:15,834][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:16,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T20:40:16,736][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T21:04:27,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73a14b10, interval=1s}] took [31766ms] which is above the warn threshold of [5000ms]
[2022-03-31T21:16:53,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5612ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:32:16,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5320754331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:35:57,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8m/1850539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:33:19,407][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.5s/22579ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5e1b599d]], which exceeds the warn threshold of [10s]
[2022-03-31T21:36:33,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6e830b69, interval=5s}] took [1850361ms] which is above the warn threshold of [5000ms]
[2022-03-31T21:38:13,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8m/1850361593952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:42:51,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:40:53,448][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [16.1s/16101ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@12ade8e], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@85afadec]], which exceeds the warn threshold of [10s]
[2022-03-31T21:45:23,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288962423394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:47:57,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:49:53,758][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.3m/82659ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@12ade8e]], which exceeds the warn threshold of [10s]
[2022-03-31T21:51:50,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/431519045870ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T21:51:51,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017eebe8@4c5666e5] took [720481ms] which is above the warn threshold of [5000ms]
[2022-03-31T21:56:32,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463846ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T21:56:27,279][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.8s/10895ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@85afadec], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5e1b599d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@be6ac145]], which exceeds the warn threshold of [10s]
[2022-03-31T22:08:35,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463420157077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:10:13,935][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.5s/32560ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@85afadec], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5e1b599d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@be6ac145]], which exceeds the warn threshold of [10s]
[2022-03-31T22:14:21,382][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T22:14:21,417][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T22:14:21,418][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T22:14:25,822][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T22:14:25,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T22:14:25,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T22:14:25,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T22:14:25,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T22:14:25,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T22:14:25,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T22:14:25,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T22:14:25,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T22:14:25,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T22:14:25,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T22:14:25,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T22:14:25,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T22:14:25,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T22:14:25,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T22:14:25,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T22:14:25,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T22:14:25,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T22:14:25,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T22:14:25,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T22:14:25,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T22:14:25,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T22:14:25,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T22:14:25,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T22:14:25,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T22:14:25,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T22:14:25,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T22:14:25,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T22:14:25,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T22:14:25,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T22:14:25,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T22:14:25,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T22:14:25,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T22:14:25,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T22:14:25,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T22:14:25,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T22:14:25,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T22:14:25,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T22:14:25,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T22:14:25,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T22:14:25,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T22:14:25,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T22:14:25,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T22:14:25,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T22:14:25,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T22:14:25,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T22:14:25,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T22:14:25,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T22:14:25,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T22:14:25,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T22:14:25,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T22:14:25,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T22:14:25,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T22:14:25,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T22:14:25,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T22:14:25,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T22:14:25,903][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103gb], net total_space [125.8gb], types [ext4]
[2022-03-31T22:14:25,904][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T22:14:26,208][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T22:14:35,115][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T22:14:35,120][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-31T22:14:35,121][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T22:14:35,122][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-31T22:14:35,123][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T22:14:35,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-31T22:14:35,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-31T22:14:35,125][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T22:14:35,125][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-31T22:14:35,126][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-31T22:14:35,126][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-31T22:14:35,127][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T22:14:35,128][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T22:14:35,128][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-31T22:14:35,129][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T22:14:36,150][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T22:14:36,278][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T22:14:36,975][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T22:14:37,661][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T22:14:37,662][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T22:14:37,731][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T22:14:37,733][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T22:14:37,937][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T22:14:39,861][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T22:14:39,974][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 171, version: 5071, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T22:14:40,155][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 171, version: 5071, reason: Publication{term=171, version=5071}
[2022-03-31T22:14:40,320][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T22:14:40,323][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T22:14:43,931][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:929) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-31T22:14:44,594][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T22:14:44,619][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T22:14:47,176][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T22:14:47,793][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T22:15:05,872][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5475ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:06,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5474565092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:15:21,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5474ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:16:49,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:52,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/112170499736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:55,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:16:57,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6893162897ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:17:08,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@5bb125ee] took [13697ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:17:09,302][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36512}] took [21192ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:17:46,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:17:56,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][27][9] duration [1.6s], collections [1]/[16.4s], total [1.6s]/[1.8s], memory [530.5mb]->[102.2mb]/[2gb], all_pools {[young] [452mb]->[8mb]/[0b]}{[old] [54.5mb]->[54.5mb]/[2gb]}{[survivor] [23.9mb]->[39.6mb]/[0b]}
[2022-03-31T22:17:55,953][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [25795ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:17:56,285][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.4s] publication of cluster state version [5081] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:17:57,064][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T22:17:57,663][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T22:17:57,813][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T22:17:59,000][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][29] overhead, spent [307ms] collecting in the last [1s]
[2022-03-31T22:18:12,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:19,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5576226086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:21,112][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][32][11] duration [2.5s], collections [1]/[10.6s], total [2.5s]/[4.7s], memory [186mb]->[99mb]/[2gb], all_pools {[young] [88mb]->[4mb]/[0b]}{[old] [91mb]->[91mb]/[2gb]}{[survivor] [7mb]->[8mb]/[0b]}
[2022-03-31T22:18:21,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9162ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [9162ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:18:23,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9162420389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:18:39,524][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][39][12] duration [1.4s], collections [1]/[2.7s], total [1.4s]/[6.1s], memory [163mb]->[101.1mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [91mb]->[91mb]/[2gb]}{[survivor] [8mb]->[10.1mb]/[0b]}
[2022-03-31T22:18:39,751][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][39] overhead, spent [1.4s] collecting in the last [2.7s]
[2022-03-31T22:18:44,730][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][41][13] duration [1.9s], collections [1]/[3.5s], total [1.9s]/[8.1s], memory [157.1mb]->[100.6mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [91mb]->[92.8mb]/[2gb]}{[survivor] [10.1mb]->[7.7mb]/[0b]}
[2022-03-31T22:18:40,504][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.SocketException: Broken pipe
	at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
	at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
	at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
	at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
	at sun.security.ssl.SSLSocketOutputRecord.flush(SSLSocketOutputRecord.java:268) ~[?:?]
	at sun.security.ssl.HandshakeOutStream.flush(HandshakeOutStream.java:89) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.onProduceFinished(Finished.java:404) ~[?:?]
	at sun.security.ssl.Finished$T12FinishedProducer.produce(Finished.java:379) ~[?:?]
	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]
	at sun.security.ssl.ServerHelloDone$ServerHelloDoneConsumer.consume(ServerHelloDone.java:182) ~[?:?]
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480) ~[?:?]
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458) ~[?:?]
	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:199) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:172) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-31T22:18:44,852][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][41] overhead, spent [1.9s] collecting in the last [3.5s]
[2022-03-31T22:18:46,677][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T22:18:46,971][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T22:18:48,467][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][44] overhead, spent [300ms] collecting in the last [1.1s]
[2022-03-31T22:18:52,475][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][46][15] duration [1s], collections [1]/[2.4s], total [1s]/[9.4s], memory [166.8mb]->[106.9mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [92.8mb]->[94.9mb]/[2gb]}{[survivor] [9.9mb]->[12mb]/[0b]}
[2022-03-31T22:18:52,765][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][46] overhead, spent [1s] collecting in the last [2.4s]
[2022-03-31T22:19:06,181][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12232ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:19:12,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@63692784] took [10664ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:19:36,325][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][64][17] duration [778ms], collections [1]/[2.2s], total [778ms]/[10.4s], memory [118.3mb]->[112mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [98.5mb]->[105.4mb]/[2gb]}{[survivor] [15.8mb]->[6.5mb]/[0b]}
[2022-03-31T22:19:36,859][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T22:19:36,964][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][64] overhead, spent [778ms] collecting in the last [2.2s]
[2022-03-31T22:19:52,180][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][70][18] duration [2.1s], collections [1]/[4.6s], total [2.1s]/[12.5s], memory [180mb]->[121.4mb]/[2gb], all_pools {[young] [72mb]->[8mb]/[0b]}{[old] [105.4mb]->[105.4mb]/[2gb]}{[survivor] [6.5mb]->[11.9mb]/[0b]}
[2022-03-31T22:19:52,927][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][70] overhead, spent [2.1s] collecting in the last [4.6s]
[2022-03-31T22:20:06,340][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.7s] publication of cluster state version [5088] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:20:33,173][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][19] duration [1s], collections [1]/[2s], total [1s]/[13.6s], memory [197.4mb]->[123.7mb]/[2gb], all_pools {[young] [79.9mb]->[0b]/[0b]}{[old] [105.4mb]->[109.7mb]/[2gb]}{[survivor] [11.9mb]->[14mb]/[0b]}
[2022-03-31T22:20:33,353][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [1s] collecting in the last [2s]
[2022-03-31T22:20:37,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][96] overhead, spent [635ms] collecting in the last [1.8s]
[2022-03-31T22:20:39,620][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][97] overhead, spent [447ms] collecting in the last [1.6s]
[2022-03-31T22:20:41,509][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][98] overhead, spent [568ms] collecting in the last [2s]
[2022-03-31T22:20:42,852][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [334ms] collecting in the last [1.2s]
[2022-03-31T22:20:44,121][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][100] overhead, spent [378ms] collecting in the last [1.4s]
[2022-03-31T22:20:49,347][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][104] overhead, spent [364ms] collecting in the last [1.1s]
[2022-03-31T22:21:03,443][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][111] overhead, spent [514ms] collecting in the last [1.2s]
[2022-03-31T22:21:05,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][29] duration [1.1s], collections [1]/[2.6s], total [1.1s]/[18.4s], memory [148.5mb]->[147.8mb]/[2gb], all_pools {[young] [40mb]->[8mb]/[0b]}{[old] [141.1mb]->[141.1mb]/[2gb]}{[survivor] [7.3mb]->[6.7mb]/[0b]}
[2022-03-31T22:21:06,193][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [1.1s] collecting in the last [2.6s]
[2022-03-31T22:21:15,542][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][118][30] duration [851ms], collections [1]/[1.5s], total [851ms]/[19.3s], memory [207.8mb]->[235.8mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [141.1mb]->[141.1mb]/[2gb]}{[survivor] [6.7mb]->[8.6mb]/[0b]}
[2022-03-31T22:21:15,627][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][118] overhead, spent [851ms] collecting in the last [1.5s]
[2022-03-31T22:21:16,865][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T22:21:29,059][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][32] duration [1.2s], collections [1]/[2.4s], total [1.2s]/[20.8s], memory [186.6mb]->[153.9mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [141.1mb]->[142.6mb]/[2gb]}{[survivor] [9.4mb]->[11.2mb]/[0b]}
[2022-03-31T22:21:29,503][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [1.2s] collecting in the last [2.4s]
[2022-03-31T22:21:37,177][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][131] overhead, spent [689ms] collecting in the last [1.9s]
[2022-03-31T22:22:00,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7208ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:22:00,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@12a49828] took [8208ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7207761193ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:22:03,865][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][136][34] duration [5.8s], collections [1]/[10.9s], total [5.8s]/[27.4s], memory [237.8mb]->[162.7mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [147.8mb]->[147.8mb]/[2gb]}{[survivor] [5.9mb]->[6.8mb]/[0b]}
[2022-03-31T22:22:04,965][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][136] overhead, spent [5.8s] collecting in the last [10.9s]
[2022-03-31T22:22:06,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6932ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:19,844][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36590}] took [7060ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:32,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5943ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:22:51,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [5118ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:10,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:12,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [7323ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:17,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5122172389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:25,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14656ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:32,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14656245187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:37,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [14656ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:40,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:47,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14929921909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:52,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:57,114][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36590}] took [12806ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:23:57,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12805636523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:23:59,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7683ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:04,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7682695925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:14,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:19,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14020763027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:28,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:36,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14210825197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:24:50,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:03,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20586438221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:04,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@7def8108] took [69306ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:25:13,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:25,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23673372005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:36,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:25:49,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22716366811ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:14,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:14,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [39522ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:26:15,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39522345403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:14,660][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36594}] took [120710ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:26:16,148][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][140][35] duration [11.2s], collections [1]/[2.9m], total [11.2s]/[38.6s], memory [182.7mb]->[160.9mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [147.8mb]->[147.8mb]/[2gb]}{[survivor] [6.8mb]->[9mb]/[0b]}
[2022-03-31T22:26:31,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5860ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:32,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5859994752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:32,749][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][145][36] duration [3s], collections [1]/[7.5s], total [3s]/[41.7s], memory [232.9mb]->[156.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [147.8mb]->[150.2mb]/[2gb]}{[survivor] [9mb]->[6.3mb]/[0b]}
[2022-03-31T22:26:32,750][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][145] overhead, spent [3s] collecting in the last [7.5s]
[2022-03-31T22:26:37,850][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=171, version=5098}] took [4.9m] which is above the warn threshold of [30s]: [running task [Publication{term=171, version=5098}]] took [0ms], [connecting to new nodes] took [71ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@60ac379] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@fee596e] took [294786ms], [org.elasticsearch.script.ScriptService@14b0327] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [0ms], [org.elasticsearch.snapshots.RestoreService@62fc05e5] took [0ms], [org.elasticsearch.ingest.IngestService@1fa415fd] took [598ms], [org.elasticsearch.action.ingest.IngestActionForwarder@46af6f15] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d4a60@1c861d89] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e57e0d1] took [0ms], [org.elasticsearch.tasks.TaskManager@21e3151b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1c8911f5] took [113ms], [org.elasticsearch.cluster.InternalClusterInfoService@bf2b5a6] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@730a58ab] took [0ms], [org.elasticsearch.indices.SystemIndexManager@3b3cfe2f] took [672ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2760f7ba] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@75ac1e03] took [85ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2cf24896] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4f271213] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@2ee5bd05] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@72eb7556] took [45ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@44cb5db6] took [1022ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@5526550] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5615ceb4] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@407b49c3] took [123ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d51aa11] took [99ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d3b3d1d] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@55233660] took [47ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [75ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4f0c5856] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f0d7284] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5a15f4de] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@742f02f7] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@78365094] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a089e17] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@73f2938a] took [19ms], [org.elasticsearch.node.ResponseCollectorService@390acf90] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5b84015b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@55f2c41a] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@32fb63ed] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5b5f5edf] took [45ms], [org.elasticsearch.indices.store.IndicesStore@345db5b8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@19bffc81] took [0ms], [org.elasticsearch.license.LicenseService@6e5a3edb] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@16fbb05e] took [47ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@32500c10] took [0ms], [org.elasticsearch.gateway.GatewayService@368df3b1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7b4bbe88] took [0ms]
[2022-03-31T22:26:49,797][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][150][37] duration [3.9s], collections [1]/[1.4s], total [3.9s]/[45.6s], memory [212.6mb]->[232.6mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [150.2mb]->[150.2mb]/[2gb]}{[survivor] [6.3mb]->[6.3mb]/[0b]}
[2022-03-31T22:26:49,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:50,410][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][150] overhead, spent [3.9s] collecting in the last [1.4s]
[2022-03-31T22:26:50,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8179788205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:26:51,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [8580ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:00,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:27:00,905][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][38] duration [3.3s], collections [1]/[13.3s], total [3.3s]/[48.9s], memory [232.6mb]->[206.5mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [150.2mb]->[150.4mb]/[2gb]}{[survivor] [6.3mb]->[14.9mb]/[0b]}
[2022-03-31T22:27:00,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5192491815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:27:00,984][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [3.3s] collecting in the last [13.3s]
[2022-03-31T22:27:00,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6647ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:15,118][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36638}] took [5619ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:15,161][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][156][39] duration [3.6s], collections [1]/[1.5s], total [3.6s]/[52.6s], memory [229.4mb]->[245.4mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [150.4mb]->[157.8mb]/[2gb]}{[survivor] [14.9mb]->[15mb]/[0b]}
[2022-03-31T22:27:15,659][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][156] overhead, spent [3.6s] collecting in the last [1.5s]
[2022-03-31T22:27:19,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [10315ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:26,355][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][157][41] duration [4.8s], collections [2]/[11.3s], total [4.8s]/[57.4s], memory [245.4mb]->[267mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [157.8mb]->[172.2mb]/[2gb]}{[survivor] [15mb]->[4.8mb]/[0b]}
[2022-03-31T22:27:27,017][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][157] overhead, spent [4.8s] collecting in the last [11.3s]
[2022-03-31T22:27:28,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7430ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:27:35,929][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [634ms] collecting in the last [2.1s]
[2022-03-31T22:27:37,811][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][162] overhead, spent [688ms] collecting in the last [1.9s]
[2022-03-31T22:27:50,460][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][168][45] duration [1s], collections [1]/[2.2s], total [1s]/[1m], memory [178.7mb]->[179.7mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [6.4mb]->[7.4mb]/[0b]}
[2022-03-31T22:27:51,079][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][168] overhead, spent [1s] collecting in the last [2.2s]
[2022-03-31T22:27:56,371][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][171][46] duration [908ms], collections [1]/[2s], total [908ms]/[1m], memory [199.7mb]->[177.2mb]/[2gb], all_pools {[young] [64mb]->[16mb]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [7.4mb]->[4.9mb]/[0b]}
[2022-03-31T22:27:56,590][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][171] overhead, spent [908ms] collecting in the last [2s]
[2022-03-31T22:28:07,381][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][177][47] duration [1.2s], collections [1]/[3.1s], total [1.2s]/[1m], memory [245.2mb]->[182.9mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [172.2mb]->[172.2mb]/[2gb]}{[survivor] [4.9mb]->[10.6mb]/[0b]}
[2022-03-31T22:28:07,514][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][177] overhead, spent [1.2s] collecting in the last [3.1s]
[2022-03-31T22:28:10,796][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][48] duration [1.1s], collections [1]/[1.2s], total [1.1s]/[1m], memory [182.9mb]->[266.9mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [172.2mb]->[174.3mb]/[2gb]}{[survivor] [10.6mb]->[9.7mb]/[0b]}
[2022-03-31T22:28:11,199][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.1s] collecting in the last [1.2s]
[2022-03-31T22:28:16,315][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][181][49] duration [917ms], collections [1]/[2.4s], total [917ms]/[1m], memory [240mb]->[192.9mb]/[2gb], all_pools {[young] [56mb]->[12mb]/[0b]}{[old] [174.3mb]->[176.9mb]/[2gb]}{[survivor] [9.7mb]->[16mb]/[0b]}
[2022-03-31T22:28:16,511][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][181] overhead, spent [917ms] collecting in the last [2.4s]
[2022-03-31T22:28:19,052][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][182][50] duration [1.2s], collections [1]/[2.8s], total [1.2s]/[1m], memory [192.9mb]->[199.5mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [176.9mb]->[192mb]/[2gb]}{[survivor] [16mb]->[7.4mb]/[0b]}
[2022-03-31T22:28:19,247][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][182] overhead, spent [1.2s] collecting in the last [2.8s]
[2022-03-31T22:28:22,406][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][183][51] duration [1.6s], collections [1]/[3s], total [1.6s]/[1.1m], memory [199.5mb]->[201.3mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [192mb]->[192mb]/[2gb]}{[survivor] [7.4mb]->[9.2mb]/[0b]}
[2022-03-31T22:28:22,622][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][183] overhead, spent [1.6s] collecting in the last [3s]
[2022-03-31T22:28:22,851][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A31032022%3Acount%3Adelay_threshold_exceeded?refresh=wait_for&require_alias=true&_source=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57996}] took [6624ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:24,185][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57998}] took [7923ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:24,382][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57994}] took [8123ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:28,545][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][186][53] duration [1.3s], collections [1]/[2.9s], total [1.3s]/[1.1m], memory [259mb]->[205.5mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [198mb]->[198mb]/[2gb]}{[survivor] [4.9mb]->[7.4mb]/[0b]}
[2022-03-31T22:28:29,194][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][186] overhead, spent [1.3s] collecting in the last [2.9s]
[2022-03-31T22:28:31,962][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][187][54] duration [920ms], collections [1]/[3.5s], total [920ms]/[1.1m], memory [205.5mb]->[204.5mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [198mb]->[198mb]/[2gb]}{[survivor] [7.4mb]->[6.5mb]/[0b]}
[2022-03-31T22:28:32,243][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [920ms] collecting in the last [3.5s]
[2022-03-31T22:28:33,092][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T22:28:44,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [8556ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:28:46,835][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12431ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:28:47,364][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [13.6s] publication of cluster state version [5105] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:28:54,774][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][196][56] duration [1s], collections [1]/[3s], total [1s]/[1.1m], memory [285.7mb]->[243.9mb]/[2gb], all_pools {[young] [88mb]->[36mb]/[0b]}{[old] [198mb]->[198mb]/[2gb]}{[survivor] [7.6mb]->[9.9mb]/[0b]}
[2022-03-31T22:28:55,492][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][196] overhead, spent [1s] collecting in the last [3s]
[2022-03-31T22:29:03,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:03,853][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][197][57] duration [3s], collections [1]/[8.3s], total [3s]/[1.2m], memory [243.9mb]->[208.1mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [198mb]->[200.7mb]/[2gb]}{[survivor] [9.9mb]->[7.4mb]/[0b]}
[2022-03-31T22:29:03,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5057021013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:05,441][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][197] overhead, spent [3s] collecting in the last [8.3s]
[2022-03-31T22:29:06,000][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [14902ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [31] unchanged indices
[2022-03-31T22:29:06,699][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.8s] publication of cluster state version [5106] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_APPLY_COMMIT]
[2022-03-31T22:29:40,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@d5e287e] took [21100ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:29:51,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:56,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8337628009ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:29:57,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36638}] took [10599ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:29:57,444][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36562}] took [10599ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:01,871][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36642}] took [12414ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:01,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:00,155][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36636}] took [10599ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:05,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9573923997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:05,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [9573ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:14,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@49e48ccf, interval=1m}] took [10354ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:30:12,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:18,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10354622820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:43,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32364ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:30:46,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32363772050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:03,661][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:57960}] took [32364ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:04,784][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36666}] took [20717ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:04,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20717ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:07,990][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][204][59] duration [20.2s], collections [2]/[49.8s], total [20.2s]/[1.5m], memory [280.1mb]->[294.4mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [200.7mb]->[202.5mb]/[2gb]}{[survivor] [7.4mb]->[9.2mb]/[0b]}
[2022-03-31T22:31:07,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20717763522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:08,959][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][204] overhead, spent [20.2s] collecting in the last [49.8s]
[2022-03-31T22:31:14,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [30488ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:15,878][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36666}] took [10572ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:18,962][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:36208}] took [34891ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:26,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:26,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [6602ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:31:26,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6202037542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:38,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:42,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5015098686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:47,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:31:53,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8624080770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:01,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13942ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:05,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13941473349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:05,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@6cf9f0d4] took [22565ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:08,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:11,500][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36666}] took [21524ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:11,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [7582ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:12,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7582428329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:15,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:18,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6744ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:32:19,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6744026290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:22,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:24,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7031943061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:32:27,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:09,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5223177857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:09,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5223ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:10,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:11,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [43723ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:12,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43723025621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:15,156][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][60] duration [16.2s], collections [1]/[46.6s], total [16.2s]/[1.8m], memory [263.8mb]->[225.2mb]/[2gb], all_pools {[young] [60mb]->[16mb]/[0b]}{[old] [202.5mb]->[202.8mb]/[2gb]}{[survivor] [9.2mb]->[6.4mb]/[0b]}
[2022-03-31T22:33:16,293][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [16.2s] collecting in the last [46.6s]
[2022-03-31T22:33:17,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6802ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:30,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:40,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5402ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:47,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5149ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:56,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5148599771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:57,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:33:57,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@63cdcee2] took [10116ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:58,535][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36676}] took [10116ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:33:58,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10116343776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:03,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5249ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:03,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:04,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249554061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:18,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:18,488][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36684}] took [14691ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:18,388][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=171, version=5106}] took [5m] which is above the warn threshold of [30s]: [running task [Publication{term=171, version=5106}]] took [44ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@60ac379] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@fee596e] took [242771ms], [org.elasticsearch.script.ScriptService@14b0327] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [0ms], [org.elasticsearch.snapshots.RestoreService@62fc05e5] took [0ms], [org.elasticsearch.ingest.IngestService@1fa415fd] took [495ms], [org.elasticsearch.action.ingest.IngestActionForwarder@46af6f15] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d4a60@1c861d89] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e57e0d1] took [72ms], [org.elasticsearch.tasks.TaskManager@21e3151b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1c8911f5] took [121ms], [org.elasticsearch.cluster.InternalClusterInfoService@bf2b5a6] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@730a58ab] took [0ms], [org.elasticsearch.indices.SystemIndexManager@3b3cfe2f] took [1551ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2760f7ba] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@75ac1e03] took [201ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2cf24896] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4f271213] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@2ee5bd05] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@72eb7556] took [140ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@44cb5db6] took [14203ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@5526550] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5615ceb4] took [1ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@407b49c3] took [8598ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d51aa11] took [364ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d3b3d1d] took [456ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@55233660] took [20070ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [926ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4f0c5856] took [5715ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f0d7284] took [64ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5a15f4de] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@742f02f7] took [1665ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@78365094] took [3003ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a089e17] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@73f2938a] took [445ms], [org.elasticsearch.node.ResponseCollectorService@390acf90] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5b84015b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@55f2c41a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@32fb63ed] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5b5f5edf] took [75ms], [org.elasticsearch.indices.store.IndicesStore@345db5b8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@19bffc81] took [0ms], [org.elasticsearch.license.LicenseService@6e5a3edb] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@16fbb05e] took [75ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@32500c10] took [0ms], [org.elasticsearch.gateway.GatewayService@368df3b1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7b4bbe88] took [0ms]
[2022-03-31T22:34:18,488][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36682}] took [14691ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:24,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859332292ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:27,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9655ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:30,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [9655ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:30,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9655016283ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:33,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:37,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5106444865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:39,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5106ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:46,747][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [5107ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:47,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:47,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@49e48ccf, interval=1m}] took [14440ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:34:48,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14440779903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:34:59,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:02,260][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11s/11005ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [95], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]], allocationId [4Wwjmsw_QcG9aU1YqDT14A], primary term [95], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-31T22:35:01,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7565499869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:04,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6039ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:06,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6039480341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:07,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [6039ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:08,045][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [6039ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:08,045][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36684}] took [6039ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:10,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36682}] took [6153ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:18,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:19,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7379298810ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:35:19,482][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][221][61] duration [6s], collections [1]/[6.6s], total [6s]/[1.9m], memory [289.2mb]->[297.2mb]/[2gb], all_pools {[young] [84mb]->[36mb]/[0b]}{[old] [202.8mb]->[202.8mb]/[2gb]}{[survivor] [6.4mb]->[8.6mb]/[0b]}
[2022-03-31T22:35:19,482][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][221] overhead, spent [6s] collecting in the last [6.6s]
[2022-03-31T22:35:19,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7779ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:37,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [5202ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:35:35,225][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [23189ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [33] unchanged indices
[2022-03-31T22:35:42,612][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36676}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:36:10,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:17,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26185324199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:20,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:10,903][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1m] publication of cluster state version [5107] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-31T22:36:24,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9804795250ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:26,040][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][226][62] duration [19.5s], collections [1]/[29.9s], total [19.5s]/[2.2m], memory [291.4mb]->[216.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [202.8mb]->[203.4mb]/[2gb]}{[survivor] [8.6mb]->[13.5mb]/[0b]}
[2022-03-31T22:36:27,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:28,642][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][226] overhead, spent [19.5s] collecting in the last [29.9s]
[2022-03-31T22:36:30,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [16760ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:36:32,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6955452073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:42,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:48,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15080530050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:36:54,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [15080ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:36:57,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:05,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14532852790ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:12,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@34ef3bd3] took [14373ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:37:13,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:23,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14373768709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:31,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:38,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19029893595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:50,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:37:57,917][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [37755ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:37:58,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18725628155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:03,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:27,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13024603440ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:29,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [13024ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:38:34,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:36,546][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:36688}] took [31906ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:38:39,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31906568087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:44,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9878ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:38:45,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4bbc258f, interval=5s}] took [9877ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:39:00,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9877385527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:10,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26177ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:23,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26176951528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:41,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:45,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16451187642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:50,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:39:50,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [23622ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:39:51,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23622479178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:40:01,286][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168555ms] ago, timed out [1.2m/75991ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{HkeYlEG3S5aq0nbTlPwbKA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [991]
[2022-03-31T22:40:08,109][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=171, version=5107}] took [3.6m] which is above the warn threshold of [30s]: [running task [Publication{term=171, version=5107}]] took [223ms], [connecting to new nodes] took [154ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@60ac379] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@fee596e] took [68090ms], [org.elasticsearch.script.ScriptService@14b0327] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [0ms], [org.elasticsearch.snapshots.RestoreService@62fc05e5] took [0ms], [org.elasticsearch.ingest.IngestService@1fa415fd] took [8247ms], [org.elasticsearch.action.ingest.IngestActionForwarder@46af6f15] took [203ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d4a60@1c861d89] took [297ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e57e0d1] took [619ms], [org.elasticsearch.tasks.TaskManager@21e3151b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1c8911f5] took [376ms], [org.elasticsearch.cluster.InternalClusterInfoService@bf2b5a6] took [285ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@730a58ab] took [696ms], [org.elasticsearch.indices.SystemIndexManager@3b3cfe2f] took [8097ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@2760f7ba] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@75ac1e03] took [656ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2cf24896] took [104ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4f271213] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@2ee5bd05] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@72eb7556] took [190ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@44cb5db6] took [112446ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@5526550] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5615ceb4] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@407b49c3] took [3784ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d51aa11] took [239ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@2d3b3d1d] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@55233660] took [2202ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@323d19d0] took [712ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4f0c5856] took [4028ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f0d7284] took [59ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@5a15f4de] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@742f02f7] took [4655ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@78365094] took [1352ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a089e17] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@73f2938a] took [51ms], [org.elasticsearch.node.ResponseCollectorService@390acf90] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5b84015b] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@55f2c41a] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@32fb63ed] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@5b5f5edf] took [48ms], [org.elasticsearch.indices.store.IndicesStore@345db5b8] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@19bffc81] took [0ms], [org.elasticsearch.license.LicenseService@6e5a3edb] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@16fbb05e] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@32500c10] took [0ms], [org.elasticsearch.gateway.GatewayService@368df3b1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@7b4bbe88] took [0ms]
[2022-03-31T22:40:15,089][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [991] timed out after [92564ms]
[2022-03-31T22:40:27,306][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0], [.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-03-31T22:40:32,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [7003ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:42:46,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/115109ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:43:18,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/115108995049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:43:53,138][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][244][63] duration [1.5m], collections [1]/[2.8s], total [1.5m]/[3.8m], memory [288.9mb]->[288.9mb]/[2gb], all_pools {[young] [72mb]->[80mb]/[0b]}{[old] [203.4mb]->[203.4mb]/[2gb]}{[survivor] [13.5mb]->[13.5mb]/[0b]}
[2022-03-31T22:43:54,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:44:25,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][244] overhead, spent [1.5m] collecting in the last [2.8s]
[2022-03-31T22:44:25,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70485299967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:45:02,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@257d51d4, interval=1s}] took [188675ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:45:09,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:45:43,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71446029910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:46:14,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:46:40,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69097238819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:47:10,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:47:49,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50481832516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:48:26,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:48:53,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81610176926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:49:32,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:50:12,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64715290468ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:51:06,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:51:48,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92506551096ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T22:52:10,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T22:52:20,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@20763b43, interval=5s}] took [66806ms] which is above the warn threshold of [5000ms]
[2022-03-31T22:52:30,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66806178633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:01:25,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/527785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:04:21,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7m/527784638637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:07:27,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:10:43,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379339498970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:14:07,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:17:12,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367369404147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:20:06,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:23:01,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392432971198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:26:18,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:29:50,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372939158069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:33:29,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:36:32,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417069742454ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:37:33,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017e9730@56ecfdd0] took [417069ms] which is above the warn threshold of [5000ms]
[2022-03-31T23:39:45,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:42:42,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382910240146ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:46:29,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/393107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:49:42,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/392574643480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T23:53:50,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/452897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T23:57:18,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/452967504255ns] on relative clock which is above the warn threshold of [5000ms]
