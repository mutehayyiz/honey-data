[2022-03-29T16:58:42,321][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T16:58:42,384][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T16:58:42,386][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T16:58:58,423][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T16:58:58,429][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T16:58:58,431][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T16:58:58,431][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T16:58:58,432][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T16:58:58,432][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T16:58:58,433][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T16:58:58,439][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T16:58:58,440][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T16:58:58,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T16:58:58,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T16:58:58,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T16:58:58,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T16:58:58,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T16:58:58,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T16:58:58,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T16:58:58,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T16:58:58,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T16:58:58,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T16:58:58,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T16:58:58,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T16:58:58,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T16:58:58,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T16:58:58,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T16:58:58,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T16:58:58,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T16:58:58,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T16:58:58,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T16:58:58,481][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T16:58:58,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T16:58:58,490][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T16:58:58,500][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T16:58:58,502][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T16:58:58,502][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T16:58:58,503][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T16:58:58,504][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T16:58:58,504][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T16:58:58,505][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T16:58:58,510][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T16:58:58,511][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T16:58:58,521][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T16:58:58,522][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T16:58:58,523][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T16:58:58,523][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T16:58:58,530][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T16:58:58,530][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T16:58:58,531][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T16:58:58,531][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T16:58:58,532][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T16:58:58,533][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T16:58:58,533][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T16:58:58,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T16:58:58,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T16:58:58,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T16:58:58,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T16:58:58,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T16:58:58,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T16:58:58,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T16:58:58,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T16:58:58,741][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.6gb], net total_space [125.8gb], types [ext4]
[2022-03-29T16:58:58,742][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T16:58:59,433][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T16:59:18,955][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T16:59:18,959][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T16:59:20,755][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T16:59:21,003][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T16:59:22,459][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T16:59:23,879][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T16:59:23,888][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T16:59:23,955][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T16:59:23,956][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T16:59:24,303][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T16:59:28,095][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T16:59:28,264][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jR_AE3WqTvG3b_wTAwv1Kw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 153, version: 4375, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jR_AE3WqTvG3b_wTAwv1Kw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T16:59:28,560][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{jR_AE3WqTvG3b_wTAwv1Kw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 153, version: 4375, reason: Publication{term=153, version=4375}
[2022-03-29T16:59:28,722][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T16:59:28,722][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T16:59:32,495][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T16:59:32,504][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T16:59:35,224][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T16:59:35,225][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T16:59:37,169][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-29T16:59:37,636][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-29T16:59:39,158][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-29T16:59:39,322][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T16:59:39,333][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T16:59:39,352][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T16:59:39,644][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-29T16:59:41,097][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-29T16:59:42,354][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T16:59:42,582][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T16:59:57,742][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][33] overhead, spent [1s] collecting in the last [1.5s]
[2022-03-29T16:59:59,877][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T17:00:15,700][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.29][0]]]).
[2022-03-29T17:00:49,535][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 515 finished with response BulkByScrollResponse[took=541.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-29T17:00:52,433][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 531 finished with response BulkByScrollResponse[took=3s,timed_out=false,sliceId=null,updated=1038,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-29T17:01:02,273][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-29T17:02:03,771][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T17:04:01,357][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T17:22:13,418][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T17:24:31,553][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-29T17:32:21,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5aef5a62, interval=1s}] took [36279ms] which is above the warn threshold of [5000ms]
[2022-03-29T17:41:01,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T18:45:52,886][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T18:45:52,930][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T18:45:52,931][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T18:45:59,636][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T18:45:59,639][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T18:45:59,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T18:45:59,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T18:45:59,642][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T18:45:59,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T18:45:59,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T18:45:59,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T18:45:59,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T18:45:59,647][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T18:45:59,647][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T18:45:59,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T18:45:59,650][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T18:45:59,650][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T18:45:59,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T18:45:59,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T18:45:59,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T18:45:59,655][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T18:45:59,655][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T18:45:59,658][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T18:45:59,658][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T18:45:59,660][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T18:45:59,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T18:45:59,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T18:45:59,663][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T18:45:59,665][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T18:45:59,666][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T18:45:59,666][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T18:45:59,667][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T18:45:59,667][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T18:45:59,668][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T18:45:59,669][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T18:45:59,671][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T18:45:59,671][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T18:45:59,672][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T18:45:59,673][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T18:45:59,673][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T18:45:59,674][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T18:45:59,675][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T18:45:59,677][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T18:45:59,680][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T18:45:59,681][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T18:45:59,682][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T18:45:59,682][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T18:45:59,683][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T18:45:59,683][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T18:45:59,683][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T18:45:59,684][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T18:45:59,685][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T18:45:59,685][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T18:45:59,686][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T18:45:59,686][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T18:45:59,687][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T18:45:59,688][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T18:45:59,690][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T18:45:59,691][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T18:45:59,693][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T18:45:59,693][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T18:45:59,694][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T18:45:59,773][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.5gb], net total_space [125.8gb], types [ext4]
[2022-03-29T18:45:59,773][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T18:46:00,167][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T18:46:12,489][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T18:46:12,494][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T18:46:12,495][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T18:46:12,496][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T18:46:12,497][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T18:46:12,498][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T18:46:12,498][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T18:46:12,499][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T18:46:12,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T18:46:12,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T18:46:12,501][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T18:46:12,502][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T18:46:12,503][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T18:46:12,504][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T18:46:12,505][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T18:46:13,980][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T18:46:14,175][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T18:46:15,098][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T18:46:16,000][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T18:46:16,001][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T18:46:16,079][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T18:46:16,081][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T18:46:16,314][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T18:46:18,446][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T18:46:18,557][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{YXLgQ-tbQ4arHLdAvjQP9A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 154, version: 4424, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{YXLgQ-tbQ4arHLdAvjQP9A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T18:46:18,744][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{YXLgQ-tbQ4arHLdAvjQP9A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 154, version: 4424, reason: Publication{term=154, version=4424}
[2022-03-29T18:46:18,846][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T18:46:18,847][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T18:46:20,028][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T18:46:20,035][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T18:46:20,861][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T18:46:20,862][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T18:46:23,340][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-29T18:46:23,356][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-29T18:46:23,382][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-29T18:46:25,873][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T18:46:26,018][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T18:46:26,024][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-29T18:46:28,040][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:29,286][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][13] overhead, spent [549ms] collecting in the last [1s]
[2022-03-29T18:46:30,933][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:31,039][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:31,104][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:31,149][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:31,183][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:31,242][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:31,292][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_update_by_query, params: {ignore_unavailable=true, refresh=true, conflicts=proceed, index=.kibana_task_manager}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.access$000(AbstractSearchAsyncAction.java:64) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:343) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$Delegating.onFailure(ActionListener.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:50) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportChannel.sendErrorResponse(TransportChannel.java:45) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.onFailure(ThreadContext.java:765) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query]]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 18 more
[2022-03-29T18:46:35,734][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T18:46:43,990][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.29][0]]]).
[2022-03-29T20:11:30,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [23058ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:12:02,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:13:10,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7995104458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:13:18,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:13:19,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [86224ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:13:41,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86224719044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:15:55,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/155926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:17:02,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/155926263486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:17:29,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:17:55,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/93335160737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:18:15,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:18:41,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.5s/42578886914ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:19:19,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:18:25,731][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [42579ms] which is above the warn threshold of [5s]
[2022-03-29T20:19:44,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68337055462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:19:25,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [68337ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:20:09,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.7s/50763ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:20:30,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.7s/50763069623ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:20:48,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:21:02,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38196127756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:21:15,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:21:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27775726145ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:21:36,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20098ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:21:51,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19996062453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:22:10,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:22:29,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34556593639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:23:00,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.8s/48819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:23:17,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.8s/48819325868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:23:31,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:23:52,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32463659441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:24:00,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:24:23,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [28870ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:24:18,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28870982273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:24:53,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:25:24,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53446736029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:26:01,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67111ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:26:38,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67110742642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:27:01,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:27:27,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57672815335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:27:54,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:28:23,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55348059814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:28:49,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.4s/55490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:27:37,308][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [57673ms] which is above the warn threshold of [5s]
[2022-03-29T20:29:08,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.4s/55489985879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:29:35,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:29:55,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45543259312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:30:26,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51s/51011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:30:51,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.7s/50718946860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:31:27,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:32:35,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.9s/57960866480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:33:11,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:33:40,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107341562999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:34:07,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.8s/56833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:34:45,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [56833ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:34:35,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.8s/56833551899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:35:18,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70992ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:35:51,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70991903315ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:36:14,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.2s/55229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:36:35,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.2s/55228408006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:37:00,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45685ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:37:17,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45685024000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:37:36,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:37:53,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.3s/36307294405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:38:16,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:38:29,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39857854437ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:38:47,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:39:08,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31278500430ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:39:25,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37775ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:40:02,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.7s/37774827976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:40:24,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:40:38,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59512335404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:41:12,919][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [30785] timed out after [476912ms]
[2022-03-29T20:41:20,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.5s/55508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:41:33,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.5s/55507824297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:41:19,189][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [34.7s/34740ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3a1f884b]], which exceeds the warn threshold of [10s]
[2022-03-29T20:41:47,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:40:39,920][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [59512ms] which is above the warn threshold of [5s]
[2022-03-29T20:42:14,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27727535530ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:42:39,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5s/51557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:42:58,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5s/51557230560ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:42:23,861][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:50008}] took [27728ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:42:20,575][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:50016}] took [55508ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:43:16,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37528ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:43:32,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37528141222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:43:45,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:43:53,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28356880484ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:44:08,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23328ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:44:18,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23327535545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:44:15,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [23327ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:44:34,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:44:55,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26131806559ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:45:16,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:45:34,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40515884550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:45:56,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40596ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:46:02,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d8258@4892bdc0] took [40595ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:46:15,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.5s/40595349418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:46:29,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:46:41,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33516508465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:46:57,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [33516ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:47:10,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41084ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:47:36,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41s/41083515166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:47:54,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:48:15,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43695100557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:48:34,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:48:49,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40059931121ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:49:08,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:49:24,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33299374005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:48:44,655][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40060ms] which is above the warn threshold of [5s]
[2022-03-29T20:49:42,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:50:05,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34193224329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:50:29,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:50:50,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41106646381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:51:11,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:51:47,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46756242969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:52:12,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:52:31,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62819128859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:52:35,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [62819ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:52:51,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:53:11,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.9s/38957566012ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:53:44,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52621ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:54:00,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.6s/52621055808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:52:57,402][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [30929] timed out after [225847ms]
[2022-03-29T20:54:46,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:55:04,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62730270805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:55:17,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31098ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:55:30,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31097390846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:54:49,797][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [39.5m/2372457ms] ago, timed out [31.5m/1895545ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{YXLgQ-tbQ4arHLdAvjQP9A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [30785]
[2022-03-29T20:55:46,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:55:57,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28738490797ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:56:17,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:56:44,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31202958364ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:57:02,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.2s/42210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:57:17,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.2s/42209778075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:57:38,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:57:56,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.4s/38413439879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:58:05,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [38413ms] which is above the warn threshold of [5000ms]
[2022-03-29T20:58:10,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:57:48,121][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [38413ms] which is above the warn threshold of [5s]
[2022-03-29T20:58:27,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.7s/31722260602ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T20:59:00,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.8s/49894ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T20:59:29,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.8s/49893872514ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:00:11,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:00:45,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71276704874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:01:06,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:01:20,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55177519453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:01:35,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:01:46,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28788059967ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:02:01,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:02:23,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26151424404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:02:40,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:02:54,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.9s/37915835941ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:03:08,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:03:22,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29500147762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:03:34,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [29500ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:03:34,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:03:49,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26201120757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:04:04,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:04:15,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30034229567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:04:38,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:04:50,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.7s/33756120958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:04:17,816][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30034ms] which is above the warn threshold of [5s]
[2022-03-29T21:05:03,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:05:15,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25165963736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:05:31,844][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:49996}] took [58922ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:05:36,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d8258@6640227] took [32830ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:05:36,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:05:50,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.8s/32830339066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:06:07,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30666ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:06:16,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30666636853ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:06:21,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [30666ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:06:26,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:06:33,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19172204929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:06:47,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21347ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:06:58,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21347184515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:07:17,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:07:29,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29948354280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:07:40,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:07:47,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [22937ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:07:50,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22937799922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:08:02,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:08:07,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5109/0x00000008017d8258@240ded95] took [22173ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:08:12,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22173713329ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:08:24,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:08:41,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21932703037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:08:56,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:09:10,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31457858082ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:09:23,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:09:24,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [27140ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:09:36,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27140658520ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:09:48,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25436ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:10:11,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25436048599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:10:23,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:10:36,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35252092574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:10:51,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27879ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:11:07,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27878710936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:11:19,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:11:34,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27610367974ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:11:06,944][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [24.9m/1496258ms] ago, timed out [21.1m/1270411ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{YXLgQ-tbQ4arHLdAvjQP9A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [30929]
[2022-03-29T21:11:48,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [55662ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:11:48,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28053ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:11:58,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28052551553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:12:13,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:12:27,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25117374277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:12:45,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32224ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:12:46,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3f2650d, interval=5s}] took [32223ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:13:02,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32223622211ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:13:23,656][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38860ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:13:38,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38860472439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:13:53,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30018ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:14:11,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30017835396ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:14:33,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39771ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:14:51,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39771104766ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:15:17,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:15:28,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [43605ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:15:37,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.6s/43605117533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:15:52,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:16:06,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35078693897ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:16:19,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:16:40,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26792161958ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:17:01,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:17:23,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42615845209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:17:43,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:17:01,628][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [31082] timed out after [221653ms]
[2022-03-29T21:18:25,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41816041060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:18:48,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:18:50,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [64536ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:19:06,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64536604650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:19:39,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.6s/49601ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:20:05,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.6s/49600963285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:20:24,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:20:54,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.1s/47193181818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:21:04,124][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.9s/14968ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3a1f884b]], which exceeds the warn threshold of [10s]
[2022-03-29T21:21:22,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:21:38,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57s/57016366020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:21:56,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:22:14,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.1s/34185071745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:22:22,063][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [34185ms] which is above the warn threshold of [5s]
[2022-03-29T21:22:34,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:23:00,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38188380119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:23:23,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:23:41,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48s/48041691703ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:24:02,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:24:19,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.1s/40188834664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:24:41,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38347ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:25:04,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38346945190ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:25:24,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43547ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:25:42,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.5s/43547440727ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:26:02,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:26:22,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.3s/38375039126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:26:42,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:27:03,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39064799657ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:27:28,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:27:48,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [46258ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:27:50,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46258803131ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:28:22,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.2s/54211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:28:27,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3f2650d, interval=5s}] took [54210ms] which is above the warn threshold of [5000ms]
[2022-03-29T21:28:46,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.2s/54210906589ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:29:14,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:29:31,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52154665988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:29:57,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42707ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:30:22,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42706946605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:30:52,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54s/54078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:31:35,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54s/54078624091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:32:09,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:33:10,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73284700946ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:34:43,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/153946ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:35:55,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/153859625323ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:38:19,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:40:40,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/184136232586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:43:27,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:47:28,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343992365331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:50:24,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:46:43,044][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [343992ms] which is above the warn threshold of [5s]
[2022-03-29T21:53:11,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/416955095862ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T21:56:05,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T21:58:25,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326727840142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:01:55,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363108ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:04:18,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362918310469ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:06:47,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/280828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:09:22,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/280934992153ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:11:36,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:13:52,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301934749374ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:17:09,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:10:10,620][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [31191] timed out after [958424ms]
[2022-03-29T22:19:30,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331379063583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:21:42,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273265ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:23:52,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273476276397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:26:13,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:29:07,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/270376194631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:31:33,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:33:32,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320519591166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:35:44,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/250741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:38:02,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/250749403799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:40:26,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:42:39,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282181201610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:45:29,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:46:54,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@62a357b9, interval=1s}] took [274110ms] which is above the warn threshold of [5000ms]
[2022-03-29T22:47:59,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/274110311480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:50:39,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T22:52:43,441][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4h/5372682ms] ago, timed out [1.4h/5151029ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{YXLgQ-tbQ4arHLdAvjQP9A}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [31082]
[2022-03-29T22:54:56,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325666476623ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T22:57:18,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/411695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:05:30,150][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-29T23:05:30,200][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-29T23:05:30,203][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-29T23:05:40,387][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-29T23:05:40,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-29T23:05:40,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-29T23:05:40,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-29T23:05:40,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-29T23:05:40,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-29T23:05:40,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-29T23:05:40,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-29T23:05:40,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-29T23:05:40,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-29T23:05:40,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-29T23:05:40,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-29T23:05:40,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-29T23:05:40,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-29T23:05:40,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-29T23:05:40,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-29T23:05:40,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-29T23:05:40,409][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-29T23:05:40,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-29T23:05:40,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-29T23:05:40,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-29T23:05:40,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-29T23:05:40,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-29T23:05:40,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-29T23:05:40,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-29T23:05:40,418][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-29T23:05:40,419][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-29T23:05:40,420][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-29T23:05:40,421][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-29T23:05:40,422][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-29T23:05:40,424][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-29T23:05:40,425][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-29T23:05:40,426][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-29T23:05:40,427][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-29T23:05:40,428][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-29T23:05:40,429][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-29T23:05:40,430][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-29T23:05:40,431][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-29T23:05:40,432][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-29T23:05:40,433][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-29T23:05:40,434][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-29T23:05:40,435][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-29T23:05:40,436][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-29T23:05:40,436][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-29T23:05:40,437][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-29T23:05:40,438][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-29T23:05:40,439][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-29T23:05:40,440][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-29T23:05:40,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-29T23:05:40,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-29T23:05:40,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-29T23:05:40,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-29T23:05:40,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-29T23:05:40,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-29T23:05:40,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-29T23:05:40,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-29T23:05:40,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-29T23:05:40,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-29T23:05:40,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-29T23:05:40,548][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.3gb], net total_space [125.8gb], types [ext4]
[2022-03-29T23:05:40,550][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-29T23:05:40,942][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-29T23:05:53,526][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-29T23:05:53,536][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-29T23:05:53,539][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-29T23:05:53,545][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-29T23:05:53,558][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T23:05:53,564][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-29T23:05:53,573][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-29T23:05:53,574][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T23:05:53,575][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-29T23:05:53,576][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-29T23:05:53,576][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-29T23:05:53,577][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-29T23:05:53,580][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-29T23:05:53,581][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-29T23:05:53,583][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-29T23:05:54,881][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-29T23:05:55,042][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-29T23:05:56,150][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-29T23:05:57,161][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-29T23:05:57,163][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-29T23:05:57,285][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-29T23:05:57,291][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-29T23:05:57,591][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-29T23:06:02,320][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-29T23:06:02,463][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{8g0PWzQfTHOEm6Vp4jQRHQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 155, version: 4468, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{8g0PWzQfTHOEm6Vp4jQRHQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-29T23:06:02,696][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{8g0PWzQfTHOEm6Vp4jQRHQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 155, version: 4468, reason: Publication{term=155, version=4468}
[2022-03-29T23:06:02,829][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-29T23:06:02,832][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-29T23:06:04,975][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-29T23:06:04,998][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-29T23:06:07,892][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-29T23:06:07,898][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-29T23:07:29,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6231b6ab, interval=5s}] took [10431ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:08:11,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:12:02,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24550550801ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:12:30,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:12:50,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309273729605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:13:05,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6d2b9767, interval=1s}] took [309273ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:13:20,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.7s/49784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:13:39,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.7s/49784398526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:14:49,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86657ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:15:38,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86656327003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:16:03,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77075ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:16:21,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77075796176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:26:34,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/632196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:26:44,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/632195607215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:26:45,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@edd1edc, interval=5s}] took [795927ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:26:55,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21479ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:27:05,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21478807048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:27:17,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21643ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:27:50,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21643021308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:28:02,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45224ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:28:12,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45223853992ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:28:26,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:28:37,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23654676120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:28:48,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:29:02,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22096786444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:29:14,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:29:24,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25969972656ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:29:35,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:29:47,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21740174182ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:30:01,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:30:16,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24643694030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:30:46,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45837ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:31:00,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45837421111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:31:10,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24410ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:31:21,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24409360099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:31:35,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:31:45,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23059659912ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:31:55,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:32:08,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22054801658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:32:26,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:32:41,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29910424854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:32:54,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:33:05,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28087444193ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:33:18,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:33:30,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24234582201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:33:41,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:33:52,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23186836166ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:34:09,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:34:20,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25236234506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:34:31,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:34:41,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24880048141ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:34:53,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:35:03,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21830551394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:35:15,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21563ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:35:29,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21563173053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:35:42,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:35:51,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28370471642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:36:01,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:36:15,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18065331505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:36:27,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:36:38,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26462136730ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:36:49,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21394ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:37:02,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21393265441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:37:15,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:37:25,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26121234318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:37:36,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20561ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:37:49,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20561487397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:38:05,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:38:19,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28548748295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:38:33,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29217ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:38:48,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29217251552ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:39:02,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28123ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:39:13,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28122477952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:39:27,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24680ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:39:44,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24680367738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:39:58,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:40:11,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31426469910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:40:24,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:40:41,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26182470430ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:40:55,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:40:55,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017dd978@41bc29e9] took [839542ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:41:09,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31128516418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:41:37,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:41:51,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41963319547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:42:10,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:42:25,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30533029811ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:42:41,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33128ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:42:57,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33128357584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:43:14,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:43:31,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.2s/32271699508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:43:44,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@21c6dab8] took [32271ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:44:01,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:44:38,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46080324956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:45:10,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:45:39,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66309670352ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:46:30,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82699ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:47:02,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6d2b9767, interval=1s}] took [149008ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:47:24,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82699000058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:47:40,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:47:52,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70402209969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:48:06,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:48:20,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25825664083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:47:43,210][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [82699ms] which is above the warn threshold of [5s]
[2022-03-29T23:48:35,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:48:45,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31899017624ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:48:51,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@edd1edc, interval=5s}] took [31899ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:49:01,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:49:11,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23209106642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:49:22,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:49:34,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23182004428ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:49:42,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:49:54,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20354664107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:50:02,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:50:14,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20343560910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:50:15,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6d2b9767, interval=1s}] took [20343ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:50:23,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:50:36,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19931904749ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:50:48,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:50:57,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25344074882ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:04,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:13,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16603250954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:21,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16333ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:30,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6d2b9767, interval=1s}] took [16333ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:51:30,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16333745063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:40,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:49,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18126218391ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:51:50,889][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [64] timed out after [515450ms]
[2022-03-29T23:51:57,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:06,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17982072904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:14,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17772ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:24,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17772143909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:32,953][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:40,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18299550269ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:16,780][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17772ms] which is above the warn threshold of [5s]
[2022-03-29T23:52:47,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:52:59,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13289861898ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:53:05,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6d2b9767, interval=1s}] took [31589ms] which is above the warn threshold of [5000ms]
[2022-03-29T23:53:09,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:53:16,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22454859257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:53:21,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:53:28,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12056638404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:53:36,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:53:43,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15355075979ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-29T23:56:02,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/124029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-29T23:59:25,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/123719408405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:39:33,302][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-30T00:39:33,361][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-30T00:39:33,362][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-30T00:39:39,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-30T00:39:39,946][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-30T00:39:39,947][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-30T00:39:39,948][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-30T00:39:39,948][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-30T00:39:39,949][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-30T00:39:39,950][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-30T00:39:39,951][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-30T00:39:39,952][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-30T00:39:39,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-30T00:39:39,953][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-30T00:39:39,954][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-30T00:39:39,955][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-30T00:39:39,956][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-30T00:39:39,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-30T00:39:39,957][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-30T00:39:39,958][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-30T00:39:39,959][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-30T00:39:39,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-30T00:39:39,960][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-30T00:39:39,961][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-30T00:39:39,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-30T00:39:39,962][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-30T00:39:39,963][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-30T00:39:39,964][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-30T00:39:39,965][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-30T00:39:39,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-30T00:39:39,966][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-30T00:39:39,967][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-30T00:39:39,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-30T00:39:39,968][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-30T00:39:39,969][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-30T00:39:39,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-30T00:39:39,970][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-30T00:39:39,971][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-30T00:39:39,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-30T00:39:39,972][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-30T00:39:39,973][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-30T00:39:39,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-30T00:39:39,974][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-30T00:39:39,975][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-30T00:39:39,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-30T00:39:39,976][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-30T00:39:39,977][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-30T00:39:39,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-30T00:39:39,978][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-30T00:39:39,979][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-30T00:39:39,979][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-30T00:39:39,980][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-30T00:39:39,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-30T00:39:39,981][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-30T00:39:39,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-30T00:39:39,982][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-30T00:39:39,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-30T00:39:39,984][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-30T00:39:39,984][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-30T00:39:39,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-30T00:39:39,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-30T00:39:39,987][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-30T00:39:40,076][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.4gb], net total_space [125.8gb], types [ext4]
[2022-03-30T00:39:40,077][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-30T00:39:40,431][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-30T00:39:53,604][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-30T00:39:53,612][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-30T00:39:55,513][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-30T00:39:59,089][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-30T00:40:00,594][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-30T00:40:04,146][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-30T00:40:04,226][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-30T00:40:40,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6838ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:40:43,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [6991ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:41:07,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6837680080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:14,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:15,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [40692ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:41:21,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.6s/40692470485ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:30,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:37,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15920027698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:44,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:45,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3f93a40c, interval=1m}] took [14646ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:41:48,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14646912988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:55,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:41:56,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [11592ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:42:03,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11592074175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:42:12,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:42:15,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [16017ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:42:20,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16017200272ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:42:29,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:42:33,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [16313ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:42:39,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16313765120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:42:46,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17289ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:42:48,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [17288ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:42:55,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17288857112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:02,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:06,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3031c174, interval=30s}] took [17218ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:43:08,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17218615777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:15,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3f93a40c, interval=1m}] took [11917ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:43:14,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:20,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11917992740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:24,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:28,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9716765089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:33,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:34,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [9729ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:43:37,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9729906541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:43,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3031c174, interval=30s}] took [9648ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:43:43,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:43:51,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9648223620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:02,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:04,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [17507ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:44:12,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17507619988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:20,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:28,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18797026066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:35,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:42,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15778455302ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:52,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [13203ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:44:52,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:44:59,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13203876970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:45:10,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [17297ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:45:07,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17298ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:45:16,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17297686001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:45:25,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:45:29,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [19570ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:45:33,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19570322671ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:45:47,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [21756ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:45:47,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:45:58,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21756997438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:46:13,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [26194ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:46:13,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26195ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:46:37,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26194350697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:46:41,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:46:42,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [28029ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:46:46,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28029413503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:46:53,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:46:54,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@cc1f8ad, interval=1m}] took [11067ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:47:00,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11067423214ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:09,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [13867ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:47:07,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:18,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13867734138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:29,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:30,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [22208ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:47:36,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22208840378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:44,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14731ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:46,579][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-30T00:47:49,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14730270867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:47:58,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15007ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:48:03,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [15007ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:48:10,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15007294970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:48:25,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:48:24,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [25747ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:48:38,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25747038800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:48:53,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26601ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:48:53,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3f93a40c, interval=1m}] took [26601ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:49:04,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26601443308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:49:17,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25830ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:49:17,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [25829ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:49:30,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25829407189ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:49:39,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:49:49,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21736879467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:49:58,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20010ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:50:01,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@cc1f8ad, interval=1m}] took [20010ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:50:08,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20010474995ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:50:17,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:50:21,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [18402ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:50:27,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18402397271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:50:27,887][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-30T00:50:36,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:50:44,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18368161712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:50:57,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:51:01,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [20411ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:51:10,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20411752533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:51:42,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:51:41,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [45597ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:51:54,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.5s/45597793796ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:52:19,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:52:25,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [36694ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:52:32,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.6s/36694286278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:52:43,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@484a6c33, interval=5s}] took [24593ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:52:43,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:52:51,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24593310151ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:52:59,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16343ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:53:01,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [16343ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:53:11,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16343032579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:53:22,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:53:22,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3f93a40c, interval=1m}] took [22365ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:53:32,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22365058686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:53:47,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:53:51,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [25235ms] which is above the warn threshold of [5000ms]
[2022-03-30T00:53:59,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25235728389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:54:21,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:54:35,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32986105601ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:54:52,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:55:06,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27910477889ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:55:25,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:55:36,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37055070159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:55:54,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:56:14,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27696920188ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:56:34,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:57:07,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38804220805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:57:52,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T00:58:52,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73737375053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T00:59:19,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:00:04,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90309887929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:00:36,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:00:55,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79159731346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:01:14,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:02:46,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.9s/36929771528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:04:12,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:04:04,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@3ef79c21] took [411603ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:04:51,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.9m/176476081317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:05:19,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:05:25,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@cc1f8ad, interval=1m}] took [67291ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:06:25,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67291197014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:07:11,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:08:26,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/111739597403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:08:53,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:09:16,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104455436899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:09:41,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.9s/47926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:10:07,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.9s/47925293372ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:10:25,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:10:42,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42760404594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:10:34,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [195141ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:10:58,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:11:24,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.2s/34201092364ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:11:47,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.3s/49335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:11:01,560][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [195141ms] which is above the warn threshold of [5s]
[2022-03-30T01:12:04,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.3s/49334938985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:12:27,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:12:47,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.3s/35356689452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:13:06,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43447ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:13:28,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.4s/43446862378ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:13:53,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:14:17,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46376254859ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:14:20,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [89823ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:14:39,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.9s/45973ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:15:03,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.9s/45973325928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:15:45,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@cc1f8ad, interval=1m}] took [64522ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:15:45,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64523ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:16:16,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64522363473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:16:49,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:17:20,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67411620266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:17:49,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:18:18,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.9s/56953619326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:18:52,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:19:30,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [187566ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:19:31,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63201049519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:20:27,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:18:35,268][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [56954ms] which is above the warn threshold of [5s]
[2022-03-30T01:20:59,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92809108230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:21:51,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:22:50,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78057242035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:24:00,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:25:04,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125186486615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:26:15,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:26:40,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [247077ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:27:52,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121891247213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:31:19,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:31:25,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@3ef79c21] took [326913ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:33:47,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326913598232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:36:57,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:39:23,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/329513448005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:42:13,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/317274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:42:32,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [646787ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:44:35,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/317273998769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:41:12,116][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [329513ms] which is above the warn threshold of [5s]
[2022-03-30T01:47:10,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/299106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:47:42,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [298966ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:50:16,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/298966946701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T01:53:16,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3f93a40c, interval=1m}] took [366623ms] which is above the warn threshold of [5000ms]
[2022-03-30T01:53:40,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T01:56:55,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366623929820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:00:18,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:03:14,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407016325432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:06:15,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [777669ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:06:22,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:09:28,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370653509492ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:12:45,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382696ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:12:54,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@3ef79c21] took [382700ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:15:33,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/382700519460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:18:38,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351299ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:20:05,174][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [351755ms] which is above the warn threshold of [5s]
[2022-03-30T02:21:20,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351754430999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:24:42,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:27:52,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363083917528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:31:15,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/395926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:31:59,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@985ee4c, interval=1s}] took [759351ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:34:10,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396267735350ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:36:58,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:37:15,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@2b011bd3, interval=5s}] took [347813ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:39:56,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/347813650977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:43:38,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:43:36,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3f93a40c, interval=1m}] took [395779ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:47:19,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/395779319301ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:56:58,665][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-30T02:56:58,698][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-30T02:56:58,701][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-30T02:57:15,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-30T02:57:15,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-30T02:57:15,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-30T02:57:15,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-30T02:57:15,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-30T02:57:15,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-30T02:57:15,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-30T02:57:15,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-30T02:57:15,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-30T02:57:15,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-30T02:57:15,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-30T02:57:15,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-30T02:57:15,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-30T02:57:15,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-30T02:57:15,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-30T02:57:15,478][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-30T02:57:15,480][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-30T02:57:15,482][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-30T02:57:15,482][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-30T02:57:15,488][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-30T02:57:15,489][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-30T02:57:15,491][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-30T02:57:15,492][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-30T02:57:15,496][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-30T02:57:15,498][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-30T02:57:15,499][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-30T02:57:15,502][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-30T02:57:15,502][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-30T02:57:15,504][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-30T02:57:15,505][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-30T02:57:15,507][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-30T02:57:15,510][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-30T02:57:15,513][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-30T02:57:15,515][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-30T02:57:15,517][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-30T02:57:15,519][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-30T02:57:15,520][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-30T02:57:15,522][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-30T02:57:15,524][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-30T02:57:15,527][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-30T02:57:15,532][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-30T02:57:15,533][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-30T02:57:15,534][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-30T02:57:15,534][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-30T02:57:15,535][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-30T02:57:15,535][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-30T02:57:15,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-30T02:57:15,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-30T02:57:15,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-30T02:57:15,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-30T02:57:15,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-30T02:57:15,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-30T02:57:15,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-30T02:57:15,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-30T02:57:15,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-30T02:57:15,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-30T02:57:15,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-30T02:57:15,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-30T02:57:15,558][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-30T02:57:15,725][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.4gb], net total_space [125.8gb], types [ext4]
[2022-03-30T02:57:15,728][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-30T02:57:16,347][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-30T02:57:44,176][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-30T02:57:44,182][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-30T02:57:45,729][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-30T02:57:45,932][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-30T02:57:46,933][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-30T02:57:47,806][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-30T02:57:47,808][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-30T02:57:47,896][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-30T02:57:47,898][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-30T02:57:48,157][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-30T02:57:50,930][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-30T02:57:51,394][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 156, version: 4475, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-30T02:57:52,043][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 156, version: 4475, reason: Publication{term=156, version=4475}
[2022-03-30T02:57:52,255][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-30T02:57:52,256][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-30T02:57:54,785][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-30T02:57:54,818][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [32] indices into cluster_state
[2022-03-30T02:58:54,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5181810500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:58:56,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [38843ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:58:57,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T02:58:58,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15249613847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T02:59:05,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@5f784320] took [8312ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:59:09,857][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51350}] took [9329ms] which is above the warn threshold of [5000ms]
[2022-03-30T02:59:36,712][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=156, version=4477}] took [1.6m] which is above the warn threshold of [30s]: [running task [Publication{term=156, version=4477}]] took [3ms], [connecting to new nodes] took [15ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@1a2c6398] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@73faa96a] took [98688ms], [org.elasticsearch.script.ScriptService@3526ac1b] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [0ms], [org.elasticsearch.snapshots.RestoreService@347cdee] took [0ms], [org.elasticsearch.ingest.IngestService@25508515] took [16ms], [org.elasticsearch.action.ingest.IngestActionForwarder@2596a3d0] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce640@2d9c4487] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1089c467] took [0ms], [org.elasticsearch.tasks.TaskManager@31d0834e] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@21c83e87] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@49eeb16e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@39954ea3] took [0ms], [org.elasticsearch.indices.SystemIndexManager@34d4830f] took [32ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@79c15417] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@691b1047] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7ab4377f] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@54968cb3] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@7b017545] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@659c610a] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@80b0243] took [195ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4a1eff4c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5a0b36b2] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@67524620] took [35ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@6f0a9ac8] took [60ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7476cc26] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@77cbcabc] took [20ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [27ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2ddf123] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@393ae76a] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@3f6107ed] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@f263156] took [2ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6fed69da] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@61c841e3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@11def15c] took [0ms], [org.elasticsearch.node.ResponseCollectorService@15f9f904] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@3e629292] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@1d060be4] took [1ms], [org.elasticsearch.shutdown.PluginShutdownService@7b345d09] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@45fabb06] took [0ms], [org.elasticsearch.indices.store.IndicesStore@44078460] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@46146beb] took [0ms], [org.elasticsearch.license.LicenseService@72bfb7ec] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1a51ac93] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@66e71974] took [0ms], [org.elasticsearch.gateway.GatewayService@1ee67e8a] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@f4a1e13] took [0ms]
[2022-03-30T02:59:38,952][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-30T02:59:39,026][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-30T02:59:41,915][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-03-30T03:00:54,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:01:33,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [40633ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:02:02,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39632726194ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:02:43,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:03:03,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/109441715401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:03:20,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53986ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:03:26,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [163651ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:03:35,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.2s/54209889388ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:03:49,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30553ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:03:49,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67886eb9, interval=5s}] took [30552ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:03:51,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30552326653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:03:59,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:04:01,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9722455068ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:04:10,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:04:11,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11200088306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:04:14,016][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-30T03:04:13,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@504de2c3] took [15158ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:04:14,404][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-30T03:04:14,405][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-30T03:04:15,382][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=156, version=4485}] took [4.3m] which is above the warn threshold of [30s]: [running task [Publication{term=156, version=4485}]] took [0ms], [connecting to new nodes] took [43ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@1a2c6398] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@73faa96a] took [212262ms], [org.elasticsearch.script.ScriptService@3526ac1b] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [0ms], [org.elasticsearch.snapshots.RestoreService@347cdee] took [0ms], [org.elasticsearch.ingest.IngestService@25508515] took [49761ms], [org.elasticsearch.action.ingest.IngestActionForwarder@2596a3d0] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce640@2d9c4487] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1089c467] took [30ms], [org.elasticsearch.tasks.TaskManager@31d0834e] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@21c83e87] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@49eeb16e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@39954ea3] took [1ms], [org.elasticsearch.indices.SystemIndexManager@34d4830f] took [0ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@79c15417] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@691b1047] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7ab4377f] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@54968cb3] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@7b017545] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@659c610a] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@80b0243] took [451ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4a1eff4c] took [1ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5a0b36b2] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@67524620] took [23ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@6f0a9ac8] took [173ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7476cc26] took [40ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@77cbcabc] took [59ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2ddf123] took [23ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@393ae76a] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@3f6107ed] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@f263156] took [85ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6fed69da] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@61c841e3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@11def15c] took [0ms], [org.elasticsearch.node.ResponseCollectorService@15f9f904] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@3e629292] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@1d060be4] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@7b345d09] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@45fabb06] took [0ms], [org.elasticsearch.indices.store.IndicesStore@44078460] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@46146beb] took [0ms], [org.elasticsearch.license.LicenseService@72bfb7ec] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1a51ac93] took [17ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@66e71974] took [0ms], [org.elasticsearch.gateway.GatewayService@1ee67e8a] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@f4a1e13] took [0ms]
[2022-03-30T03:04:24,067][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-03-30T03:04:24,264][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-03-30T03:04:54,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:05:04,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20674858501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:05:15,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:05:19,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [20277ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:05:21,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20277650203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:05:32,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:05:40,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17000562411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:05:53,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:03,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20572840198ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:13,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20052ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:22,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20051738215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:31,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:34,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19334587203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:46,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [88203ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:06:46,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:06:55,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11243733966ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:07:01,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:07:18,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17920887091ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:07:28,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:07:34,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26492329110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:07:42,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:08:07,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14576504970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:08:52,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68814ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:09:18,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/68813447403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:09:21,581][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51408}] took [216006ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:09:23,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:09:45,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32332886864ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:09:49,612][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:10:14,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26048854712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:10:39,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49498ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:10:43,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49498055904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:11:03,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24716ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:11:07,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24715655105ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:11:09,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5689ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:11:10,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5689270660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:11:10,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@3ab453aa] took [207098ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:11:24,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [6461ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:11:39,023][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=156, version=4494}] took [7.1m] which is above the warn threshold of [30s]: [running task [Publication{term=156, version=4494}]] took [0ms], [connecting to new nodes] took [37ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@1a2c6398] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@73faa96a] took [399216ms], [org.elasticsearch.script.ScriptService@3526ac1b] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [0ms], [org.elasticsearch.snapshots.RestoreService@347cdee] took [0ms], [org.elasticsearch.ingest.IngestService@25508515] took [115ms], [org.elasticsearch.action.ingest.IngestActionForwarder@2596a3d0] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce640@2d9c4487] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1089c467] took [0ms], [org.elasticsearch.tasks.TaskManager@31d0834e] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@21c83e87] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@49eeb16e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@39954ea3] took [0ms], [org.elasticsearch.indices.SystemIndexManager@34d4830f] took [0ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@79c15417] took [1ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@691b1047] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7ab4377f] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@54968cb3] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@7b017545] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@659c610a] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@80b0243] took [24037ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4a1eff4c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5a0b36b2] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@67524620] took [308ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@6f0a9ac8] took [28ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7476cc26] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@77cbcabc] took [61ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2ddf123] took [2327ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@393ae76a] took [15ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@3f6107ed] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@f263156] took [1006ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6fed69da] took [48ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@61c841e3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@11def15c] took [43ms], [org.elasticsearch.node.ResponseCollectorService@15f9f904] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@3e629292] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@1d060be4] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@7b345d09] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@45fabb06] took [0ms], [org.elasticsearch.indices.store.IndicesStore@44078460] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@46146beb] took [0ms], [org.elasticsearch.license.LicenseService@72bfb7ec] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1a51ac93] took [32ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@66e71974] took [0ms], [org.elasticsearch.gateway.GatewayService@1ee67e8a] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@f4a1e13] took [0ms]
[2022-03-30T03:12:00,727][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][13] duration [2.7s], collections [1]/[8.9s], total [2.7s]/[3.4s], memory [1.2gb]->[136.5mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [71mb]->[71mb]/[2gb]}{[survivor] [2.8mb]->[65.4mb]/[0b]}
[2022-03-30T03:12:01,094][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [2.7s] collecting in the last [8.9s]
[2022-03-30T03:12:02,990][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-03-30T03:12:05,080][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-03-30T03:12:24,064][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][15] duration [1.2s], collections [1]/[2.6s], total [1.2s]/[4.9s], memory [221.5mb]->[145.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [135.5mb]->[135.5mb]/[2gb]}{[survivor] [6mb]->[10mb]/[0b]}
[2022-03-30T03:12:24,225][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [1.2s] collecting in the last [2.6s]
[2022-03-30T03:12:33,840][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-03-30T03:12:35,459][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][101] overhead, spent [659ms] collecting in the last [1.3s]
[2022-03-30T03:12:41,271][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][103][18] duration [1.8s], collections [1]/[4s], total [1.8s]/[7.5s], memory [184mb]->[159.9mb]/[2gb], all_pools {[young] [40mb]->[8mb]/[0b]}{[old] [137mb]->[144.3mb]/[2gb]}{[survivor] [15mb]->[15.5mb]/[0b]}
[2022-03-30T03:12:41,523][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][103] overhead, spent [1.8s] collecting in the last [4s]
[2022-03-30T03:12:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [11063ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:15:32,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:18:36,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.4s/45433425812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:19:32,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:19:45,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324895354019ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:19:59,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:20:19,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26930403191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:20:31,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:20:34,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67886eb9, interval=5s}] took [32963ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:20:41,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32963300017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:20:52,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20312ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:20:18,281][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51438}] took [351826ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:21:02,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20311546738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:21:16,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24231ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:21:28,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24230876103ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:21:41,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:21:52,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23511354313ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:05,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:19,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25249866257ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:30,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:37,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [50354ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:22:38,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25104802787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:40,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11491ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:46,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11491086051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:49,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:50,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8374415137ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:54,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:54,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5178022394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:22:55,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@52645990] took [6665ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:23:11,911][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [158830ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [28] unchanged indices
[2022-03-30T03:23:26,348][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][112][19] duration [8.5s], collections [1]/[11.6s], total [8.5s]/[16.1s], memory [235.9mb]->[169.8mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [144.3mb]->[153.8mb]/[2gb]}{[survivor] [15.5mb]->[16mb]/[0b]}
[2022-03-30T03:23:26,439][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.7m] publication of cluster state version [4501] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T03:23:26,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:23:26,764][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][112] overhead, spent [8.5s] collecting in the last [11.6s]
[2022-03-30T03:23:26,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10148016134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:23:31,681][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][116] overhead, spent [460ms] collecting in the last [1.1s]
[2022-03-30T03:23:32,944][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T03:23:32,769][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T03:23:36,438][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][119][21] duration [723ms], collections [1]/[1.8s], total [723ms]/[17.3s], memory [218.3mb]->[177.1mb]/[2gb], all_pools {[young] [48mb]->[4mb]/[0b]}{[old] [161.3mb]->[161.3mb]/[2gb]}{[survivor] [9mb]->[15.8mb]/[0b]}
[2022-03-30T03:23:36,959][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][119] overhead, spent [723ms] collecting in the last [1.8s]
[2022-03-30T03:23:37,476][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-30T03:23:37,502][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-30T03:23:29,503][WARN ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] could not delete old chunks for geoip database [GeoLite2-ASN.mmdb]
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.SearchScrollAsyncAction.onShardFailure(SearchScrollAsyncAction.java:293) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction$1.onFailure(SearchScrollAsyncAction.java:193) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:1086) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.access$100(TransportService.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$3.sendRequest(TransportService.java:143) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendRequestInternal(TransportService.java:975) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:874) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendChildRequest(TransportService.java:937) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendChildRequest(TransportService.java:925) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService.sendExecuteScrollFetch(SearchTransportService.java:295) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollQueryAndFetchAsyncAction.executeInitialPhase(SearchScrollQueryAndFetchAsyncAction.java:49) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.run(SearchScrollAsyncAction.java:203) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.lambda$run$0(SearchScrollAsyncAction.java:94) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.collectNodesAndRun(SearchScrollAsyncAction.java:116) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.run(SearchScrollAsyncAction.java:90) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchScrollAction.doExecute(TransportSearchScrollAction.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchScrollAction.doExecute(TransportSearchScrollAction.java:24) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.ParentTaskAssigningClient.doExecute(ParentTaskAssigningClient.java:55) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.searchScroll(AbstractClient.java:562) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ClientScrollableHitSource.doStartNextScroll(ClientScrollableHitSource.java:83) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ScrollableHitSource.startNextScroll(ScrollableHitSource.java:90) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ScrollableHitSource.startNextScroll(ScrollableHitSource.java:86) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ScrollableHitSource$1.done(ScrollableHitSource.java:107) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction$ScrollConsumableHitsResponse.done(AbstractAsyncBulkByScrollAction.java:1007) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction.notifyDone(AbstractAsyncBulkByScrollAction.java:499) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction.lambda$prepareBulkRequest$1(AbstractAsyncBulkByScrollAction.java:396) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction.onBulkResponse(AbstractAsyncBulkByScrollAction.java:482) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction$2.onResponse(AbstractAsyncBulkByScrollAction.java:421) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction$2.onResponse(AbstractAsyncBulkByScrollAction.java:417) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.Retry$RetryHandler.finishHim(Retry.java:168) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onResponse(Retry.java:105) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onResponse(Retry.java:71) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:88) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$RunBeforeActionListener.onResponse(ActionListener.java:389) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$1.finishHim(TransportBulkAction.java:625) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$1.onResponse(TransportBulkAction.java:601) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$1.onResponse(TransportBulkAction.java:590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:88) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishOnSuccess(TransportReplicationAction.java:1070) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:978) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:969) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleResponse(TransportService.java:847) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1549) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1529) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:33) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:16) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$RunBeforeActionListener.onResponse(ActionListener.java:389) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.lambda$runWithPrimaryShardReference$2(TransportReplicationAction.java:494) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.finish(ReplicationOperation.java:396) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.decPendingAndFinishIfNeeded(ReplicationOperation.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.access$300(ReplicationOperation.java:46) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation$1.onResponse(ReplicationOperation.java:160) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation$1.onResponse(ReplicationOperation.java:152) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$WritePrimaryResult$1.onSuccess(TransportWriteAction.java:286) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$AsyncAfterWriteAction.maybeFinish(TransportWriteAction.java:428) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$AsyncAfterWriteAction.lambda$run$1(TransportWriteAction.java:457) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AsyncIOProcessor.notifyList(AsyncIOProcessor.java:111) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AsyncIOProcessor.drainAndProcessAndRelease(AsyncIOProcessor.java:89) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AsyncIOProcessor.put(AsyncIOProcessor.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.sync(IndexShard.java:3754) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$AsyncAfterWriteAction.run(TransportWriteAction.java:455) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$WritePrimaryResult.runPostReplicationActions(TransportWriteAction.java:293) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.handlePrimaryResult(ReplicationOperation.java:152) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener.completeWith(ActionListener.java:447) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction$2.finishRequest(TransportShardBulkAction.java:233) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:196) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:134) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:196) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query+fetch/scroll]]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [10]
	at org.elasticsearch.search.SearchService.findReaderContext(SearchService.java:824) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:747) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService.lambda$registerRequestHandler$12(SearchTransportService.java:547) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:67) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:1037) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 89 more
[2022-03-30T03:23:38,881][WARN ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] could not delete old chunks for geoip database [GeoLite2-City.mmdb]
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.SearchScrollAsyncAction.onShardFailure(SearchScrollAsyncAction.java:293) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction$1.onFailure(SearchScrollAsyncAction.java:193) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:48) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:651) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleException(TransportService.java:853) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1481) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1564) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:1086) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.access$100(TransportService.java:66) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$3.sendRequest(TransportService.java:143) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendRequestInternal(TransportService.java:975) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:874) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendChildRequest(TransportService.java:937) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendChildRequest(TransportService.java:925) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService.sendExecuteScrollFetch(SearchTransportService.java:295) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollQueryAndFetchAsyncAction.executeInitialPhase(SearchScrollQueryAndFetchAsyncAction.java:49) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.run(SearchScrollAsyncAction.java:203) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.lambda$run$0(SearchScrollAsyncAction.java:94) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.collectNodesAndRun(SearchScrollAsyncAction.java:116) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchScrollAsyncAction.run(SearchScrollAsyncAction.java:90) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchScrollAction.doExecute(TransportSearchScrollAction.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchScrollAction.doExecute(TransportSearchScrollAction.java:24) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.ParentTaskAssigningClient.doExecute(ParentTaskAssigningClient.java:55) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.searchScroll(AbstractClient.java:562) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ClientScrollableHitSource.doStartNextScroll(ClientScrollableHitSource.java:83) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ScrollableHitSource.startNextScroll(ScrollableHitSource.java:90) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ScrollableHitSource.startNextScroll(ScrollableHitSource.java:86) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.reindex.ScrollableHitSource$1.done(ScrollableHitSource.java:107) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction$ScrollConsumableHitsResponse.done(AbstractAsyncBulkByScrollAction.java:1007) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction.notifyDone(AbstractAsyncBulkByScrollAction.java:499) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction.lambda$prepareBulkRequest$1(AbstractAsyncBulkByScrollAction.java:396) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction.onBulkResponse(AbstractAsyncBulkByScrollAction.java:482) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction$2.onResponse(AbstractAsyncBulkByScrollAction.java:421) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.reindex.AbstractAsyncBulkByScrollAction$2.onResponse(AbstractAsyncBulkByScrollAction.java:417) [reindex-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.Retry$RetryHandler.finishHim(Retry.java:168) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onResponse(Retry.java:105) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.Retry$RetryHandler.onResponse(Retry.java:71) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:88) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$RunBeforeActionListener.onResponse(ActionListener.java:389) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$1.finishHim(TransportBulkAction.java:625) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$1.onResponse(TransportBulkAction.java:601) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportBulkAction$BulkOperation$1.onResponse(TransportBulkAction.java:590) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:88) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishOnSuccess(TransportReplicationAction.java:1070) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:978) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:969) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$4.handleResponse(TransportService.java:847) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1549) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1529) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:41) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:33) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:16) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$RunBeforeActionListener.onResponse(ActionListener.java:389) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.lambda$runWithPrimaryShardReference$2(TransportReplicationAction.java:494) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.finish(ReplicationOperation.java:396) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.decPendingAndFinishIfNeeded(ReplicationOperation.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.access$300(ReplicationOperation.java:46) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation$1.onResponse(ReplicationOperation.java:160) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation$1.onResponse(ReplicationOperation.java:152) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$WritePrimaryResult$1.onSuccess(TransportWriteAction.java:286) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$AsyncAfterWriteAction.maybeFinish(TransportWriteAction.java:428) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$AsyncAfterWriteAction.lambda$run$1(TransportWriteAction.java:457) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AsyncIOProcessor.notifyList(AsyncIOProcessor.java:111) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AsyncIOProcessor.drainAndProcessAndRelease(AsyncIOProcessor.java:89) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AsyncIOProcessor.put(AsyncIOProcessor.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.shard.IndexShard.sync(IndexShard.java:3754) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$AsyncAfterWriteAction.run(TransportWriteAction.java:455) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$WritePrimaryResult.runPostReplicationActions(TransportWriteAction.java:293) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.ReplicationOperation.handlePrimaryResult(ReplicationOperation.java:152) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener.completeWith(ActionListener.java:447) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction$2.finishRequest(TransportShardBulkAction.java:233) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:196) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:134) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:196) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.transport.RemoteTransportException: [tpotcluster-node-01][127.0.0.1:9300][indices:data/read/search[phase/query+fetch/scroll]]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [11]
	at org.elasticsearch.search.SearchService.findReaderContext(SearchService.java:824) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:747) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.SearchTransportService.lambda$registerRequestHandler$12(SearchTransportService.java:547) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:67) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:1037) ~[elasticsearch-7.17.0.jar:7.17.0]
	... 89 more
[2022-03-30T03:23:40,884][ERROR][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] failed to download database [GeoLite2-City.mmdb]
org.elasticsearch.ResourceNotFoundException: chunk document with id [GeoLite2-City.mmdb_610_1648559855479] not found
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:371) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-30T03:23:44,948][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-30T03:23:46,931][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T03:23:47,040][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T03:23:49,923][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][22] duration [865ms], collections [1]/[1.9s], total [865ms]/[18.1s], memory [237.1mb]->[178.1mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [161.3mb]->[174.9mb]/[2gb]}{[survivor] [15.8mb]->[3.1mb]/[0b]}
[2022-03-30T03:23:50,263][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [865ms] collecting in the last [1.9s]
[2022-03-30T03:23:50,479][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T03:23:51,016][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T03:24:16,644][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-30T03:24:22,746][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][143][24] duration [1.8s], collections [1]/[1.8s], total [1.8s]/[20.4s], memory [244.8mb]->[260.8mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [174.9mb]->[174.9mb]/[2gb]}{[survivor] [5.9mb]->[5.8mb]/[0b]}
[2022-03-30T03:24:23,514][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][143] overhead, spent [1.8s] collecting in the last [1.8s]
[2022-03-30T03:24:35,037][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [600ms] collecting in the last [1.6s]
[2022-03-30T03:24:35,137][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51460}] took [12978ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:24:39,806][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.03.30] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-03-30T03:24:43,786][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T03:25:01,346][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_monitoring/bulk?system_id=kibana&system_api_version=7&interval=10000ms][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53086}] took [5619ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:25:05,035][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.03.30][0]]]).
[2022-03-30T03:25:37,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [10408ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:26:13,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][195][26] duration [1.5s], collections [1]/[2.6s], total [1.5s]/[22.5s], memory [264.4mb]->[179.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [174.9mb]->[174.9mb]/[2gb]}{[survivor] [5.4mb]->[4.9mb]/[0b]}
[2022-03-30T03:26:13,474][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][195] overhead, spent [1.5s] collecting in the last [2.6s]
[2022-03-30T03:26:13,312][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA] update_mapping [_doc]
[2022-03-30T03:26:13,672][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.8s/11897ms] to compute cluster state update for [put-mapping [logstash-2022.03.29/4CNLAl-yQqmObSIQcEvFWA][_doc]], which exceeds the warn threshold of [10s]
[2022-03-30T03:26:14,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:46186}] took [14929ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:26:19,468][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:19,588][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:21,647][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:23,772][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:23,918][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:25,487][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:26,094][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:26,510][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:26,916][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:26,932][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:31,110][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:31,904][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:43,421][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.8s/10886ms] to compute cluster state update for [put-mapping [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw][_doc]], which exceeds the warn threshold of [10s]
[2022-03-30T03:26:46,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [13465ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:26:54,474][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][215][31] duration [1.6s], collections [1]/[3.8s], total [1.6s]/[24.8s], memory [235.3mb]->[184.3mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [176.3mb]->[176.3mb]/[2gb]}{[survivor] [7mb]->[8mb]/[0b]}
[2022-03-30T03:26:54,605][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][215] overhead, spent [1.6s] collecting in the last [3.8s]
[2022-03-30T03:26:55,275][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:26:58,299][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:27:03,070][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][220][32] duration [1.7s], collections [1]/[3.6s], total [1.7s]/[26.6s], memory [264.3mb]->[185.8mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [176.3mb]->[176.3mb]/[2gb]}{[survivor] [8mb]->[9.4mb]/[0b]}
[2022-03-30T03:27:03,656][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][220] overhead, spent [1.7s] collecting in the last [3.6s]
[2022-03-30T03:27:11,408][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][224][33] duration [1.3s], collections [1]/[2.5s], total [1.3s]/[28s], memory [253.8mb]->[184.2mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [176.3mb]->[177mb]/[2gb]}{[survivor] [9.4mb]->[7.2mb]/[0b]}
[2022-03-30T03:27:11,582][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][224] overhead, spent [1.3s] collecting in the last [2.5s]
[2022-03-30T03:27:11,923][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:27:24,501][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10401ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [32] unchanged indices
[2022-03-30T03:27:30,379][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [18.1s] publication of cluster state version [4521] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T03:28:05,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67886eb9, interval=5s}] took [11145ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:28:05,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:28:06,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10900608838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:28:07,553][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53078}] took [26364ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:28:07,847][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][237][34] duration [7.9s], collections [1]/[17s], total [7.9s]/[36s], memory [252.2mb]->[184.6mb]/[2gb], all_pools {[young] [71.9mb]->[0b]/[0b]}{[old] [177mb]->[177mb]/[2gb]}{[survivor] [7.2mb]->[7.5mb]/[0b]}
[2022-03-30T03:28:08,209][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][237] overhead, spent [7.9s] collecting in the last [17s]
[2022-03-30T03:28:18,325][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=156, version=4521}] took [42.8s] which is above the warn threshold of [30s]: [running task [Publication{term=156, version=4521}]] took [188ms], [connecting to new nodes] took [479ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@1a2c6398] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@73faa96a] took [4881ms], [org.elasticsearch.script.ScriptService@3526ac1b] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [0ms], [org.elasticsearch.snapshots.RestoreService@347cdee] took [0ms], [org.elasticsearch.ingest.IngestService@25508515] took [276ms], [org.elasticsearch.action.ingest.IngestActionForwarder@2596a3d0] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce640@2d9c4487] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1089c467] took [0ms], [org.elasticsearch.tasks.TaskManager@31d0834e] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@21c83e87] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@49eeb16e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@39954ea3] took [0ms], [org.elasticsearch.indices.SystemIndexManager@34d4830f] took [3440ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@79c15417] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@691b1047] took [424ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7ab4377f] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@54968cb3] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406c08@7b017545] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@659c610a] took [113ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@80b0243] took [27946ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4a1eff4c] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@5a0b36b2] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@67524620] took [3191ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@6f0a9ac8] took [0ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7476cc26] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@77cbcabc] took [910ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [38ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2ddf123] took [215ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@393ae76a] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@3f6107ed] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@f263156] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6fed69da] took [41ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@61c841e3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@11def15c] took [190ms], [org.elasticsearch.node.ResponseCollectorService@15f9f904] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@3e629292] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@1d060be4] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@7b345d09] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@45fabb06] took [0ms], [org.elasticsearch.indices.store.IndicesStore@44078460] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@46146beb] took [0ms], [org.elasticsearch.license.LicenseService@72bfb7ec] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1a51ac93] took [43ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@66e71974] took [0ms], [org.elasticsearch.gateway.GatewayService@1ee67e8a] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@f4a1e13] took [0ms]
[2022-03-30T03:28:27,825][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:28:41,859][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11.6s] publication of cluster state version [4522] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T03:28:54,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:28:55,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8983320563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:28:55,943][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][254][35] duration [7s], collections [1]/[10.4s], total [7s]/[43s], memory [256.6mb]->[188.8mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [177mb]->[177mb]/[2gb]}{[survivor] [7.5mb]->[11.8mb]/[0b]}
[2022-03-30T03:28:56,264][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][254] overhead, spent [7s] collecting in the last [10.4s]
[2022-03-30T03:28:56,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [8983ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:29:30,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:29:30,870][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][265][36] duration [8.7s], collections [1]/[12.3s], total [8.7s]/[51.7s], memory [256.8mb]->[191mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [177mb]->[180.2mb]/[2gb]}{[survivor] [11.8mb]->[10.8mb]/[0b]}
[2022-03-30T03:29:30,975][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][265] overhead, spent [8.7s] collecting in the last [12.3s]
[2022-03-30T03:29:30,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10871676647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:29:30,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [10871ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:29:31,810][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:29:36,314][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][266][37] duration [2.6s], collections [1]/[1.7s], total [2.6s]/[54.4s], memory [191mb]->[190.3mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [180.2mb]->[182.3mb]/[2gb]}{[survivor] [10.8mb]->[8mb]/[0b]}
[2022-03-30T03:29:36,559][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][266] overhead, spent [2.6s] collecting in the last [1.7s]
[2022-03-30T03:29:42,096][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][267][38] duration [2.8s], collections [1]/[9.1s], total [2.8s]/[57.2s], memory [190.3mb]->[188.6mb]/[2gb], all_pools {[young] [0b]->[8mb]/[0b]}{[old] [182.3mb]->[182.3mb]/[2gb]}{[survivor] [8mb]->[6.3mb]/[0b]}
[2022-03-30T03:29:42,460][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][267] overhead, spent [2.8s] collecting in the last [9.1s]
[2022-03-30T03:29:42,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [5048ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:29:53,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@128b752f] took [5049ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:30:07,225][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.8s/11842ms] to compute cluster state update for [put-mapping [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw][_doc]], which exceeds the warn threshold of [10s]
[2022-03-30T03:30:08,778][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.30/Lx7u_woySQuQ_zD8F6Y2bw] update_mapping [_doc]
[2022-03-30T03:30:14,556][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][277][39] duration [2.5s], collections [1]/[1.2s], total [2.5s]/[59.8s], memory [260.6mb]->[280.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [182.3mb]->[182.3mb]/[2gb]}{[survivor] [6.3mb]->[8.8mb]/[0b]}
[2022-03-30T03:30:15,100][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][277] overhead, spent [2.5s] collecting in the last [1.2s]
[2022-03-30T03:30:15,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [5773ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:30:20,627][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][278][40] duration [3.5s], collections [1]/[10.7s], total [3.5s]/[1m], memory [280.6mb]->[191.7mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [182.3mb]->[182.7mb]/[2gb]}{[survivor] [8.8mb]->[9mb]/[0b]}
[2022-03-30T03:30:21,038][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][278] overhead, spent [3.5s] collecting in the last [10.7s]
[2022-03-30T03:30:32,140][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][283][41] duration [2.4s], collections [1]/[1.6s], total [2.4s]/[1m], memory [251.7mb]->[275.7mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [182.7mb]->[182.7mb]/[2gb]}{[survivor] [9mb]->[6.8mb]/[0b]}
[2022-03-30T03:30:32,386][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][283] overhead, spent [2.4s] collecting in the last [1.6s]
[2022-03-30T03:30:51,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [5350ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:30:56,797][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53168}] took [13519ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:31:12,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@15d9d2b9] took [6220ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:31:10,513][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6709ms] which is above the warn threshold of [5s]
[2022-03-30T03:31:50,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:31:52,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30483779891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:31:55,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:31:58,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5384358260ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:32:00,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5205ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:32:00,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5204378547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:32:00,158][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][291][42] duration [25.2s], collections [1]/[53.8s], total [25.2s]/[1.5m], memory [249.5mb]->[191.2mb]/[2gb], all_pools {[young] [64mb]->[0b]/[0b]}{[old] [182.7mb]->[182.7mb]/[2gb]}{[survivor] [6.8mb]->[8.5mb]/[0b]}
[2022-03-30T03:32:00,390][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][291] overhead, spent [25.2s] collecting in the last [53.8s]
[2022-03-30T03:32:00,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [5204ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:32:06,196][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][292][43] duration [2.6s], collections [1]/[5.2s], total [2.6s]/[1.5m], memory [191.2mb]->[275.2mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [182.7mb]->[182.7mb]/[2gb]}{[survivor] [8.5mb]->[8mb]/[0b]}
[2022-03-30T03:32:06,799][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][292] overhead, spent [2.6s] collecting in the last [5.2s]
[2022-03-30T03:32:07,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [6142ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:32:26,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [8052ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:32:42,198][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51564}] took [9063ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:32:46,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [7148ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:33:24,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@2b649495] took [29768ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:33:26,577][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51564}] took [34302ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:33:56,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:03,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6419006605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:00,458][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51564}] took [10219ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:34:08,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:15,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12757859072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:25,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:32,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15392421314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:36,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [28150ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:34:40,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:34:52,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15335051136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:35:06,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:35:13,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25569887822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:35:20,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15471ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:35:27,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15470692274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:35:44,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:35:58,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23122442002ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:36:10,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:36:26,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27095632262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:36:42,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:36:19,141][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [27096ms] which is above the warn threshold of [5s]
[2022-03-30T03:36:44,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [30929ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:36:45,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30929027943ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:36:57,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14850ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:36:57,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [14850ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:37:05,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14850294860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:37:09,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:37:10,447][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53176}] took [59214ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:37:12,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13434451595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:37:21,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [20953ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:37:46,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [9052ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:37:59,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:01,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12196202622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:16,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:17,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [8754ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:38:25,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7079926707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:31,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:36,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15111431760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:41,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9568ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:44,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [24679ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:38:44,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9567680300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:50,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:55,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7918504285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:38:58,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:39:03,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9387583065ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:39:01,615][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1151] timed out after [166392ms]
[2022-03-30T03:39:03,648][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.5m/333996ms] ago, timed out [2.7m/167604ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1151]
[2022-03-30T03:39:04,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:39:04,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5974878209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:39:13,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [6617ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:39:34,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [11699ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:39:30,514][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10920ms] which is above the warn threshold of [5s]
[2022-03-30T03:40:01,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [10327ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:40:27,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@64cd4510] took [12945ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:40:48,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [9800ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:41:17,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [9980ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:41:15,558][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1228] timed out after [39772ms]
[2022-03-30T03:41:33,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [6408ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:41:42,914][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.2m/75379ms] ago, timed out [35.6s/35607ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1228]
[2022-03-30T03:41:57,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [11740ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:41:51,995][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6352ms] which is above the warn threshold of [5s]
[2022-03-30T03:42:17,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [7346ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:42:41,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@af1a99a] took [17243ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:43:18,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [9744ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:44:35,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [8784ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:45:12,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67886eb9, interval=5s}] took [16599ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:45:51,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [14528ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:46:12,859][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/205908ms] ago, timed out [1.6m/101310ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yo_4jrqwTgysCiW97Om-NQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1286]
[2022-03-30T03:46:01,303][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1286] timed out after [104598ms]
[2022-03-30T03:47:12,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3fe8e45c, interval=5s}] took [5508ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:48:11,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@67886eb9, interval=5s}] took [23640ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:47:44,522][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [30238ms] which is above the warn threshold of [5s]
[2022-03-30T03:50:56,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@28332a48, interval=1s}] took [51133ms] which is above the warn threshold of [5000ms]
[2022-03-30T03:52:27,956][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-30T03:52:27,995][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-30T03:52:27,996][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-30T03:52:33,630][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-30T03:52:33,632][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-30T03:52:33,633][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-30T03:52:33,634][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-30T03:52:33,635][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-30T03:52:33,637][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-30T03:52:33,640][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-30T03:52:33,641][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-30T03:52:33,643][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-30T03:52:33,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-30T03:52:33,644][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-30T03:52:33,645][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-30T03:52:33,646][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-30T03:52:33,647][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-30T03:52:33,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-30T03:52:33,648][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-30T03:52:33,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-30T03:52:33,649][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-30T03:52:33,650][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-30T03:52:33,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-30T03:52:33,651][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-30T03:52:33,652][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-30T03:52:33,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-30T03:52:33,653][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-30T03:52:33,654][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-30T03:52:33,655][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-30T03:52:33,655][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-30T03:52:33,656][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-30T03:52:33,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-30T03:52:33,657][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-30T03:52:33,658][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-30T03:52:33,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-30T03:52:33,659][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-30T03:52:33,660][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-30T03:52:33,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-30T03:52:33,661][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-30T03:52:33,662][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-30T03:52:33,663][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-30T03:52:33,663][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-30T03:52:33,664][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-30T03:52:33,665][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-30T03:52:33,665][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-30T03:52:33,666][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-30T03:52:33,666][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-30T03:52:33,667][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-30T03:52:33,668][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-30T03:52:33,668][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-30T03:52:33,669][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-30T03:52:33,670][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-30T03:52:33,670][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-30T03:52:33,671][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-30T03:52:33,672][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-30T03:52:33,672][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-30T03:52:33,673][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-30T03:52:33,674][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-30T03:52:33,674][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-30T03:52:33,675][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-30T03:52:33,675][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-30T03:52:33,677][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-30T03:52:33,781][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.3gb], net total_space [125.8gb], types [ext4]
[2022-03-30T03:52:33,783][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-30T03:52:34,131][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-30T03:52:47,738][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-30T03:52:47,747][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-30T03:52:47,748][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T03:52:47,752][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-30T03:52:47,752][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T03:52:47,754][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-30T03:52:47,756][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-30T03:52:47,757][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T03:52:47,759][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-30T03:52:47,768][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-30T03:52:47,770][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-30T03:52:47,771][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T03:52:47,772][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T03:52:47,776][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T03:52:47,777][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-30T03:52:49,485][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-30T03:52:49,707][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-30T03:52:50,789][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-30T03:52:51,760][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-30T03:52:51,763][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-30T03:52:51,870][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-30T03:52:51,880][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-30T03:52:52,157][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-30T03:52:54,717][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-30T03:52:54,916][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 157, version: 4525, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-30T03:52:55,145][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 157, version: 4525, reason: Publication{term=157, version=4525}
[2022-03-30T03:52:55,264][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-30T03:52:55,265][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-30T03:52:56,822][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-30T03:52:56,840][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [33] indices into cluster_state
[2022-03-30T03:52:57,930][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-30T03:52:57,933][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-30T03:52:59,245][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-30T03:52:59,808][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-30T03:53:00,708][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-30T03:53:00,727][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-30T03:53:00,745][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-30T03:53:01,445][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-30T03:53:02,668][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T03:53:03,197][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T03:53:09,910][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][17] overhead, spent [481ms] collecting in the last [1.2s]
[2022-03-30T03:53:09,981][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T03:53:14,070][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][20] overhead, spent [663ms] collecting in the last [1.7s]
[2022-03-30T03:55:03,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:55:30,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6812093964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:55:37,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39119ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:55:46,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.1s/39118512544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:55:52,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:55:57,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16110069600ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:56:01,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8477ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:55:59,341][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [158341ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [32] unchanged indices
[2022-03-30T03:56:06,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8476823658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:56:14,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:56:22,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12805623776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:56:28,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:56:21,787][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3m] publication of cluster state version [4551] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T03:56:37,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14169930594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:56:46,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:57:00,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17171259170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:57:13,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25767ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:57:26,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25767038787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:57:42,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:57:54,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30248101793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:58:15,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:58:26,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33245130792ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:58:40,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:58:52,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24153725633ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:59:04,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24460ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:59:16,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24459894070ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:59:29,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T03:59:44,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24819024444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T03:59:58,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:00:12,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.3s/29330691133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:00:26,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:00:37,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27700154517ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:00:46,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:00:57,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20777135460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:01:10,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:01:19,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22754559072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:01:30,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:01:44,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21007920077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:01:57,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:02:07,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26791902581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:01:52,227][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:51830}] took [475642ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:02:20,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22231ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:02:34,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22231614282ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:02:49,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29074ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:03:03,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29074018736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:03:17,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:03:28,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28543825159ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:03:42,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:04:04,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25197991398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:04:14,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:04:24,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32387502092ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:04:35,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:04:44,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19962010143ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:04:55,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:05:05,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19975654528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:05:16,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22416ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:05:25,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22415759155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:05:36,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:05:46,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19622294231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:05:56,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:06:07,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19909994674ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:06:19,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:06:32,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22404804506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:06:50,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30785ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:07:03,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30785205307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:07:15,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:07:31,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26422956828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:07:43,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:07:53,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26115463873ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:08:04,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:07:49,018][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=157, version=4551}] took [9.1m] which is above the warn threshold of [30s]: [running task [Publication{term=157, version=4551}]] took [231ms], [connecting to new nodes] took [1490ms], [applying settings] took [251ms], [org.elasticsearch.repositories.RepositoriesService@4cd26a53] took [588ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@7acb18da] took [97240ms], [org.elasticsearch.script.ScriptService@58a38cef] took [347ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c3ab31e] took [0ms], [org.elasticsearch.snapshots.RestoreService@63e5c638] took [0ms], [org.elasticsearch.ingest.IngestService@41ca0f56] took [10111ms], [org.elasticsearch.action.ingest.IngestActionForwarder@54a3774] took [1052ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce200@6c01d680] took [167ms], [org.elasticsearch.indices.TimestampFieldMapperService@2198193e] took [468ms], [org.elasticsearch.tasks.TaskManager@303902e4] took [334ms], [org.elasticsearch.snapshots.SnapshotsService@649c7200] took [1043ms], [org.elasticsearch.cluster.InternalClusterInfoService@ca40123] took [41088ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@798af648] took [1025ms], [org.elasticsearch.indices.SystemIndexManager@48d596bf] took [8387ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@d7f5757] took [261ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@48c6cae8] took [2432ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@1d903cd3] took [676ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3ec6e3be] took [260ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@241a0d03] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3940a133] took [159ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@37225ecb] took [180962ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7e427580] took [239ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@16ff92d2] took [297ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@7bfc0bf8] took [64089ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2ded3cf3] took [1278ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@6c91b286] took [988ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@612d4c4] took [48699ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c3ab31e] took [3356ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1daaac35] took [29444ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@59e411d7] took [435ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@60a700fa] took [77ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48107214] took [28901ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@bafdecc] took [16431ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@3dc825d5] took [322ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@a842e5b] took [1723ms], [org.elasticsearch.node.ResponseCollectorService@7f28c879] took [254ms], [org.elasticsearch.snapshots.SnapshotShardsService@1b43bd86] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@67d9c5b6] took [261ms], [org.elasticsearch.shutdown.PluginShutdownService@4c04d9e0] took [158ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1d0873ac] took [239ms], [org.elasticsearch.indices.store.IndicesStore@20a8f7c] took [445ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7a795d01] took [298ms], [org.elasticsearch.license.LicenseService@a80378b] took [244ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2eca4fe] took [154ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4828f69b] took [0ms], [org.elasticsearch.gateway.GatewayService@48ec042f] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@33c0551f] took [0ms]
[2022-03-30T04:08:15,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21966924927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:08:18,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@6772fca2] took [877959ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:08:26,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22207ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:08:36,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22207727177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:08:48,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21939ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:09:00,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21938086715ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:09:10,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23001ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:09:19,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23001879247ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:09:29,850][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [15.6m/936305ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [15.6m/941513ms]
[2022-03-30T04:09:31,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:09:40,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19483665097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:10:13,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:10:10,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3f04bc41, interval=5s}] took [64423ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:10:23,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.5s/41511711779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:10:35,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:10:46,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.1s/23173409622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:10:56,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:11:01,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a534c4b, interval=1s}] took [44183ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:10:58,769][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [35.7s/35753ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [46], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [46], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [46], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.20][0]], allocationId [OkbTmFqSR4iKtOyMAWv7tQ], primary term [46], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-30T04:11:14,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21010492986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:11:24,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:11:35,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28431184225ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:11:45,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:12:10,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20107161305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:12:21,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35137ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:12:33,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35136943678ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:12:43,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:12:54,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23266418496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:13:06,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22886ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:13:16,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22885565252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:13:26,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20164ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:13:35,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20164383003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:13:45,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:13:55,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18931609438ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:14:06,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20670ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:14:20,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20669939547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:14:32,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:14:40,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26163363186ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:14:49,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17550ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:15:01,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17549839684ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:15:11,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:15:19,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21796142838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:15:29,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17948ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:15:39,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17947700148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:16:03,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:16:11,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34605196074ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:16:13,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@703f8d40] took [34605ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:16:20,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16106ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:16:28,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16106156412ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:17:24,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:17:54,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62710427279ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:19:26,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121497ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:19:46,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121388849359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:20:07,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:20:32,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41970891129ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:20:53,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44849ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:20:05,512][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [184100ms] which is above the warn threshold of [5s]
[2022-03-30T04:21:14,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44848279569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:21:39,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:22:55,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.1s/42155683731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:23:04,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3f04bc41, interval=5s}] took [42155ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:23:17,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:23:42,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/104320286291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:24:06,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44878ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:24:26,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44878534421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:25:14,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65408ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:25:26,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a534c4b, interval=1s}] took [110286ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:25:33,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65407715848ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:25:57,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.4s/47474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:26:20,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.4s/47474042728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:26:49,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.3s/51355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:27:15,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.3s/51354770426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:27:50,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60044ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:28:18,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/60043734865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:28:48,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.2s/55232ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:29:15,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.2s/55232864128ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:29:53,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57623ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:30:49,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57622577418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:31:46,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122868ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:32:20,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122868234004ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:32:43,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:33:12,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61144336015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:32:50,999][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [1223507ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [32] unchanged indices
[2022-03-30T04:34:04,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:34:15,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80734364551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:34:21,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3f04bc41, interval=5s}] took [80734ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:34:26,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:34:37,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21727598086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:34:17,352][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [22.8m] publication of cluster state version [4552] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{yzh1XZvgRX2wEQro6KG2YQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T04:34:49,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23427ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:34:53,933][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23428ms] which is above the warn threshold of [5s]
[2022-03-30T04:35:02,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23427621460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:35:15,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:35:26,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25421801833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:35:28,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a534c4b, interval=1s}] took [48849ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:35:36,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:35:47,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21161436093ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:00,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:10,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23511407151ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:19,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20398ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:30,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20398661175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:41,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21292ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:50,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21291904762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:36:35,777][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [186] timed out after [809162ms]
[2022-03-30T04:37:02,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:37:17,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19703691052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:37:31,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:37:35,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3f04bc41, interval=5s}] took [31260ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:37:42,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31260087336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:37:55,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:38:08,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21491431575ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:38:18,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:38:32,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24756962431ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:38:44,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:38:51,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a534c4b, interval=1s}] took [70170ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:38:56,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23922307061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:39:14,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:39:14,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6f0c95bb, interval=5s}] took [30060ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:39:27,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30s/30060035557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:39:39,152][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:39:53,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26148300994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:40:05,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:40:17,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25614285222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:40:31,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25739ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:40:14,667][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25614ms] which is above the warn threshold of [5s]
[2022-03-30T04:40:42,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25738211594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:40:49,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19538ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:40:51,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3f04bc41, interval=5s}] took [19538ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:40:59,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19538094447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:41:09,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:41:18,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19794082910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:41:30,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:41:41,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20117063940ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:41:52,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22238ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:01,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.2s/22238154433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:11,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:09,431][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=157, version=4552}] took [7m] which is above the warn threshold of [30s]: [running task [Publication{term=157, version=4552}]] took [348ms], [connecting to new nodes] took [4146ms], [applying settings] took [154ms], [org.elasticsearch.repositories.RepositoriesService@4cd26a53] took [238ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@7acb18da] took [66790ms], [org.elasticsearch.script.ScriptService@58a38cef] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c3ab31e] took [0ms], [org.elasticsearch.snapshots.RestoreService@63e5c638] took [0ms], [org.elasticsearch.ingest.IngestService@41ca0f56] took [8873ms], [org.elasticsearch.action.ingest.IngestActionForwarder@54a3774] took [1077ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ce200@6c01d680] took [323ms], [org.elasticsearch.indices.TimestampFieldMapperService@2198193e] took [1860ms], [org.elasticsearch.tasks.TaskManager@303902e4] took [76ms], [org.elasticsearch.snapshots.SnapshotsService@649c7200] took [430ms], [org.elasticsearch.cluster.InternalClusterInfoService@ca40123] took [88ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@798af648] took [527ms], [org.elasticsearch.indices.SystemIndexManager@48d596bf] took [13464ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@d7f5757] took [276ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012dee58@48c6cae8] took [1628ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@1d903cd3] took [403ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@3ec6e3be] took [113ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402c08@241a0d03] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3940a133] took [164ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@37225ecb] took [180282ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@7e427580] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@16ff92d2] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@7bfc0bf8] took [38022ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2ded3cf3] took [595ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@6c91b286] took [575ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@612d4c4] took [25171ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3c3ab31e] took [4333ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@1daaac35] took [21107ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@59e411d7] took [235ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@60a700fa] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@48107214] took [27065ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@bafdecc] took [15734ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@3dc825d5] took [262ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@a842e5b] took [2369ms], [org.elasticsearch.node.ResponseCollectorService@7f28c879] took [76ms], [org.elasticsearch.snapshots.SnapshotShardsService@1b43bd86] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@67d9c5b6] took [773ms], [org.elasticsearch.shutdown.PluginShutdownService@4c04d9e0] took [327ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@1d0873ac] took [75ms], [org.elasticsearch.indices.store.IndicesStore@20a8f7c] took [537ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7a795d01] took [319ms], [org.elasticsearch.license.LicenseService@a80378b] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2eca4fe] took [229ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4828f69b] took [0ms], [org.elasticsearch.gateway.GatewayService@48ec042f] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@33c0551f] took [390ms]
[2022-03-30T04:42:16,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a534c4b, interval=1s}] took [61772ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:42:19,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19416928197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:29,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18025ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:38,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18025236867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:38,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@6f0c95bb, interval=5s}] took [18025ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:42:42,745][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [49.2m/2957169ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [49.4m/2968070ms]
[2022-03-30T04:42:44,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:42:52,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15897266659ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:43:10,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:43:20,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25765378818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:43:31,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18535ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:43:40,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18534711686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:43:50,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:44:01,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19983372609ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:44:11,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:44:21,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22588240646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:44:50,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18752ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:45:06,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18752150486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:45:19,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48951ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:45:33,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.9s/48950776276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:45:45,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:45:47,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da000@33165f75] took [180441ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:45:59,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25867323757ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:46:37,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:49:30,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.9s/50963157277ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:52:48,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:53:15,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3f04bc41, interval=5s}] took [349983ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:56:44,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/349983367271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:55:28,569][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.3m/743646ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-30T04:58:08,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/336640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:58:52,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/337046928262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:58:59,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.3s/56339ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:59:04,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.4s/56470091521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:59:12,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:59:18,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12410872446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:59:27,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T04:59:38,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15101725301ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T04:59:43,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3a534c4b, interval=1s}] took [27512ms] which is above the warn threshold of [5000ms]
[2022-03-30T04:59:47,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20283ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:00:02,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20283297787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:00:37,641][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-30T05:00:37,664][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-30T05:00:37,666][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-30T05:00:43,517][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-30T05:00:43,519][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-30T05:00:43,521][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-30T05:00:43,522][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-30T05:00:43,523][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-30T05:00:43,524][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-30T05:00:43,525][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-30T05:00:43,526][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-30T05:00:43,527][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-30T05:00:43,528][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-30T05:00:43,529][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-30T05:00:43,530][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-30T05:00:43,531][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-30T05:00:43,532][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-30T05:00:43,533][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-30T05:00:43,534][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-30T05:00:43,535][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-30T05:00:43,535][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-30T05:00:43,536][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-30T05:00:43,537][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-30T05:00:43,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-30T05:00:43,538][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-30T05:00:43,539][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-30T05:00:43,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-30T05:00:43,540][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-30T05:00:43,541][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-30T05:00:43,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-30T05:00:43,542][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-30T05:00:43,543][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-30T05:00:43,544][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-30T05:00:43,545][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-30T05:00:43,545][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-30T05:00:43,546][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-30T05:00:43,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-30T05:00:43,547][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-30T05:00:43,548][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-30T05:00:43,549][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-30T05:00:43,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-30T05:00:43,550][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-30T05:00:43,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-30T05:00:43,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-30T05:00:43,552][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-30T05:00:43,553][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-30T05:00:43,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-30T05:00:43,554][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-30T05:00:43,555][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-30T05:00:43,556][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-30T05:00:43,556][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-30T05:00:43,557][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-30T05:00:43,558][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-30T05:00:43,558][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-30T05:00:43,559][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-30T05:00:43,559][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-30T05:00:43,560][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-30T05:00:43,561][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-30T05:00:43,569][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-30T05:00:43,571][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-30T05:00:43,571][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-30T05:00:43,573][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-30T05:00:43,664][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.3gb], net total_space [125.8gb], types [ext4]
[2022-03-30T05:00:43,665][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-30T05:00:44,147][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-30T05:02:46,691][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-30T05:02:46,697][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-30T05:02:46,698][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T05:02:46,701][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-30T05:02:46,701][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T05:02:46,702][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-30T05:02:46,703][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-30T05:02:46,704][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T05:02:46,705][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-30T05:02:46,705][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-30T05:02:46,706][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-30T05:02:46,707][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T05:02:46,707][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T05:02:46,708][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T05:02:46,709][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-30T05:02:48,139][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-30T05:02:48,348][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-30T05:02:49,342][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-30T05:02:50,217][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-30T05:02:50,220][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-30T05:02:50,304][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-30T05:02:50,306][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-30T05:02:50,590][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-30T05:02:53,442][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-30T05:02:53,658][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{iFkV66LJSFS0xN-56A_XPQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 158, version: 4553, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{iFkV66LJSFS0xN-56A_XPQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-30T05:02:53,920][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{iFkV66LJSFS0xN-56A_XPQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 158, version: 4553, reason: Publication{term=158, version=4553}
[2022-03-30T05:02:54,070][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-30T05:02:54,071][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-30T05:02:56,101][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-30T05:02:56,112][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [33] indices into cluster_state
[2022-03-30T05:02:57,741][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-30T05:02:57,745][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-30T05:02:59,500][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-30T05:03:00,058][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-30T05:03:01,543][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-30T05:03:01,580][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-30T05:03:01,590][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-30T05:03:02,086][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-30T05:03:03,499][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T05:03:03,897][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T05:03:08,482][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T05:03:56,013][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10874ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [30] unchanged indices
[2022-03-30T05:03:58,685][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.9s] publication of cluster state version [4588] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{iFkV66LJSFS0xN-56A_XPQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T05:04:17,791][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:52216}] took [33685ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:04:32,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51b933c7, interval=1s}] took [7359ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:04:45,885][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][63][15] duration [2.5s], collections [1]/[5.5s], total [2.5s]/[3.2s], memory [854.4mb]->[168.4mb]/[2gb], all_pools {[young] [740mb]->[0b]/[0b]}{[old] [102.4mb]->[102.4mb]/[2gb]}{[survivor] [16mb]->[66mb]/[0b]}
[2022-03-30T05:04:47,132][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][63] overhead, spent [2.5s] collecting in the last [5.5s]
[2022-03-30T05:04:48,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51b933c7, interval=1s}] took [8601ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:04:53,688][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0], [.kibana-event-log-7.16.2-000001][0]]]).
[2022-03-30T05:05:16,511][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [18611ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [31] unchanged indices
[2022-03-30T05:05:20,247][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [25.8s] publication of cluster state version [4590] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{iFkV66LJSFS0xN-56A_XPQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T05:06:03,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51b933c7, interval=1s}] took [16414ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:06:03,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:06:04,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10808753957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:06:04,887][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][16] duration [7.1s], collections [1]/[17.4s], total [7.1s]/[10.3s], memory [192.4mb]->[172.5mb]/[2gb], all_pools {[young] [24mb]->[0b]/[0b]}{[old] [102.4mb]->[166.5mb]/[2gb]}{[survivor] [66mb]->[6mb]/[0b]}
[2022-03-30T05:06:05,017][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:46954}] took [104442ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:06:04,887][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=158, version=4590}] took [40.6s] which is above the warn threshold of [30s]: [running task [Publication{term=158, version=4590}]] took [95ms], [connecting to new nodes] took [250ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@1a2c6398] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@73faa96a] took [481ms], [org.elasticsearch.script.ScriptService@3526ac1b] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [0ms], [org.elasticsearch.snapshots.RestoreService@347cdee] took [0ms], [org.elasticsearch.ingest.IngestService@25508515] took [429ms], [org.elasticsearch.action.ingest.IngestActionForwarder@2596a3d0] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ceec0@2d9c4487] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1089c467] took [55ms], [org.elasticsearch.tasks.TaskManager@31d0834e] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@21c83e87] took [63ms], [org.elasticsearch.cluster.InternalClusterInfoService@49eeb16e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@79752251] took [0ms], [org.elasticsearch.indices.SystemIndexManager@524d4622] took [1539ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@b69c0bf] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@fca42c7] took [1120ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@528a03c0] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5c493c9b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ec08@ea3eb39] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@75c92eeb] took [117ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@76e3895c] took [20393ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3a6969b9] took [295ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@6f54518b] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1e81d979] took [15159ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@d727c5c] took [50ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@34d4830f] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@79c15417] took [118ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7a8e4184] took [63ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@691b1047] took [87ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@779511a4] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@25730a44] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@46156def] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@659c610a] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@4a1eff4c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@cd08012] took [0ms], [org.elasticsearch.node.ResponseCollectorService@ac224b3] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@5a0b36b2] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@67524620] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@6f0a9ac8] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@510eb6dc] took [0ms], [org.elasticsearch.indices.store.IndicesStore@523eacc9] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@4572bd32] took [0ms], [org.elasticsearch.license.LicenseService@721b841e] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@71f8d22a] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@15f9f904] took [0ms], [org.elasticsearch.gateway.GatewayService@3e629292] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@b035a19] took [0ms]
[2022-03-30T05:06:05,596][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [7.1s] collecting in the last [17.4s]
[2022-03-30T05:06:36,244][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [359ms] collecting in the last [1s]
[2022-03-30T05:09:09,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@684c9e99, interval=5s}] took [112919ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:09:32,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:11:32,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107984538860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:15:08,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295851ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:24:36,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295850614462ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:33:52,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9m/1199400ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:36:21,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@55a18f6a, interval=5s}] took [1494941ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:36:47,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9m/1199091279716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:36:22,469][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53986}] took [1199091ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:39:53,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:43:18,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358586418405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:45:12,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@51b933c7, interval=1s}] took [358586ms] which is above the warn threshold of [5000ms]
[2022-03-30T05:46:39,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407360ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T05:50:08,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/407231656251ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T05:54:10,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/451379ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:00:24,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/451202952434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:03:44,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/561841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:07:49,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/562457816954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:11:07,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/455028ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:14:30,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/454528046868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:17:45,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398122ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:21:08,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398113727638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:34:56,744][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-30T06:34:56,758][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-30T06:34:56,758][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-30T06:35:02,983][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-30T06:35:02,985][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-30T06:35:02,986][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-30T06:35:02,988][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-30T06:35:02,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-30T06:35:02,989][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-30T06:35:02,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-30T06:35:02,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-30T06:35:02,990][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-30T06:35:02,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-30T06:35:02,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-30T06:35:02,991][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-30T06:35:02,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-30T06:35:02,992][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-30T06:35:02,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-30T06:35:02,993][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-30T06:35:02,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-30T06:35:02,994][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-30T06:35:02,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-30T06:35:02,995][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-30T06:35:02,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-30T06:35:02,996][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-30T06:35:02,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-30T06:35:02,997][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-30T06:35:02,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-30T06:35:02,998][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-30T06:35:02,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-30T06:35:02,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-30T06:35:02,999][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-30T06:35:03,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-30T06:35:03,000][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-30T06:35:03,001][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-30T06:35:03,001][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-30T06:35:03,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-30T06:35:03,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-30T06:35:03,002][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-30T06:35:03,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-30T06:35:03,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-30T06:35:03,003][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-30T06:35:03,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-30T06:35:03,004][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-30T06:35:03,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-30T06:35:03,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-30T06:35:03,005][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-30T06:35:03,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-30T06:35:03,006][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-30T06:35:03,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-30T06:35:03,007][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-30T06:35:03,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-30T06:35:03,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-30T06:35:03,008][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-30T06:35:03,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-30T06:35:03,009][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-30T06:35:03,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-30T06:35:03,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-30T06:35:03,010][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-30T06:35:03,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-30T06:35:03,011][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-30T06:35:03,012][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-30T06:35:03,101][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.3gb], net total_space [125.8gb], types [ext4]
[2022-03-30T06:35:03,102][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-30T06:35:03,516][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-30T06:35:17,398][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-30T06:35:17,405][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-30T06:35:17,407][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T06:35:17,409][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-30T06:35:17,410][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T06:35:17,412][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-30T06:35:17,413][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-30T06:35:17,415][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T06:35:17,416][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-30T06:35:17,418][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-30T06:35:17,420][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-30T06:35:17,422][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T06:35:17,423][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T06:35:17,424][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-30T06:35:17,425][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-30T06:35:18,815][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-30T06:35:18,993][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-30T06:35:19,914][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-30T06:35:20,772][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-30T06:35:20,774][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-30T06:35:20,853][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-30T06:35:20,864][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-30T06:35:21,175][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-30T06:35:24,164][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-30T06:35:24,306][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 159, version: 4591, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-30T06:35:24,529][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 159, version: 4591, reason: Publication{term=159, version=4591}
[2022-03-30T06:35:24,741][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-30T06:35:24,742][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-30T06:35:25,951][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-30T06:35:25,968][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [33] indices into cluster_state
[2022-03-30T06:35:27,083][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-30T06:35:27,099][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-30T06:35:29,110][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-30T06:35:29,580][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-30T06:35:31,227][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-30T06:35:31,386][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-30T06:35:31,439][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-30T06:35:33,442][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-30T06:36:44,083][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [34416ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:36:45,470][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [66248ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [33] unchanged indices
[2022-03-30T06:36:48,438][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.1m] publication of cluster state version [4606] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T06:37:26,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@2ebaa3b0] took [38207ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:37:40,178][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-30T06:37:47,324][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-30T06:37:50,872][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [102] timed out after [16571ms]
[2022-03-30T06:38:02,540][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [35.9s/35902ms] ago, timed out [19.3s/19331ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [102]
[2022-03-30T06:38:16,010][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16648ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [31] unchanged indices
[2022-03-30T06:38:21,166][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24s] publication of cluster state version [4608] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T06:38:41,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@7c5d7148] took [13878ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:38:51,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [6214ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:39:28,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [9003ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:39:29,214][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [141] timed out after [51818ms]
[2022-03-30T06:39:43,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [6603ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:39:36,159][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [143] timed out after [50300ms]
[2022-03-30T06:40:06,135][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10168ms] which is above the warn threshold of [5s]
[2022-03-30T06:40:19,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [16907ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:40:58,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@49e599b2, interval=5s}] took [22497ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:41:19,515][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [13207ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:41:47,677][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:52536}] took [288781ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:41:47,677][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:52534}] took [288781ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:42:10,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@6b2d38e6] took [46823ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:42:34,485][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [172] timed out after [65826ms]
[2022-03-30T06:42:34,553][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [174] timed out after [21604ms]
[2022-03-30T06:43:05,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:43:14,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22693574217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:43:17,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:43:19,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13843878802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:43:44,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][52][14] duration [16.8s], collections [1]/[4.1s], total [16.8s]/[17.8s], memory [388mb]->[388mb]/[2gb], all_pools {[young] [292mb]->[296mb]/[0b]}{[old] [92mb]->[92mb]/[2gb]}{[survivor] [4mb]->[23mb]/[0b]}
[2022-03-30T06:43:47,679][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [16.8s] collecting in the last [4.1s]
[2022-03-30T06:43:46,460][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=159, version=4608}] took [5m] which is above the warn threshold of [30s]: [running task [Publication{term=159, version=4608}]] took [29ms], [connecting to new nodes] took [36ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@472cfe7c] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@421a65ba] took [209198ms], [org.elasticsearch.script.ScriptService@c19bbd0] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [0ms], [org.elasticsearch.snapshots.RestoreService@5f4af226] took [0ms], [org.elasticsearch.ingest.IngestService@4eec7bc0] took [8111ms], [org.elasticsearch.action.ingest.IngestActionForwarder@fea9ef6] took [66ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016c2000@596c6315] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@51bf61dc] took [64ms], [org.elasticsearch.tasks.TaskManager@7c203250] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27660e0c] took [173ms], [org.elasticsearch.cluster.InternalClusterInfoService@126bac8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@13a2996f] took [56ms], [org.elasticsearch.indices.SystemIndexManager@62043339] took [2296ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@71f19f43] took [76ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@1de89dda] took [801ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@246296e3] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5595788d] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013b6c08@46ec71cd] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@467721a5] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@292abadb] took [29802ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@584ebfe1] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@3dbb51bc] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1027505d] took [28130ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@5bc89d6b] took [399ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@445543e7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2384dc60] took [2462ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [2028ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@90bc5f5] took [8391ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1637227a] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2a02e3a8] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@3491d0b0] took [6768ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@706ed0d2] took [4081ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@204e3680] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5541afc6] took [757ms], [org.elasticsearch.node.ResponseCollectorService@53af1dfc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1d70f4d4] took [1ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4cdb2bcc] took [66ms], [org.elasticsearch.shutdown.PluginShutdownService@143c920e] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@2c472ee0] took [29ms], [org.elasticsearch.indices.store.IndicesStore@3d1a23f8] took [102ms], [org.elasticsearch.persistent.PersistentTasksNodeService@735c2c98] took [0ms], [org.elasticsearch.license.LicenseService@6473ba4d] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1dbf403c] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4d967014] took [0ms], [org.elasticsearch.gateway.GatewayService@19f350e1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@40ed2650] took [0ms]
[2022-03-30T06:43:49,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [71150ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:44:12,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@2cbfd07c] took [6859ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:44:21,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [7351ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:44:40,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [9069ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:44:38,130][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [198] timed out after [25075ms]
[2022-03-30T06:44:46,066][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [199] timed out after [33489ms]
[2022-03-30T06:44:53,137][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.4m/384375ms] ago, timed out [5.5m/332557ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [141]
[2022-03-30T06:45:05,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [5809ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:45:00,018][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.2m/377301ms] ago, timed out [5.4m/327001ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [143]
[2022-03-30T06:45:18,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [5008ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:45:24,457][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.9m/237533ms] ago, timed out [2.8m/171707ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [172]
[2022-03-30T06:45:30,237][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.3m/198314ms] ago, timed out [2.9m/176710ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [174]
[2022-03-30T06:45:33,648][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85885ms] ago, timed out [1m/60810ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [198]
[2022-03-30T06:45:48,210][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [81581ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [31] unchanged indices
[2022-03-30T06:45:57,217][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.7m] publication of cluster state version [4609] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T06:46:05,233][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.8m/112254ms] ago, timed out [1.3m/78765ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [199]
[2022-03-30T06:46:13,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@4939f5e9] took [41006ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:46:13,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:17,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5777306905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:24,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:28,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11614861306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:30,366][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [11614ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:46:32,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:35,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6777232465ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:38,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:40,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6550761548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:46:49,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [6023ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:47:01,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [5808ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:47:13,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [5214ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:47:12,035][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5815ms] which is above the warn threshold of [5s]
[2022-03-30T06:47:29,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [6122ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:47:32,677][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=159, version=4609}] took [1.4m] which is above the warn threshold of [30s]: [running task [Publication{term=159, version=4609}]] took [198ms], [connecting to new nodes] took [259ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@472cfe7c] took [1ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@421a65ba] took [26123ms], [org.elasticsearch.script.ScriptService@c19bbd0] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [0ms], [org.elasticsearch.snapshots.RestoreService@5f4af226] took [0ms], [org.elasticsearch.ingest.IngestService@4eec7bc0] took [5060ms], [org.elasticsearch.action.ingest.IngestActionForwarder@fea9ef6] took [275ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016c2000@596c6315] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@51bf61dc] took [72ms], [org.elasticsearch.tasks.TaskManager@7c203250] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27660e0c] took [209ms], [org.elasticsearch.cluster.InternalClusterInfoService@126bac8f] took [65ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@13a2996f] took [0ms], [org.elasticsearch.indices.SystemIndexManager@62043339] took [1353ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@71f19f43] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@1de89dda] took [195ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@246296e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5595788d] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013b6c08@46ec71cd] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@467721a5] took [63ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@292abadb] took [28368ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@584ebfe1] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@3dbb51bc] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1027505d] took [7681ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@5bc89d6b] took [57ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@445543e7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2384dc60] took [3867ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [219ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@90bc5f5] took [4327ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1637227a] took [38ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2a02e3a8] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@3491d0b0] took [3598ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@706ed0d2] took [3359ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@204e3680] took [59ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5541afc6] took [2119ms], [org.elasticsearch.node.ResponseCollectorService@53af1dfc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1d70f4d4] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4cdb2bcc] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@143c920e] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@2c472ee0] took [60ms], [org.elasticsearch.indices.store.IndicesStore@3d1a23f8] took [708ms], [org.elasticsearch.persistent.PersistentTasksNodeService@735c2c98] took [50ms], [org.elasticsearch.license.LicenseService@6473ba4d] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1dbf403c] took [57ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4d967014] took [0ms], [org.elasticsearch.gateway.GatewayService@19f350e1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@40ed2650] took [0ms]
[2022-03-30T06:48:36,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:48:39,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10365288794ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:48:38,570][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.8s/30848ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002][0]], allocationId [mM7n0nEhTeOf33zASpblVQ], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002][0]], allocationId [mM7n0nEhTeOf33zASpblVQ], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [CqyzXh5mSZWwwNbZ5RgHFw], primary term [20], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [CqyzXh5mSZWwwNbZ5RgHFw], primary term [20], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002][0]], allocationId [mM7n0nEhTeOf33zASpblVQ], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002][0]], allocationId [mM7n0nEhTeOf33zASpblVQ], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [CqyzXh5mSZWwwNbZ5RgHFw], primary term [20], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-1970.01.01][0]], allocationId [CqyzXh5mSZWwwNbZ5RgHFw], primary term [20], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-03-30T06:48:42,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:48:44,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5457657156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:48:51,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@7db2f64d] took [63989ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:48:47,939][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-30T06:49:03,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [5005ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:49:32,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [8604ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:49:46,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6847ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:49:49,041][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [53.9s/53993ms] ago, timed out [9.4s/9449ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [259]
[2022-03-30T06:49:46,710][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [259] timed out after [44544ms]
[2022-03-30T06:49:50,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6847179097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:49:54,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:49:57,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7861599246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:02,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:05,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7342405878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:10,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8950ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:15,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [8950ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:50:15,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8950192637ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:17,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54232}] took [387277ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:50:20,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:24,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10171228273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:30,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:36,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8913919707ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:39,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [8913ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:50:40,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:43,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11492460687ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:47,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6253ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:51,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6253282267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:50:58,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11267ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:06,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11267363969ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:12,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:17,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13587594665ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:23,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:27,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11226868740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:32,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:37,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9792792676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:42,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:45,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9799007743ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:51:49,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@160d9910] took [62852ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:51:49,748][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [169491ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [31] unchanged indices
[2022-03-30T06:51:53,249][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.1m] publication of cluster state version [4610] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T06:52:41,237][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=159, version=4610}] took [44.3s] which is above the warn threshold of [30s]: [running task [Publication{term=159, version=4610}]] took [139ms], [connecting to new nodes] took [86ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@472cfe7c] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@421a65ba] took [1048ms], [org.elasticsearch.script.ScriptService@c19bbd0] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [0ms], [org.elasticsearch.snapshots.RestoreService@5f4af226] took [0ms], [org.elasticsearch.ingest.IngestService@4eec7bc0] took [796ms], [org.elasticsearch.action.ingest.IngestActionForwarder@fea9ef6] took [52ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016c2000@596c6315] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@51bf61dc] took [0ms], [org.elasticsearch.tasks.TaskManager@7c203250] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@27660e0c] took [98ms], [org.elasticsearch.cluster.InternalClusterInfoService@126bac8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@13a2996f] took [105ms], [org.elasticsearch.indices.SystemIndexManager@62043339] took [1108ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@71f19f43] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@1de89dda] took [93ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@246296e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5595788d] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013b6c08@46ec71cd] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@467721a5] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@292abadb] took [15213ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@584ebfe1] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@3dbb51bc] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1027505d] took [5373ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@5bc89d6b] took [154ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@445543e7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2384dc60] took [6348ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [218ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@90bc5f5] took [4407ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1637227a] took [41ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2a02e3a8] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@3491d0b0] took [6180ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@706ed0d2] took [2083ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@204e3680] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5541afc6] took [315ms], [org.elasticsearch.node.ResponseCollectorService@53af1dfc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1d70f4d4] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4cdb2bcc] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@143c920e] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@2c472ee0] took [0ms], [org.elasticsearch.indices.store.IndicesStore@3d1a23f8] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@735c2c98] took [0ms], [org.elasticsearch.license.LicenseService@6473ba4d] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1dbf403c] took [32ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4d967014] took [0ms], [org.elasticsearch.gateway.GatewayService@19f350e1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@40ed2650] took [0ms]
[2022-03-30T06:52:46,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@1b95e209] took [20308ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:52:48,636][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.8m/531637ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [7.8m/471553ms]
[2022-03-30T06:53:17,044][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16892ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [29] unchanged indices
[2022-03-30T06:53:19,831][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [23.8s] publication of cluster state version [4611] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T06:53:35,371][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][94][15] duration [2s], collections [1]/[3.3s], total [2s]/[19.9s], memory [183.1mb]->[115.2mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [92mb]->[111.8mb]/[2gb]}{[survivor] [23mb]->[3.3mb]/[0b]}
[2022-03-30T06:53:35,594][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [2s] collecting in the last [3.3s]
[2022-03-30T06:53:52,186][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][103][16] duration [888ms], collections [1]/[2.3s], total [888ms]/[20.8s], memory [147.2mb]->[207.2mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [111.8mb]->[111.8mb]/[2gb]}{[survivor] [3.3mb]->[9.3mb]/[0b]}
[2022-03-30T06:53:52,808][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][103] overhead, spent [888ms] collecting in the last [2.3s]
[2022-03-30T06:54:42,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [5351ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:54:58,863][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:52584}] took [40419ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:54:59,278][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [26162ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [31] unchanged indices
[2022-03-30T06:55:02,331][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [32.7s] publication of cluster state version [4613] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T06:55:09,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@23bfc7f0] took [11810ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:55:15,532][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:52588}] took [10005ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:56:06,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15681ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:10,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15680688709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:15,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:19,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10575273798ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:22,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6466ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:24,235][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=159, version=4613}] took [1.2m] which is above the warn threshold of [30s]: [running task [Publication{term=159, version=4613}]] took [142ms], [connecting to new nodes] took [109ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@472cfe7c] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@421a65ba] took [1627ms], [org.elasticsearch.script.ScriptService@c19bbd0] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [0ms], [org.elasticsearch.snapshots.RestoreService@5f4af226] took [0ms], [org.elasticsearch.ingest.IngestService@4eec7bc0] took [1273ms], [org.elasticsearch.action.ingest.IngestActionForwarder@fea9ef6] took [213ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016c2000@596c6315] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@51bf61dc] took [39ms], [org.elasticsearch.tasks.TaskManager@7c203250] took [44ms], [org.elasticsearch.snapshots.SnapshotsService@27660e0c] took [86ms], [org.elasticsearch.cluster.InternalClusterInfoService@126bac8f] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@13a2996f] took [43ms], [org.elasticsearch.indices.SystemIndexManager@62043339] took [1330ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@71f19f43] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@1de89dda] took [197ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@246296e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@5595788d] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013b6c08@46ec71cd] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@467721a5] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@292abadb] took [21093ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@584ebfe1] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@3dbb51bc] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@1027505d] took [7026ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@5bc89d6b] took [92ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@445543e7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2384dc60] took [5911ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@7fb53cb2] took [46ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@90bc5f5] took [22354ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@1637227a] took [52ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@2a02e3a8] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@3491d0b0] took [8539ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@706ed0d2] took [3556ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@204e3680] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5541afc6] took [1155ms], [org.elasticsearch.node.ResponseCollectorService@53af1dfc] took [150ms], [org.elasticsearch.snapshots.SnapshotShardsService@1d70f4d4] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4cdb2bcc] took [108ms], [org.elasticsearch.shutdown.PluginShutdownService@143c920e] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@2c472ee0] took [102ms], [org.elasticsearch.indices.store.IndicesStore@3d1a23f8] took [676ms], [org.elasticsearch.persistent.PersistentTasksNodeService@735c2c98] took [2ms], [org.elasticsearch.license.LicenseService@6473ba4d] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@1dbf403c] took [101ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4d967014] took [0ms], [org.elasticsearch.gateway.GatewayService@19f350e1] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@40ed2650] took [0ms]
[2022-03-30T06:56:26,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6466075653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:33,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:38,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10775971784ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:39,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@520b8557] took [48500ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:56:40,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7853ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:42,246][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852327471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T06:56:47,250][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][134][17] duration [9.9s], collections [1]/[1m], total [9.9s]/[30.8s], memory [201.2mb]->[128.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [111.8mb]->[114.2mb]/[2gb]}{[survivor] [9.3mb]->[14.3mb]/[0b]}
[2022-03-30T06:56:49,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [8949ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:57:02,423][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [23.4s/23469ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-30T06:57:35,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [10252ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:58:08,186][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [460] timed out after [43130ms]
[2022-03-30T06:58:46,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [36771ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:59:11,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@144cf011, interval=5s}] took [8541ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:59:54,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@49e599b2, interval=5s}] took [28244ms] which is above the warn threshold of [5000ms]
[2022-03-30T06:59:59,143][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.5m/150247ms] ago, timed out [1.7m/107117ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [460]
[2022-03-30T07:02:13,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [54703ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:02:43,774][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [15588ms] which is above the warn threshold of [5s]
[2022-03-30T07:04:16,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@33842454] took [108168ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:04:48,286][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [453229ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [29] unchanged indices
[2022-03-30T07:05:05,430][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [7.9m] publication of cluster state version [4614] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-30T07:05:17,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [17061ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:05:28,252][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [494] timed out after [69902ms]
[2022-03-30T07:05:35,745][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7779ms] which is above the warn threshold of [5s]
[2022-03-30T07:05:39,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [13378ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:06:19,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [17183ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:07:58,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@40fc0520] took [77189ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:08:56,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [25192ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:09:09,345][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.8m/288320ms] ago, timed out [3.6m/218418ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [494]
[2022-03-30T07:10:08,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:10:18,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8670ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:10:33,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14112104382ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:10:35,809][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [540] timed out after [117335ms]
[2022-03-30T07:10:19,004][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24045ms] which is above the warn threshold of [5s]
[2022-03-30T07:10:40,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [14112ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:10:43,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:10:52,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25227939007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:00,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17904ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:09,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17903426925ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:22,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:30,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20615511567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:40,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:47,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [16861ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:11:49,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16861941813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:11:59,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:01,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@144cf011, interval=5s}] took [20261ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:12:09,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20261589921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:17,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:26,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18266463602ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:31,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:36,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15370443570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:45,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:12:54,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12759687349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:03,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17648ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:10,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17647710253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:16,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:25,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12258851185ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:36,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19855ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:43,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017e1bb8@7cb547fd] took [96158ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:13:46,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19855143981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:13:56,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:14:06,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.6s/20671732794ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:14:16,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:14:34,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20482577306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:14:37,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [20482ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:14:37,137][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/381971ms] ago, timed out [4.4m/264636ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{9orahI9OSPeo4zvSMDD4ug}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [540]
[2022-03-30T07:14:44,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28825ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:14:54,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28824563110ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:15:01,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:15:01,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@49e599b2, interval=5s}] took [16722ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:15:05,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16722370387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:15:12,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:15:39,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11943909921ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:18:45,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/193645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:15:12,883][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [580] timed out after [86701ms]
[2022-03-30T07:21:48,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/193323610459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:15:59,739][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11944ms] which is above the warn threshold of [5s]
[2022-03-30T07:24:13,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:26:36,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328657050421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:29:53,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/335191ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:32:52,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/335651241899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:35:44,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@36430591, interval=1s}] took [335651ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:36:11,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/380497ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:39:00,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/380201338288ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:42:11,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:43:27,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@49e599b2, interval=5s}] took [362145ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:45:10,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362145872580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:48:15,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366363ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:51:14,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366165914867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T07:54:19,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T07:54:12,667][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@61231747] took [366890ms] which is above the warn threshold of [5000ms]
[2022-03-30T07:57:30,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/366890375884ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T08:02:49,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503359ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T08:06:09,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/503280930634ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T08:09:46,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T08:12:49,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406784081137ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-30T08:17:14,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-30T08:21:09,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403768023043ns] on relative clock which is above the warn threshold of [5000ms]
