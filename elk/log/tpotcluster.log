[2022-03-25T04:55:36,607][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T04:55:36,654][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T04:55:36,656][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T04:55:54,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T04:55:54,716][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T04:55:54,717][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T04:55:54,717][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T04:55:54,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T04:55:54,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T04:55:54,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T04:55:54,720][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T04:55:54,720][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T04:55:54,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T04:55:54,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T04:55:54,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T04:55:54,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T04:55:54,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T04:55:54,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T04:55:54,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T04:55:54,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T04:55:54,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T04:55:54,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T04:55:54,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T04:55:54,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T04:55:54,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T04:55:54,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T04:55:54,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T04:55:54,760][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T04:55:54,761][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T04:55:54,762][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T04:55:54,762][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T04:55:54,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T04:55:54,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T04:55:54,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T04:55:54,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T04:55:54,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T04:55:54,776][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T04:55:54,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T04:55:54,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T04:55:54,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T04:55:54,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T04:55:54,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T04:55:54,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T04:55:54,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T04:55:54,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T04:55:54,817][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T04:55:54,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T04:55:54,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T04:55:54,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T04:55:54,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T04:55:54,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T04:55:54,852][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T04:55:54,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T04:55:54,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T04:55:54,861][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T04:55:54,861][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T04:55:54,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T04:55:54,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T04:55:54,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T04:55:54,862][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T04:55:54,863][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T04:55:54,875][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T04:55:55,173][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.1gb], net total_space [125.8gb], types [ext4]
[2022-03-25T04:55:55,181][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T04:55:56,003][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T04:56:21,320][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T04:56:21,323][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T04:56:23,604][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T04:56:23,949][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T04:56:25,836][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T04:56:27,828][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T04:56:27,829][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T04:56:27,883][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T04:56:27,884][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T04:56:28,248][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T04:56:31,574][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T04:56:31,781][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 113, version: 3198, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T04:56:32,145][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 113, version: 3198, reason: Publication{term=113, version=3198}
[2022-03-25T04:56:32,399][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T04:56:32,400][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T04:56:36,164][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T04:56:36,180][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T04:56:37,880][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T04:56:37,881][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T04:56:39,272][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-25T04:56:39,560][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-25T04:56:40,366][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-25T04:56:40,473][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-25T04:56:40,499][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-25T04:56:40,505][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-25T04:56:41,819][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T04:56:42,124][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T04:56:47,654][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana-event-log-7.16.2-000001][0]]]).
[2022-03-25T04:56:48,077][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T04:57:03,809][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-25T04:57:04,276][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-25T04:58:12,190][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 491 finished with response BulkByScrollResponse[took=770.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T04:58:17,248][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 509 finished with response BulkByScrollResponse[took=5.4s,timed_out=false,sliceId=null,updated=1046,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-25T04:58:28,220][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-25T05:09:13,347][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T05:29:48,833][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T05:38:36,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T05:38:46,885][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T05:55:29,826][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T06:00:18,935][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3825] overhead, spent [643ms] collecting in the last [1.2s]
[2022-03-25T06:04:00,598][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T06:11:35,980][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-25T06:11:36,086][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-25T06:44:36,382][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T07:02:26,183][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T07:15:49,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [5737ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:16:36,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [8179ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:16:51,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:03,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9268521539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:07,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:09,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19863217321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:12,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:12,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [19863ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:17:15,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5833394628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:19,243][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:22,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6245572944ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:25,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [6245ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:17:27,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:30,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7551830771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:32,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:37,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [5772ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:17:36,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5772782593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:42,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@386aab0e, interval=5s}] took [9871ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:17:42,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9871ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:46,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9871052331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:48,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:51,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6295523495ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:17:55,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:18:00,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [6484ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:20:18,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6484857502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:21:38,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/222358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:22:58,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/222358684850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:23:44,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:24:18,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/125653897018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:25:05,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:24:46,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [125653ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:25:46,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/75434426809ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:27:27,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/147867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:28:40,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/147867266205ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:29:45,493][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:27:55,207][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [64847] timed out after [22637ms]
[2022-03-25T07:31:39,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/137624211768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:34:01,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/254920ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:36:40,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/254919482742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:39:28,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:40:39,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327219056817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:41:22,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:41:51,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107651804261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:04,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:13,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [50119ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:42:14,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.1s/50119737325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:06,280][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [107652ms] which is above the warn threshold of [5s]
[2022-03-25T07:42:23,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19212ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:30,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19211440371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:37,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13736ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:37,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@386aab0e, interval=5s}] took [13736ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:42:42,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13736474086ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:42,818][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [26.9m/1619020ms] ago, timed out [26.6m/1596383ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [64847]
[2022-03-25T07:42:43,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6237ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:42:43,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6237359301ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:43:35,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:43:36,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6097633696ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:43:37,155][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][8378][159] duration [4.9s], collections [1]/[7.4s], total [4.9s]/[11.3s], memory [1.3gb]->[169.4mb]/[2gb], all_pools {[young] [1.1gb]->[28mb]/[0b]}{[old] [162.6mb]->[162.6mb]/[2gb]}{[survivor] [6.1mb]->[6.8mb]/[0b]}
[2022-03-25T07:43:37,412][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][8378] overhead, spent [4.9s] collecting in the last [7.4s]
[2022-03-25T07:43:37,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [8270ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:43:53,664][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][8385][160] duration [1.8s], collections [1]/[3.4s], total [1.8s]/[13.2s], memory [249.4mb]->[170mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [162.6mb]->[163.1mb]/[2gb]}{[survivor] [6.8mb]->[6.9mb]/[0b]}
[2022-03-25T07:43:58,449][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][8385] overhead, spent [1.8s] collecting in the last [3.4s]
[2022-03-25T07:44:01,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [9429ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:44:27,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [5862ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:44:47,166][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5773ms] which is above the warn threshold of [5s]
[2022-03-25T07:44:59,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [7662ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:45:25,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [10192ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:45:42,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@386aab0e, interval=5s}] took [6720ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:46:10,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [11397ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:47:46,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:47:52,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72729391820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:01,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:07,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15582389034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:00,477][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65152] timed out after [65182ms]
[2022-03-25T07:48:16,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:24,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15026339758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:33,839][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:35,731][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][8396][161] duration [1m], collections [1]/[1.8m], total [1m]/[1.2m], memory [234mb]->[177.6mb]/[2gb], all_pools {[young] [64mb]->[16mb]/[0b]}{[old] [163.1mb]->[163.1mb]/[2gb]}{[survivor] [6.9mb]->[6.5mb]/[0b]}
[2022-03-25T07:48:40,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16967036928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:44,326][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][8396] overhead, spent [1m] collecting in the last [1.8m]
[2022-03-25T07:48:48,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:48:54,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [61617ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:49:04,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14041645029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:49:17,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:49:29,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27221948207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:49:09,885][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.5m/213673ms] ago, timed out [2.4m/148491ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65152]
[2022-03-25T07:49:42,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:49:52,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.6s/26605146528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:50:05,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:50:11,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [50318ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:50:17,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23713667416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:50:29,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24124ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:50:36,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5126/0x00000008017e86c0@2139e51c] took [24124ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:50:42,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24124708303ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:50:54,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:50:58,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@386aab0e, interval=5s}] took [24886ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:51:06,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24886564004ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:51:17,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:51:26,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23559172290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:51:35,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:51:44,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17475738098ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:51:54,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [36190ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:51:57,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18715ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:09,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18715261220ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:17,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:25,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23678431018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:16,031][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65212] timed out after [84637ms]
[2022-03-25T07:52:37,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20114ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:47,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20114754436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:57,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:52:57,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [39844ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:53:04,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19729949684ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:53:15,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@16efc840, interval=5s}] took [17747ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:53:15,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17748ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:53:26,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17747235550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:53:34,057][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.7m/165908ms] ago, timed out [1.3m/81271ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65212]
[2022-03-25T07:53:39,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:53:48,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23804806579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:53:26,057][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17748ms] which is above the warn threshold of [5s]
[2022-03-25T07:53:59,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:54:10,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20513814057ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:54:20,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21560ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:54:30,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21559474922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:54:50,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:55:04,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23943676626ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:55:12,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [23943ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:55:18,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:55:42,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33556663017ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:56:05,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:56:17,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5126/0x00000008017e86c0@1a5ac592] took [46789ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:56:19,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.7s/46789736793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:56:36,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:56:56,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31015239412ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:57:15,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:57:40,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.2s/37278174434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:57:55,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:58:21,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41965970566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:58:39,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:59:08,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43713482697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T07:59:32,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T07:59:31,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [43713ms] which is above the warn threshold of [5000ms]
[2022-03-25T07:59:58,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.2s/53284575595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:00:21,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:01:04,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.5s/48548813028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:02:17,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114048ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:01:13,441][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [48549ms] which is above the warn threshold of [5s]
[2022-03-25T08:04:21,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114048263918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:03:28,011][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65271] timed out after [207257ms]
[2022-03-25T08:07:11,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287204ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:08:42,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287203459222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:10:30,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207206ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:11:51,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207205863712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:13:44,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/194777ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:15:45,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/194777143202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:15:46,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [194777ms] which is above the warn threshold of [5000ms]
[2022-03-25T08:18:33,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:21:40,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287328494816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:20:41,468][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [22.4m/1346369ms] ago, timed out [18.9m/1139112ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65271]
[2022-03-25T08:25:04,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:28:18,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362164629082ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:27:38,610][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.3s/10341ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746]], which exceeds the warn threshold of [10s]
[2022-03-25T08:31:18,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/393488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:34:15,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/393369753664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:38:31,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/443714ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:44:03,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/443664506549ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:35:24,913][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.7s/24713ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T08:46:55,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/502486ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:49:58,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3m/502687615700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T08:50:58,065][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [29.3s/29384ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T08:53:01,346][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/365466ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T08:56:08,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/365269372610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:04:14,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8m/650042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:05:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@540a29bd, interval=1s}] took [650023ms] which is above the warn threshold of [5000ms]
[2022-03-25T09:09:11,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8m/650023462263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:12:16,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/505590ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:12:56,163][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [21.8m/1311621ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [24.9m/1498632ms]
[2022-03-25T09:15:14,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/505512792213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:16:13,636][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [41.1m/2467157ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [43.2m/2595137ms]
[2022-03-25T09:12:07,224][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [650024ms] which is above the warn threshold of [5s]
[2022-03-25T09:18:46,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389991ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:20:01,790][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [47.6m/2857690ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [36.9m/2214672ms]
[2022-03-25T09:22:02,982][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/390533110833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:25:16,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388444ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:25:38,115][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [33.8s/33818ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T09:28:25,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388137873621ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:30:54,480][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.2m/73217ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T09:31:15,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361090ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:36:06,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361405140575ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:39:20,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:47:27,956][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3607233ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [40.9m/2454402ms]
[2022-03-25T09:46:52,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8m/485259305239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:51:15,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9m/714330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:53:56,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8m/713971702871ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T09:56:30,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315136ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T09:56:34,708][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.1h/4092493ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [56.5m/3395760ms]
[2022-03-25T09:59:21,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315494903636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T10:00:01,311][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.4h/5121959ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [1h/3852530ms]
[2022-03-25T10:02:24,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T10:03:31,005][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.5h/5462878ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [32m/1921551ms]
[2022-03-25T10:03:42,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@386aab0e, interval=5s}] took [340918ms] which is above the warn threshold of [5000ms]
[2022-03-25T10:05:29,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340918201853ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T10:08:27,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T10:09:02,407][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.2s/30206ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T10:11:56,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/374734661818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T10:12:38,759][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22s/22082ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T10:08:56,896][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [65349] timed out after [2780872ms]
[2022-03-25T10:14:10,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T10:15:29,821][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.7h/6180697ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [43m/2585240ms]
[2022-03-25T10:16:35,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/343084304443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T10:20:08,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332359ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T10:20:25,138][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [29.7s/29733ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T10:15:51,267][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.3h/4869076ms] ago, timed out [34.8m/2088204ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{hI7L10KzRJaLqpu2BJNgmA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [65349]
[2022-03-25T10:23:58,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332625114962ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T10:24:54,437][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [23.1s/23151ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@439486a4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c72bda63], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@7c4f99fd]], which exceeds the warn threshold of [10s]
[2022-03-25T10:22:29,243][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [332625ms] which is above the warn threshold of [5s]
[2022-03-25T10:27:27,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T10:28:58,982][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.9h/6977373ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [14.2m/857200ms]
[2022-03-25T10:30:36,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464051228530ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T10:32:27,394][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.4s/10494ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bf0fb746]], which exceeds the warn threshold of [10s]
[2022-03-25T10:33:36,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370784ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T10:43:08,271][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T10:43:08,454][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T10:43:08,459][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T10:43:24,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T10:43:24,209][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T10:43:24,211][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T10:43:24,211][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T10:43:24,213][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T10:43:24,216][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T10:43:24,217][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T10:43:24,219][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T10:43:24,220][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T10:43:24,222][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T10:43:24,223][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T10:43:24,224][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T10:43:24,226][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T10:43:24,228][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T10:43:24,230][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T10:43:24,232][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T10:43:24,233][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T10:43:24,234][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T10:43:24,235][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T10:43:24,238][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T10:43:24,239][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T10:43:24,240][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T10:43:24,240][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T10:43:24,241][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T10:43:24,244][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T10:43:24,245][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T10:43:24,246][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T10:43:24,246][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T10:43:24,247][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T10:43:24,249][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T10:43:24,250][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T10:43:24,251][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T10:43:24,252][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T10:43:24,253][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T10:43:24,254][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T10:43:24,255][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T10:43:24,255][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T10:43:24,256][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T10:43:24,258][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T10:43:24,259][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T10:43:24,262][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T10:43:24,263][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T10:43:24,264][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T10:43:24,264][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T10:43:24,265][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T10:43:24,265][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T10:43:24,266][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T10:43:24,267][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T10:43:24,268][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T10:43:24,268][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T10:43:24,269][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T10:43:24,269][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T10:43:24,270][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T10:43:24,271][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T10:43:24,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T10:43:24,275][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T10:43:24,276][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T10:43:24,277][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T10:43:24,278][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T10:43:24,392][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.9gb], net total_space [125.8gb], types [ext4]
[2022-03-25T10:43:24,393][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T10:43:24,863][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T10:43:46,321][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T10:43:46,326][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-25T10:43:46,328][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T10:43:46,330][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-25T10:43:46,340][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T10:43:46,342][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-25T10:43:46,344][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-25T10:43:46,347][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T10:43:46,349][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-25T10:43:46,351][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-25T10:43:46,354][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-25T10:43:46,355][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T10:43:46,357][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T10:43:46,358][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T10:43:46,359][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T10:43:49,159][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T10:43:49,571][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T10:43:51,721][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T10:43:53,753][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T10:43:53,763][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T10:43:53,824][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T10:43:53,829][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T10:43:54,251][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T10:43:59,117][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T10:43:59,302][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 114, version: 3241, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T10:43:59,604][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 114, version: 3241, reason: Publication{term=114, version=3241}
[2022-03-25T10:43:59,828][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T10:43:59,829][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T10:44:02,762][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T10:44:02,784][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T10:44:05,652][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T10:44:05,653][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T10:44:07,773][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-25T10:44:08,607][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-25T10:44:10,123][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-25T10:44:10,388][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-25T10:44:10,449][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-25T10:44:10,450][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-25T10:44:14,752][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T10:44:14,937][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][21] overhead, spent [384ms] collecting in the last [1s]
[2022-03-25T10:44:15,257][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T10:44:16,961][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][23] overhead, spent [279ms] collecting in the last [1s]
[2022-03-25T10:44:21,008][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][27] overhead, spent [302ms] collecting in the last [1s]
[2022-03-25T10:44:29,941][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-25T10:44:35,044][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T10:46:06,589][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T10:46:14,357][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T10:46:15,070][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T11:09:13,741][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.25/MxSo4ICFRF2lkCe_AUkUNQ] update_mapping [_doc]
[2022-03-25T11:13:24,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [23297ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:13:44,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [5571ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:14:05,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [13887ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:15:15,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7080ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:17:13,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7013436125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:17:40,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/196810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:17:53,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/196876275994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:18:12,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33573ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:18:32,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.5s/33572845670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:18:46,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.9s/34939ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:19:12,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.9s/34939335296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:19:18,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [34939ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:20:00,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:20:26,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71955730824ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:20:38,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39865ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:20:48,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39864901369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:20:53,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:00,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15574189940ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:08,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:18,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14158230801ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:28,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:38,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233255223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:46,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [36434ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:21:44,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16202ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:49,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16201743971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:21:57,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10966ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:12,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10965914909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:18,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [10965ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:22:23,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:33,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27445271300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:40,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:48,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [17692ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:22:48,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17692665763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:55,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:23:02,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14859033313ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:22:33,121][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35894}] took [16202ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:23:11,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15884ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:23:28,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.8s/15883794140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:24:05,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:24:21,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.9s/53956553308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:24:26,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [53956ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:24:34,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:24:49,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28921789120ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:24:52,942][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35886}] took [53956ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:24:57,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:25:14,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22740541608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:25:28,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:25:30,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [31661ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:25:46,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.6s/31661648433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:26:04,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:26:18,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.6s/30606413414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:26:24,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:26:34,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25639460117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:26:35,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [25639ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:26:40,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:26:51,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15002904774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:27:01,590][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13244] timed out after [49738ms]
[2022-03-25T11:27:07,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [26755ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:27:06,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:27:14,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26755664771ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:27:27,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:27:40,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20800642252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:27:48,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20791ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:27:55,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20790470237ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:28:02,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:28:08,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [14259ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:28:08,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14259587739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:28:09,785][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:28:09,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [7538ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:28:10,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7538828325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:28:12,356][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14.6m/878636ms] ago, timed out [13.8m/828898ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13244]
[2022-03-25T11:31:34,374][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:47188}] took [7059ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:32:22,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [6912ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:32:22,370][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6112ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:32:23,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6111689680ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:32:27,191][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:32:30,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158481669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:32:33,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:32:36,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6412803849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:32:51,543][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1862][50] duration [4.7s], collections [1]/[9.9s], total [4.7s]/[8.5s], memory [1.3gb]->[160.3mb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [152.7mb]->[152.7mb]/[2gb]}{[survivor] [8mb]->[7.5mb]/[0b]}
[2022-03-25T11:33:00,217][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1862] overhead, spent [4.7s] collecting in the last [9.9s]
[2022-03-25T11:33:06,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [44419ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:33:06,794][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35880}] took [8317ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:33:26,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [5403ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:33:44,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [6007ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:34:00,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:34:00,901][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1867][51] duration [6.6s], collections [1]/[12s], total [6.6s]/[15.1s], memory [240.3mb]->[244.3mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [152.7mb]->[152.8mb]/[2gb]}{[survivor] [7.5mb]->[9.3mb]/[0b]}
[2022-03-25T11:34:01,322][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1867] overhead, spent [6.6s] collecting in the last [12s]
[2022-03-25T11:34:01,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9576785361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:34:02,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [9977ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:34:38,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:34:40,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6552590806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:34:40,437][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1878][52] duration [3.4s], collections [1]/[2.1s], total [3.4s]/[18.5s], memory [238.1mb]->[246.1mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [152.8mb]->[153.2mb]/[2gb]}{[survivor] [9.3mb]->[6.3mb]/[0b]}
[2022-03-25T11:34:42,640][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1878] overhead, spent [3.4s] collecting in the last [2.1s]
[2022-03-25T11:34:44,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [13187ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:35:08,347][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1887][53] duration [3.3s], collections [1]/[6.4s], total [3.3s]/[21.9s], memory [211.5mb]->[159.7mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [153.2mb]->[153.2mb]/[2gb]}{[survivor] [6.3mb]->[6.4mb]/[0b]}
[2022-03-25T11:35:10,092][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1887] overhead, spent [3.3s] collecting in the last [6.4s]
[2022-03-25T11:35:11,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [8935ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:35:43,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8134ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:35:43,745][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1893][54] duration [5.4s], collections [1]/[1.8s], total [5.4s]/[27.3s], memory [231.7mb]->[243.7mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [153.2mb]->[153.2mb]/[2gb]}{[survivor] [6.4mb]->[8mb]/[0b]}
[2022-03-25T11:35:44,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8133808015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:35:44,796][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1893] overhead, spent [5.4s] collecting in the last [1.8s]
[2022-03-25T11:35:45,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [8734ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:36:05,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:36:06,822][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1900][55] duration [4.8s], collections [1]/[2.2s], total [4.8s]/[32.2s], memory [245.2mb]->[245.2mb]/[2gb], all_pools {[young] [84mb]->[84mb]/[0b]}{[old] [153.2mb]->[153.2mb]/[2gb]}{[survivor] [8mb]->[8mb]/[0b]}
[2022-03-25T11:36:06,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6085195297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:36:12,581][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1900] overhead, spent [4.8s] collecting in the last [2.2s]
[2022-03-25T11:36:12,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [7285ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:36:12,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7203ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:36:12,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7203221701ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:36:12,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [7203ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:36:14,766][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1901][56] duration [4.2s], collections [1]/[15.1s], total [4.2s]/[36.4s], memory [245.2mb]->[168.5mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [153.2mb]->[154.2mb]/[2gb]}{[survivor] [8mb]->[6.3mb]/[0b]}
[2022-03-25T11:36:15,724][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1901] overhead, spent [4.2s] collecting in the last [15.1s]
[2022-03-25T11:36:21,995][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1903][57] duration [2s], collections [1]/[1.5s], total [2s]/[38.4s], memory [180.5mb]->[248.5mb]/[2gb], all_pools {[young] [20mb]->[4mb]/[0b]}{[old] [154.2mb]->[154.2mb]/[2gb]}{[survivor] [6.3mb]->[8mb]/[0b]}
[2022-03-25T11:36:22,639][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1903] overhead, spent [2s] collecting in the last [1.5s]
[2022-03-25T11:36:26,526][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1904][58] duration [2.4s], collections [1]/[7.4s], total [2.4s]/[40.9s], memory [248.5mb]->[167.6mb]/[2gb], all_pools {[young] [4mb]->[48mb]/[0b]}{[old] [154.2mb]->[154.2mb]/[2gb]}{[survivor] [8mb]->[9.4mb]/[0b]}
[2022-03-25T11:36:28,958][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1904] overhead, spent [2.4s] collecting in the last [7.4s]
[2022-03-25T11:36:53,043][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1905][59] duration [1.2s], collections [1]/[3.8s], total [1.2s]/[42.1s], memory [167.6mb]->[195mb]/[2gb], all_pools {[young] [48mb]->[36mb]/[0b]}{[old] [154.2mb]->[155.6mb]/[2gb]}{[survivor] [9.4mb]->[7.3mb]/[0b]}
[2022-03-25T11:37:02,139][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1905] overhead, spent [1.2s] collecting in the last [3.8s]
[2022-03-25T11:37:10,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [40060ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:37:28,721][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017e4258@53eea61c] took [7180ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:38:40,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:38:46,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62288381160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:38:55,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15422ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:05,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15421760233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:15,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:26,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19936948454ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:38,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23657ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:43,532][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:47188}] took [23657ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:39:46,322][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1906][60] duration [53.9s], collections [1]/[2.4m], total [53.9s]/[1.6m], memory [195mb]->[166.4mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [155.6mb]->[155.6mb]/[2gb]}{[survivor] [7.3mb]->[6.8mb]/[0b]}
[2022-03-25T11:39:45,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23657591644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:56,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18445ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:39:56,890][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1906] overhead, spent [53.9s] collecting in the last [2.4m]
[2022-03-25T11:40:02,294][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18445108165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:40:03,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [77461ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:40:11,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@79081ab0, interval=1m}] took [12786ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:40:10,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12787ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:40:19,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12786585681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:40:31,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20181ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:40:41,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20181450949ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:40:51,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:40:39,846][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20182ms] which is above the warn threshold of [5s]
[2022-03-25T11:41:01,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20506553203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:41:40,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:41:59,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47657579255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:42:11,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [47657ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:42:21,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:42:39,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43257570023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:42:54,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33088ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:43:07,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33s/33088227825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:42:33,462][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13932] timed out after [161731ms]
[2022-03-25T11:43:22,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:43:29,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26914575543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:43:37,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:43:45,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16199303619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:03,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:04,427][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48022}] took [20738ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:44:11,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20737886789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:20,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21691ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:32,785][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48022}] took [21692ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:44:35,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21691302184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:37,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [21691ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:44:43,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:44,526][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48022}] took [23076ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:44:48,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23s/23076599326ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:44:55,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11838ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:01,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11837834499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:09,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:00,315][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11838ms] which is above the warn threshold of [5s]
[2022-03-25T11:45:16,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14259201937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:21,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:27,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12424571104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:31,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [12424ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:45:33,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12182ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:38,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12181892332ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:45,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:45,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f953c0, interval=5s}] took [11274ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:45:51,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11274941711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:45:59,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14040ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:10,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14040262772ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:19,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:25,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19865745847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:30,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [19865ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:46:33,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:40,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13664513298ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:33,516][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13995] timed out after [61979ms]
[2022-03-25T11:46:50,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17258ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:38,908][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve stats for node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]] request_id [13994] timed out after [61979ms]
[2022-03-25T11:46:59,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17258316913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:47:10,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [17980ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:47:08,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:47:27,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17980502827ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:46:58,582][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.1m/126807ms] ago, timed out [1m/64828ms] ago, action [cluster:monitor/nodes/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13994]
[2022-03-25T11:47:34,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26822ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:47:45,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26822076919ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:47:54,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19316ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:01,692][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [46138ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:48:02,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19316517629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:10,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:20,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16389254114ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:28,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:30,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [18036ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:48:36,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18036984865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:39,660][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11m/660395ms] ago, timed out [8.3m/498664ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13932]
[2022-03-25T11:48:43,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:50,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14489280020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:57,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:49:09,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14713432868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:48:56,274][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14489ms] which is above the warn threshold of [5s]
[2022-03-25T11:49:18,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017e4258@323dd01d] took [14713ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:49:20,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:49:26,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22530583842ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:49:40,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:49:49,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16743945953ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:49:52,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [16743ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:49:59,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22382ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:50:08,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22381636877ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:50:17,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:50:17,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@768ea5fc] took [16722ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:50:26,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16722321962ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:50:34,222][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:50:41,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18008559839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:50:50,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:00,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16142763168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:16,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:20,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [40699ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:51:27,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.5s/24557018927ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:39,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:48,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24792663769ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:52,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12598ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:54,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12598456072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:51:59,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:01,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [7222ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:52:05,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7222092857ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:08,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:10,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [9374ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:52:11,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9374531814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:18,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:23,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [9199ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:52:22,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9199545849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:25,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:26,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [7730ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:52:28,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7730198020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:52:42,712][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [7.9m/479243ms] ago, timed out [6.9m/417264ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13995]
[2022-03-25T11:53:23,581][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1942][62] duration [2.2s], collections [1]/[4.2s], total [2.2s]/[1.6m], memory [224.2mb]->[169.3mb]/[2gb], all_pools {[young] [68mb]->[44mb]/[0b]}{[old] [155.6mb]->[155.6mb]/[2gb]}{[survivor] [4.6mb]->[5.7mb]/[0b]}
[2022-03-25T11:53:23,833][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1942] overhead, spent [2.2s] collecting in the last [4.2s]
[2022-03-25T11:53:34,281][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1947][63] duration [2.6s], collections [1]/[3.9s], total [2.6s]/[1.6m], memory [229.3mb]->[163.7mb]/[2gb], all_pools {[young] [72mb]->[80mb]/[0b]}{[old] [155.6mb]->[155.6mb]/[2gb]}{[survivor] [5.7mb]->[8.1mb]/[0b]}
[2022-03-25T11:53:34,887][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1947] overhead, spent [2.6s] collecting in the last [3.9s]
[2022-03-25T11:53:40,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [10443ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:53:40,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5024ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:53:41,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5023883207ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:53:42,068][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1948][64] duration [3.4s], collections [1]/[8.2s], total [3.4s]/[1.7m], memory [163.7mb]->[175.2mb]/[2gb], all_pools {[young] [80mb]->[32mb]/[0b]}{[old] [155.6mb]->[155.6mb]/[2gb]}{[survivor] [8.1mb]->[7.5mb]/[0b]}
[2022-03-25T11:53:42,212][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1948] overhead, spent [3.4s] collecting in the last [8.2s]
[2022-03-25T11:53:56,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:53:57,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6840688899ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:54:04,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:54:04,410][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1952][66] duration [10.6s], collections [2]/[8s], total [10.6s]/[1.9m], memory [195.2mb]->[165.1mb]/[2gb], all_pools {[young] [36mb]->[84mb]/[0b]}{[old] [155.6mb]->[155.6mb]/[2gb]}{[survivor] [7.5mb]->[9.5mb]/[0b]}
[2022-03-25T11:54:05,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7909842339ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:54:05,973][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1952] overhead, spent [10.6s] collecting in the last [8s]
[2022-03-25T11:54:06,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [14750ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:54:18,882][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1957][67] duration [2.4s], collections [1]/[4.2s], total [2.4s]/[1.9m], memory [220.8mb]->[164.8mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [158.2mb]->[158.2mb]/[2gb]}{[survivor] [6.5mb]->[6.6mb]/[0b]}
[2022-03-25T11:54:21,543][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1957] overhead, spent [2.4s] collecting in the last [4.2s]
[2022-03-25T11:54:27,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [5232ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:54:32,887][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1958][69] duration [5.1s], collections [2]/[9.7s], total [5.1s]/[2m], memory [164.8mb]->[227.2mb]/[2gb], all_pools {[young] [4mb]->[64mb]/[0b]}{[old] [158.2mb]->[159.6mb]/[2gb]}{[survivor] [6.6mb]->[7.5mb]/[0b]}
[2022-03-25T11:54:34,910][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1958] overhead, spent [5.1s] collecting in the last [9.7s]
[2022-03-25T11:54:35,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [7310ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:54:48,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:54:48,564][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1959][71] duration [8.1s], collections [2]/[11.7s], total [8.1s]/[2.1m], memory [227.2mb]->[167.2mb]/[2gb], all_pools {[young] [64mb]->[32mb]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [7.5mb]->[6.8mb]/[0b]}
[2022-03-25T11:54:48,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7100475099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:54:48,801][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1959] overhead, spent [8.1s] collecting in the last [11.7s]
[2022-03-25T11:54:48,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [7901ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:55:00,674][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1963][72] duration [2.7s], collections [1]/[1.7s], total [2.7s]/[2.2m], memory [234.5mb]->[254.5mb]/[2gb], all_pools {[young] [68mb]->[36mb]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [6.8mb]->[4.5mb]/[0b]}
[2022-03-25T11:55:01,146][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1963] overhead, spent [2.7s] collecting in the last [1.7s]
[2022-03-25T11:55:11,517][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1968][73] duration [1.7s], collections [1]/[2.8s], total [1.7s]/[2.2m], memory [228.2mb]->[166.2mb]/[2gb], all_pools {[young] [68mb]->[12mb]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [4.5mb]->[6.5mb]/[0b]}
[2022-03-25T11:55:12,454][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1968] overhead, spent [1.7s] collecting in the last [2.8s]
[2022-03-25T11:55:14,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [5994ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:55:20,049][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1969][74] duration [2.9s], collections [1]/[8.3s], total [2.9s]/[2.3m], memory [166.2mb]->[166.4mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [6.5mb]->[6.7mb]/[0b]}
[2022-03-25T11:55:20,535][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1969] overhead, spent [2.9s] collecting in the last [8.3s]
[2022-03-25T11:55:26,204][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1971][75] duration [1.7s], collections [1]/[1.2s], total [1.7s]/[2.3m], memory [250.4mb]->[250.4mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [6.7mb]->[5.1mb]/[0b]}
[2022-03-25T11:55:27,167][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1971] overhead, spent [1.7s] collecting in the last [1.2s]
[2022-03-25T11:55:28,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [5360ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:55:38,923][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1974][76] duration [3.1s], collections [1]/[5.1s], total [3.1s]/[2.3m], memory [188.8mb]->[166.6mb]/[2gb], all_pools {[young] [24mb]->[32mb]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [5.1mb]->[6.9mb]/[0b]}
[2022-03-25T11:55:40,599][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1974] overhead, spent [3.1s] collecting in the last [5.1s]
[2022-03-25T11:55:41,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [9197ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:55:57,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:55:57,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5421304838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:56:01,186][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1979][77] duration [3.9s], collections [1]/[7.1s], total [3.9s]/[2.4m], memory [230.6mb]->[222.3mb]/[2gb], all_pools {[young] [64mb]->[60mb]/[0b]}{[old] [159.6mb]->[159.6mb]/[2gb]}{[survivor] [6.9mb]->[10.7mb]/[0b]}
[2022-03-25T11:56:05,667][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1979] overhead, spent [3.9s] collecting in the last [7.1s]
[2022-03-25T11:56:05,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:56:05,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [8018ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:56:05,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8018470754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:56:07,813][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1980][79] duration [4.8s], collections [2]/[9.7s], total [4.8s]/[2.5m], memory [222.3mb]->[205.1mb]/[2gb], all_pools {[young] [60mb]->[36mb]/[0b]}{[old] [159.6mb]->[161.4mb]/[2gb]}{[survivor] [10.7mb]->[7.6mb]/[0b]}
[2022-03-25T11:56:09,649][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1980] overhead, spent [4.8s] collecting in the last [9.7s]
[2022-03-25T11:56:16,923][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1982][80] duration [2.8s], collections [1]/[5.4s], total [2.8s]/[2.5m], memory [233.1mb]->[218.6mb]/[2gb], all_pools {[young] [64mb]->[52mb]/[0b]}{[old] [161.4mb]->[161.4mb]/[2gb]}{[survivor] [7.6mb]->[9.2mb]/[0b]}
[2022-03-25T11:56:20,783][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1982] overhead, spent [2.8s] collecting in the last [5.4s]
[2022-03-25T11:56:21,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [5056ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:56:23,698][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1983][81] duration [2.5s], collections [1]/[6.1s], total [2.5s]/[2.6m], memory [218.6mb]->[206.2mb]/[2gb], all_pools {[young] [52mb]->[40mb]/[0b]}{[old] [161.4mb]->[162.5mb]/[2gb]}{[survivor] [9.2mb]->[7.7mb]/[0b]}
[2022-03-25T11:56:24,368][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1983] overhead, spent [2.5s] collecting in the last [6.1s]
[2022-03-25T11:56:32,348][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1985][82] duration [1s], collections [1]/[2.1s], total [1s]/[2.6m], memory [210.2mb]->[258.2mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [162.5mb]->[162.5mb]/[2gb]}{[survivor] [7.7mb]->[5.6mb]/[0b]}
[2022-03-25T11:56:33,086][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1985] overhead, spent [1s] collecting in the last [2.1s]
[2022-03-25T11:57:14,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [6031ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:57:44,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:57:44,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017e4258@22ad63ec] took [17851ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:57:44,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17008576172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:57:45,316][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1996][83] duration [12.2s], collections [1]/[23.7s], total [12.2s]/[2.8m], memory [216.1mb]->[171.5mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [162.5mb]->[162.5mb]/[2gb]}{[survivor] [5.6mb]->[4.9mb]/[0b]}
[2022-03-25T11:57:45,993][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1996] overhead, spent [12.2s] collecting in the last [23.7s]
[2022-03-25T11:57:58,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:57:58,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f953c0, interval=5s}] took [5760ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:57:59,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5760779975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:58:08,348][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1999][85] duration [8.5s], collections [2]/[8s], total [8.5s]/[2.9m], memory [219.5mb]->[212.9mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [162.5mb]->[162.5mb]/[2gb]}{[survivor] [4.9mb]->[7.4mb]/[0b]}
[2022-03-25T11:58:08,139][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48056}] took [9252ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:58:08,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9252ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:58:08,139][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48052}] took [9252ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:58:10,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9252435304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:58:10,120][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1999] overhead, spent [8.5s] collecting in the last [8s]
[2022-03-25T11:58:12,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [9252ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:58:24,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [7031ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:58:37,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T11:58:38,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11239715901ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T11:58:54,872][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2001][86] duration [7.1s], collections [1]/[28.3s], total [7.1s]/[3.1m], memory [185.9mb]->[196.3mb]/[2gb], all_pools {[young] [20mb]->[24mb]/[0b]}{[old] [162.5mb]->[162.5mb]/[2gb]}{[survivor] [7.4mb]->[9.7mb]/[0b]}
[2022-03-25T11:58:58,237][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2001] overhead, spent [7.1s] collecting in the last [28.3s]
[2022-03-25T11:59:05,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [17934ms] which is above the warn threshold of [5000ms]
[2022-03-25T11:59:19,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [7949ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:00:07,872][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48052}] took [5248ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:02:14,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:02:25,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119586905384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:02:31,260][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2003][87] duration [1.7m], collections [1]/[50.1s], total [1.7m]/[4.9m], memory [204.3mb]->[244.3mb]/[2gb], all_pools {[young] [36mb]->[0b]/[0b]}{[old] [162.5mb]->[165mb]/[2gb]}{[survivor] [9.7mb]->[6.2mb]/[0b]}
[2022-03-25T12:02:32,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:02:37,414][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2003] overhead, spent [1.7m] collecting in the last [50.1s]
[2022-03-25T12:02:39,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18691627984ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:02:46,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [150730ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:02:48,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:02:59,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13999521639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:03:09,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:03:12,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017e4258@64ae9b61] took [22150ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:03:23,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.1s/22150469255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:03:34,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:03:50,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.1s/25107247754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:04:03,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:04:16,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29s/29033644230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:04:36,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:04:47,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28811772523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:05:03,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:05:12,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29996554335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:04:46,675][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [28811ms] which is above the warn threshold of [5s]
[2022-03-25T12:05:23,651][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:05:31,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21295645597ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:05:43,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:05:49,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [71328ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:05:52,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20036727006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:03,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19484ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:17,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19483465566ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:29,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:40,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23860803395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:50,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:55,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22495157627ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:06:41,048][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [14561] timed out after [173765ms]
[2022-03-25T12:07:05,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:07:13,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15984681525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:07:21,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [15984ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:07:23,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:07:30,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17892646908ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:07:38,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:07:45,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15274623880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:07:55,234][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:06,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16996064080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:07,072][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [32270ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:08:16,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19980ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:25,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19980160924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:32,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:36,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5125/0x00000008017e4258@43532ee0] took [17012ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:08:39,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17s/17012550051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:49,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:57,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17724982395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:08:47,631][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17012ms] which is above the warn threshold of [5s]
[2022-03-25T12:09:06,907][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16991ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:14,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16990690172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:14,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [34715ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:09:19,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12241ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:24,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12241308594ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:32,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:29,752][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [46.9s/46957ms] ago, timed out [0s/0ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [14623]
[2022-03-25T12:09:36,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10730133860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:38,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [10730ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:09:44,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14675ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:51,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14674428656ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:09:57,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12414ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:10:04,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12413666197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:10:14,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17174ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:10:00,015][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [14623] timed out after [46957ms]
[2022-03-25T12:10:20,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [29588ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:10:21,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17174677918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:10:30,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:10:37,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15461458397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:12:43,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118441ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:13:01,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118441541165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:13:35,244][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.3m/559114ms] ago, timed out [6.4m/385349ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [14561]
[2022-03-25T12:13:35,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:14:04,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65955579620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:14:35,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61285ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:14:59,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61285601373ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:16:11,130][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][2010][88] duration [1.3m], collections [1]/[4m], total [1.3m]/[6.2m], memory [255.2mb]->[170.2mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [165mb]->[165mb]/[2gb]}{[survivor] [6.2mb]->[5.2mb]/[0b]}
[2022-03-25T12:16:22,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:16:45,552][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][2010] overhead, spent [1.3m] collecting in the last [4m]
[2022-03-25T12:16:17,358][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [42.5s/42581ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6df6acca]], which exceeds the warn threshold of [10s]
[2022-03-25T12:16:54,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107179996746ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:17:40,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77772ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:17:27,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [168465ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:18:23,536][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [23s/23045ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e971dd6c], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f18e0089]], which exceeds the warn threshold of [10s]
[2022-03-25T12:18:40,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77772015222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:19:19,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:19:43,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/96432073395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:20:04,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:20:24,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46981104714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:20:51,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:20:50,221][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [45369ms] which is above the warn threshold of [5s]
[2022-03-25T12:21:25,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.3s/45369116953ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:22:06,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77194ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:22:20,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5ff0bebb, interval=5s}] took [77193ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:22:22,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77193977644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:22:39,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33175ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:22:55,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33175431698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:23:26,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39068ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:23:51,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39s/39068243419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:24:04,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:24:17,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.2s/46283462780ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:24:33,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:24:49,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27539606403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:25:08,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35730ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:25:23,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.7s/35729220708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:25:38,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:25:52,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [66274ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:25:56,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30545475512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:26:12,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33324ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:26:27,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33323382262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:26:42,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30141ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:26:49,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30141310818ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:26:59,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:26:58,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f953c0, interval=5s}] took [15658ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:27:07,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15658015368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:27:16,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:27:26,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18949174580ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:27:36,720][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19614ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:27:42,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [38563ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:27:24,698][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [14683] timed out after [415209ms]
[2022-03-25T12:27:27,927][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18949ms] which is above the warn threshold of [5s]
[2022-03-25T12:27:46,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19614224620ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:27:54,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:28:01,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18077549460ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:28:10,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:28:14,046][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [8.8m/533728ms] ago, timed out [1.9m/118519ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [14683]
[2022-03-25T12:28:18,187][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16078897802ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:28:33,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:28:42,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22746216850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:28:52,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19974ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:29:08,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19973791534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:29:17,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24304ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:29:26,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24304767829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:29:30,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [24304ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:29:36,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:29:46,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18529239419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:29:55,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:30:08,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19413799187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:30:18,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:30:30,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22379031722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:30:46,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26345ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:30:58,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26345635632ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:31:06,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [26345ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:31:13,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:31:28,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27962244322ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:31:40,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6f953c0, interval=5s}] took [28044ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:31:40,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:31:54,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28s/28044352027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:32:04,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:31:42,977][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [14770] timed out after [104601ms]
[2022-03-25T12:32:14,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24296944846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:32:25,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:32:05,949][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [24297ms] which is above the warn threshold of [5s]
[2022-03-25T12:32:41,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20440169392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:32:53,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:33:07,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28294605201ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:34:28,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1ba4ee88, interval=1s}] took [28294ms] which is above the warn threshold of [5000ms]
[2022-03-25T12:36:02,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/188369ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:37:25,089][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.5s/21525ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a6b1c023]], which exceeds the warn threshold of [10s]
[2022-03-25T12:39:10,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.1m/188368867646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:41:07,541][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.4s/10417ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a6b1c023]], which exceeds the warn threshold of [10s]
[2022-03-25T12:42:37,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394821ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:44:04,939][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [14m/843174ms] ago, timed out [12.3m/738573ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EInU_N--QqGX9PkrYEuGGA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [14770]
[2022-03-25T12:45:54,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/394820772863ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:48:40,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:47:26,722][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [58.3s/58329ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e971dd6c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6df6acca], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f18e0089]], which exceeds the warn threshold of [10s]
[2022-03-25T12:51:20,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/363165184026ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T12:52:33,260][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.3s/24302ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e971dd6c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6df6acca], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f18e0089]], which exceeds the warn threshold of [10s]
[2022-03-25T12:54:20,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T12:58:43,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339336262981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:02:07,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:04:59,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467511628893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:08:06,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:11:15,735][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/359082460910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:13:07,026][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.7m/702502ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [17.7m/1064919ms]
[2022-03-25T13:16:46,066][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [31.5m/1891431ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [34.8m/2090580ms]
[2022-03-25T13:14:08,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/362423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:27:26,046][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T13:27:26,107][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T13:27:26,110][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T13:27:33,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T13:27:33,773][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T13:27:33,774][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T13:27:33,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T13:27:33,775][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T13:27:33,777][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T13:27:33,778][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T13:27:33,779][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T13:27:33,780][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T13:27:33,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T13:27:33,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T13:27:33,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T13:27:33,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T13:27:33,786][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T13:27:33,788][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T13:27:33,789][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T13:27:33,790][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T13:27:33,791][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T13:27:33,791][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T13:27:33,795][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T13:27:33,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T13:27:33,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T13:27:33,797][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T13:27:33,798][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T13:27:33,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T13:27:33,799][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T13:27:33,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T13:27:33,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T13:27:33,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T13:27:33,803][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T13:27:33,804][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T13:27:33,806][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T13:27:33,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T13:27:33,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T13:27:33,812][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T13:27:33,813][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T13:27:33,813][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T13:27:33,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T13:27:33,816][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T13:27:33,818][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T13:27:33,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T13:27:33,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T13:27:33,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T13:27:33,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T13:27:33,824][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T13:27:33,825][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T13:27:33,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T13:27:33,826][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T13:27:33,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T13:27:33,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T13:27:33,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T13:27:33,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T13:27:33,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T13:27:33,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T13:27:33,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T13:27:33,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T13:27:33,859][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T13:27:33,860][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T13:27:33,864][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T13:27:34,019][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.9gb], net total_space [125.8gb], types [ext4]
[2022-03-25T13:27:34,022][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T13:27:34,467][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T13:27:49,478][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T13:27:49,491][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-25T13:27:49,493][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T13:27:49,495][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-25T13:27:49,496][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T13:27:49,497][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-25T13:27:49,497][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-25T13:27:49,498][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T13:27:49,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-25T13:27:49,501][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-25T13:27:49,502][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-25T13:27:49,503][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T13:27:49,504][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T13:27:49,505][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-25T13:27:49,506][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T13:27:51,099][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T13:27:51,283][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T13:27:52,456][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T13:27:53,375][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T13:27:53,376][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T13:27:53,477][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T13:27:53,490][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T13:27:53,763][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T13:27:56,214][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T13:27:56,370][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{etCyQgIBS7isDpGApHabMg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 115, version: 3275, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{etCyQgIBS7isDpGApHabMg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T13:27:56,531][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{etCyQgIBS7isDpGApHabMg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 115, version: 3275, reason: Publication{term=115, version=3275}
[2022-03-25T13:27:56,676][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T13:27:56,676][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T13:27:57,602][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T13:27:57,611][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T13:27:59,812][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T13:27:59,852][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T13:35:57,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:41:40,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88449498435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:43:59,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2m/857459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:46:20,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2m/857295696595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:48:44,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/280903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:51:17,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/280681554050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:53:58,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T13:56:57,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313862145848ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T13:59:29,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330973ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:01:53,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330749876006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:04:22,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:00:03,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@490b94ab, interval=5s}] took [1451839ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:07:00,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/291911544787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:09:42,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:12:46,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320559749427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:15:10,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6f6822f0, interval=1s}] took [320559ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:15:39,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:18:21,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352434648937ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:21:14,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:23:51,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339540574136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:26:34,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:29:26,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326194612388ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:32:20,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:41:17,268][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-19-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-25T14:41:17,294][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-25T14:41:17,297][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-25T14:41:25,273][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-25T14:41:25,282][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-25T14:41:25,284][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-25T14:41:25,285][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-25T14:41:25,286][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-25T14:41:25,287][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-25T14:41:25,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-25T14:41:25,288][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-25T14:41:25,289][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-25T14:41:25,290][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-25T14:41:25,291][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-25T14:41:25,292][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-25T14:41:25,292][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-25T14:41:25,293][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-25T14:41:25,294][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-25T14:41:25,295][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-25T14:41:25,295][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-25T14:41:25,296][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-25T14:41:25,297][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-25T14:41:25,297][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-25T14:41:25,298][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-25T14:41:25,299][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-25T14:41:25,299][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-25T14:41:25,300][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-25T14:41:25,301][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-25T14:41:25,302][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-25T14:41:25,302][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-25T14:41:25,303][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-25T14:41:25,304][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-25T14:41:25,305][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-25T14:41:25,305][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-25T14:41:25,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-25T14:41:25,307][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-25T14:41:25,307][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-25T14:41:25,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-25T14:41:25,309][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-25T14:41:25,310][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-25T14:41:25,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-25T14:41:25,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-25T14:41:25,312][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-25T14:41:25,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-25T14:41:25,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-25T14:41:25,314][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-25T14:41:25,314][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-25T14:41:25,315][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-25T14:41:25,316][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-25T14:41:25,316][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-25T14:41:25,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-25T14:41:25,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-25T14:41:25,318][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-25T14:41:25,318][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-25T14:41:25,319][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-25T14:41:25,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-25T14:41:25,321][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-25T14:41:25,321][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-25T14:41:25,322][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-25T14:41:25,323][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-25T14:41:25,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-25T14:41:25,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-25T14:41:25,492][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104gb], net total_space [125.8gb], types [ext4]
[2022-03-25T14:41:25,493][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-25T14:41:25,897][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-25T14:42:43,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11385ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:42:42,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@7c40bc7f, interval=30s}] took [7124ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:42:48,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11385130650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:42:49,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:42:49,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7269758751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:43:13,827][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-25T14:43:13,834][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-25T14:43:17,167][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-25T14:43:17,571][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-25T14:43:19,923][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-25T14:43:23,289][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-25T14:43:23,302][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-25T14:43:23,456][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-25T14:43:23,460][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-25T14:43:24,286][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-25T14:43:29,880][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-25T14:43:30,182][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{aX-xAsVrQaCJTSucQDKpvA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 116, version: 3280, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{aX-xAsVrQaCJTSucQDKpvA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-25T14:43:30,363][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{aX-xAsVrQaCJTSucQDKpvA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 116, version: 3280, reason: Publication{term=116, version=3280}
[2022-03-25T14:43:30,535][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-25T14:43:30,536][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-25T14:43:32,761][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-25T14:43:32,786][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [27] indices into cluster_state
[2022-03-25T14:43:35,031][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-25T14:43:35,049][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-25T14:43:38,645][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-25T14:43:38,670][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-25T14:43:38,679][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-25T14:43:40,547][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-25T14:43:45,140][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-25T14:43:45,140][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-25T14:43:52,017][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-25T14:44:10,147][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][37][18] duration [1.8s], collections [1]/[4.6s], total [1.8s]/[4.2s], memory [203.4mb]->[156.1mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [129.6mb]->[140.8mb]/[2gb]}{[survivor] [17.8mb]->[11.3mb]/[0b]}
[2022-03-25T14:44:10,262][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][37] overhead, spent [1.8s] collecting in the last [4.6s]
[2022-03-25T14:44:13,509][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][38][19] duration [1.4s], collections [1]/[3.4s], total [1.4s]/[5.6s], memory [156.1mb]->[157.2mb]/[2gb], all_pools {[young] [4mb]->[4mb]/[0b]}{[old] [140.8mb]->[146.8mb]/[2gb]}{[survivor] [11.3mb]->[10.3mb]/[0b]}
[2022-03-25T14:44:14,565][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][38] overhead, spent [1.4s] collecting in the last [3.4s]
[2022-03-25T14:44:21,218][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0], [logstash-2022.03.13][0], [.kibana-event-log-7.16.2-000001][0]]]).
[2022-03-25T14:44:27,358][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [24.3s/24311ms] ago, timed out [7.7s/7735ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{aX-xAsVrQaCJTSucQDKpvA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [191]
[2022-03-25T14:44:37,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3e6cd6cb, interval=5s}] took [6681ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:44:37,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:44:38,378][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [191] timed out after [16576ms]
[2022-03-25T14:44:38,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6481674954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:44:38,506][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12510ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [24] unchanged indices
[2022-03-25T14:44:38,697][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][46][20] duration [3.3s], collections [1]/[8.3s], total [3.3s]/[9s], memory [225.2mb]->[167.7mb]/[2gb], all_pools {[young] [72mb]->[8mb]/[0b]}{[old] [146.8mb]->[151.9mb]/[2gb]}{[survivor] [10.3mb]->[7.7mb]/[0b]}
[2022-03-25T14:44:38,698][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][46] overhead, spent [3.3s] collecting in the last [8.3s]
[2022-03-25T14:45:31,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25333ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:45:04,496][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37504}] took [14700ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:45:31,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25333090039ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:45:46,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5187ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:45:48,276][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][55][21] duration [19.2s], collections [1]/[33.3s], total [19.2s]/[28.2s], memory [187.7mb]->[158.7mb]/[2gb], all_pools {[young] [28mb]->[4mb]/[0b]}{[old] [151.9mb]->[151.9mb]/[2gb]}{[survivor] [7.7mb]->[6.8mb]/[0b]}
[2022-03-25T14:45:48,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5187404681ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:45:49,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][55] overhead, spent [19.2s] collecting in the last [33.3s]
[2022-03-25T14:45:50,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [15291ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:45:59,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017d9b18@42471784] took [7524ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:45:59,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:45:59,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5569246479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:46:01,306][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][56][22] duration [4.2s], collections [1]/[25.5s], total [4.2s]/[32.5s], memory [158.7mb]->[163.9mb]/[2gb], all_pools {[young] [4mb]->[8mb]/[0b]}{[old] [151.9mb]->[151.9mb]/[2gb]}{[survivor] [6.8mb]->[3.9mb]/[0b]}
[2022-03-25T14:46:05,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [5171ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:46:11,181][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48750}] took [6976ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:46:12,515][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48726}] took [10393ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:46:21,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [5453ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:46:21,648][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37488}] took [6654ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:46:44,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6089ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:46:44,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6089234991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:46:44,999][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][66][23] duration [4.4s], collections [1]/[1.3s], total [4.4s]/[36.9s], memory [239.9mb]->[247.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [151.9mb]->[151.9mb]/[2gb]}{[survivor] [3.9mb]->[7mb]/[0b]}
[2022-03-25T14:46:45,330][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][66] overhead, spent [4.4s] collecting in the last [1.3s]
[2022-03-25T14:46:46,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [8507ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:47:05,796][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][72][24] duration [3s], collections [1]/[5s], total [3s]/[40s], memory [243mb]->[159.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [151.9mb]->[151.9mb]/[2gb]}{[survivor] [7mb]->[8mb]/[0b]}
[2022-03-25T14:47:05,929][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][72] overhead, spent [3s] collecting in the last [5s]
[2022-03-25T14:47:09,277][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][74][25] duration [971ms], collections [1]/[1.9s], total [971ms]/[41s], memory [243.9mb]->[162mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [151.9mb]->[151.9mb]/[2gb]}{[survivor] [8mb]->[10mb]/[0b]}
[2022-03-25T14:47:10,033][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][74] overhead, spent [971ms] collecting in the last [1.9s]
[2022-03-25T14:47:16,010][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][76][26] duration [2s], collections [1]/[3.3s], total [2s]/[43s], memory [170mb]->[162.7mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [151.9mb]->[153.8mb]/[2gb]}{[survivor] [10mb]->[8.8mb]/[0b]}
[2022-03-25T14:47:16,358][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][76] overhead, spent [2s] collecting in the last [3.3s]
[2022-03-25T14:47:19,136][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][77] overhead, spent [679ms] collecting in the last [1.7s]
[2022-03-25T14:47:27,774][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][28] duration [1.8s], collections [1]/[1.5s], total [1.8s]/[45.5s], memory [182.6mb]->[242.6mb]/[2gb], all_pools {[young] [24mb]->[32mb]/[0b]}{[old] [153.8mb]->[153.8mb]/[2gb]}{[survivor] [8.7mb]->[7.1mb]/[0b]}
[2022-03-25T14:47:27,980][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [1.8s] collecting in the last [1.5s]
[2022-03-25T14:47:33,174][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][82][29] duration [1.2s], collections [1]/[1.3s], total [1.2s]/[46.8s], memory [208.9mb]->[232.9mb]/[2gb], all_pools {[young] [52mb]->[0b]/[0b]}{[old] [153.8mb]->[153.8mb]/[2gb]}{[survivor] [7.1mb]->[10.6mb]/[0b]}
[2022-03-25T14:47:33,410][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][82] overhead, spent [1.2s] collecting in the last [1.3s]
[2022-03-25T14:47:41,020][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][85][30] duration [2s], collections [1]/[3.8s], total [2s]/[48.9s], memory [176.4mb]->[163.3mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [153.8mb]->[156.6mb]/[2gb]}{[survivor] [10.6mb]->[6.6mb]/[0b]}
[2022-03-25T14:47:41,441][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][85] overhead, spent [2s] collecting in the last [3.8s]
[2022-03-25T14:47:46,181][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][86][31] duration [2.8s], collections [1]/[5.3s], total [2.8s]/[51.8s], memory [163.3mb]->[163.2mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [156.6mb]->[156.6mb]/[2gb]}{[survivor] [6.6mb]->[6.5mb]/[0b]}
[2022-03-25T14:47:46,425][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][86] overhead, spent [2.8s] collecting in the last [5.3s]
[2022-03-25T14:48:02,362][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48756}] took [5625ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:48:02,360][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48750}] took [5224ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:48:05,983][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][91][32] duration [3s], collections [1]/[8.5s], total [3s]/[54.8s], memory [243.2mb]->[165.2mb]/[2gb], all_pools {[young] [80mb]->[40mb]/[0b]}{[old] [156.6mb]->[156.6mb]/[2gb]}{[survivor] [6.5mb]->[8.6mb]/[0b]}
[2022-03-25T14:48:07,200][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][91] overhead, spent [3s] collecting in the last [8.5s]
[2022-03-25T14:48:14,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6286ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:48:14,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [12676ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:48:14,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6285812996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:48:17,231][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][92][33] duration [3.8s], collections [1]/[14.9s], total [3.8s]/[58.6s], memory [165.2mb]->[222.4mb]/[2gb], all_pools {[young] [40mb]->[56mb]/[0b]}{[old] [156.6mb]->[156.9mb]/[2gb]}{[survivor] [8.6mb]->[9.5mb]/[0b]}
[2022-03-25T14:48:17,514][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][92] overhead, spent [3.8s] collecting in the last [14.9s]
[2022-03-25T14:48:22,530][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][34] duration [2.1s], collections [1]/[5s], total [2.1s]/[1m], memory [222.4mb]->[166.9mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [156.9mb]->[157.3mb]/[2gb]}{[survivor] [9.5mb]->[9.6mb]/[0b]}
[2022-03-25T14:48:23,441][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [2.1s] collecting in the last [5s]
[2022-03-25T14:48:48,817][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3e6cd6cb, interval=5s}] took [8805ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:48:55,235][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][99][35] duration [2.5s], collections [1]/[1.3s], total [2.5s]/[1m], memory [242.9mb]->[164.1mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [157.3mb]->[159.3mb]/[2gb]}{[survivor] [9.6mb]->[4.8mb]/[0b]}
[2022-03-25T14:48:55,506][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][99] overhead, spent [2.5s] collecting in the last [1.3s]
[2022-03-25T14:49:17,097][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][107][36] duration [3.4s], collections [1]/[5.5s], total [3.4s]/[1.1m], memory [236.1mb]->[164.3mb]/[2gb], all_pools {[young] [72mb]->[28mb]/[0b]}{[old] [159.3mb]->[159.3mb]/[2gb]}{[survivor] [4.8mb]->[5mb]/[0b]}
[2022-03-25T14:49:17,678][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [3.4s] collecting in the last [5.5s]
[2022-03-25T14:49:42,982][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:45442}] took [88362ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:49:50,037][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][119][37] duration [1.7s], collections [1]/[1.6s], total [1.7s]/[1.1m], memory [248.3mb]->[252.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [159.3mb]->[159.3mb]/[2gb]}{[survivor] [5mb]->[5.4mb]/[0b]}
[2022-03-25T14:49:50,381][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][119] overhead, spent [1.7s] collecting in the last [1.6s]
[2022-03-25T14:50:07,171][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][38] duration [2.4s], collections [1]/[4.9s], total [2.4s]/[1.1m], memory [240.7mb]->[167.3mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [159.3mb]->[159.3mb]/[2gb]}{[survivor] [5.4mb]->[8mb]/[0b]}
[2022-03-25T14:50:07,709][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][127] overhead, spent [2.4s] collecting in the last [4.9s]
[2022-03-25T14:50:20,682][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][132][39] duration [2.1s], collections [1]/[3.9s], total [2.1s]/[1.2m], memory [203.3mb]->[169.4mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [159.3mb]->[159.3mb]/[2gb]}{[survivor] [8mb]->[10.1mb]/[0b]}
[2022-03-25T14:50:21,210][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][132] overhead, spent [2.1s] collecting in the last [3.9s]
[2022-03-25T14:50:38,946][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][139][40] duration [3.5s], collections [1]/[5.9s], total [3.5s]/[1.2m], memory [241.4mb]->[180.6mb]/[2gb], all_pools {[young] [76mb]->[24mb]/[0b]}{[old] [159.3mb]->[161.2mb]/[2gb]}{[survivor] [10.1mb]->[7.3mb]/[0b]}
[2022-03-25T14:50:39,596][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][139] overhead, spent [3.5s] collecting in the last [5.9s]
[2022-03-25T14:51:22,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3e6cd6cb, interval=5s}] took [5551ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:51:27,140][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][146][41] duration [3.4s], collections [1]/[9.1s], total [3.4s]/[1.3m], memory [208.6mb]->[173.2mb]/[2gb], all_pools {[young] [40mb]->[4mb]/[0b]}{[old] [161.2mb]->[161.2mb]/[2gb]}{[survivor] [7.3mb]->[8mb]/[0b]}
[2022-03-25T14:51:27,935][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][146] overhead, spent [3.4s] collecting in the last [9.1s]
[2022-03-25T14:51:29,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:52:35,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:52:52,504][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:52:53,765][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8226983643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:52:55,983][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][162][42] duration [5.7s], collections [1]/[2.6s], total [5.7s]/[1.4m], memory [253.2mb]->[253.2mb]/[2gb], all_pools {[young] [84mb]->[16mb]/[0b]}{[old] [161.2mb]->[161.2mb]/[2gb]}{[survivor] [8mb]->[9.2mb]/[0b]}
[2022-03-25T14:52:59,044][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][162] overhead, spent [5.7s] collecting in the last [2.6s]
[2022-03-25T14:52:59,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [16371ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:53:49,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14366ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:53:49,779][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][169][43] duration [8.4s], collections [1]/[16s], total [8.4s]/[1.5m], memory [226.5mb]->[172.6mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [161.2mb]->[162.2mb]/[2gb]}{[survivor] [9.2mb]->[10.4mb]/[0b]}
[2022-03-25T14:53:50,494][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][169] overhead, spent [8.4s] collecting in the last [16s]
[2022-03-25T14:53:50,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14366629269ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:54:30,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12627ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:54:37,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12627007192ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:54:47,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:54:55,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14754099280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:55:02,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17374ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:55:11,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17374485544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:55:11,840][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48772}] took [32128ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:55:12,935][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][176][44] duration [8.9s], collections [1]/[4.2s], total [8.9s]/[1.7m], memory [244.6mb]->[256.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [162.2mb]->[163.5mb]/[2gb]}{[survivor] [10.4mb]->[7.7mb]/[0b]}
[2022-03-25T14:55:14,858][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:55:14,756][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][176] overhead, spent [8.9s] collecting in the last [4.2s]
[2022-03-25T14:55:16,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12753146152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:55:16,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [58309ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:55:59,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [7638ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:56:33,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [6747ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:56:46,518][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:48772}] took [6259ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:57:15,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:57:17,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8999798517ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:57:19,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [10200ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:57:21,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:57:22,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6624100354ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:57:53,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@19515160, interval=5s}] took [21578ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:57:53,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T14:57:54,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20328929694ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T14:58:03,480][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][186][45] duration [17.1s], collections [1]/[29.8s], total [17.1s]/[2m], memory [255.3mb]->[171.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [163.5mb]->[163.5mb]/[2gb]}{[survivor] [7.7mb]->[7.6mb]/[0b]}
[2022-03-25T14:58:06,282][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][186] overhead, spent [17.1s] collecting in the last [29.8s]
[2022-03-25T14:58:08,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [14918ms] which is above the warn threshold of [5000ms]
[2022-03-25T14:58:22,197][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][191] overhead, spent [660ms] collecting in the last [1.3s]
[2022-03-25T14:58:24,832][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][193] overhead, spent [496ms] collecting in the last [1.2s]
[2022-03-25T14:58:32,789][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][199][48] duration [963ms], collections [1]/[1.6s], total [963ms]/[2m], memory [207.5mb]->[167.9mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [163.5mb]->[163.5mb]/[2gb]}{[survivor] [3.9mb]->[4.4mb]/[0b]}
[2022-03-25T14:58:32,925][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][199] overhead, spent [963ms] collecting in the last [1.6s]
[2022-03-25T14:58:35,294][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][201] overhead, spent [465ms] collecting in the last [1s]
[2022-03-25T15:01:31,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@19515160, interval=5s}] took [5443ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:06:18,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [5973ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:07:08,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [7806ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:12:16,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:14:43,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5405144548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:16:13,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/516660ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:17:45,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/517039687761ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:18:29,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/143341ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:17:47,809][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.8s/20826ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d]], which exceeds the warn threshold of [10s]
[2022-03-25T15:18:56,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/143458782232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:19:24,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:19:55,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54754758439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:20:01,727][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.1s/26158ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d]], which exceeds the warn threshold of [10s]
[2022-03-25T15:21:23,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:23:32,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72157033203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:24:38,042][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37550}] took [72157ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:24:38,042][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [54.9s/54958ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2c71034]], which exceeds the warn threshold of [10s]
[2022-03-25T15:25:10,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:26:01,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/269364536291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:26:10,086][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017d9b18@7ed285c8] took [269364ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:26:27,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:26:41,150][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82228469156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:26:52,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:27:01,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@3e6cd6cb, interval=5s}] took [23351ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:27:08,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23351557414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:27:23,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32441ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:27:37,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.4s/32440765325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:27:53,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:28:06,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.3s/30332703668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:27:36,723][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23352ms] which is above the warn threshold of [5s]
[2022-03-25T15:28:18,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:28:24,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22417430817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:28:32,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15987ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:28:42,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15986725050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:28:56,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:29:09,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23925929511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:29:18,396][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [23925ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:29:22,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:29:37,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25484346573ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:30:01,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:30:33,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28641446891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:31:50,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/117676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:31:25,125][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3481] timed out after [256168ms]
[2022-03-25T15:32:19,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/117675661916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:32:53,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:33:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64955125819ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:33:21,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [64955ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:33:32,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39334ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:33:43,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.3s/39334115353ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:33:52,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:34:00,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.7s/19727938367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:34:10,334][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:34:23,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18396082956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:34:38,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26817ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:34:48,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26816392361ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:34:43,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [26816ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:35:00,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@19515160, interval=5s}] took [21293ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:34:58,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:35:13,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.2s/21293437125ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:35:25,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:35:46,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26431091314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:35:48,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5107/0x00000008017d9b18@258a1cca] took [26431ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:35:57,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:36:09,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.6s/32661612522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:36:31,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30795ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:36:50,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30795733408ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:37:11,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:37:52,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.7s/42754999817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:38:52,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/100391ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:39:24,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/100390521554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:37:56,154][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [42755ms] which is above the warn threshold of [5s]
[2022-03-25T15:39:46,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [155016ms] which is above the warn threshold of [5000ms]
[2022-03-25T15:39:46,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.6s/54626ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:38:51,376][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [12m/725652ms] ago, timed out [7.8m/469484ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{aX-xAsVrQaCJTSucQDKpvA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [3481]
[2022-03-25T15:40:10,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.6s/54626094878ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:41:14,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:42:19,179][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86200911839ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:43:27,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:45:00,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/134078989614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:45:29,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122670ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:45:59,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122762144781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:46:34,078][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:47:03,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64019135122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:47:28,624][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [3601] timed out after [261229ms]
[2022-03-25T15:47:45,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70978ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:47:20,649][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.7s/11728ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb]], which exceeds the warn threshold of [10s]
[2022-03-25T15:49:46,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70691083269ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:53:14,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:55:23,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314482903370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T15:57:35,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275249ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T15:59:44,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275378003506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:01:47,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:03:01,648][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.4s/15482ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d]], which exceeds the warn threshold of [10s]
[2022-03-25T16:04:17,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251809323540ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:06:18,974][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [28.7s/28762ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2c71034]], which exceeds the warn threshold of [10s]
[2022-03-25T16:06:27,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/280515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:08:41,899][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/280582102823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:09:57,322][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [25.7s/25748ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb]], which exceeds the warn threshold of [10s]
[2022-03-25T16:10:38,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249738ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:12:48,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/249696900178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:13:47,994][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/90176ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb]], which exceeds the warn threshold of [10s]
[2022-03-25T16:15:39,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:16:49,550][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_7.17.0/_doc/config%3A7.17.0][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37552}] took [295193ms] which is above the warn threshold of [5000ms]
[2022-03-25T16:17:49,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295193735946ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:17:57,389][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [295193ms] which is above the warn threshold of [5000ms]
[2022-03-25T16:20:52,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:23:05,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318632075993ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:25:40,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288321ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:30:24,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/288432500614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:30:22,406][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [13.7m/825472ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [9.4m/565810ms]
[2022-03-25T16:32:42,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:33:34,250][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [30.8m/1853249ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [23.6m/1419345ms]
[2022-03-25T16:34:57,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420712216107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:31:23,231][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [288433ms] which is above the warn threshold of [5s]
[2022-03-25T16:40:17,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:40:33,272][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/108835ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2c71034]], which exceeds the warn threshold of [10s]
[2022-03-25T16:42:34,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456410975047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:44:30,949][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.2s/20287ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2c71034]], which exceeds the warn threshold of [10s]
[2022-03-25T16:46:28,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/370766ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:47:30,091][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [44.6m/2680905ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [36.5m/2192165ms]
[2022-03-25T16:48:21,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/371244954501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:52:05,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:52:28,202][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.4m/84521ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a]], which exceeds the warn threshold of [10s]
[2022-03-25T16:54:57,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301237289879ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T16:55:27,744][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.8s/15899ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a]], which exceeds the warn threshold of [10s]
[2022-03-25T16:57:01,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331269ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T16:58:41,886][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [55.2m/3313495ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [10.7m/647864ms]
[2022-03-25T16:58:17,325][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37542}] took [331353ms] which is above the warn threshold of [5000ms]
[2022-03-25T16:59:20,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331352880426ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:04:44,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T17:04:45,215][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.9m/178843ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2c71034]], which exceeds the warn threshold of [10s]
[2022-03-25T17:07:23,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/463277285576ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:08:43,971][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [28.3s/28338ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@15872d7d], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1da3509a], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@d2c71034]], which exceeds the warn threshold of [10s]
[2022-03-25T17:10:17,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332876ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T17:12:48,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332679987403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:12:35,146][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.1h/4109453ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [16.3m/979300ms]
[2022-03-25T17:12:58,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1a514468, interval=1s}] took [332679ms] which is above the warn threshold of [5000ms]
[2022-03-25T17:12:31,049][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37548}] took [795958ms] which is above the warn threshold of [5000ms]
[2022-03-25T17:14:44,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T17:16:06,295][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [36.5s/36562ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9a0bfcdb]], which exceeds the warn threshold of [10s]
[2022-03-25T17:17:08,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/266772308490ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:21:10,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/385710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T17:22:15,678][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37536}] took [385578ms] which is above the warn threshold of [5000ms]
[2022-03-25T17:23:29,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/385577749392ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:27:03,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354138ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T17:31:13,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354141954268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:34:08,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-25T17:36:25,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424122693660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-25T17:36:18,602][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37554}] took [424122ms] which is above the warn threshold of [5000ms]
[2022-03-25T17:32:06,222][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [354142ms] which is above the warn threshold of [5s]
[2022-03-25T17:38:53,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/283675ms] on absolute clock which is above the warn threshold of [5000ms]
