[2022-04-07T17:21:54,404][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-07T17:21:54,435][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-07T17:21:54,436][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-07T17:22:03,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-07T17:22:03,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-07T17:22:03,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-07T17:22:03,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-07T17:22:03,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-07T17:22:03,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-07T17:22:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-07T17:22:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-07T17:22:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-07T17:22:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-07T17:22:03,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-07T17:22:03,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-07T17:22:03,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-07T17:22:03,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-07T17:22:03,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-07T17:22:03,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-07T17:22:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-07T17:22:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-07T17:22:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-07T17:22:03,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-07T17:22:03,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-07T17:22:03,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-07T17:22:03,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-07T17:22:03,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-07T17:22:03,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-07T17:22:03,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-07T17:22:03,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-07T17:22:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-07T17:22:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-07T17:22:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-07T17:22:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-07T17:22:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-07T17:22:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-07T17:22:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-07T17:22:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-07T17:22:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-07T17:22:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-07T17:22:03,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-07T17:22:03,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-07T17:22:03,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-07T17:22:03,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-07T17:22:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-07T17:22:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-07T17:22:03,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-07T17:22:03,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-07T17:22:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-07T17:22:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-07T17:22:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-07T17:22:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-07T17:22:03,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-07T17:22:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-07T17:22:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-07T17:22:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-07T17:22:03,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-07T17:22:03,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-07T17:22:03,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-07T17:22:03,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-07T17:22:03,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-07T17:22:03,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-07T17:22:03,569][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.2gb], net total_space [125.8gb], types [ext4]
[2022-04-07T17:22:03,570][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-07T17:22:03,886][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-07T17:22:16,294][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-07T17:22:16,297][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-07T17:22:17,350][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-07T17:22:17,532][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-07T17:22:18,224][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-07T17:22:19,054][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-07T17:22:19,055][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-07T17:22:19,096][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-07T17:22:19,097][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-07T17:22:19,307][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-07T17:22:21,931][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-07T17:22:22,095][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2R__AD4AQIit4e9WQnq1Zg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 213, version: 7796, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2R__AD4AQIit4e9WQnq1Zg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-07T17:22:22,287][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2R__AD4AQIit4e9WQnq1Zg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 213, version: 7796, reason: Publication{term=213, version=7796}
[2022-04-07T17:22:22,411][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-07T17:22:22,411][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-07T17:22:23,728][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-07T17:22:23,735][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [40] indices into cluster_state
[2022-04-07T17:22:24,535][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-07T17:22:24,536][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-07T17:22:25,246][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-07T17:22:25,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-07T17:22:25,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-07T17:22:25,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-07T17:22:26,229][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T17:22:26,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T17:22:29,742][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-07T17:22:29,806][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-07T17:22:29,824][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-07T17:22:30,561][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T17:22:30,562][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T17:22:31,191][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T17:22:39,360][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-07T17:22:39,849][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-07T17:22:43,384][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-07T17:22:43,416][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-07T17:22:43,419][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-07T17:22:45,001][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-07T17:22:45,024][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-07T17:22:45,435][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T17:22:45,450][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T17:22:47,931][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T17:22:47,944][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T17:22:53,285][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.06][0]]]).
[2022-04-07T17:23:29,401][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 929 finished with response BulkByScrollResponse[took=165.2ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-07T17:23:31,916][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 954 finished with response BulkByScrollResponse[took=2.3s,timed_out=false,sliceId=null,updated=1021,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-07T17:23:41,705][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-07T17:24:21,512][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.07] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-07T17:24:21,707][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.07][0]]]).
[2022-04-07T17:24:21,878][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:21,998][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,007][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,126][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,312][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,417][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,533][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:24,196][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:24,284][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:33,965][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,118][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,232][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,247][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,645][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:09,020][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:09,274][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:09,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:16,298][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:27,240][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:50,365][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:50,494][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:26:43,467][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:27:15,388][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:27:35,427][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:27:36,430][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,290][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,388][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,546][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,663][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:38,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:38,353][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:38,549][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:39,297][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:32:04,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:32:04,841][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:33:40,836][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:34:15,551][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:34:35,890][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:34:35,982][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:41:37,252][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:43:00,790][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:51:54,956][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:54:12,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [31253ms] which is above the warn threshold of [5000ms]
[2022-04-07T17:54:25,876][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33836}] took [8623ms] which is above the warn threshold of [5000ms]
[2022-04-07T17:54:45,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017ebbb8@31fca5f0] took [7937ms] which is above the warn threshold of [5000ms]
[2022-04-07T17:55:16,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8849ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:24,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8849445577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:25,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50d26e41, interval=5s}] took [328997ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:00:25,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:25,826][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [328997ms] which is above the warn threshold of [5s]
[2022-04-07T18:00:25,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328997085101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:34,752][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:09:57,311][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:11:37,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:11:38,018][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:11:38,455][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:21:48,597][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:26:47,314][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:26:47,338][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:26:47,444][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:27:48,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [7888ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:27:59,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [5642ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:28:15,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3461][68] duration [726ms], collections [1]/[2s], total [726ms]/[3.2s], memory [1.4gb]->[1.4gb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [6.9mb]->[7.3mb]/[0b]}
[2022-04-07T18:28:16,050][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3461] overhead, spent [726ms] collecting in the last [2s]
[2022-04-07T18:28:47,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50d26e41, interval=5s}] took [14464ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:28:59,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [8777ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:29:01,101][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7084ms] which is above the warn threshold of [5s]
[2022-04-07T18:29:46,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:29:47,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3471][69] duration [18.3s], collections [1]/[4.7s], total [18.3s]/[21.6s], memory [391.2mb]->[411.2mb]/[2gb], all_pools {[young] [172mb]->[192mb]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [7.3mb]->[7.3mb]/[0b]}
[2022-04-07T18:29:48,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21999617535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:29:49,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3471] overhead, spent [18.3s] collecting in the last [4.7s]
[2022-04-07T18:29:50,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [32904ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:30:12,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [5390ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:30:43,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:31:09,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [26523ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:33:36,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:35:11,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173256655183ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:35:37,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:36:03,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122651057349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:36:15,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@76093688, interval=5s}] took [122651ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:37:38,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:38:30,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120581543895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:39:31,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:39:50,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113070760631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:40:52,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:41:56,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79841862698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:43:30,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/158618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:43:56,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/158878714798ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:44:59,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:43:30,824][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35004}] took [192913ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:45:45,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88646648755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:14,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:41,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135119683585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:53,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:59,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@264be63d, interval=1m}] took [39530ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:48:15,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39530588650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:55:40,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:55:54,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467033840028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:56:27,380][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3475][70] duration [5.9m], collections [1]/[18.1m], total [5.9m]/[6.3m], memory [288.2mb]->[320.2mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [231.9mb]->[231.9mb]/[2gb]}{[survivor] [8.3mb]->[9.5mb]/[0b]}
[2022-04-07T18:56:31,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:56:45,383][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3475] overhead, spent [5.9m] collecting in the last [18.1m]
[2022-04-07T18:56:46,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47759418269ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:57:00,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [514793ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:57:03,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:57:21,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34682724464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:56:36,949][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35870}] took [514794ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:57:43,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:58:00,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40049006132ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:58:17,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:58:33,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34577235486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:57:55,789][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40049ms] which is above the warn threshold of [5s]
[2022-04-07T18:58:52,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.5s/32577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:06,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.5s/32577481768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:24,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:40,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33867265791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:38,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50d26e41, interval=5s}] took [33867ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:59:53,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:00:02,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29819932069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:47,384][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [26532] timed out after [43865ms]
[2022-04-07T19:00:22,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:00:34,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28591647216ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:00:50,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:01:02,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27901041062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:02:46,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:02:53,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116199767134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:02,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:09,276][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3476][71] duration [1.2m], collections [1]/[14.4m], total [1.2m]/[7.6m], memory [320.2mb]->[261.4mb]/[2gb], all_pools {[young] [8mb]->[24mb]/[0b]}{[old] [231.9mb]->[232.8mb]/[2gb]}{[survivor] [9.5mb]->[8.5mb]/[0b]}
[2022-04-07T19:03:17,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246199935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:27,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24328ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:28,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [16246ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:03:39,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24328160647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:51,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:04:16,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24051298386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:04:42,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:05:11,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51473200267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:05:10,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017ebbb8@14a77b80] took [51473ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:05:38,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54836ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:05:52,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54836522923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:06:37,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.4s/59435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:06:42,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33886}] took [114271ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:06:52,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.4s/59435184013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:08,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:10,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [86645ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:07:18,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27209919236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:30,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25845ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:38,304][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33868}] took [25845ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:07:40,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25844808055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:51,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:13:13,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21113146196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:14:20,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:15:11,829][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3478][72] duration [3.9m], collections [1]/[1.4m], total [3.9m]/[11.6m], memory [281.4mb]->[305.4mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [232.8mb]->[232.8mb]/[2gb]}{[survivor] [8.5mb]->[7.9mb]/[0b]}
[2022-04-07T19:14:38,853][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33846}] took [389407ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:07:52,495][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21113ms] which is above the warn threshold of [5s]
[2022-04-07T19:16:19,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389406459956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:16:39,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3478] overhead, spent [3.9m] collecting in the last [1.4m]
[2022-04-07T19:18:35,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/241294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:19:07,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [651545ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:20:33,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/241025719786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:19:36,142][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.1s/12113ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@49939439]], which exceeds the warn threshold of [10s]
[2022-04-07T19:23:01,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:23:22,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@76093688, interval=5s}] took [275131ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:25:11,877][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.4s/12483ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21ff3fea], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@824ea792]], which exceeds the warn threshold of [10s]
[2022-04-07T19:26:11,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275131003606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:29:37,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:32:50,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399190505779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:32:22,366][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.1s/15172ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21ff3fea], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@824ea792]], which exceeds the warn threshold of [10s]
[2022-04-07T19:35:38,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:28:54,724][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [26688] timed out after [141481ms]
[2022-04-07T19:30:11,970][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33888}] took [1304754ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:38:35,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360724824635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:41:42,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:45:07,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364261256195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:46:55,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:49:16,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313090050034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:52:35,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:55:22,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322150236591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:56:09,411][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [12.6m/759916ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [13.7m/824601ms]
[2022-04-07T19:57:36,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:58:41,185][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [34.6m/2079313ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [33.5m/2014671ms]
[2022-04-07T20:00:56,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319895305015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:05:04,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:08:05,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420333093181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:04:48,220][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.5s/14549ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c50ec4db], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@49939439], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21ff3fea], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@824ea792]], which exceeds the warn threshold of [10s]
