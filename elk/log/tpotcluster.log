[2022-03-31T17:12:52,786][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-31T17:12:52,827][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-31T17:12:52,828][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-31T17:12:59,390][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-31T17:12:59,391][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-31T17:12:59,392][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-31T17:12:59,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-31T17:12:59,393][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-31T17:12:59,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-31T17:12:59,394][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-31T17:12:59,395][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-31T17:12:59,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-31T17:12:59,396][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-31T17:12:59,397][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-31T17:12:59,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-31T17:12:59,398][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-31T17:12:59,399][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-31T17:12:59,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-31T17:12:59,400][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-31T17:12:59,401][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-31T17:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-31T17:12:59,402][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-31T17:12:59,403][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-31T17:12:59,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-31T17:12:59,404][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-31T17:12:59,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-31T17:12:59,405][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-31T17:12:59,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-31T17:12:59,406][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-31T17:12:59,407][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-31T17:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-31T17:12:59,408][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-31T17:12:59,409][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-31T17:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-31T17:12:59,410][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-31T17:12:59,411][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-31T17:12:59,412][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-31T17:12:59,413][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-31T17:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-31T17:12:59,414][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-31T17:12:59,415][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-31T17:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-31T17:12:59,416][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-31T17:12:59,417][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-31T17:12:59,479][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [103.2gb], net total_space [125.8gb], types [ext4]
[2022-03-31T17:12:59,480][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-31T17:12:59,707][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-31T17:13:09,457][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-31T17:13:09,459][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-31T17:13:10,433][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-31T17:13:10,552][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-31T17:13:11,275][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-31T17:13:12,000][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-31T17:13:12,001][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-31T17:13:12,024][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-31T17:13:12,026][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-31T17:13:12,200][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-31T17:13:14,014][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-31T17:13:14,133][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 169, version: 4918, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-31T17:13:14,304][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 169, version: 4918, reason: Publication{term=169, version=4918}
[2022-03-31T17:13:14,406][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-31T17:13:14,407][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-31T17:13:15,087][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-31T17:13:15,093][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [34] indices into cluster_state
[2022-03-31T17:13:15,796][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-31T17:13:15,797][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-31T17:13:16,358][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:16,488][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:16,776][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-31T17:13:16,780][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-31T17:13:16,783][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-31T17:13:16,906][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-31T17:13:17,218][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-31T17:13:17,376][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-31T17:13:19,684][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-31T17:13:20,336][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0], [.ds-.logs-deprecation.elasticsearch-default-2022.03.12-000001][0]]]).
[2022-03-31T17:13:39,973][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-31T17:13:40,092][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-31T17:14:19,372][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 527 finished with response BulkByScrollResponse[took=333.6ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-31T17:14:21,900][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 543 finished with response BulkByScrollResponse[took=2.6s,timed_out=false,sliceId=null,updated=1036,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-31T17:14:29,045][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-31T17:14:51,442][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:14:51,569][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:14:51,581][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:10,197][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:11,029][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:11,166][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:12,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:15,029][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:18,202][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:22,191][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,037][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,116][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,235][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:15:24,306][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:07,289][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:16,333][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:16,402][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:36,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:16:38,285][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:17:44,428][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,358][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,420][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,425][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,521][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:10,759][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:23:11,391][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:39,477][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:39,537][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:40,472][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:26:45,769][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:28:36,844][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:34:02,052][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:35:12,088][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:35:12,162][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:41:11,426][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:41:11,503][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:44:26,573][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,585][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,675][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,686][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:11,790][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:12,577][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,584][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,644][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:45:13,650][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:52:51,841][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:56:02,982][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:57:23,034][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T17:57:23,130][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:04:52,544][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:04:53,475][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:06:00,466][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:06:00,562][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:12:47,828][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:13:05,830][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:14:24,807][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:31:06,738][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:33:17,155][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:38:29,565][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:47:19,147][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T18:55:12,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8323ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:55:49,269][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60610}] took [8546ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:56:00,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8662ms] which is above the warn threshold of [5000ms]
[2022-03-31T18:56:26,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:04,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/464200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:18,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8m/469179476688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:04:33,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:09,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32037024964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:06,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60590}] took [469179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:06,944][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:60612}] took [469179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:24,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:26,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52467198605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:05:30,819][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [9.8m/593109ms] ago, timed out [9.3m/558079ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [42739]
[2022-03-31T19:05:41,280][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6078][104] duration [6.6m], collections [1]/[9.1m], total [6.6m]/[6.6m], memory [1.3gb]->[252.7mb]/[2gb], all_pools {[young] [1.1gb]->[64mb]/[0b]}{[old] [180.1mb]->[180.1mb]/[2gb]}{[survivor] [7.5mb]->[8.6mb]/[0b]}
[2022-03-31T19:05:41,280][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [42739] timed out after [35030ms]
[2022-03-31T19:05:41,834][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6078] overhead, spent [6.6m] collecting in the last [9.1m]
[2022-03-31T19:05:41,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [16994ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:05:44,299][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6079][105] duration [2.6s], collections [1]/[15.9s], total [2.6s]/[6.6m], memory [252.7mb]->[267.3mb]/[2gb], all_pools {[young] [64mb]->[80mb]/[0b]}{[old] [180.1mb]->[180.3mb]/[2gb]}{[survivor] [8.6mb]->[7mb]/[0b]}
[2022-03-31T19:05:55,479][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6086][106] duration [1s], collections [1]/[2.3s], total [1s]/[6.7m], memory [267.3mb]->[198.6mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [180.3mb]->[180.3mb]/[2gb]}{[survivor] [7mb]->[6.3mb]/[0b]}
[2022-03-31T19:05:55,655][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6086] overhead, spent [1s] collecting in the last [2.3s]
[2022-03-31T19:05:59,540][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T19:06:02,425][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6091][107] duration [742ms], collections [1]/[1.4s], total [742ms]/[6.7m], memory [246.6mb]->[210.3mb]/[2gb], all_pools {[young] [64mb]->[16mb]/[0b]}{[old] [180.3mb]->[180.3mb]/[2gb]}{[survivor] [6.3mb]->[13.9mb]/[0b]}
[2022-03-31T19:06:02,656][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6091] overhead, spent [742ms] collecting in the last [1.4s]
[2022-03-31T19:06:04,387][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.31/QQZ9Vm1mRv6iv6z0MTqFNA] update_mapping [_doc]
[2022-03-31T19:06:08,023][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6094][110] duration [883ms], collections [1]/[2.1s], total [883ms]/[6.7m], memory [196.2mb]->[196.1mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.6mb]->[5.5mb]/[0b]}
[2022-03-31T19:06:08,462][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6094] overhead, spent [883ms] collecting in the last [2.1s]
[2022-03-31T19:06:11,520][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6095][111] duration [1.1s], collections [1]/[3.4s], total [1.1s]/[6.7m], memory [196.1mb]->[195.8mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.5mb]->[5.2mb]/[0b]}
[2022-03-31T19:06:11,722][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6095] overhead, spent [1.1s] collecting in the last [3.4s]
[2022-03-31T19:06:14,195][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6096][112] duration [1.1s], collections [1]/[2.8s], total [1.1s]/[6.7m], memory [195.8mb]->[195.3mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.2mb]->[4.7mb]/[0b]}
[2022-03-31T19:06:14,400][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6096] overhead, spent [1.1s] collecting in the last [2.8s]
[2022-03-31T19:06:21,337][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6101][113] duration [923ms], collections [1]/[1.8s], total [923ms]/[6.8m], memory [243.3mb]->[197.5mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [4.7mb]->[6.9mb]/[0b]}
[2022-03-31T19:06:22,879][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6101] overhead, spent [923ms] collecting in the last [1.8s]
[2022-03-31T19:06:31,947][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6108][115] duration [714ms], collections [1]/[1.2s], total [714ms]/[6.8m], memory [236.5mb]->[194.8mb]/[2gb], all_pools {[young] [44mb]->[0b]/[0b]}{[old] [190.6mb]->[190.6mb]/[2gb]}{[survivor] [5.9mb]->[4.2mb]/[0b]}
[2022-03-31T19:06:32,291][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6108] overhead, spent [714ms] collecting in the last [1.2s]
[2022-03-31T19:06:42,071][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6116] overhead, spent [538ms] collecting in the last [1.5s]
[2022-03-31T19:06:45,674][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6118] overhead, spent [578ms] collecting in the last [1.7s]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35248}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,939][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6135][122] duration [3.4s], collections [1]/[5.3s], total [3.4s]/[6.9m], memory [217.3mb]->[202.7mb]/[2gb], all_pools {[young] [80mb]->[16mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [3.4mb]->[4.9mb]/[0b]}
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:14,546][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6135] overhead, spent [3.4s] collecting in the last [5.3s]
[2022-03-31T19:07:14,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6259ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:13,815][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35242}] took [5459ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:32,057][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6141][123] duration [2.3s], collections [1]/[5.9s], total [2.3s]/[6.9m], memory [234.7mb]->[200.9mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [4.9mb]->[7.1mb]/[0b]}
[2022-03-31T19:07:32,729][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6141] overhead, spent [2.3s] collecting in the last [5.9s]
[2022-03-31T19:07:42,647][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:07:42,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [14930ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:07:43,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9389036208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:07:47,726][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6142][124] duration [6.7s], collections [1]/[12.7s], total [6.7s]/[7m], memory [200.9mb]->[224.5mb]/[2gb], all_pools {[young] [8mb]->[24mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [7.1mb]->[6.7mb]/[0b]}
[2022-03-31T19:07:49,688][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6142] overhead, spent [6.7s] collecting in the last [12.7s]
[2022-03-31T19:07:51,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7308ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:10,362][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [9177ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:57,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44789ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:08:57,370][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [46990ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:08:58,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.7s/44788663816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:04,942][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5601ms] which is above the warn threshold of [5s]
[2022-03-31T19:09:05,820][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6144][125] duration [38.7s], collections [1]/[59.4s], total [38.7s]/[7.7m], memory [228.5mb]->[222.4mb]/[2gb], all_pools {[young] [32mb]->[24mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [6.7mb]->[8.6mb]/[0b]}
[2022-03-31T19:09:09,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6144] overhead, spent [38.7s] collecting in the last [59.4s]
[2022-03-31T19:09:13,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [13122ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:25,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5802ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,814][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6146][126] duration [10.8s], collections [1]/[6.9s], total [10.8s]/[7.9m], memory [226.4mb]->[286.4mb]/[2gb], all_pools {[young] [24mb]->[32mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [8.6mb]->[8mb]/[0b]}
[2022-03-31T19:09:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15579015529ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:43,958][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6146] overhead, spent [10.8s] collecting in the last [6.9s]
[2022-03-31T19:09:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [16179ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:52,011][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6148][127] duration [3.2s], collections [1]/[1.2s], total [3.2s]/[7.9m], memory [249.8mb]->[257.8mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [193.8mb]->[193.8mb]/[2gb]}{[survivor] [8mb]->[8.7mb]/[0b]}
[2022-03-31T19:09:52,471][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6148] overhead, spent [3.2s] collecting in the last [1.2s]
[2022-03-31T19:09:52,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6291ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6131ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,600][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6149][128] duration [4.5s], collections [1]/[12.9s], total [4.5s]/[8m], memory [257.8mb]->[201.8mb]/[2gb], all_pools {[young] [8mb]->[12mb]/[0b]}{[old] [193.8mb]->[194.2mb]/[2gb]}{[survivor] [8.7mb]->[7.6mb]/[0b]}
[2022-03-31T19:09:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6130551581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:09:59,730][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6149] overhead, spent [4.5s] collecting in the last [12.9s]
[2022-03-31T19:09:59,730][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6130ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:15,863][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6153][129] duration [5s], collections [1]/[2s], total [5s]/[8.1m], memory [273.8mb]->[289.8mb]/[2gb], all_pools {[young] [72mb]->[52mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.6mb]->[7.5mb]/[0b]}
[2022-03-31T19:10:15,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7103ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:10:16,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7102418865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:10:16,106][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6153] overhead, spent [5s] collecting in the last [2s]
[2022-03-31T19:10:16,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7302ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:34,024][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6157][130] duration [3.2s], collections [1]/[2.2s], total [3.2s]/[8.1m], memory [261.8mb]->[289.8mb]/[2gb], all_pools {[young] [60mb]->[84mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.5mb]->[7.8mb]/[0b]}
[2022-03-31T19:10:34,634][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6157] overhead, spent [3.2s] collecting in the last [2.2s]
[2022-03-31T19:10:35,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11017ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:10:37,386][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6158][131] duration [2.4s], collections [1]/[12s], total [2.4s]/[8.2m], memory [289.8mb]->[209.2mb]/[2gb], all_pools {[young] [84mb]->[8mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7.8mb]->[7mb]/[0b]}
[2022-03-31T19:10:49,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6162][132] duration [2.1s], collections [1]/[5.4s], total [2.1s]/[8.2m], memory [213.2mb]->[213.5mb]/[2gb], all_pools {[young] [12mb]->[20mb]/[0b]}{[old] [194.2mb]->[194.2mb]/[2gb]}{[survivor] [7mb]->[7.2mb]/[0b]}
[2022-03-31T19:10:49,721][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6162] overhead, spent [2.1s] collecting in the last [5.4s]
[2022-03-31T19:11:01,436][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6163][134] duration [6.1s], collections [2]/[7s], total [6.1s]/[8.3m], memory [213.5mb]->[215.8mb]/[2gb], all_pools {[young] [20mb]->[4mb]/[0b]}{[old] [194.2mb]->[195.5mb]/[2gb]}{[survivor] [7.2mb]->[10mb]/[0b]}
[2022-03-31T19:11:02,244][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6163] overhead, spent [6.1s] collecting in the last [7s]
[2022-03-31T19:11:04,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8125ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:07,210][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6165] overhead, spent [582ms] collecting in the last [1.2s]
[2022-03-31T19:11:28,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6186ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:29,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6185628504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:29,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6172][136] duration [4.2s], collections [1]/[3.3s], total [4.2s]/[8.4m], memory [282.8mb]->[290.8mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [6.4mb]->[6.5mb]/[0b]}
[2022-03-31T19:11:29,189][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6172] overhead, spent [4.2s] collecting in the last [3.3s]
[2022-03-31T19:11:29,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6385ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:47,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:48,197][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6178][137] duration [2.6s], collections [1]/[6.8s], total [2.6s]/[8.4m], memory [278.9mb]->[204.3mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [6.5mb]->[8mb]/[0b]}
[2022-03-31T19:11:53,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5424975194ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:54,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:53,920][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6178] overhead, spent [2.6s] collecting in the last [6.8s]
[2022-03-31T19:11:55,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6898208933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:11:55,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [12323ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:11:58,820][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6179][140] duration [6.1s], collections [3]/[10.8s], total [6.1s]/[8.5m], memory [204.3mb]->[201.7mb]/[2gb], all_pools {[young] [4mb]->[0b]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [8mb]->[5.3mb]/[0b]}
[2022-03-31T19:11:59,176][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6179] overhead, spent [6.1s] collecting in the last [10.8s]
[2022-03-31T19:12:11,413][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6184][141] duration [2.4s], collections [1]/[4.6s], total [2.4s]/[8.6m], memory [249.7mb]->[217.8mb]/[2gb], all_pools {[young] [52mb]->[16mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [5.3mb]->[5.4mb]/[0b]}
[2022-03-31T19:12:12,040][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6184] overhead, spent [2.4s] collecting in the last [4.6s]
[2022-03-31T19:12:12,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5124ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:22,888][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6190][142] duration [1.4s], collections [1]/[3.4s], total [1.4s]/[8.6m], memory [237.8mb]->[200.8mb]/[2gb], all_pools {[young] [36mb]->[8mb]/[0b]}{[old] [196.3mb]->[196.3mb]/[2gb]}{[survivor] [5.4mb]->[4.4mb]/[0b]}
[2022-03-31T19:12:23,535][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6190] overhead, spent [1.4s] collecting in the last [3.4s]
[2022-03-31T19:12:46,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:46,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [6463ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:46,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5663364526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9259ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,884][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [9259ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:12:55,857][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6200][144] duration [9.1s], collections [2]/[14.9s], total [9.1s]/[8.8m], memory [280.8mb]->[208.4mb]/[2gb], all_pools {[young] [87.9mb]->[36mb]/[0b]}{[old] [196.3mb]->[196.4mb]/[2gb]}{[survivor] [4.4mb]->[8mb]/[0b]}
[2022-03-31T19:12:56,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9258564750ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:12:56,110][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6200] overhead, spent [9.1s] collecting in the last [14.9s]
[2022-03-31T19:12:56,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [9258ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:13:20,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:23,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23855602023ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:33,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:40,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13291120682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:46,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:51,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12579457648ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:52,686][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6201][145] duration [17.8s], collections [1]/[27.4s], total [17.8s]/[9m], memory [208.4mb]->[210.2mb]/[2gb], all_pools {[young] [36mb]->[12mb]/[0b]}{[old] [196.4mb]->[196.4mb]/[2gb]}{[survivor] [8mb]->[9.7mb]/[0b]}
[2022-03-31T19:13:54,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8214ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:13:59,078][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6201] overhead, spent [17.8s] collecting in the last [27.4s]
[2022-03-31T19:13:59,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2s/8213830658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35242}] took [13734ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35282}] took [13734ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:09,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:04,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [34084ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:14:14,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13734178316ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:21,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:28,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13160752307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:40,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:47,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18635288522ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:56,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:57,870][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [16505ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:02,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16505714411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:14:58,707][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16506ms] which is above the warn threshold of [5s]
[2022-03-31T19:15:11,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:12,839][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [14804ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:19,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14804030000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:52,236][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6203][146] duration [16.8s], collections [1]/[20.6s], total [16.8s]/[9.3m], memory [222.2mb]->[246.2mb]/[2gb], all_pools {[young] [16mb]->[72mb]/[0b]}{[old] [196.4mb]->[196.4mb]/[2gb]}{[survivor] [9.7mb]->[9.7mb]/[0b]}
[2022-03-31T19:15:52,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,719][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6203] overhead, spent [16.8s] collecting in the last [20.6s]
[2022-03-31T19:15:53,614][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35232}] took [41329ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [56132ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:15:53,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.3s/41328448939ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:15:50,319][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [43515] timed out after [113954ms]
[2022-03-31T19:16:10,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [14070ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:16:10,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:10,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14070574936ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:14,791][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/208340ms] ago, timed out [1.5m/94386ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{_aLEhBRnQ2a651GlpLszyg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [43515]
[2022-03-31T19:16:14,906][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6204][147] duration [9.1s], collections [1]/[1m], total [9.1s]/[9.5m], memory [246.2mb]->[214.3mb]/[2gb], all_pools {[young] [72mb]->[16mb]/[0b]}{[old] [196.4mb]->[198.3mb]/[2gb]}{[survivor] [9.7mb]->[8mb]/[0b]}
[2022-03-31T19:16:26,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:28,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8607236776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:27,905][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6206][148] duration [6.2s], collections [1]/[9.9s], total [6.2s]/[9.6m], memory [246.3mb]->[205.5mb]/[2gb], all_pools {[young] [72mb]->[28mb]/[0b]}{[old] [198.3mb]->[198.3mb]/[2gb]}{[survivor] [8mb]->[7.2mb]/[0b]}
[2022-03-31T19:16:28,567][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6206] overhead, spent [6.2s] collecting in the last [9.9s]
[2022-03-31T19:16:45,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:48,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11320004956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:48,744][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6208][149] duration [6.9s], collections [1]/[2.3s], total [6.9s]/[9.7m], memory [281.5mb]->[285.5mb]/[2gb], all_pools {[young] [80mb]->[88mb]/[0b]}{[old] [198.3mb]->[198.3mb]/[2gb]}{[survivor] [7.2mb]->[7.2mb]/[0b]}
[2022-03-31T19:16:51,048][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6208] overhead, spent [6.9s] collecting in the last [2.3s]
[2022-03-31T19:16:50,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:16:58,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [18679ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:16:58,568][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5158197752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:02,332][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:03,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [11221ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:06,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11221683414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:12,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:14,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5145/0x00000008017f1d80@6a3a4261] took [10518ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:17,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10518083841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:20,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:26,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8425527213ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:27,728][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [8425ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:33,641][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35344}] took [10911ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:32,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:36,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910392266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:41,150][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35244}] took [8859ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:40,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [8859ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:17:43,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8859358328ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:48,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7245998531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:17:52,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [7245ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:18:01,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13192ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:11,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13191718366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:19,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18498ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:18:59,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [18497ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:19:31,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18497778394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:32,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@c12697f, interval=1m}] took [72591ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:19:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72591ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:33,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72591565071ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:19:40,100][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6213][151] duration [48.4s], collections [2]/[1.3m], total [48.4s]/[10.5m], memory [261.9mb]->[237.8mb]/[2gb], all_pools {[young] [60mb]->[36mb]/[0b]}{[old] [198.3mb]->[200.4mb]/[2gb]}{[survivor] [7.5mb]->[9.3mb]/[0b]}
[2022-03-31T19:19:42,986][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6213] overhead, spent [48.4s] collecting in the last [1.3m]
[2022-03-31T19:19:44,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [12280ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:06,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:08,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10717680333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:08,571][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6218][152] duration [7.8s], collections [1]/[12.6s], total [7.8s]/[10.6m], memory [285.8mb]->[208mb]/[2gb], all_pools {[young] [80mb]->[12mb]/[0b]}{[old] [200.4mb]->[201.5mb]/[2gb]}{[survivor] [9.3mb]->[6.5mb]/[0b]}
[2022-03-31T19:20:15,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:15,838][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6218] overhead, spent [7.8s] collecting in the last [12.6s]
[2022-03-31T19:20:16,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9862216387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:20:16,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [20579ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:18,105][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6219][153] duration [3.5s], collections [1]/[11.3s], total [3.5s]/[10.7m], memory [208mb]->[288.6mb]/[2gb], all_pools {[young] [12mb]->[80mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [6.5mb]->[7mb]/[0b]}
[2022-03-31T19:20:19,279][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6219] overhead, spent [3.5s] collecting in the last [11.3s]
[2022-03-31T19:20:34,982][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6222][154] duration [2.8s], collections [1]/[2.1s], total [2.8s]/[10.7m], memory [288.6mb]->[292.6mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [7mb]->[3.7mb]/[0b]}
[2022-03-31T19:20:37,254][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6222] overhead, spent [2.8s] collecting in the last [2.1s]
[2022-03-31T19:20:37,618][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11597ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:20:57,815][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6232][155] duration [706ms], collections [1]/[1.2s], total [706ms]/[10.8m], memory [225.3mb]->[229.3mb]/[2gb], all_pools {[young] [20mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [3.7mb]->[3.1mb]/[0b]}
[2022-03-31T19:20:58,065][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6232] overhead, spent [706ms] collecting in the last [1.2s]
[2022-03-31T19:21:29,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:21:30,421][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6240][156] duration [5.6s], collections [1]/[4.3s], total [5.6s]/[10.9m], memory [276.7mb]->[276.7mb]/[2gb], all_pools {[young] [72mb]->[92mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [3.1mb]->[3.1mb]/[0b]}
[2022-03-31T19:21:30,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9093295596ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:21:30,914][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6240] overhead, spent [5.6s] collecting in the last [4.3s]
[2022-03-31T19:21:30,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [10941ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:21:45,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [11645ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:16,449][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35376}] took [24149ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:16,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:22:17,297][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6245][157] duration [13.8s], collections [1]/[2.9s], total [13.8s]/[11.1m], memory [250.6mb]->[262.6mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [5mb]->[6.5mb]/[0b]}
[2022-03-31T19:22:17,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19858473482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:22:18,357][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6245] overhead, spent [13.8s] collecting in the last [2.9s]
[2022-03-31T19:22:20,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [27101ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:32,370][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6249][158] duration [1s], collections [1]/[3.1s], total [1s]/[11.1m], memory [224.1mb]->[210.9mb]/[2gb], all_pools {[young] [16mb]->[0b]/[0b]}{[old] [201.5mb]->[201.5mb]/[2gb]}{[survivor] [6.5mb]->[9.3mb]/[0b]}
[2022-03-31T19:22:32,863][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6249] overhead, spent [1s] collecting in the last [3.1s]
[2022-03-31T19:22:35,500][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [5117ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:22:37,551][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][6250][160] duration [1.6s], collections [2]/[4.5s], total [1.6s]/[11.1m], memory [210.9mb]->[295.7mb]/[2gb], all_pools {[young] [0b]->[28mb]/[0b]}{[old] [201.5mb]->[203.9mb]/[2gb]}{[survivor] [9.3mb]->[4.7mb]/[0b]}
[2022-03-31T19:22:38,087][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][6250] overhead, spent [1.6s] collecting in the last [4.5s]
[2022-03-31T19:26:17,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/185094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:32:32,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/185435813173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:34:51,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@40cca32e, interval=1s}] took [208966ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:35:13,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:38:30,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9m/540616712550ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:42:21,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:43:56,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4fba6bd7, interval=5s}] took [427142ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:45:52,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/427142976773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:48:28,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:47:11,064][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.2m/76359ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26]], which exceeds the warn threshold of [10s]
[2022-03-31T19:50:51,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367016408265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:53:50,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T19:53:57,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@5c0a69ee, interval=5s}] took [679083ms] which is above the warn threshold of [5000ms]
[2022-03-31T19:56:38,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/312067572289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T19:59:07,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:01:51,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/327075915271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:04:47,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:06:39,686][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.5s/15526ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7]], which exceeds the warn threshold of [10s]
[2022-03-31T20:07:27,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339996989053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:10:13,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324963ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:12:23,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325021196471ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:11:38,325][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.5s/11517ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:14:24,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:16:36,737][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.3s/26315ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1e58d7], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:17:04,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251476839909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:20:01,038][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328558ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-31T20:20:31,204][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [11.5s/11501ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:22:58,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328540951672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-31T20:23:41,486][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.9s/24908ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@a32dddc8], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@27b2ad26], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@606dc07f]], which exceeds the warn threshold of [10s]
[2022-03-31T20:20:45,644][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [905039ms] which is above the warn threshold of [5s]
[2022-03-31T20:25:54,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361787ms] on absolute clock which is above the warn threshold of [5000ms]
