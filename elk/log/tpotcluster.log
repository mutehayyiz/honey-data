[2022-04-05T16:41:34,689][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T16:41:34,705][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T16:41:34,706][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T16:41:40,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T16:41:40,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T16:41:40,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T16:41:40,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T16:41:40,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T16:41:40,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T16:41:40,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T16:41:40,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T16:41:40,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T16:41:40,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T16:41:40,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T16:41:40,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T16:41:40,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T16:41:40,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T16:41:40,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T16:41:40,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T16:41:40,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T16:41:40,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T16:41:40,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T16:41:40,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T16:41:40,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T16:41:40,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T16:41:40,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T16:41:40,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T16:41:40,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T16:41:40,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T16:41:40,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T16:41:40,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T16:41:40,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T16:41:40,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T16:41:40,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T16:41:40,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T16:41:40,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T16:41:40,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T16:41:40,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T16:41:40,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T16:41:40,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T16:41:40,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T16:41:40,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T16:41:40,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T16:41:40,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T16:41:40,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T16:41:40,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T16:41:40,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T16:41:40,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T16:41:40,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T16:41:40,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T16:41:40,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T16:41:40,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T16:41:40,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T16:41:40,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T16:41:40,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T16:41:40,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T16:41:40,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T16:41:40,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T16:41:40,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T16:41:40,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T16:41:40,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T16:41:40,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T16:41:40,442][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.5gb], net total_space [125.8gb], types [ext4]
[2022-04-05T16:41:40,443][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T16:41:40,734][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T16:41:51,479][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T16:41:51,483][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T16:41:52,637][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T16:41:52,781][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T16:41:53,970][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T16:41:54,935][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T16:41:54,936][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T16:41:54,981][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T16:41:54,983][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T16:41:55,205][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T16:41:58,244][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T16:41:58,426][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{gaYA5D8dQCWPK49CSMzNGQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 206, version: 7479, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{gaYA5D8dQCWPK49CSMzNGQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T16:41:58,669][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{gaYA5D8dQCWPK49CSMzNGQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 206, version: 7479, reason: Publication{term=206, version=7479}
[2022-04-05T16:41:58,795][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T16:41:58,796][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T16:42:00,593][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T16:42:00,606][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T16:42:01,681][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T16:42:01,683][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T16:42:02,462][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-05T16:42:02,695][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-05T16:42:03,176][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-05T16:42:03,260][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T16:42:03,268][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T16:42:03,272][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T16:42:04,044][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T16:42:04,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T16:42:07,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T16:42:17,867][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.05][0]]]).
[2022-04-05T16:42:21,367][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-05T16:42:21,567][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-05T16:43:05,096][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 565 finished with response BulkByScrollResponse[took=366.8ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-05T16:43:07,497][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 582 finished with response BulkByScrollResponse[took=2.4s,timed_out=false,sliceId=null,updated=1025,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-05T16:43:15,827][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-05T16:43:36,555][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T16:43:37,209][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T16:43:54,151][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:04,371][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:28,450][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:36,866][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:06:37,494][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:17:46,779][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:17:47,266][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:19:51,026][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:20:02,020][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:20:11,092][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:24:04,192][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T17:25:03,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@41484e5c, interval=1s}] took [29543ms] which is above the warn threshold of [5000ms]
[2022-04-05T17:26:22,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@16e41a2c, interval=5s}] took [13022ms] which is above the warn threshold of [5000ms]
[2022-04-05T17:30:58,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T17:37:46,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5182468799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T17:41:01,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14m/845726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T17:37:07,808][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [10.6s/10669ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T17:43:34,977][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14m/845405294539ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T17:45:34,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/274263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T17:45:21,503][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.3s/21374ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T17:47:49,580][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/274359640101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T17:56:31,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9m/655552ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:03:36,106][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [18.6m/1119765ms] which is longer than the warn threshold of [300000ms]; there are currently [6] pending tasks, the oldest of which has age [13.7m/824405ms]
[2022-04-05T18:02:48,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9m/655730999227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:10:58,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4m/867921ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:09:21,927][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [29.5m/1775496ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [33.5m/2011447ms]
[2022-04-05T18:16:15,850][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [44m/2643462ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [31.3m/1878116ms]
[2022-04-05T18:15:57,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4m/867965713071ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:15:50,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b2101c9, interval=5s}] took [867965ms] which is above the warn threshold of [5000ms]
[2022-04-05T18:21:34,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/635723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:22:03,930][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13s/13054ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5c00c5e0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T18:24:27,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5m/635228745177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:17:05,672][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [867966ms] which is above the warn threshold of [5s]
[2022-04-05T18:26:05,885][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [30.4s/30492ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@ff104ad1], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@83951a2f], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5c00c5e0], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@bc502d88]], which exceeds the warn threshold of [10s]
[2022-04-05T18:27:26,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:31:41,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/351527032424ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:37:28,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/602155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:41:45,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10m/602474728126ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:42:55,744][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1h/3630217ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [43.2m/2595728ms]
[2022-04-05T18:44:56,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/448342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:55:43,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/447948718976ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T18:55:27,087][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.3h/4680641ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [58.8m/3528870ms]
[2022-04-05T18:59:10,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14m/844965ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T18:59:29,891][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [1.5h/5525469ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [1.1h/4256545ms]
[2022-04-05T19:04:09,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-05T19:04:09,485][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-05T19:04:09,486][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-05T19:04:14,819][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-05T19:04:14,827][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-05T19:04:14,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-05T19:04:14,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-05T19:04:14,828][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-05T19:04:14,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-05T19:04:14,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-05T19:04:14,829][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-05T19:04:14,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-05T19:04:14,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-05T19:04:14,830][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-05T19:04:14,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-05T19:04:14,831][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-05T19:04:14,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-05T19:04:14,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-05T19:04:14,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-05T19:04:14,833][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-05T19:04:14,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-05T19:04:14,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-05T19:04:14,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-05T19:04:14,835][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-05T19:04:14,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-05T19:04:14,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-05T19:04:14,836][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-05T19:04:14,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-05T19:04:14,837][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-05T19:04:14,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-05T19:04:14,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-05T19:04:14,838][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-05T19:04:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-05T19:04:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-05T19:04:14,839][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-05T19:04:14,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-05T19:04:14,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-05T19:04:14,840][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-05T19:04:14,841][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-05T19:04:14,842][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-05T19:04:14,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-05T19:04:14,843][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-05T19:04:14,844][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-05T19:04:14,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-05T19:04:14,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-05T19:04:14,846][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-05T19:04:14,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-05T19:04:14,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-05T19:04:14,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-05T19:04:14,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-05T19:04:14,848][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-05T19:04:14,849][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-05T19:04:14,851][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-05T19:04:14,852][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-05T19:04:14,854][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-05T19:04:14,855][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-05T19:04:14,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-05T19:04:14,856][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-05T19:04:14,857][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-05T19:04:14,968][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.3gb], net total_space [125.8gb], types [ext4]
[2022-04-05T19:04:14,969][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-05T19:04:16,307][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-05T19:06:10,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/106544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:06:10,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/106613821073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:06:24,181][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-05T19:06:24,188][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-05T19:06:24,196][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T19:06:24,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-05T19:06:24,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T19:06:24,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-05T19:06:24,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-05T19:06:24,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T19:06:24,203][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-05T19:06:24,203][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-05T19:06:24,204][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-05T19:06:24,205][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T19:06:24,206][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T19:06:24,208][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-05T19:06:24,208][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-05T19:06:25,531][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-05T19:06:25,714][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-05T19:06:26,468][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-05T19:06:27,339][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-05T19:06:27,340][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-05T19:06:27,444][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-05T19:06:27,446][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-05T19:06:27,737][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-05T19:06:29,951][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-05T19:06:30,142][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 207, version: 7546, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-05T19:06:30,310][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 207, version: 7546, reason: Publication{term=207, version=7546}
[2022-04-05T19:06:30,449][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-05T19:06:30,450][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-05T19:06:31,356][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-05T19:06:31,368][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [39] indices into cluster_state
[2022-04-05T19:06:32,221][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-05T19:06:32,222][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-05T19:06:33,515][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-05T19:06:34,263][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-05T19:06:34,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-05T19:06:34,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-05T19:08:00,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5518ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:39,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5010497828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:48,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/244577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:50,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/245142213515ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:10:48,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7092ae78, interval=5s}] took [245142ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:10:52,930][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [257995ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [39] unchanged indices
[2022-04-05T19:10:55,079][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.3m] publication of cluster state version [7557] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:10:59,233][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][10][9] duration [946ms], collections [1]/[4m], total [946ms]/[1.2s], memory [622.3mb]->[93.9mb]/[2gb], all_pools {[young] [548mb]->[0b]/[0b]}{[old] [54.3mb]->[54.3mb]/[2gb]}{[survivor] [23.9mb]->[39.5mb]/[0b]}
[2022-04-05T19:11:01,568][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-05T19:11:04,353][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53912}] took [12630ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:11:13,166][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][17][11] duration [1.4s], collections [1]/[3.8s], total [1.4s]/[3s], memory [183.1mb]->[104mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [92.5mb]->[92.5mb]/[2gb]}{[survivor] [6.6mb]->[11.5mb]/[0b]}
[2022-04-05T19:11:13,610][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][17] overhead, spent [1.4s] collecting in the last [3.8s]
[2022-04-05T19:11:29,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [13433ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:11:44,346][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-05T19:12:01,653][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7092ae78, interval=5s}] took [28514ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:10,499][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-05T19:12:21,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [6166ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:37,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5922ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:36,873][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:53924}] took [7240ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:12:40,279][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [71264ms] which is above the warn threshold of [10s]; wrote global metadata [true] and metadata for [0] indices and skipped [39] unchanged indices
[2022-04-05T19:12:45,474][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.5m] publication of cluster state version [7559] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:12:53,523][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][26][12] duration [1.2s], collections [1]/[2.6s], total [1.2s]/[4.2s], memory [180mb]->[106.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [92.5mb]->[95.7mb]/[2gb]}{[survivor] [11.5mb]->[10.6mb]/[0b]}
[2022-04-05T19:12:53,902][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][26] overhead, spent [1.2s] collecting in the last [2.6s]
[2022-04-05T19:12:57,184][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-05T19:13:09,873][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][34][13] duration [3.3s], collections [1]/[5.6s], total [3.3s]/[7.5s], memory [182.4mb]->[112.8mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [95.7mb]->[102.1mb]/[2gb]}{[survivor] [10.6mb]->[10.6mb]/[0b]}
[2022-04-05T19:13:10,257][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][34] overhead, spent [3.3s] collecting in the last [5.6s]
[2022-04-05T19:13:26,518][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][42] overhead, spent [629ms] collecting in the last [1.1s]
[2022-04-05T19:13:31,273][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-05T19:13:34,692][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][47][15] duration [803ms], collections [1]/[2s], total [803ms]/[9s], memory [165.6mb]->[114.4mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [105.6mb]->[105.6mb]/[2gb]}{[survivor] [8mb]->[8.8mb]/[0b]}
[2022-04-05T19:13:35,520][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][47] overhead, spent [803ms] collecting in the last [2s]
[2022-04-05T19:13:42,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5314ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:14:10,973][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][53][16] duration [1.6s], collections [1]/[3.1s], total [1.6s]/[10.6s], memory [198.4mb]->[117.2mb]/[2gb], all_pools {[young] [84mb]->[12mb]/[0b]}{[old] [105.6mb]->[110.9mb]/[2gb]}{[survivor] [8.8mb]->[6.3mb]/[0b]}
[2022-04-05T19:14:20,222][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][53] overhead, spent [1.6s] collecting in the last [3.1s]
[2022-04-05T19:14:22,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [21885ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:14:33,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6201ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:14:33,926][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][54][17] duration [4.5s], collections [1]/[25.3s], total [4.5s]/[15.1s], memory [117.2mb]->[193.2mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [110.9mb]->[110.9mb]/[2gb]}{[survivor] [6.3mb]->[11.7mb]/[0b]}
[2022-04-05T19:14:33,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6200901331ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:14:34,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [7074ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:15:00,496][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.3s] publication of cluster state version [7564] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:15:10,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][67][18] duration [3.3s], collections [1]/[6.1s], total [3.3s]/[18.5s], memory [206.6mb]->[131mb]/[2gb], all_pools {[young] [84mb]->[4mb]/[0b]}{[old] [110.9mb]->[115.8mb]/[2gb]}{[survivor] [11.7mb]->[11.2mb]/[0b]}
[2022-04-05T19:15:10,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][67] overhead, spent [3.3s] collecting in the last [6.1s]
[2022-04-05T19:15:24,133][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43380}] took [51320ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:15:28,232][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [12365ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [37] unchanged indices
[2022-04-05T19:15:29,429][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14s] publication of cluster state version [7565] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:15:46,178][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][19] duration [2s], collections [1]/[5s], total [2s]/[20.5s], memory [203mb]->[211mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [115.8mb]->[121.4mb]/[2gb]}{[survivor] [11.2mb]->[8mb]/[0b]}
[2022-04-05T19:15:46,327][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [2s] collecting in the last [5s]
[2022-04-05T19:15:52,787][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][85] overhead, spent [427ms] collecting in the last [1.3s]
[2022-04-05T19:15:57,745][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][87][21] duration [1.5s], collections [1]/[3.3s], total [1.5s]/[22.5s], memory [199mb]->[134.1mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [121.4mb]->[125.6mb]/[2gb]}{[survivor] [9.5mb]->[8.4mb]/[0b]}
[2022-04-05T19:15:58,048][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][87] overhead, spent [1.5s] collecting in the last [3.3s]
[2022-04-05T19:16:00,887][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [541ms] collecting in the last [1.6s]
[2022-04-05T19:16:07,178][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][94] overhead, spent [489ms] collecting in the last [1.4s]
[2022-04-05T19:16:25,250][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][107] overhead, spent [496ms] collecting in the last [1.4s]
[2022-04-05T19:16:34,527][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][113][28] duration [1.2s], collections [1]/[2.6s], total [1.2s]/[25.6s], memory [229.8mb]->[163.9mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [145.8mb]->[153.3mb]/[2gb]}{[survivor] [12mb]->[10.6mb]/[0b]}
[2022-04-05T19:16:35,957][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][113] overhead, spent [1.2s] collecting in the last [2.6s]
[2022-04-05T19:16:42,748][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][114][30] duration [3s], collections [2]/[3.3s], total [3s]/[28.6s], memory [163.9mb]->[233.2mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [153.3mb]->[156.3mb]/[2gb]}{[survivor] [10.6mb]->[13.2mb]/[0b]}
[2022-04-05T19:16:43,161][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][114] overhead, spent [3s] collecting in the last [3.3s]
[2022-04-05T19:16:43,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5928ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:16:49,940][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][117][31] duration [1s], collections [1]/[1.3s], total [1s]/[29.7s], memory [197.5mb]->[213.5mb]/[2gb], all_pools {[young] [28mb]->[4mb]/[0b]}{[old] [156.3mb]->[161mb]/[2gb]}{[survivor] [13.2mb]->[13.1mb]/[0b]}
[2022-04-05T19:16:50,119][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][117] overhead, spent [1s] collecting in the last [1.3s]
[2022-04-05T19:16:52,334][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][118][32] duration [792ms], collections [1]/[4.2s], total [792ms]/[30.5s], memory [213.5mb]->[174.7mb]/[2gb], all_pools {[young] [4mb]->[20mb]/[0b]}{[old] [161mb]->[167.7mb]/[2gb]}{[survivor] [13.1mb]->[6.9mb]/[0b]}
[2022-04-05T19:17:07,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:17:07,917][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][125][33] duration [3.3s], collections [1]/[1.6s], total [3.3s]/[33.8s], memory [258.7mb]->[176.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [167.7mb]->[167.7mb]/[2gb]}{[survivor] [6.9mb]->[8.3mb]/[0b]}
[2022-04-05T19:17:07,825][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5507261345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:17:08,562][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][125] overhead, spent [3.3s] collecting in the last [1.6s]
[2022-04-05T19:17:09,608][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [8228ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:04,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [8113ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:12,576][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=207, version=7576}] took [45.2s] which is above the warn threshold of [30s]: [running task [Publication{term=207, version=7576}]] took [0ms], [connecting to new nodes] took [70ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@4556b7d] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@5dec30b5] took [767ms], [org.elasticsearch.script.ScriptService@4998b0d] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [0ms], [org.elasticsearch.snapshots.RestoreService@1ba57fc7] took [0ms], [org.elasticsearch.ingest.IngestService@27f08d9e] took [670ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6ff9ab8e] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016cddc0@7ccb2b64] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@4ca0287d] took [0ms], [org.elasticsearch.tasks.TaskManager@6f9fd00a] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@4d18a3ac] took [33ms], [org.elasticsearch.cluster.InternalClusterInfoService@6589f19e] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@72403552] took [0ms], [org.elasticsearch.indices.SystemIndexManager@66e3b204] took [113ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@433b486a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@359b7368] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2406de71] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4a00ceae] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@4714c437] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@793b4ed9] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@7641461] took [11440ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3aa9c8e9] took [99ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@653547d6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@11af3519] took [2372ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2a330bae] took [194ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@44aff015] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@beb04a1] took [8553ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [3466ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6304ff38] took [7476ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@51d8f68d] took [67ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7d688122] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@71f3dd86] took [4076ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@10c2a3ab] took [2268ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@334f0aa0] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@68cf110c] took [1021ms], [org.elasticsearch.node.ResponseCollectorService@6d953845] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@540aac] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4b050a0c] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@5b522511] took [56ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@71302f6d] took [152ms], [org.elasticsearch.indices.store.IndicesStore@2029b7d2] took [84ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7529b199] took [0ms], [org.elasticsearch.license.LicenseService@722ebc70] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f6f0ea8] took [193ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@651913b1] took [0ms], [org.elasticsearch.gateway.GatewayService@184d8c89] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2fc7d780] took [0ms]
[2022-04-05T19:18:20,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@4fd9bf2c] took [9633ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:28,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:46,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [6607ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:47,567][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53756}] took [112489ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:47,567][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53758}] took [109535ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:18:56,202][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5555ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:18:57,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5554829875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:01,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5263ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:19:01,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5263ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:02,749][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5263304178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:10,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:13,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9000252321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:16,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5884ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:20,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5883780781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:23,308][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54000}] took [5884ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:19:23,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7449ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:23,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7449067157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:25,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@4d482020] took [23449ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:19:35,896][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [54206ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [35] unchanged indices
[2022-04-05T19:19:35,619][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:36,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9127127523ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:19:36,111][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1m] publication of cluster state version [7577] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:19:36,251][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][137][34] duration [7s], collections [1]/[38.7s], total [7s]/[40.8s], memory [252.1mb]->[190.8mb]/[2gb], all_pools {[young] [76mb]->[16mb]/[0b]}{[old] [167.7mb]->[167.7mb]/[2gb]}{[survivor] [8.3mb]->[11mb]/[0b]}
[2022-04-05T19:19:45,380][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53764}] took [55586ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:04,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e7b1b78, interval=5s}] took [10078ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:04,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:05,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9478553935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:10,348][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][144][35] duration [6s], collections [1]/[14s], total [6s]/[46.9s], memory [242.8mb]->[230.4mb]/[2gb], all_pools {[young] [68mb]->[52mb]/[0b]}{[old] [167.7mb]->[171.3mb]/[2gb]}{[survivor] [11mb]->[11.1mb]/[0b]}
[2022-04-05T19:20:13,466][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][144] overhead, spent [6s] collecting in the last [14s]
[2022-04-05T19:20:14,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [9325ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:21,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@66bcf642] took [6137ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:20:38,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:39,780][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13764573174ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:20:46,845][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][145][36] duration [8.2s], collections [1]/[33.9s], total [8.2s]/[55.2s], memory [230.4mb]->[180.7mb]/[2gb], all_pools {[young] [52mb]->[68mb]/[0b]}{[old] [171.3mb]->[175.2mb]/[2gb]}{[survivor] [11.1mb]->[5.4mb]/[0b]}
[2022-04-05T19:20:49,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [10136ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:21:05,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [7934ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:21:36,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:21:37,103][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][147][37] duration [14.3s], collections [1]/[13.2s], total [14.3s]/[1.1m], memory [256.7mb]->[264.7mb]/[2gb], all_pools {[young] [80mb]->[84mb]/[0b]}{[old] [175.2mb]->[175.2mb]/[2gb]}{[survivor] [5.4mb]->[5.4mb]/[0b]}
[2022-04-05T19:21:37,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20505714014ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:21:38,262][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][147] overhead, spent [14.3s] collecting in the last [13.2s]
[2022-04-05T19:21:40,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [29457ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:21:57,603][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54040}] took [10821ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:08,570][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53760}] took [301131ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:09,277][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:53762}] took [301131ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:29,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@57124d1c] took [40439ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:52,344][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:43432}] took [21228ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:53,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [7204ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:22:53,882][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54044}] took [10406ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:23:08,429][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54038}] took [5204ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:23:31,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:32,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9421407603ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:32,503][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][151][38] duration [6.8s], collections [1]/[4.7s], total [6.8s]/[1.2m], memory [228.3mb]->[236.3mb]/[2gb], all_pools {[young] [52mb]->[88mb]/[0b]}{[old] [175.2mb]->[175.2mb]/[2gb]}{[survivor] [5mb]->[5mb]/[0b]}
[2022-04-05T19:23:32,821][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:33,027][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14031345950ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:23:33,027][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][151] overhead, spent [6.8s] collecting in the last [4.7s]
[2022-04-05T19:23:33,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [26454ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:23:33,997][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=207, version=7577}] took [3.9m] which is above the warn threshold of [30s]: [running task [Publication{term=207, version=7577}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@4556b7d] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@5dec30b5] took [185510ms], [org.elasticsearch.script.ScriptService@4998b0d] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [0ms], [org.elasticsearch.snapshots.RestoreService@1ba57fc7] took [0ms], [org.elasticsearch.ingest.IngestService@27f08d9e] took [1920ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6ff9ab8e] took [152ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016cddc0@7ccb2b64] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@4ca0287d] took [158ms], [org.elasticsearch.tasks.TaskManager@6f9fd00a] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@4d18a3ac] took [148ms], [org.elasticsearch.cluster.InternalClusterInfoService@6589f19e] took [1ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@72403552] took [71ms], [org.elasticsearch.indices.SystemIndexManager@66e3b204] took [610ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@433b486a] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@359b7368] took [1242ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@2406de71] took [78ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@4a00ceae] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801406000@4714c437] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@793b4ed9] took [96ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@7641461] took [21960ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@3aa9c8e9] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@653547d6] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@11af3519] took [23603ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@2a330bae] took [149ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@44aff015] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@beb04a1] took [440ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@3484f4a9] took [42ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@6304ff38] took [92ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@51d8f68d] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@7d688122] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@71f3dd86] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@10c2a3ab] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@334f0aa0] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@68cf110c] took [117ms], [org.elasticsearch.node.ResponseCollectorService@6d953845] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@540aac] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@4b050a0c] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@5b522511] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@71302f6d] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2029b7d2] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@7529b199] took [0ms], [org.elasticsearch.license.LicenseService@722ebc70] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f6f0ea8] took [50ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@651913b1] took [0ms], [org.elasticsearch.gateway.GatewayService@184d8c89] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@2fc7d780] took [0ms]
[2022-04-05T19:23:39,036][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.7m/406159ms] which is longer than the warn threshold of [300000ms]; there are currently [7] pending tasks, the oldest of which has age [6.6m/399807ms]
[2022-04-05T19:23:59,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:24:22,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:24:22,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@79289f93, interval=30s}] took [13100ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:24:25,323][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54054}] took [17703ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:24:25,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.1s/13100323452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:24:29,306][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][160][40] duration [7.8s], collections [2]/[20.5s], total [7.8s]/[1.4m], memory [252.7mb]->[272.8mb]/[2gb], all_pools {[young] [76mb]->[12mb]/[0b]}{[old] [175.2mb]->[181.4mb]/[2gb]}{[survivor] [9.4mb]->[8.9mb]/[0b]}
[2022-04-05T19:24:29,631][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][160] overhead, spent [7.8s] collecting in the last [20.5s]
[2022-04-05T19:24:33,867][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][161][41] duration [2.3s], collections [1]/[7.4s], total [2.3s]/[1.4m], memory [272.8mb]->[193mb]/[2gb], all_pools {[young] [12mb]->[0b]/[0b]}{[old] [181.4mb]->[181.4mb]/[2gb]}{[survivor] [8.9mb]->[11.5mb]/[0b]}
[2022-04-05T19:24:34,200][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][161] overhead, spent [2.3s] collecting in the last [7.4s]
[2022-04-05T19:24:34,729][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [43644ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T19:24:35,119][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [47.5s] publication of cluster state version [7578] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:24:39,495][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][164][42] duration [1s], collections [1]/[2.7s], total [1s]/[1.4m], memory [253mb]->[196.5mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [181.4mb]->[186mb]/[2gb]}{[survivor] [11.5mb]->[10.5mb]/[0b]}
[2022-04-05T19:24:39,801][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][164] overhead, spent [1s] collecting in the last [2.7s]
[2022-04-05T19:24:42,537][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][166] overhead, spent [445ms] collecting in the last [1.1s]
[2022-04-05T19:24:54,780][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T19:24:56,001][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.05/SW7PcCANRW2HpJ810Rh22A] update_mapping [_doc]
[2022-04-05T19:25:17,959][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:32,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8834739669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:46,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28312ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:52,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28312034660ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:25:59,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12724ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:08,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12724460606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:25,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:32,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.8s/26810742571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:29,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017da708@33f70c4b] took [85823ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:26:32,735][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54068}] took [67848ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:26:41,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:47,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16033847638ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:48,293][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [16033ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:26:49,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:50,573][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8749332054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:26:58,470][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][45] duration [2.9s], collections [1]/[4.5s], total [2.9s]/[1.5m], memory [265.7mb]->[200.8mb]/[2gb], all_pools {[young] [72mb]->[48mb]/[0b]}{[old] [191.5mb]->[191.5mb]/[2gb]}{[survivor] [6.1mb]->[9.2mb]/[0b]}
[2022-04-05T19:26:59,587][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [2.9s] collecting in the last [4.5s]
[2022-04-05T19:27:00,401][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [121886ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [38] unchanged indices
[2022-04-05T19:27:02,303][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2m] publication of cluster state version [7580] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{k29sRSEMTyydKDcEJZerrA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-05T19:27:05,318][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][181][46] duration [1.3s], collections [1]/[3.3s], total [1.3s]/[1.5m], memory [256.8mb]->[200mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [191.5mb]->[194.4mb]/[2gb]}{[survivor] [9.2mb]->[5.5mb]/[0b]}
[2022-04-05T19:27:05,673][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][181] overhead, spent [1.3s] collecting in the last [3.3s]
[2022-04-05T19:27:07,133][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][182] overhead, spent [538ms] collecting in the last [1.8s]
[2022-04-05T19:27:11,356][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][185] overhead, spent [381ms] collecting in the last [1.4s]
[2022-04-05T19:27:15,042][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][187][49] duration [838ms], collections [1]/[1.9s], total [838ms]/[1.5m], memory [215.8mb]->[202.7mb]/[2gb], all_pools {[young] [36mb]->[4mb]/[0b]}{[old] [194.4mb]->[195.6mb]/[2gb]}{[survivor] [9.3mb]->[7.1mb]/[0b]}
[2022-04-05T19:27:15,304][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][187] overhead, spent [838ms] collecting in the last [1.9s]
[2022-04-05T19:27:17,447][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][188][50] duration [795ms], collections [1]/[2.6s], total [795ms]/[1.5m], memory [202.7mb]->[236.6mb]/[2gb], all_pools {[young] [4mb]->[32mb]/[0b]}{[old] [195.6mb]->[195.6mb]/[2gb]}{[survivor] [7.1mb]->[9mb]/[0b]}
[2022-04-05T19:27:17,579][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][188] overhead, spent [795ms] collecting in the last [2.6s]
[2022-04-05T19:27:27,610][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][192][51] duration [1.1s], collections [1]/[3.6s], total [1.1s]/[1.6m], memory [272.6mb]->[207.1mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [195.6mb]->[196.4mb]/[2gb]}{[survivor] [9mb]->[10.7mb]/[0b]}
[2022-04-05T19:27:27,880][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][192] overhead, spent [1.1s] collecting in the last [3.6s]
[2022-04-05T19:31:12,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/210872ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:33:44,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/210281672917ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:35:21,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/256972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:36:36,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257562235712ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:39:23,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/240875ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:39:05,676][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][194][52] duration [2.9m], collections [1]/[1.3s], total [2.9m]/[4.5m], memory [247.1mb]->[291.1mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [196.4mb]->[201.7mb]/[2gb]}{[survivor] [10.7mb]->[11.2mb]/[0b]}
[2022-04-05T19:40:29,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/240875226688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:40:39,766][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][194] overhead, spent [2.9m] collecting in the last [1.3s]
[2022-04-05T19:43:01,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/214513ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:44:04,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@4ef5fdae, interval=1s}] took [922711ms] which is above the warn threshold of [5000ms]
[2022-04-05T19:46:29,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/213792345061ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T19:51:11,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/490270ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T19:57:24,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1m/490324784844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:00:33,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/562071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:00:41,313][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [20.3m/1222070ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.19][0]], allocationId [1VOd_d6vRTicjiMR5cSWAg], primary term [75], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.19][0]], allocationId [1VOd_d6vRTicjiMR5cSWAg], primary term [75], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-05T20:04:24,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/562261520926ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:08:14,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/458797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:12:16,282][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/458714447386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:18:35,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7m/586604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:22:43,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7m/586357282371ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:27:16,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2m/557623ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:28:45,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7092ae78, interval=5s}] took [2165760ms] which is above the warn threshold of [5000ms]
[2022-04-05T20:30:38,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3m/558427343380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:33:55,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/400049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:37:11,464][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399718786048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-05T20:40:43,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/406284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-05T20:46:31,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405862338479ns] on relative clock which is above the warn threshold of [5000ms]
