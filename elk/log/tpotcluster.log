[2022-03-27T17:46:52,777][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T17:46:52,797][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T17:46:52,800][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T17:47:09,675][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T17:47:09,687][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T17:47:09,695][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T17:47:09,696][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T17:47:09,703][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T17:47:09,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T17:47:09,704][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T17:47:09,705][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T17:47:09,717][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T17:47:09,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T17:47:09,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T17:47:09,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T17:47:09,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T17:47:09,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T17:47:09,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T17:47:09,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T17:47:09,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T17:47:09,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T17:47:09,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T17:47:09,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T17:47:09,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T17:47:09,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T17:47:09,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T17:47:09,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T17:47:09,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T17:47:09,752][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T17:47:09,753][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T17:47:09,766][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T17:47:09,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T17:47:09,767][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T17:47:09,768][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T17:47:09,769][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T17:47:09,771][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T17:47:09,781][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T17:47:09,782][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T17:47:09,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T17:47:09,783][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T17:47:09,784][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T17:47:09,785][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T17:47:09,787][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T17:47:09,796][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T17:47:09,801][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T17:47:09,802][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T17:47:09,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T17:47:09,809][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T17:47:09,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T17:47:09,810][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T17:47:09,811][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T17:47:09,812][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T17:47:09,814][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T17:47:09,821][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T17:47:09,822][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T17:47:09,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T17:47:09,823][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T17:47:09,832][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T17:47:09,834][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T17:47:09,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T17:47:09,845][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T17:47:09,847][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T17:47:10,020][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.7gb], net total_space [125.8gb], types [ext4]
[2022-03-27T17:47:10,021][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T17:47:11,013][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T17:47:31,741][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T17:47:31,744][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T17:47:33,753][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T17:47:34,013][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T17:47:35,529][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T17:47:36,991][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T17:47:36,992][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T17:47:37,040][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T17:47:37,042][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T17:47:37,441][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T17:47:41,679][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T17:47:41,855][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 131, version: 3734, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T17:47:42,182][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{P0ppsdc3Q_OcfB5snvTueg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 131, version: 3734, reason: Publication{term=131, version=3734}
[2022-03-27T17:47:42,341][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T17:47:42,362][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T17:47:44,026][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T17:47:44,033][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T17:47:45,468][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T17:47:45,469][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T17:47:47,134][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:47,324][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:48,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-27T17:47:48,061][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-27T17:47:48,065][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-27T17:47:48,364][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-03-27T17:47:50,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T17:47:50,671][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T17:47:58,958][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T17:48:15,184][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][38] overhead, spent [270ms] collecting in the last [1s]
[2022-03-27T17:48:19,121][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-03-27T17:48:19,603][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-03-27T17:48:50,578][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.03.26][0]]]).
[2022-03-27T17:49:27,559][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 574 finished with response BulkByScrollResponse[took=327.9ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-27T17:49:30,368][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 599 finished with response BulkByScrollResponse[took=2.5s,timed_out=false,sliceId=null,updated=1039,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-03-27T17:49:42,455][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-03-27T17:50:41,627][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:42,207][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:42,572][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:46,125][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:49,220][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:50:53,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,592][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,745][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:50,987][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:51,574][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:54,651][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:51:54,892][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:52:03,704][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:07,779][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:08,711][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:53:45,784][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:55:21,835][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:57:33,083][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T17:59:32,216][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:26:08,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:26:08,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.03.27/a2wbbW3JSrultjMfCS3dRQ] update_mapping [_doc]
[2022-03-27T18:29:35,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@50f1bee5, interval=1s}] took [31305ms] which is above the warn threshold of [5000ms]
[2022-03-27T18:29:51,496][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:59330}] took [12865ms] which is above the warn threshold of [5000ms]
[2022-03-27T18:29:35,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T18:33:24,635][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.4s/14468ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T18:40:30,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38s/38043637312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T18:50:37,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/999161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T18:50:39,427][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [21.3s/21302ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78ac69ee]], which exceeds the warn threshold of [10s]
[2022-03-27T19:01:30,751][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.8s/13860ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@78ac69ee]], which exceeds the warn threshold of [10s]
[2022-03-27T19:01:31,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6m/998758196133ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:05:42,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4m/1224665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:06:45,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5094/0x00000008017ed730@62f2767e] took [1224762ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:07:58,146][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4m/1224762295911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:06:07,449][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.4s/15422ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f4279a90], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5118159f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T19:09:36,972][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:38120}] took [1224762ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:09:58,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257116ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:12:30,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257421582102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:12:31,275][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.3s/22325ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f4279a90], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@5118159f], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b1677d47]], which exceeds the warn threshold of [10s]
[2022-03-27T19:14:46,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/272121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:14:40,518][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:38098}] took [271770ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:16:55,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/271770693812ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:18:48,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/256996ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:20:07,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/257226656122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:15:35,890][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [1753954ms] which is above the warn threshold of [5s]
[2022-03-27T19:21:53,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/181604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:24:07,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3m/181472293679ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:25:41,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/228160ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:31:58,905][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T19:31:58,921][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T19:31:58,922][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T19:32:06,551][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T19:32:06,558][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T19:32:06,560][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T19:32:06,561][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T19:32:06,563][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T19:32:06,564][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T19:32:06,565][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T19:32:06,567][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T19:32:06,568][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T19:32:06,570][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T19:32:06,571][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T19:32:06,572][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T19:32:06,574][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T19:32:06,575][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T19:32:06,576][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T19:32:06,577][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T19:32:06,577][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T19:32:06,579][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T19:32:06,580][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T19:32:06,581][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T19:32:06,582][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T19:32:06,582][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T19:32:06,583][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T19:32:06,584][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T19:32:06,584][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T19:32:06,585][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T19:32:06,586][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T19:32:06,586][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T19:32:06,587][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T19:32:06,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T19:32:06,588][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T19:32:06,589][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T19:32:06,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T19:32:06,590][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T19:32:06,591][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T19:32:06,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T19:32:06,592][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T19:32:06,593][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T19:32:06,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T19:32:06,594][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T19:32:06,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T19:32:06,595][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T19:32:06,596][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T19:32:06,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T19:32:06,597][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T19:32:06,598][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T19:32:06,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T19:32:06,599][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T19:32:06,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T19:32:06,600][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T19:32:06,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T19:32:06,601][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T19:32:06,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T19:32:06,602][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T19:32:06,603][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T19:32:06,604][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T19:32:06,605][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T19:32:06,721][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.5gb], net total_space [125.8gb], types [ext4]
[2022-03-27T19:32:06,722][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T19:32:07,143][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T19:32:23,141][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T19:32:23,165][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,167][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T19:32:23,170][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,171][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,172][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,173][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,174][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,175][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-27T19:32:23,176][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-27T19:32:23,177][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-27T19:32:23,178][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T19:32:23,179][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T19:32:23,180][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T19:32:23,183][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T19:32:26,734][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T19:32:27,025][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T19:32:28,827][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T19:32:30,420][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T19:32:30,422][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T19:32:30,601][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T19:32:30,603][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T19:32:31,093][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T19:32:34,054][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T19:32:34,195][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 132, version: 3812, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T19:32:34,388][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 132, version: 3812, reason: Publication{term=132, version=3812}
[2022-03-27T19:32:34,593][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T19:32:34,594][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T19:32:35,992][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T19:32:36,004][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T19:32:37,480][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T19:32:37,483][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T19:32:39,882][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-03-27T19:32:39,895][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-03-27T19:32:39,911][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-03-27T19:32:42,578][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T19:32:42,641][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T19:32:42,773][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-03-27T19:32:51,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7033ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:07,958][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13522ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:33:18,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:25,815][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9003156048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:25,736][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [31.9s] publication of cluster state version [3828] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:33:26,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:27,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9339417975ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:33:37,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4fc38b8a] took [31389ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:51,729][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][19][13] duration [7.2s], collections [2]/[34.8s], total [7.2s]/[8s], memory [152.3mb]->[107.3mb]/[2gb], all_pools {[young] [56mb]->[8mb]/[0b]}{[old] [64.3mb]->[94.6mb]/[2gb]}{[survivor] [32mb]->[4.7mb]/[0b]}
[2022-03-27T19:33:53,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15060ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:33:54,189][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T19:34:03,984][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [134] timed out after [20550ms]
[2022-03-27T19:34:14,052][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3829}] took [34.6s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3829}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [21663ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [1200ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [39ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [79ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [42ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [1118ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [281ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [106ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [2118ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [2165ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [148ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [4008ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [49ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [1197ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [63ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [112ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [93ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:34:26,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:34:26,581][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][29][14] duration [5.5s], collections [1]/[8.3s], total [5.5s]/[13.6s], memory [175.3mb]->[104mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [94.6mb]->[94.6mb]/[2gb]}{[survivor] [4.7mb]->[9.3mb]/[0b]}
[2022-03-27T19:34:26,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7071082635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:34:26,648][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][29] overhead, spent [5.5s] collecting in the last [8.3s]
[2022-03-27T19:34:29,393][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [50.4s/50467ms] ago, timed out [29.9s/29917ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [134]
[2022-03-27T19:34:35,760][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16834ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:34:37,448][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [19.4s] publication of cluster state version [3830] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:34:57,934][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [164] timed out after [16161ms]
[2022-03-27T19:34:58,792][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [16.7s/16761ms] ago, timed out [600ms/600ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [164]
[2022-03-27T19:35:23,986][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [24532ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [28] unchanged indices
[2022-03-27T19:35:25,176][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [26.6s] publication of cluster state version [3832] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:35:36,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@667f0d42] took [7381ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:35:42,841][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][52][15] duration [3s], collections [1]/[10.2s], total [3s]/[16.6s], memory [172mb]->[184mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [94.6mb]->[97.9mb]/[2gb]}{[survivor] [9.3mb]->[12mb]/[0b]}
[2022-03-27T19:35:43,063][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][52] overhead, spent [3s] collecting in the last [10.2s]
[2022-03-27T19:35:43,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5137ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:35:43,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60696}] took [98798ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:07,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5805ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:13,016][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3832}] took [45.6s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3832}]] took [0ms], [connecting to new nodes] took [60ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [20017ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [178ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [63ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [66ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [69ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [505ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [26ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [571ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [4522ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [204ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [3760ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [48ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [6903ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [55ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [5378ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [2404ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [244ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [153ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [162ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [57ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:36:21,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@54b3e502] took [7004ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:36:44,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8605ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:01,501][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6537ms] which is above the warn threshold of [5s]
[2022-03-27T19:37:02,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [11135ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:10,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@7dcc4038] took [5172ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6662087605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:37:43,572][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][69][16] duration [5s], collections [1]/[1.7s], total [5s]/[21.7s], memory [177.9mb]->[113.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [97.9mb]->[103.5mb]/[2gb]}{[survivor] [12mb]->[10.1mb]/[0b]}
[2022-03-27T19:37:43,626][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][69] overhead, spent [5s] collecting in the last [1.7s]
[2022-03-27T19:37:43,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6662ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:37:45,869][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10813ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:37:46,007][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.4s] publication of cluster state version [3833] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:37:51,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][74][17] duration [861ms], collections [1]/[1.1s], total [861ms]/[22.6s], memory [189.6mb]->[193.6mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [103.5mb]->[107.2mb]/[2gb]}{[survivor] [10.1mb]->[9.8mb]/[0b]}
[2022-03-27T19:37:52,025][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][74] overhead, spent [861ms] collecting in the last [1.1s]
[2022-03-27T19:38:04,994][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][80][18] duration [786ms], collections [1]/[1.1s], total [786ms]/[23.4s], memory [177.1mb]->[201.1mb]/[2gb], all_pools {[young] [60mb]->[4mb]/[0b]}{[old] [107.2mb]->[112.7mb]/[2gb]}{[survivor] [9.8mb]->[6.7mb]/[0b]}
[2022-03-27T19:38:05,681][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][80] overhead, spent [786ms] collecting in the last [1.1s]
[2022-03-27T19:38:11,625][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][82][19] duration [1.1s], collections [1]/[2.3s], total [1.1s]/[24.5s], memory [187.4mb]->[203.4mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [112.7mb]->[112.7mb]/[2gb]}{[survivor] [6.7mb]->[11.9mb]/[0b]}
[2022-03-27T19:38:11,713][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][82] overhead, spent [1.1s] collecting in the last [2.3s]
[2022-03-27T19:38:12,174][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:39522}] took [19077ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:38:36,069][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][93][21] duration [1s], collections [1]/[2.1s], total [1s]/[26.1s], memory [192.5mb]->[128.2mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [117.4mb]->[124.3mb]/[2gb]}{[survivor] [11.1mb]->[3.9mb]/[0b]}
[2022-03-27T19:38:36,906][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][93] overhead, spent [1s] collecting in the last [2.1s]
[2022-03-27T19:39:07,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6871ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:39:16,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7472ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:39:25,633][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [37470ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T19:39:30,615][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [49.6s] publication of cluster state version [3838] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:39:41,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@43b9b420] took [15078ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:01,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:17,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6232ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:37,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@32cdb515] took [5203ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:40:56,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:40:57,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349957776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:40:58,138][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][109][22] duration [7s], collections [1]/[2.1s], total [7s]/[33.2s], memory [220.2mb]->[220.2mb]/[2gb], all_pools {[young] [92mb]->[4mb]/[0b]}{[old] [124.3mb]->[124.3mb]/[2gb]}{[survivor] [3.9mb]->[7.5mb]/[0b]}
[2022-03-27T19:40:58,980][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][109] overhead, spent [7s] collecting in the last [2.1s]
[2022-03-27T19:41:00,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14983ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:41:29,484][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3838}] took [1.8m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3838}]] took [0ms], [connecting to new nodes] took [27ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [105728ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [178ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [35ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [31ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [486ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [91ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [1028ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [689ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [117ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [2410ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [674ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [1ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [20ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [18ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [1ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [68ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [21ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:41:40,181][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][123][23] duration [3.4s], collections [1]/[1.6s], total [3.4s]/[36.6s], memory [195.8mb]->[219.8mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [124.3mb]->[124.3mb]/[2gb]}{[survivor] [7.5mb]->[12.6mb]/[0b]}
[2022-03-27T19:41:41,320][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][123] overhead, spent [3.4s] collecting in the last [1.6s]
[2022-03-27T19:41:42,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7624ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:41:57,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5143ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:11,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6439ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:21,043][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3839}] took [39.8s] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3839}]] took [0ms], [connecting to new nodes] took [287ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [3802ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [727ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [43ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [37ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [33ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [697ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [28ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [12380ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [3267ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [382ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [8676ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [111ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [2454ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [10ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [2836ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [3465ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [48ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [50ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [51ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [58ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [47ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:42:31,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@c16e751] took [8631ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:42:32,649][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60706}] took [18363ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:43:12,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@fc3cb3] took [5898ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:43:12,917][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [37081ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [28] unchanged indices
[2022-03-27T19:43:17,753][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [42.3s] publication of cluster state version [3840] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:43:45,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6383ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:43:46,315][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3s/6382318119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:43:48,223][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][153][24] duration [3s], collections [1]/[8.4s], total [3s]/[39.6s], memory [188.9mb]->[143.5mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [124.3mb]->[129.2mb]/[2gb]}{[survivor] [12.6mb]->[14.2mb]/[0b]}
[2022-03-27T19:43:48,934][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][153] overhead, spent [3s] collecting in the last [8.4s]
[2022-03-27T19:44:16,163][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60708}] took [6557ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:44:23,827][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [29970ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:44:26,175][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [34s] publication of cluster state version [3841] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:45:05,837][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [19s/19087ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T19:45:07,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6123ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:21,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@5f2b336d] took [10113ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:37,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6061ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:45:56,339][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [34249ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [26] unchanged indices
[2022-03-27T19:46:05,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@261e316d] took [10017ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:46:04,045][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [48.9s] publication of cluster state version [3842] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:46:21,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:46:22,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101079961ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:46:22,733][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][25] duration [5.9s], collections [1]/[4.1s], total [5.9s]/[45.6s], memory [211.5mb]->[215.5mb]/[2gb], all_pools {[young] [68mb]->[76mb]/[0b]}{[old] [129.2mb]->[136.3mb]/[2gb]}{[survivor] [14.2mb]->[7.6mb]/[0b]}
[2022-03-27T19:46:23,277][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [5.9s] collecting in the last [4.1s]
[2022-03-27T19:46:23,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [12907ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:47:01,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:47:02,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6884763574ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:47:03,519][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][189][26] duration [4.5s], collections [1]/[7.9s], total [4.5s]/[50.2s], memory [212mb]->[147.2mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [136.3mb]->[136.3mb]/[2gb]}{[survivor] [7.6mb]->[10.9mb]/[0b]}
[2022-03-27T19:47:03,896][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][189] overhead, spent [4.5s] collecting in the last [7.9s]
[2022-03-27T19:47:49,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [20191ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:48:00,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [5749ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:48:08,517][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9864ms] which is above the warn threshold of [5s]
[2022-03-27T19:48:36,154][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3842}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3842}]] took [53ms], [connecting to new nodes] took [45ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [62866ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [278ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [358ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [51ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [43ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [590ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [41ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [45ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [33117ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [17235ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1193ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [981ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [13369ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1052ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [6136ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [3708ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [4129ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [58ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [122ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [55ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:48:40,064][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4dc16bb] took [11016ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:23,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:49:23,730][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [38.8s/38844ms] to notify listeners on successful publication of cluster state (version: 3842, uuid: sF2-4i7USP28ab9GQa93ig) for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T19:49:24,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37s/37016416254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:49:25,884][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][198][27] duration [28.1s], collections [1]/[1.2m], total [28.1s]/[1.3m], memory [207.2mb]->[227.2mb]/[2gb], all_pools {[young] [60mb]->[0b]/[0b]}{[old] [136.3mb]->[140.3mb]/[2gb]}{[survivor] [10.9mb]->[8.9mb]/[0b]}
[2022-03-27T19:49:26,460][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][198] overhead, spent [28.1s] collecting in the last [1.2m]
[2022-03-27T19:49:27,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [43519ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:28,166][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:60708}] took [5502ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:49:49,950][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/69115ms] ago, timed out [5.8s/5849ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [699]
[2022-03-27T19:49:49,148][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [699] timed out after [63266ms]
[2022-03-27T19:50:03,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [9827ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:16,247][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [50.6s/50605ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b6318fb5]], which exceeds the warn threshold of [10s]
[2022-03-27T19:50:17,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5009ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:35,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7719ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:50:35,358][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6377ms] which is above the warn threshold of [5s]
[2022-03-27T19:51:12,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:50:53,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4f26bc78] took [8508ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:51:12,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.3s/18300319452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:51:17,426][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][203][28] duration [13.3s], collections [1]/[45.1s], total [13.3s]/[1.5m], memory [213.2mb]->[156.8mb]/[2gb], all_pools {[young] [64mb]->[8mb]/[0b]}{[old] [140.3mb]->[141.2mb]/[2gb]}{[survivor] [8.9mb]->[15.5mb]/[0b]}
[2022-03-27T19:51:20,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][203] overhead, spent [13.3s] collecting in the last [45.1s]
[2022-03-27T19:51:23,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [10841ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:51:55,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [6324ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:52:12,095][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@dba517a] took [8735ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:52:21,935][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [93571ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [29] unchanged indices
[2022-03-27T19:52:25,972][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [3843] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T19:54:36,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118638ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T19:54:37,826][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118637624293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T19:54:41,842][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][209][29] duration [1.8m], collections [1]/[2m], total [1.8m]/[3.3m], memory [228.8mb]->[163mb]/[2gb], all_pools {[young] [75.9mb]->[0b]/[0b]}{[old] [141.2mb]->[148.4mb]/[2gb]}{[survivor] [15.5mb]->[14.6mb]/[0b]}
[2022-03-27T19:54:44,670][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][209] overhead, spent [1.8m] collecting in the last [2m]
[2022-03-27T19:54:46,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [10157ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:55:30,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@da53de0] took [18026ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:55:27,020][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10951ms] which is above the warn threshold of [5s]
[2022-03-27T19:56:07,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15885ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:56:33,199][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [796] timed out after [60735ms]
[2022-03-27T19:56:39,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [7666ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:56:44,521][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3843}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3843}]] took [48ms], [connecting to new nodes] took [160ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [4333ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [175ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [0ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [46ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [0ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [119922ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [565ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [67ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [54770ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [73ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [13303ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [740ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [556ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [17900ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [1036ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [20453ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [11867ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [3384ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [674ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [67ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [222ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [265ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [0ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [173ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [68ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [0ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T19:57:07,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8682ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:57:31,109][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.6m/399019ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [7.5m/451579ms]
[2022-03-27T19:57:36,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [13375ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:57:57,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@13000f0a] took [16055ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:58:31,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [15228ms] which is above the warn threshold of [5000ms]
[2022-03-27T19:58:30,248][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.1s/26117ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.03.13][0]], allocationId [zEEmyQ4zTiiC9z2GM-JELg], primary term [64], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [57], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.14][0]], allocationId [NqPyQR8iQIex-ABZw1hN4A], primary term [57], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.15][0]], allocationId [2UnyQ--uSYG-6FXjK__UNw], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-27T19:58:48,625][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [838] timed out after [50437ms]
[2022-03-27T19:58:55,070][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.3m/203357ms] ago, timed out [2.3m/142622ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [796]
[2022-03-27T19:58:51,094][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9657ms] which is above the warn threshold of [5s]
[2022-03-27T19:59:06,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14722ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:00:01,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [28237ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:00:55,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@44f0025c] took [28562ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:01:33,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24200ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:01:43,017][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.7m/223437ms] ago, timed out [2.8m/173000ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [838]
[2022-03-27T20:02:50,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [16767ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:03:17,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [6228ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:03:20,579][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10406ms] which is above the warn threshold of [5s]
[2022-03-27T20:03:23,315][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [867] timed out after [137504ms]
[2022-03-27T20:04:40,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [51792ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:06:12,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [27097ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:07:22,597][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.3m/383094ms] ago, timed out [4m/245590ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [867]
[2022-03-27T20:07:40,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [35065ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:08:12,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@5552023a] took [24045ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:08:12,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [524113ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T20:08:35,900][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [9.6m] publication of cluster state version [3844] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T20:09:10,804][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [12444ms] which is above the warn threshold of [5s]
[2022-03-27T20:09:10,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [5150ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:09:47,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [21795ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:09:59,347][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.7m/106182ms] ago, timed out [6.1s/6180ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [919]
[2022-03-27T20:09:55,134][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [919] timed out after [100002ms]
[2022-03-27T20:10:18,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14027ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:10:42,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [5971ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:11:45,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [47667ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:12:49,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24250ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:14:07,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@195eb929] took [64812ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:14:38,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10243ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:15:08,357][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3844}] took [6m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3844}]] took [232ms], [connecting to new nodes] took [1275ms], [applying settings] took [55ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [24192ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [1ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [4268ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [64ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [230ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [55ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [193ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [273ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [4279ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [81ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [1111ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [55ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [111ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [91601ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [114ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [41732ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1541ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [495ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [47017ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [12415ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [45127ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [51867ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [26758ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [4570ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [4ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [178ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [1328ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [185ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [158ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [175ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [115ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T20:15:31,171][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [25.1m/1510215ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [26m/1562177ms]
[2022-03-27T20:15:58,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [27463ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:15:49,907][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14605ms] which is above the warn threshold of [5s]
[2022-03-27T20:16:43,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [22930ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:17:12,529][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [978] timed out after [176940ms]
[2022-03-27T20:18:23,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [39110ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:18:56,844][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.9m/178736ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-03-27T20:19:02,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [23340ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:19:23,130][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5.2m/312670ms] ago, timed out [2.2m/135730ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [978]
[2022-03-27T20:19:32,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [5629ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:20:56,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@7e92a380] took [48876ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:21:42,893][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [17759ms] which is above the warn threshold of [5s]
[2022-03-27T20:22:15,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [39511ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:22:38,603][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1026] timed out after [103285ms]
[2022-03-27T20:22:48,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5089ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:22:56,178][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.9m/119801ms] ago, timed out [16.5s/16516ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1026]
[2022-03-27T20:23:20,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [21846ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:24:29,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@6772b3ca] took [27915ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:24:46,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [5604ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:25:19,186][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7321ms] which is above the warn threshold of [5s]
[2022-03-27T20:25:56,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [18786ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:25:43,874][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1085] timed out after [68711ms]
[2022-03-27T20:26:30,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [20018ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:26:29,347][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [279690ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [27] unchanged indices
[2022-03-27T20:26:47,393][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6m] publication of cluster state version [3845] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-03-27T20:26:58,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [7043ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:27:21,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [13209ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:27:57,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:27:59,436][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.1m/190750ms] ago, timed out [2m/122039ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1085]
[2022-03-27T20:28:05,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16825438317ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:14,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17362ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@4e73b4c3] took [42731ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:28:22,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17362213066ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:29,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:38,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14679117913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:47,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18117ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:28:54,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18117163042ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:07,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:24,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19247363113ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:24,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [19247ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:29:36,196][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28993ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:48,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.9s/28993809631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:29:59,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22753ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:11,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22753000999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:27,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24729ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:40,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24728472255ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:30:55,145][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][233][30] duration [11.5s], collections [1]/[2.6m], total [11.5s]/[3.5m], memory [239mb]->[164.6mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [148.4mb]->[155.3mb]/[2gb]}{[survivor] [14.6mb]->[9.3mb]/[0b]}
[2022-03-27T20:30:55,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:07,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [79581ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:31:10,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.1s/32100080869ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:21,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:35,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25694212854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:46,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:46,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [25633ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:31:54,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25633141896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:02,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:31:44,433][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1137] timed out after [186314ms]
[2022-03-27T20:31:50,331][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [25633ms] which is above the warn threshold of [5s]
[2022-03-27T20:32:10,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15093939999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14615ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:25,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14614823828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:33,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:34,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [31292ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:32:39,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16678143508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:47,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:32:54,822][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [4.5m/273411ms] ago, timed out [1.4m/87097ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1137]
[2022-03-27T20:32:54,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15077283324ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:02,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:07,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [14140ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:33:08,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14140083525ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:17,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:22,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260697297ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:29,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11019ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:37,277][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11018838527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:46,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:52,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18581165535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:54,809][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@6cd4f775] took [18581ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:33:56,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:33:57,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10419ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:00,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10419718030ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:05,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:10,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [8531ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:10,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8531978994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:14,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:18,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8985204318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:24,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:29,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8952649543ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:33,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [10105ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:33,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:38,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10105174178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:45,213][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11628ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:55,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [11627ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:34:55,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11627633758ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:34:40,898][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1195] timed out after [46994ms]
[2022-03-27T20:35:04,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19171ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:06,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [19171ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:35:13,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.1s/19171286203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:23,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:32,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [18276ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:35:32,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18276305171ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:42,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:35:50,842][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18740670765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:00,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:14,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.2s/18248094947ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:28,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26955ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:42,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.9s/26954798913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:36:49,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [26954ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:36:53,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25962ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:07,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25962274991ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:18,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25527ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:35,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25526846640ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:37:48,294][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.5m/211502ms] ago, timed out [2.7m/164508ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1195]
[2022-03-27T20:37:50,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:03,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.3s/31339587271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:08,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [31339ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:38:14,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:27,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23909940241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:41,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:38:53,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24943892931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:07,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28222ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:16,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@136fe03e] took [53165ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:39:16,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.2s/28221257233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:25,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@57d50e4f] took [18188ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:39:25,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18188ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:37,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18188467646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:47,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21828ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:39:58,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21828073612ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:11,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24311ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:24,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24310632644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:37,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:40:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [49918ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:40:52,357][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25607597041ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:00,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:06,787][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23438504147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:13,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:18,706][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12428853891ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:24,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:24,309][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1247] timed out after [125802ms]
[2022-03-27T20:41:29,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788367706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:35,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:42,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10305378160ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:49,018][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:53,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14110632368ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:41:55,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [24416ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:41:56,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:02,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7959492081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:07,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:10,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10175928789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:15,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7761ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:23,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7761496414ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:35,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19087ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:37,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [26848ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:42:42,290][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19087156028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:44,335][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.4m/206991ms] ago, timed out [1.3m/81189ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1247]
[2022-03-27T20:42:48,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14841ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:42:52,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14840324199ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:02,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:10,700][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13296679268ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:16,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@1d079082] took [13296ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:43:18,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:25,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16030035027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:34,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16393ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:42,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16392333020ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:43:49,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:00,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15139983162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:12,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:21,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20233696434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:26,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:32,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [37738ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:44:34,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17505077830ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:43,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16175ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:44:52,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16174176456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:04,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20183ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:15,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20183898334ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:24,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20301ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:36,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20300483278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:46,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:45:35,229][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20300ms] which is above the warn threshold of [5s]
[2022-03-27T20:45:57,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21925566309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:11,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24817ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:18,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [24816ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:46:25,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24816384157ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:35,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24340ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:42,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24339864044ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:29,635][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1295] timed out after [163885ms]
[2022-03-27T20:46:50,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:46:58,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15133221280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:07,422][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.8m/228174ms] ago, timed out [1m/64289ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1295]
[2022-03-27T20:47:10,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18863ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:15,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [33996ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:47:18,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.8s/18863533872ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:33,910][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:47,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24461050421ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:47:56,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:07,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640464718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:14,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:19,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17336224135ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:27,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:35,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13439139330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:42,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:48:51,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15064924266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:00,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:05,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [33540ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:49:08,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18476023291ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:19,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:20,353][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@d7cd0c9] took [14866ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:49:41,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14866100400ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:49:56,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:07,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.7s/39709477527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:21,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@d71048f, interval=5s}] took [26198ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:50:36,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.1s/26198119200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:50:53,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28344ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:29,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.3s/28343756170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:39,897][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=132, version=3845}] took [23.5m] which is above the warn threshold of [30s]: [running task [Publication{term=132, version=3845}]] took [202ms], [connecting to new nodes] took [656ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@461e1681] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40f0cea3] took [1103661ms], [org.elasticsearch.script.ScriptService@4725d83a] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [0ms], [org.elasticsearch.snapshots.RestoreService@62096eb0] took [0ms], [org.elasticsearch.ingest.IngestService@24ca3184] took [4866ms], [org.elasticsearch.action.ingest.IngestActionForwarder@4dc7d92f] took [168ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016d1540@372de812] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@1e787ca9] took [315ms], [org.elasticsearch.tasks.TaskManager@6b4b28f3] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@18b68a14] took [163ms], [org.elasticsearch.cluster.InternalClusterInfoService@256ab0c0] took [173ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@227118e0] took [236ms], [org.elasticsearch.indices.SystemIndexManager@4a8a2fa9] took [6214ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@216a2a29] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@45c350ea] took [835ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32439a9b] took [199ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@2577889b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@ef2f3aa] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@463d7684] took [259ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@215875ea] took [140986ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@699b3c47] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@c6da66e] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3ca4caa0] took [34850ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@1a2861e5] took [1281ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@3736d8c7] took [1156ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@56894699] took [17689ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5495d0bd] took [4016ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@4c627722] took [33019ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@151c1002] took [383ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@57c88e40] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@2cfab4cb] took [39132ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7931320b] took [26710ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@72a6fba3] took [268ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@121a30ef] took [8503ms], [org.elasticsearch.node.ResponseCollectorService@62cbe9cd] took [27ms], [org.elasticsearch.snapshots.SnapshotShardsService@4613c453] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@32d44d1] took [598ms], [org.elasticsearch.shutdown.PluginShutdownService@40f38113] took [648ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@6090e45c] took [2139ms], [org.elasticsearch.indices.store.IndicesStore@6af5d69b] took [12725ms], [org.elasticsearch.persistent.PersistentTasksNodeService@2ad192f0] took [880ms], [org.elasticsearch.license.LicenseService@5cdb301a] took [952ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@7f8bc52d] took [548ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@7b9abaf9] took [480ms], [org.elasticsearch.gateway.GatewayService@3788464b] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@69cbc5f3] took [0ms]
[2022-03-27T20:51:54,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:51:57,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [64247ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:52:21,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64247303644ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:52:46,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:52:59,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.6s/51669305195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:09,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:16,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.5s/22508826118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:23,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:29,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15243305737ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:36,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12365ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:41,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12364733777ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:44,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [27608ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:53:47,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:52,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11932884977ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:58,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:53:51,747][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1356] timed out after [272218ms]
[2022-03-27T20:54:07,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10712227548ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:07,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [10712ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:54:14,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:21,726][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.5s/16533450647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:27,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:33,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12641232270ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:37,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [12641ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:54:43,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15967ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:51,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9s/15966779239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:54:56,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:02,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [13544ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:55:02,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13544893101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:13,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:21,669][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16063888697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:32,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:55:58,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.4s/17478913240ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:04,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [17478ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:56:18,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:33,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.1s/46113928980ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:56:54,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33989ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:16,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33988533773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:17,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e6768@3780334f] took [33988ms] which is above the warn threshold of [5000ms]
[2022-03-27T20:57:30,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:57:47,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39565754619ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:03,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:20,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30818865923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:58:37,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:00,129][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34894961639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:23,363][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44919ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T20:59:46,565][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44919082625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:02,296][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [10m/605460ms] ago, timed out [5.5m/333242ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{UI91IDFTQDCqWeqk2j-Qtw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1356]
[2022-03-27T21:00:06,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:26,127][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41980904773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:00:45,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40221ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:05,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.2s/40221138752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:25,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:01:36,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [80632ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:01:48,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40411213608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:02:15,808][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:02:57,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49043056224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:03:40,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:04:08,044][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/87288378605ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:04:52,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:06:16,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/67550365728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:07:39,186][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168211ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/168211697494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:23,449][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45813ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:08:48,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.8s/45812378987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:07,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:30,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.8s/44851752474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:09:51,658][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:10:22,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44156290742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:02,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:47,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/71121667643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:11:47,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@73f6567e, interval=1s}] took [115277ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:12:30,461][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81981ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:14:43,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81980393740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:17:19,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:19:34,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/282399642241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:22:03,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292147ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:24:26,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292246678048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:20:13,141][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [282400ms] which is above the warn threshold of [5s]
[2022-03-27T21:24:53,595][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.2m/852960ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [76], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.ds-ilm-history-5-2022.03.12-000001][0]], allocationId [_ZVLKqGLTC-ikoHi_g8ICA], primary term [76], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-03-27T21:26:58,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294931ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:29:39,506][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/294914318924ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:32:03,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306809ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:35:10,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/306725344844ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:37:59,313][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:41:00,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354317640516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:44:12,758][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334661ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:47:13,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/334498848209ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:49:44,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372352ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:51:37,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@1b938271, interval=5s}] took [372621ms] which is above the warn threshold of [5000ms]
[2022-03-27T21:52:16,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/372621398078ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T21:55:21,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T21:58:09,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/330487766639ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:01:07,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339624ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:03:59,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339610358054ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:06:49,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:09:42,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345444829682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:12:21,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331750ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:14:57,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/331983568281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:17:24,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311620ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:19:49,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/311551271416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:22:41,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/307999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:25:02,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308305704554ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:27:27,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T22:30:27,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293575908107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T22:33:32,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/358293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:00:53,142][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-03-27T23:00:53,168][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-03-27T23:00:53,170][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-03-27T23:01:00,348][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-03-27T23:01:00,349][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-03-27T23:01:00,350][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-03-27T23:01:00,351][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-03-27T23:01:00,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-03-27T23:01:00,352][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-03-27T23:01:00,353][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-03-27T23:01:00,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-03-27T23:01:00,354][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-03-27T23:01:00,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-03-27T23:01:00,355][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-03-27T23:01:00,356][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-03-27T23:01:00,357][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-03-27T23:01:00,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-03-27T23:01:00,358][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-03-27T23:01:00,359][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-03-27T23:01:00,360][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-03-27T23:01:00,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-03-27T23:01:00,361][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-03-27T23:01:00,362][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-03-27T23:01:00,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-03-27T23:01:00,363][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-03-27T23:01:00,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-03-27T23:01:00,364][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-03-27T23:01:00,365][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-03-27T23:01:00,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-03-27T23:01:00,366][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-03-27T23:01:00,367][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-03-27T23:01:00,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-03-27T23:01:00,368][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-03-27T23:01:00,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-03-27T23:01:00,369][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-03-27T23:01:00,370][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-03-27T23:01:00,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-03-27T23:01:00,371][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-03-27T23:01:00,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-03-27T23:01:00,372][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-03-27T23:01:00,373][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-03-27T23:01:00,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-03-27T23:01:00,374][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-03-27T23:01:00,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-03-27T23:01:00,375][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-03-27T23:01:00,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-03-27T23:01:00,376][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-03-27T23:01:00,377][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-03-27T23:01:00,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-03-27T23:01:00,378][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-03-27T23:01:00,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-03-27T23:01:00,379][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-03-27T23:01:00,380][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-03-27T23:01:00,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-03-27T23:01:00,381][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-03-27T23:01:00,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-03-27T23:01:00,382][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-03-27T23:01:00,383][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-03-27T23:01:00,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-03-27T23:01:00,384][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-03-27T23:01:00,385][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-03-27T23:01:00,386][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-03-27T23:01:00,474][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [102.4gb], net total_space [125.8gb], types [ext4]
[2022-03-27T23:01:00,475][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-03-27T23:01:01,085][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-03-27T23:01:14,624][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-03-27T23:01:14,630][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,631][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-03-27T23:01:14,633][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,633][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,634][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,635][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,636][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-03-27T23:01:14,637][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-03-27T23:01:14,638][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-03-27T23:01:14,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-03-27T23:01:14,642][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-03-27T23:01:14,643][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-03-27T23:01:14,644][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-03-27T23:01:16,380][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-03-27T23:01:16,575][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-03-27T23:01:17,977][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-03-27T23:01:18,936][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-03-27T23:01:18,937][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-03-27T23:01:19,036][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-03-27T23:01:19,039][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-03-27T23:01:19,373][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-03-27T23:01:22,045][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-03-27T23:01:22,220][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 133, version: 3846, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-03-27T23:01:22,452][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{uJk1RQVnQ2ekajAK6FPAbw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 133, version: 3846, reason: Publication{term=133, version=3846}
[2022-03-27T23:01:22,549][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-03-27T23:01:22,550][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-03-27T23:01:26,100][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-03-27T23:01:26,141][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [30] indices into cluster_state
[2022-03-27T23:01:27,804][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-03-27T23:01:27,806][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-03-27T23:01:29,503][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-03-27T23:01:30,036][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-03-27T23:06:31,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:15:00,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6431950931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:11:27,428][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54b4b00c, interval=1s}] took [19754ms] which is above the warn threshold of [5000ms]
[2022-03-27T23:18:06,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9m/958094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:21:29,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.9m/958345697704ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:23:19,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:24:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318874594432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:27:02,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/208516ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:30:33,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/207951828223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:34:10,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:38:03,025][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1m/428727983565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:41:43,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453335ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:45:05,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5m/453032585493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:48:18,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403512ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:49:20,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@41485283, interval=5s}] took [1812162ms] which is above the warn threshold of [5000ms]
[2022-03-27T23:51:14,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/403575013840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-03-27T23:54:51,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388744ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-03-27T23:58:47,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388873656035ns] on relative clock which is above the warn threshold of [5000ms]
